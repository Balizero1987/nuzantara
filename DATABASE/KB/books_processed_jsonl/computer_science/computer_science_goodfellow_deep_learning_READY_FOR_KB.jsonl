{"id": "computer_science_goodfellow_deep_learning_chunk_0000_365e33aa", "text": "Deep L ea r ni n g\nI a n G o o d f e l l o w\nY o s h u a B e n g i o\nA a r o n C o u r v i l l e\nCon ten ts\nWebsite vii\nAcknowledgments viii\nNotation xi\n1 Introduction 1\n1.1 WhoShouldReadThisBook? . . . . . . . . . . . . . . . . . . . . 8\n1.2 HistoricalTrendsinDeepLearning . . . . . . . . . . . . . . . . . 11\nI AppliedMathandMachineLearningBasics 29\n2 LinearAlgebra 31\n2.1 Scalars,Vectors,MatricesandTensors . . . . . . . . . . . . . . . 31\n2.2 MultiplyingMatricesandVectors . . . . . . . . . . . . . . . . . . 34\n2.3 IdentityandInverseMatrices . . . . . . . . . . . . . . . . . . . . 36\n2.4 LinearDependenceandSpan . . . . . . . . . . . . . . . . . . . . 37\n2.5 Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n2.6 SpecialKindsofMatricesandVectors . . . . . . . . . . . . . . . 40\n2.7 Eigendecomposition . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n2.8 SingularValueDecomposition . . . . . . . . . . . . . . . . . . . . 44\n2.9 TheMoore-PenrosePseudoinverse . . . . . . . . . . . . . . . . . . 45\n2.10 TheTraceOperator . . . . . . . . . . . . . . . . . . . . . . . . . 46\n2.11 TheDeterminant . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n2.12 Example: PrincipalComponentsAnalysis . . . . . . . . . . . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 0, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1254}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0001_f7dedef7", "text": ". . . . . . . . . . . . . . . . . 45\n2.10 TheTraceOperator . . . . . . . . . . . . . . . . . . . . . . . . . 46\n2.11 TheDeterminant . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n2.12 Example: PrincipalComponentsAnalysis . . . . . . . . . . . . . 48\n3 ProbabilityandInformationTheory 53\n3.1 WhyProbability? . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\ni\nCO NTE NT S\n3.2RandomVariables. . . . .. . . . . . . . . . . . . . . . . . . . .56\n3.3ProbabilityDistributions. . . . . . . . . . . . . . . . . . . . . . .56\n3.4MarginalProbability. . . . . . . . . . . . . . . . . . . . . . . . . 58\n3.5ConditionalProbability. .. . . . . . . . . . . . . . . . . . . . .59\n3.6TheChainRuleofConditionalProbabilities. . . . . . . . . . . .59\n3.7IndependenceandConditionalIndependence. . . . . . . . . . . .60\n3.8Expectation,VarianceandCovariance. . . . . . . . . . . . . . .60\n3.9CommonProbabilityDistributions. . . . . . . . . . . . . . . . .62\n3.10UsefulPropertiesofCommonFunctions. . .. . . . . . . . . . .67\n3.11Bayes’Rule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .70\n3.12TechnicalDetailsofContinuousVariables. . . . . .. . . . . . . 71\n3.13InformationTheory. . . . . . . . . . . . . . . . . . . . . . . . . . 73\n3.14StructuredProbabilisticModels. . . .. . . . . . . . . . . . . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1308}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0002_acc53e67", "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . .70\n3.12TechnicalDetailsofContinuousVariables. . . . . .. . . . . . . 71\n3.13InformationTheory. . . . . . . . . . . . . . . . . . . . . . . . . . 73\n3.14StructuredProbabilisticModels. . . .. . . . . . . . . . . . . . . 75\n4NumericalComputation 80\n4.1OverﬂowandUnderﬂow. . . . . . . . . . . . . . . . . . . . . . .80\n4.2PoorConditioning . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n4.3Gradient-BasedOptimization . . . . . . .. . . . . . . . . . . . .82\n4.4ConstrainedOptimization . . . . . . . . . . . . . . . . . . . . . .93\n4.5Example:LinearLeastSquares. . . . . . .. . . . . . . . . . . .96\n5MachineLearningBasics 98\n5.1LearningAlgorithms. . . . . . . . . . . . . . . . . . . . . . . . .99\n5.2Capacity,OverﬁttingandUnderﬁtting. .. . . . . . . . . . . . .110\n5.3HyperparametersandValidationSets.. . . . . . . . . . . . . . .120\n5.4Estimators,BiasandVariance. . . . . .. . . . . . . . . . . . . .122\n5.5MaximumLikelihoodEstimation. . . . . .. . . . . . . . . . . .131\n5.6BayesianStatistics. . . . . . . . . . . . . . . . . . . . . . . . . .135\n5.7SupervisedLearningAlgorithms. . .. . . . . . . . . . . . . . . . 140\n5.8UnsupervisedLearningAlgorithms. . . . . . . . . . . . . . . . .146\n5.9StochasticGradientDescent. . . .. . . . . . . . . . . . . . . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 2, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1320}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0003_db0670ea", "text": ". . . . . . . . . . . . . . . . . . . . . . . . .135\n5.7SupervisedLearningAlgorithms. . .. . . . . . . . . . . . . . . . 140\n5.8UnsupervisedLearningAlgorithms. . . . . . . . . . . . . . . . .146\n5.9StochasticGradientDescent. . . .. . . . . . . . . . . . . . . . . 151\n5.10BuildingaMachineLearningAlgorithm. . . . . . . . . . . . . .153\n5.11ChallengesMotivatingDeepLearning. . . . .. . . . . . . . . . .155\nIIDeepNetworks:ModernPractices 166\n6DeepFeedforwardNetworks 168\n6.1Example:LearningXOR.. . . . . . . . . . . . . . . . . . . . . .171\n6.2Gradient-BasedLearning.. . . . . . . . . . . . . . . . . . . . . .177\ni i\nCO NTE NT S\n6.3HiddenUnits. . . . . .. . . . . . . . . . . . . . . . . . . . . . .191\n6.4ArchitectureDesign. . . . . . . . . . . . . . . . . . . . . . . . . .197\n6.5Back-PropagationandOtherDiﬀerentiationAlgorithms. . . . .204\n6.6HistoricalNotes. . . . . . .. . . . . . . . . . . . . . . . . . . . .224\n7RegularizationforDeepLearning 228\n7.1ParameterNormPenalties. . . . .. . . . . . . . . . . . . . . . . 230\n7.2NormPenaltiesasConstrainedOptimization . . . . . . . . . . . .237\n7.3RegularizationandUnder-ConstrainedProblems. .. . . . . . .239\n7.4DatasetAugmentation. . . . . . . . . . . . . . . . . . . . . . . .240\n7.5NoiseRobustness. . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 3, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1302}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0004_7e7ef580", "text": ". . . . . . . . . . .237\n7.3RegularizationandUnder-ConstrainedProblems. .. . . . . . .239\n7.4DatasetAugmentation. . . . . . . . . . . . . . . . . . . . . . . .240\n7.5NoiseRobustness. . . . . . . . . . . . . . . . . . . . . . . . . . .242\n7.6Semi-SupervisedLearning. . . . . . . . . . . . . . . . . . . . . .243\n7.7Multi-TaskLearning. . . . . . . . . . . . . . . . . . . . . . . . .244\n7.8EarlyStopping. . . . . . . . . . . . . . . . . . . . . . . . . . . .246\n7.9ParameterTyingandParameterSharing . . . . . . . . . . . . . . 253\n7.10SparseRepresentations. . . . . . . . . . . . . . . . . . . . . . . .254\n7.11BaggingandOtherEnsembleMethods.. . . . . . . . . . . . . .256\n7.12Dropout. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .258\n7.13AdversarialTraining. . . . . . . . . . . . . . . . . . . . . . . . . 268\n7.14TangentDistance,TangentProp,andManifoldTangentClassiﬁer270\n8OptimizationforTrainingDeepModels 274\n8.1HowLearningDiﬀersfromPureOptimization . . . . . . . . . . . 275\n8.2ChallengesinNeuralNetworkOptimization . . . . .. . . . . . .282\n8.3BasicAlgorithms. . . . . . . . . . . . . . . . . . . . . . . . . . .294\n8.4ParameterInitialization Strategies.. . . . . . . . . . . . . . . .301\n8.5AlgorithmswithAdaptiveLearningRates. . . . . . .. . . . . .306\n8.6ApproximateSecond-Order Methods. . . .. . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 4, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1322}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0005_c6ec4ba1", "text": ". . . . . . . . . . . . . . . . . . . . . . . . . .294\n8.4ParameterInitialization Strategies.. . . . . . . . . . . . . . . .301\n8.5AlgorithmswithAdaptiveLearningRates. . . . . . .. . . . . .306\n8.6ApproximateSecond-Order Methods. . . .. . . . . . . . . . . .310\n8.7Optimization StrategiesandMeta-Algorithms. . . . .. . . . . .317\n9ConvolutionalNetworks 330\n9.1TheConvolutionOperation. . . . . . . . . . . . . . . . . . . . .331\n9.2Motivation. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .335\n9.3Pooling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .339\n9.4ConvolutionandPoolingasanInﬁnitelyStrongPrior. .. . . . .345\n9.5VariantsoftheBasicConvolutionFunction. . . . . . . . . . . . 347\n9.6StructuredOutputs.. . . . . . . . . . . . . . . . . . . . . . . . . 358\n9.7DataTypes. . . . . .. . . . . . . . . . . . . . . . . . . . . . . . 360\n9.8EﬃcientConvolutionAlgorithms. . . . . . . . . . . . . . . . . .362\n9.9RandomorUnsupervisedFeatures. . . . . . . . . . . . . . . . .363\ni i i\nCO NTE NT S\n9.10TheNeuroscientiﬁcBasisforConvolutionalNetworks. . . . . ..364\n9.11ConvolutionalNetworksandtheHistoryofDeepLearning. . . .371\n10 SequenceModeling:RecurrentandRecursiveNets373\n10.1UnfoldingComputational Graphs. . . . . . . . . . . . . . . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 5, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1266}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0006_1e59a5c1", "text": ". . . . ..364\n9.11ConvolutionalNetworksandtheHistoryofDeepLearning. . . .371\n10 SequenceModeling:RecurrentandRecursiveNets373\n10.1UnfoldingComputational Graphs. . . . . . . . . . . . . . . . . .375\n10.2RecurrentNeuralNetworks. . .. . . . . . . . . . . . . . . . . .378\n10.3BidirectionalRNNs . . . . . . . . . . . . . . . . . . . . . . . . . .394\n10.4Encoder-DecoderSequence-to-SequenceArchitectures. . . . . ..396\n10.5DeepRecurrentNetworks. . . . . . . . . . . . . . . . . . . . . .398\n10.6RecursiveNeuralNetworks. . . . .. . . . . . . . . . . . . . . . . 400\n10.7TheChallengeofLong-TermDependencies. . . . . . . . . . . . .401\n10.8EchoStateNetworks. . . . . . . . . . . . . . . . . . . . . . . . .404\n10.9LeakyUnitsandOtherStrategiesforMultipleTimeScales. . ..406\n10.10 TheLongShort-TermMemoryandOtherGatedRNNs. .. . . .408\n10.11 Optimization forLong-TermDependencies. . . . . . . . . . . . .413\n10.12 Explicit Memory. . . . . . . . . . . . . . . . . . . . . . . . . . . 416\n11 PracticalMethodology 421\n11.1PerformanceMetrics. . . . . . . . . . . . . . . . . . . . . . . . .422\n11.2DefaultBaselineModels. . . . . . . . . . . . . . . . . . . . . . .425\n11.3DeterminingWhethertoGatherMoreData. . . . . . . . . . . . 426\n11.4SelectingHyperparameters. . . . . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 6, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1260}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0007_6dc13662", "text": ". . . . . . . . . . . . . . . . . . . . . . . .422\n11.2DefaultBaselineModels. . . . . . . . . . . . . . . . . . . . . . .425\n11.3DeterminingWhethertoGatherMoreData. . . . . . . . . . . . 426\n11.4SelectingHyperparameters. . . . . . . . . . . . . . . . . . . . . .427\n11.5DebuggingStrategies. . . . .. . . . . . . . . . . . . . . . . . . .436\n11.6Example:Multi-DigitNumberRecognition. . . . .. . . . . . . . 440\n12 Applications 443\n12.1Large-ScaleDeepLearning.. . . . . . . . . . . . . . . . . . . . .443\n12.2ComputerVision. . . . . . . . . . . . . . . . . . . . . . . . . . .452\n12.3SpeechRecognition . . . . . .. . . . . . . . . . . . . . . . . . . .458\n12.4NaturalLanguageProcessing. . .. . . . . . . . . . . . . . . . .461\n12.5OtherApplications. . . . . . . . . . . . . . . . . . . . . . . . . .478\nIIIDeepLearningResearch 486\n13 LinearFactorModels 489\n13.1ProbabilisticPCAandFactorAnalysis. . . . . . .. . . . . . . . 490\n13.2IndependentComponentAnalysis(ICA). . . . . . . . . . . . . .491\n13.3SlowFeatureAnalysis. . . . . .. . . . . . . . . . . . . . . . . .493\n13.4SparseCoding. . . . . .. . . . . . . . . . . . . . . . . . . . . . .496\ni v\nCO NTE NT S\n13.5ManifoldInterpretation ofPCA. . . . . . . . . . . . . . . . . . .499\n14 Autoencoders 502\n14.1Undercomplete Autoencoders. . . . . . . . . . . . . . . . . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 7, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1318}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0008_02992ce7", "text": ". . . . . . .493\n13.4SparseCoding. . . . . .. . . . . . . . . . . . . . . . . . . . . . .496\ni v\nCO NTE NT S\n13.5ManifoldInterpretation ofPCA. . . . . . . . . . . . . . . . . . .499\n14 Autoencoders 502\n14.1Undercomplete Autoencoders. . . . . . . . . . . . . . . . . . . .503\n14.2RegularizedAutoencoders. . . . . . . . . . . . . . . . . . . . . .504\n14.3RepresentationalPower,LayerSizeandDepth. . . . . .. . . . .508\n14.4StochasticEncodersandDecoders. . . . . . . . . . . . . . . . . .509\n14.5DenoisingAutoencoders. .. . . . . . . . . . . . . . . . . . . . .510\n14.6LearningManifoldswithAutoencoders. . . . . .. . . . . . . . . 515\n14.7ContractiveAutoencoders.. . . . . . . . . . . . . . . . . . . . .521\n14.8PredictiveSparseDecomposition. . . . . . . . . . . . . . . . . .523\n14.9ApplicationsofAutoencoders. . . . .. . . . . . . . . . . . . . .524\n15 RepresentationLearning 526\n15.1GreedyLayer-WiseUnsupervisedPretraining. . . . . .. . . . .528\n15.2TransferLearningandDomainAdaptation. . . .. . . . . . . . .536\n15.3Semi-SupervisedDisentanglingofCausalFactors. . . . .. . . .541\n15.4DistributedRepresentation. . . . . . . . . . . . . . . . . . . . . .546\n15.5ExponentialGainsfromDepth. . . . . . . . . . . . . . . . . . .553\n15.6ProvidingCluestoDiscoverUnderlyingCauses. . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 8, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1274}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0009_787e61bd", "text": ". . . .. . . .541\n15.4DistributedRepresentation. . . . . . . . . . . . . . . . . . . . . .546\n15.5ExponentialGainsfromDepth. . . . . . . . . . . . . . . . . . .553\n15.6ProvidingCluestoDiscoverUnderlyingCauses. . . .. . . . . .554\n16 StructuredProbabilisticModelsforDeepLearning558\n16.1TheChallengeofUnstructuredModeling.. . . . . . . . . . . . .559\n16.2UsingGraphstoDescribeModelStructure. .. . . . . . . . . . .563\n16.3SamplingfromGraphicalModels. . .. . . . . . . . . . . . . . .580\n16.4AdvantagesofStructuredModeling .. . . . . . . . . . . . . . . .582\n16.5LearningaboutDependencies. . . .. . . . . . . . . . . . . . . . 582\n16.6InferenceandApproximateInference. . . . . . . . . . . . . . . .584\n16.7TheDeepLearningApproachtoStructuredProbabilisticModels585\n17 MonteCarloMethods 590\n17.1SamplingandMonteCarloMethods. . . . . . . . . . . . . . . . 590\n17.2ImportanceSampling. . . . . . . . . . . . . . . . . . . . . . . . .592\n17.3MarkovChainMonteCarloMethods. . . . .. . . . . . . . . . .595\n17.4GibbsSampling . . . . . . .. . . . . . . . . . . . . . . . . . . . .599\n17.5TheChallengeofMixingbetweenSeparatedModes. . . . . . ..599\n18 ConfrontingthePartitionFunction 605\n18.1TheLog-LikelihoodGradient.. . . . . . . . . . . . . . . . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 9, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1238}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0010_3d7c4f83", "text": ". .595\n17.4GibbsSampling . . . . . . .. . . . . . . . . . . . . . . . . . . . .599\n17.5TheChallengeofMixingbetweenSeparatedModes. . . . . . ..599\n18 ConfrontingthePartitionFunction 605\n18.1TheLog-LikelihoodGradient.. . . . . . . . . . . . . . . . . . .606\n18.2StochasticMaximumLikelihoodandContrastiveDivergence. . .607\nv\nCO NTE NT S\n18.3Pseudolikelihood. . . . . . . . . . . . . . . . . . . . . . . . . . .615\n18.4ScoreMatchingandRatioMatching. . . . . . . . . . . . . . . . 617\n18.5DenoisingScoreMatching. . . . . . . . . . . . . . . . . . . . . .619\n18.6Noise-ContrastiveEstimation. . . . .. . . . . . . . . . . . . . .620\n18.7EstimatingthePartitionFunction. . . . . . . . . . . . . . . . . .623\n19 ApproximateInference 631\n19.1InferenceasOptimization .. . . . . . . . . . . . . . . . . . . . .633\n19.2ExpectationMaximization . .. . . . . . . . . . . . . . . . . . . .634\n19.3MAPInferenceandSparseCoding.. . . . . . . . . . . . . . . .635\n19.4VariationalInferenceandLearning. . . . . . . . . . . . . . . . .638\n19.5LearnedApproximateInference. . .. . . . . . . . . . . . . . . .651\n20 DeepGenerativeModels 654\n20.1BoltzmannMachines. . . . . . . . . . . . . . . . . . . . . . . . .654\n20.2RestrictedBoltzmannMachines. . . . . . .. . . . . . . . . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 10, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1252}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0011_88c0434e", "text": ". . . . . . . . . . . . . . .638\n19.5LearnedApproximateInference. . .. . . . . . . . . . . . . . . .651\n20 DeepGenerativeModels 654\n20.1BoltzmannMachines. . . . . . . . . . . . . . . . . . . . . . . . .654\n20.2RestrictedBoltzmannMachines. . . . . . .. . . . . . . . . . . .656\n20.3DeepBeliefNetworks.. . . . . . . . . . . . . . . . . . . . . . . . 660\n20.4DeepBoltzmannMachines. . . . . . . . . . . . . . . . . . . . . .663\n20.5BoltzmannMachinesforReal-ValuedData. . . . . . . . . . . . .676\n20.6ConvolutionalBoltzmannMachines . . . . . . . . . . . . . . . . .683\n20.7BoltzmannMachinesforStructuredorSequentialOutputs. . . .685\n20.8OtherBoltzmannMachines. . . . .. . . . . . . . . . . . . . . . 686\n20.9Back-PropagationthroughRandomOperations. . . . . .. . . .687\n20.10 DirectedGenerativeNets. . . . . . . . . . . . . . . . . . . . . . .692\n20.11 DrawingSamplesfromAutoencoders. . . . .. . . . . . . . . . .711\n20.12 Generativ eStochasticNetworks. . .. . . . . . . . . . . . . . . . 714\n20.13 OtherGenerationSchemes. . . . . . . . . . . . . . . . . . . . . . 716\n20.14 EvaluatingGenerativeModels . . . . . . . . . . . . . . . . . . . .717\n20.15 Conclus ion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 11, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1215}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0012_81be9654", "text": ". .. . . . . . . . . . . . . . . . 714\n20.13 OtherGenerationSchemes. . . . . . . . . . . . . . . . . . . . . . 716\n20.14 EvaluatingGenerativeModels . . . . . . . . . . . . . . . . . . . .717\n20.15 Conclus ion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .720\nBibliography 721\nIndex 777\nv i\nW e b s i t e\nwww.deeplearningb ook.org\nThisbookisaccompanied bytheabovewebsite.Thewebsiteprovidesa\nvarietyofsupplementarymaterial,includingexercises,lectureslides,correctionsof\nmistakes,andotherresourcesthatshouldbeusefultobothreadersandinstructors. vii\nAcknowledgments\nThisbookwouldnothavebeenpossiblewithoutthecontributionsofmanypeople. Wewouldliketothankthosewhocommentedonourproposalforthebook\nandhelpedplanitscontentsandorganization: GuillaumeAlain,KyunghyunCho,\nÇağlarGülçehre,DavidKrueger,HugoLarochelle,RazvanPascanuandThomas\nRohée.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 12, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 845}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0013_dbfc3301", "text": "Wewouldliketothankthepeoplewhooﬀeredfeedbackonthecontentofthe\nbookitself.Someoﬀeredfeedbackonmanychapters:MartínAbadi,Guillaume\nAlain,IonAndroutsopoulos ,FredBertsch,OlexaBilaniuk,UfukCanBiçici,Matko\nBošnjak,JohnBoersma,GregBrockman,AlexandredeBrébisson,PierreLuc\nCarrier,SarathChandar,PawelChilinski,MarkDaoust,OlegDashevskii,Laurent\nDinh,StephanDreseitl,JimFan,MiaoFan,MeireFortunato,FrédéricFrancis,\nNando deFreitas,Çağlar Gülçehre, Jurgen V anGael,JavierAlonso García,\nJonathanHunt,GopiJeyaram,ChingizKabytayev,LukaszKaiser,VarunKanade,\nAsifullahKhan,AkielKhan,JohnKing,DiederikP.Kingma,YannLeCun,Rudolf\nMathey,MatíasMattamala,AbhinavMaurya,KevinMurphy,OlegMürk,Roman\nNovak,AugustusQ.Odena,SimonPavlik,KarlPichotta,EddiePierce,KariPulli,\nRousselRahman,TapaniRaiko,AnuragRanjan,JohannesRoith,MihaelaRosca,\nHalisSak, CésarSalgado,GrigorySapunov,YoshinoriSasaki, MikeSchuster,\nJulianSerban,NirShabat,KenShirriﬀ,AndreSimpelo,ScottStanley,David\nSussillo,IlyaSutskever,CarlesGeladaSáez,GrahamTaylor,ValentinTolmer,\nMassimilianoTomassoli,AnTran,ShubhenduTrivedi,AlexeyUmnov,Vincent\nVanhoucke,MarcoVisentini-Scarzanella,MartinVita,DavidWarde-Farley,Dustin\nWebb,KelvinXu,WeiXue,KeYang,LiYao,ZygmuntZającandOzanÇağlayan.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 13, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1214}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0014_8700aad6", "text": "Wewouldalsoliketothankthosewhoprovideduswithusefulfeedbackon\nindividualchapters:\n•Notation:ZhangYuanhang. •Chapter, :YusufAkgul,SebastienBratieres,SamiraEbrahimi, 1Introduction\nviii\nCO NTE NT S\nCharlieGorichanaz,BrendanLoudermilk,EricMorris,CosminPârvulescu\nandAlfredoSolano. •Chapter, :AmjadAlmahairi,NikolaBanić,KevinBennett, 2LinearAlgebra\nPhilippeCastonguay,OscarChang,EricFosler-Lussier,AndreyKhalyavin,\nSergeyOreshkov,IstvánPetrás,DennisPrangle,ThomasRohée,Gitanjali\nGulveSehgal,ColbyToland,AlessandroVitaleandBobWelland. •Chapter, :JohnPhilipAnderson,Kai 3ProbabilityandInformationTheory\nArulkumaran,VincentDumoulin,RuiFa,StephanGouws,ArtemOboturov,\nAnttiRasmus,AlexeySurkovandVolkerTresp. •Chapter ,  :Tran LamAnIan Fischer andHu 4NumericalComputation\nYuhuang. •Chapter, :DzmitryBahdanau,JustinDomingue, 5MachineLearningBasics\nNikhilGarg,MakotoOtsuka,BobPepin,PhilipPopien,EmmanuelRayner,\nPeterShepard,Kee-BongSong,ZhengSunandAndyWu.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 14, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 941}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0015_2ccd992f", "text": "•Chapter, :DzmitryBahdanau,JustinDomingue, 5MachineLearningBasics\nNikhilGarg,MakotoOtsuka,BobPepin,PhilipPopien,EmmanuelRayner,\nPeterShepard,Kee-BongSong,ZhengSunandAndyWu. •Chapter,6DeepFeedforwardNetworks:UrielBerdugo,FabrizioBottarel,\nElizabethBurl,IshanDurugkar,JeﬀHlywa,JongWookKim,DavidKrueger\nandAdityaKumarPraharaj. •Chapter, :MortenKolbæk,KshitijLauria, 7RegularizationforDeepLearning\nInkyuLee,SunilMohan,HaiPhongPhanandJoshuaSalisbury. •Chapter,8Optimization forTrainingDeepModels:MarcelAckermann,Peter\nArmitage,RowelAtienza,AndrewBrock,TeganMaharaj,JamesMartens,\nKashifRasul,KlausStroblandNicholasTurner. •Chapter,9ConvolutionalNetworks:MartínArjovsky,EugeneBrevdo,Kon-\nstantinDivilov,EricJensen,MehdiMirza,AlexPaino,MarjorieSayer,Ryan\nStoutandWentaoWu. •Chapter,10SequenceModeling:RecurrentandRecursiveNets:Gökçen\nEraslan,StevenHickson,RazvanPascanu,LorenzovonRitter,RuiRodrigues,\nDmitriySerdyuk,DongyuShiandKaiyuYang. •Chapter, :DanielBeckstein.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 15, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 958}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0016_1ca65664", "text": "•Chapter,10SequenceModeling:RecurrentandRecursiveNets:Gökçen\nEraslan,StevenHickson,RazvanPascanu,LorenzovonRitter,RuiRodrigues,\nDmitriySerdyuk,DongyuShiandKaiyuYang. •Chapter, :DanielBeckstein. 11PracticalMethodology\n•Chapter, :GeorgeDahl,VladimirNekrasovandRibana 12Applications\nRoscher. •Chapter,13LinearFactorModels:JayanthKoushik. i x\nCO NTE NT S\n•Chapter, :KunalGhosh. 15RepresentationLearning\n•Chapter, : MinhLê 16StructuredProbabilisticModelsforDeepLearning\nandAntonVarfolom. •Chapter,18ConfrontingthePartitionFunction:SamBowman. •Chapter, :YujiaBao. 19ApproximateInference\n•Chapter,20DeepGenerativeModels:NicolasChapados,DanielGalvez,\nWenmingMa,FadyMedhat,ShakirMohamedandGrégoireMontavon. •Bibliography:LukasMichelbacherandLeslieN.Smith. Wealsowanttothankthosewhoallowedustoreproduceimages,ﬁguresor\ndatafromtheirpublications.Weindicatetheircontributionsintheﬁgurecaptions\nthroughoutthetext.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 16, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 899}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0017_f5ccefaa", "text": "•Bibliography:LukasMichelbacherandLeslieN.Smith. Wealsowanttothankthosewhoallowedustoreproduceimages,ﬁguresor\ndatafromtheirpublications.Weindicatetheircontributionsintheﬁgurecaptions\nthroughoutthetext. WewouldliketothankLuWangforwritingpdf2htmlEX,whichweusedto\nmakethewebversionofthebook,andforoﬀeringsupporttoimprovethequality\noftheresultingHTML. We would liketothank Ian’swifeDaniela FloriGoodfellowforpatiently\nsupportingIanduringthewritingofthebookaswellasforhelpwithproofreading. WewouldliketothanktheGoogleBrainteamforprovidinganintellectual\nenvironmentwhereIancoulddevoteatremendousamountoftimetowritingthis\nbookandreceivefeedbackandguidancefromcolleagues.Wewouldespeciallylike\ntothankIan’sformermanager,GregCorrado,andhiscurrentmanager,Samy\nBengio,fortheirsupportofthisproject.Finally,wewouldliketothankGeoﬀrey\nHintonforencouragement whenwritingwasdiﬃcult.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 17, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 864}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0018_acdf72a8", "text": "x\nN ot at i o n\nThissectionprovidesaconcisereferencedescribingthenotationusedthroughout\nthisbook.Ifyouareunfamiliarwithanyofthecorrespondingmathematical\nconcepts,wedescribemostoftheseideasinchapters2–4. Num b e r s and Ar r a y s\naAscalar(integerorreal)\naAvector\nAAmatrix\nAAtensor\nI nIdentitymatrixwithrowsandcolumns n n\nIIdentitymatrixwithdimensionalityimpliedby\ncontext\ne( ) iStandardbasisvector[0 , . . . ,0 ,1 ,0 , . . . ,0]witha\n1atposition i\ndiag()aAsquare,diagonalmatrixwithdiagonalentries\ngivenbya\naAscalarrandomvariable\naAvector-valuedrandomvariable\nAAmatrix-valuedrandomvariable\nxi\nCO NTE NT S\nSet s and G r aphs\nAAset\nRThesetofrealnumbers\n{}01 ,Thesetcontaining0and1\n{ } 01 , , . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 18, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 694}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0019_a77bf4fc", "text": ". . , nThesetofallintegersbetweenand0 n\n[] a , bTherealintervalincludingand a b\n(] a , bTherealintervalexcludingbutincluding a b\nA B\\Setsubtraction,i.e., thesetcontainingtheele-\nmentsofthatarenotin A B\nGAgraph\nP a G(x i)Theparentsofx iinG\nI ndexing\na iElement iofvectora,withindexingstartingat1\na − iAllelementsofvectorexceptforelementa i\nA i , jElementofmatrix i , jA\nA i , :Rowofmatrix iA\nA : , iColumnofmatrix iA\nA i , j , kElementofa3-Dtensor ( ) i , j , k A\nA : : , , i2-Dsliceofa3-Dtensor\na iElementoftherandomvector i a\nL i near Al g e br a O p e r at i o ns\nATransposeofmatrixA\nA+Moore-PenrosepseudoinverseofA\nABElement-wise(Hadamard)productofandAB\ndet()ADeterminantofA\nx i i\nCO NTE NT S\nCal c ul usd y\nd xDerivativeofwithrespectto y x\n∂ y\n∂ xPartialderivativeofwithrespectto y x\n∇ x yGradientofwithrespectto y x\n∇ X yMatrixderivativesofwithrespectto y X\n∇ X yTensorcontainingderivativesof ywithrespectto\nX\n∂ f\n∂xJacobianmatrixJ∈ Rm n ×of f: Rn→ Rm\n∇2\nx f f f () (xorH)()xTheHessianmatrixofatinputpointx\nf d()xxDeﬁniteintegralovertheentiredomainofx\n\nSf d()xx x Deﬁniteintegralwithrespecttoovertheset S\nP r o babil i t y and I nf o r m at i o n T heor y\nabTherandomvariablesaandbareindependent ⊥\nabcTheyareconditionallyindependentgivenc ⊥|\nP()aAprobabilitydistributionoveradiscretevariable\np()aAprobabilitydistributionoveracontinuousvari-\nable,oroveravariablewhosetypehasnotbeen\nspeciﬁed\na Randomvariableahasdistribution ∼ P P\nE x ∼ P[()] () () () f xor E f xExpectationof f xwithrespectto Px\nVar(()) f xVarianceofunderx f x() P()\nCov(()()) f x , g xCovarianceofandunderx f x() g x() P()\nH()xShannonentropyoftherandomvariablex\nD K L( ) P QKullback-LeiblerdivergenceofPandQ\nN(; )xµ ,ΣGaussiandistributionoverxwithmeanµand\ncovarianceΣ\nx i i i\nCO NTE NT S\nF unc t i o ns\nf f : A B→Thefunctionwithdomainandrange A B\nf g f g ◦Compositionofthefunctionsand\nf(;)xθAfunctionofxparametrized byθ.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 19, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1897}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0020_2152ca9b", "text": "(Sometimes\nwewrite f(x)andomittheargumentθtolighten\nnotation)\nlog x x Naturallogarithmof\nσ x()Logisticsigmoid,1\n1+exp()− x\nζ x x () log(1+exp( Softplus, ))\n||||x p Lpnormofx\n||||x L2normofx\nx+Positivepartof,i.e., x max(0) , x\n1 c o ndi t i o nis1iftheconditionistrue,0otherwise\nSometimesweuseafunction fwhoseargumentisascalarbutapplyittoa\nvector,matrix,ortensor: f(x), f(X),or f( X).Thisdenotestheapplicationof f\ntothearrayelement-wise. Forexample,if C= σ( X),then C i , j , k= σ( X i , j , k)forall\nvalidvaluesof,and.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 20, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 518}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0021_5ff31335", "text": "Forexample,if C= σ( X),then C i , j , k= σ( X i , j , k)forall\nvalidvaluesof,and. i j k\nD at aset s and D i st r i but i o n s\np da t aThedatageneratingdistribution\nˆ p da t aTheempiricaldistributiondeﬁnedbythetraining\nset\nXAsetoftrainingexamples\nx( ) iThe-thexample(input)fromadataset i\ny( ) iory( ) iThetargetassociatedwithx( ) iforsupervisedlearn-\ning\nXThe m n×matrixwithinputexamplex( ) iinrow\nX i , :\nx i v\nC h a p t e r 1\nI n t ro d u ct i on\nInventorshavelongdreamedofcreatingmachinesthatthink.Thisdesiredates\nbacktoatleastthetimeofancientGreece.ThemythicalﬁguresPygmalion,\nDaedalus,andHephaestusmayallbeinterpretedaslegendaryinventors,and\nGalatea,Talos,andPandoramayallberegardedasartiﬁciallife( , OvidandMartin\n2004Sparkes1996Tandy1997 ;,;,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 21, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 751}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0022_55cc791c", "text": "Whenprogrammable computerswereﬁrstconceived,peoplewonderedwhether\nsuchmachinesmightbecomeintelligent,overahundredyearsbeforeonewas\nbuilt(Lovelace1842,).Today, ar t i ﬁc i al i n t e l l i g e nc e(AI)isathrivingﬁeldwith\nmanypracticalapplicationsandactiveresearchtopics.Welooktointelligent\nsoftwaretoautomateroutinelabor,understandspeechorimages,makediagnoses\ninmedicineandsupportbasicscientiﬁcresearch. Intheearlydaysofartiﬁcialintelligence,theﬁeldrapidlytackledandsolved\nproblemsthatareintellectually diﬃcultforhumanbeingsbutrelativelystraight-\nforwardforcomputers—problemsthatcanbedescribedbyalistofformal,math-\nematicalrules. Thetruechallengetoartiﬁcialintelligenceprovedtobesolving\nthetasksthatareeasyforpeopletoperformbuthardforpeopletodescribe\nformally—probl emsthatwesolveintuitively,thatfeelautomatic,likerecognizing\nspokenwordsorfacesinimages.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 22, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 852}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0023_522249a3", "text": "Thisbookisaboutasolutiontothesemoreintuitiveproblems.Thissolutionis\ntoallowcomputerstolearnfromexperienceandunderstandtheworldintermsofa\nhierarchyofconcepts,witheachconceptdeﬁnedintermsofitsrelationtosimpler\nconcepts.Bygatheringknowledgefromexperience,thisapproachavoidstheneed\nforhumanoperatorstoformallyspecifyalloftheknowledgethatthecomputer\nneeds.Thehierarchyofconceptsallowsthecomputertolearncomplicatedconcepts\nbybuildingthemoutofsimplerones.Ifwedrawagraphshowinghowthese\n1\nCHAPTER1.INTRODUCTION\nconceptsarebuiltontopofeachother,thegraphisdeep,withmanylayers.For\nthisreason,wecallthisapproachtoAI .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 23, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 604}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0024_9b541e2e", "text": "deep l e ar ni ng\nManyoftheearlysuccessesofAItookplaceinrelativelysterileandformal\nenvironmentsanddidnotrequirecomputerstohavemuchknowledgeabout\ntheworld.Forexample,IBM’sDeepBluechess-playingsystemdefeatedworld\nchampionGarryKasparovin1997(,).Chessisofcourseaverysimple Hsu2002\nworld,containingonlysixty-fourlocationsandthirty-twopiecesthatcanmove\ninonlyrigidlycircumscribedways.Devisingasuccessfulchessstrategyis a\ntremendousaccomplishment, butthechallengeisnotduetothediﬃcultyof\ndescribingthesetofchesspiecesandallowablemovestothecomputer.Chess\ncanbecompletelydescribedbyaverybrieflistofcompletelyformalrules,easily\nprovidedaheadoftimebytheprogrammer.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 24, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 652}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0025_8a59af7a", "text": "Ironically,abstractandformaltasksthatareamongthemostdiﬃcultmental\nundertakings forahumanbeingareamongtheeasiestforacomputer.Computers\nhavelongbeenabletodefeateventhebesthumanchessplayer,butareonly\nrecentlymatchingsomeoftheabilitiesofaveragehumanbeingstorecognizeobjects\norspeech.Aperson’severydayliferequiresanimmenseamountofknowledge\nabouttheworld.Muchofthisknowledgeissubjectiveandintuitive,andtherefore\ndiﬃculttoarticulateinaformalway.Computersneedtocapturethissame\nknowledgeinordertobehaveinanintelligentway.Oneofthekeychallengesin\nartiﬁcialintelligenceishowtogetthisinformalknowledgeintoacomputer.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 25, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 602}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0026_4125c482", "text": "Severalartiﬁcialintelligenceprojectshavesoughttohard-codeknowledgeabout\ntheworldinformallanguages.Acomputercanreasonaboutstatementsinthese\nformallanguagesautomatically usinglogicalinferencerules.Thisisknownasthe\nk no wl e dge baseapproachtoartiﬁcialintelligence.Noneoftheseprojectshasled\ntoamajorsuccess.OneofthemostfamoussuchprojectsisCyc( , LenatandGuha\n1989).Cycisaninferenceengineandadatabaseofstatementsinalanguage\ncalledCycL.Thesestatementsareenteredbyastaﬀofhumansupervisors.Itisan\nunwieldyprocess.Peoplestruggletodeviseformalruleswithenoughcomplexity\ntoaccuratelydescribetheworld.Forexample,Cycfailedtounderstandastory\naboutapersonnamedFredshavinginthemorning(,).Itsinference Linde1992\nenginedetectedaninconsistencyinthestory: itknewthatpeopledonothave\nelectricalparts,butbecauseFredwasholdinganelectricrazor,itbelievedthe\nentity“FredWhileShaving”containedelectricalparts.Itthereforeaskedwhether\nFredwasstillapersonwhilehewasshaving.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 26, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 941}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0027_49f7eacf", "text": "Thediﬃcultiesfacedbysystemsrelyingonhard-codedknowledgesuggest\nthatAIsystemsneedtheabilitytoacquiretheirownknowledge,byextracting\npatternsfromrawdata.Thiscapabilityisknownas m ac hi ne l e ar ni ng.The\n2\nCHAPTER1.INTRODUCTION\nintroductionofmachinelearningallowedcomputerstotackleproblemsinvolving\nknowledgeoftherealworldandmakedecisionsthatappearsubjective.Asimple\nmachinelearningalgorithmcalled l o g i st i c r e g r e ssi o ncandeterminewhetherto\nrecommendcesareandelivery(Mor-Yosef1990 e t a l .,).Asimplemachinelearning\nalgorithmcalled nai v e B a y e scanseparatelegitimatee-mailfromspame-mail.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 27, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 600}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0028_c44842d0", "text": "Theperformanceofthesesimplemachinelearningalgorithmsdependsheavily\nonthe r e pr e se n t at i o nofthedatatheyaregiven.Forexample,whenlogistic\nregressionisusedtorecommendcesareandelivery,theAIsystemdoesnotexamine\nthepatientdirectly.Instead,thedoctortellsthesystemseveralpiecesofrelevant\ninformation, suchasthepresenceorabsenceofauterinescar.Eachpieceof\ninformationincludedintherepresentationofthepatientisknownasa f e at ur e. Logisticregressionlearnshoweachofthesefeaturesofthepatientcorrelateswith\nvariousoutcomes.However,itcannotinﬂuencethewaythatthefeaturesare\ndeﬁnedinanyway. IflogisticregressionwasgivenanMRIscanofthepatient,\nratherthanthedoctor’sformalizedreport,itwouldnotbeabletomakeuseful\npredictions.IndividualpixelsinanMRIscanhavenegligiblecorrelationwithany\ncomplications thatmightoccurduringdelivery.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 28, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 814}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0029_e01996c4", "text": "Thisdependenceonrepresentationsisageneralphenomenon thatappears\nthroughoutcomputerscienceandevendailylife.Incomputerscience,opera-\ntionssuchassearchingacollectionofdatacanproceedexponentiallyfasterif\nthecollectionisstructuredandindexedintelligently.Peoplecaneasilyperform\narithmeticonArabicnumerals,butﬁndarithmeticonRomannumeralsmuch\nmoretime-consuming. Itisnotsurprisingthatthechoiceofrepresentationhasan\nenormouseﬀectontheperformanceofmachinelearningalgorithms.Forasimple\nvisualexample,seeﬁgure.1.1\nManyartiﬁcialintelligencetaskscanbesolvedbydesigningtherightsetof\nfeaturestoextractforthattask,thenprovidingthesefeaturestoasimplemachine\nlearningalgorithm.Forexample,ausefulfeatureforspeakeridentiﬁcationfrom\nsoundisanestimateofthesizeofspeaker’svocaltract.Itthereforegivesastrong\nclueastowhetherthespeakerisaman,woman,orchild. However,formanytasks,itisdiﬃculttoknowwhatfeaturesshouldbeextracted. Forexample,supposethatwewouldliketowriteaprogramtodetectcarsin\nphotographs.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 29, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 974}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0030_74c8da8e", "text": "However,formanytasks,itisdiﬃculttoknowwhatfeaturesshouldbeextracted. Forexample,supposethatwewouldliketowriteaprogramtodetectcarsin\nphotographs. Weknowthatcarshavewheels,sowemightliketousethepresence\nofawheelasafeature.Unfortunately,itisdiﬃculttodescribeexactlywhata\nwheellookslikeintermsofpixelvalues.Awheelhasasimplegeometricshapebut\nitsimagemaybecomplicatedbyshadowsfallingonthewheel,thesunglaringoﬀ\nthemetalpartsofthewheel,thefenderofthecaroranobjectintheforeground\nobscuringpartofthewheel,andsoon. 3\nCHAPTER1.INTRODUCTION\n                \n                \nFigure1.1:Exampleofdiﬀerentrepresentations:supposewewanttoseparatetwo\ncategoriesofdatabydrawingalinebetweentheminascatterplot.Intheplotontheleft,\nwerepresentsomedatausingCartesiancoordinates,andthetaskisimpossible.Intheplot\nontheright,werepresentthedatawithpolarcoordinatesandthetaskbecomessimpleto\nsolvewithaverticalline.FigureproducedincollaborationwithDavidWarde-Farley.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 30, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 976}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0031_b80c731a", "text": "Onesolutiontothisproblemistousemachinelearningtodiscovernotonly\nthemappingfromrepresentationtooutputbutalsotherepresentationitself. Thisapproachisknownas r e pr e se n t at i o n l e ar ni ng. Learnedrepresentations\noftenresultinmuchbetterperformancethancanbeobtainedwithhand-designed\nrepresentations.TheyalsoallowAIsystemstorapidlyadapttonewtasks,with\nminimalhumanintervention.Arepresentationlearningalgorithmcandiscovera\ngoodsetoffeaturesforasimpletaskinminutes,oracomplextaskinhoursto\nmonths.Manuallydesigningfeaturesforacomplextaskrequiresagreatdealof\nhumantimeandeﬀort;itcantakedecadesforanentirecommunityofresearchers.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 31, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 624}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0032_4bd83f3f", "text": "Thequintessentialexampleofarepresentationlearningalgorithmisthe au-\nt o e nc o der.Anautoencoderisthecombinationofan e nc o derfunctionthat\nconvertstheinputdataintoadiﬀerentrepresentation,anda dec o derfunction\nthatconvertsthenewrepresentationbackintotheoriginalformat.Autoencoders\naretrainedtopreserveasmuchinformationaspossiblewhenaninputisrun\nthroughtheencoderandthenthedecoder,butarealsotrainedtomakethenew\nrepresentationhavevariousniceproperties.Diﬀerentkindsofautoencodersaimto\nachievediﬀerentkindsofproperties. Whendesigningfeaturesoralgorithmsforlearningfeatures,ourgoalisusually\ntoseparatethe f ac t o r s o f v ar i at i o nthatexplaintheobserveddata.Inthis\ncontext,weusetheword“factors”simplytorefertoseparatesourcesofinﬂuence;\nthefactorsareusuallynotcombinedbymultiplication. Suchfactorsareoftennot\n4\nCHAPTER1.INTRODUCTION\nquantitiesthataredirectlyobserved.Instead,theymayexisteitherasunobserved\nobjectsorunobservedforcesinthephysicalworldthataﬀectobservablequantities.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 32, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 981}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0033_49bedc84", "text": "Suchfactorsareoftennot\n4\nCHAPTER1.INTRODUCTION\nquantitiesthataredirectlyobserved.Instead,theymayexisteitherasunobserved\nobjectsorunobservedforcesinthephysicalworldthataﬀectobservablequantities. Theymayalsoexistasconstructsinthehumanmindthatprovideusefulsimplifying\nexplanationsorinferredcausesoftheobserveddata.Theycanbethoughtofas\nconceptsorabstractionsthathelpusmakesenseoftherichvariabilityinthedata. Whenanalyzingaspeechrecording,thefactorsofvariationincludethespeaker’s\nage,theirsex,theiraccentandthewordsthattheyarespeaking.Whenanalyzing\nanimageofacar,thefactorsofvariationincludethepositionofthecar,itscolor,\nandtheangleandbrightnessofthesun. Amajorsourceofdiﬃcultyinmanyreal-worldartiﬁcialintelligenceapplications\nisthatmanyofthefactorsofvariationinﬂuenceeverysinglepieceofdataweare\nabletoobserve.Theindividualpixelsinanimageofaredcarmightbeveryclose\ntoblackatnight.Theshapeofthecar’ssilhouettedependsontheviewingangle.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 33, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 927}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0034_4f4ef8ee", "text": "Mostapplicationsrequireusto thefactorsofvariationanddiscardthe d i s e nt a ng l e\nonesthatwedonotcareabout. Ofcourse,itcanbeverydiﬃculttoextractsuchhigh-level,abstractfeatures\nfromrawdata.Manyofthesefactorsofvariation,suchasaspeaker’saccent,\ncanbeidentiﬁedonlyusingsophisticated,nearlyhuman-levelunderstandingof\nthedata.Whenitisnearlyasdiﬃculttoobtainarepresentationastosolvethe\noriginalproblem,representationlearningdoesnot,atﬁrstglance,seemtohelpus. D e e p l e ar ni ngsolvesthiscentralprobleminrepresentationlearningbyintro-\nducingrepresentationsthatareexpressedintermsofother,simplerrepresentations. Deeplearningallowsthecomputertobuildcomplexconceptsoutofsimplercon-\ncepts.Figureshowshowadeeplearningsystemcanrepresenttheconceptof 1.2\nanimageofapersonbycombiningsimplerconcepts,suchascornersandcontours,\nwhichareinturndeﬁnedintermsofedges.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 34, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 846}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0035_e676c631", "text": "Thequintessentialexampleofadeeplearningmodelisthefeedforwarddeep\nnetworkor m ul t i l a y e r p e r c e pt r o n(MLP).Amultilayerperceptronisjusta\nmathematical functionmappingsomesetofinputvaluestooutputvalues.The\nfunctionisformedbycomposingmanysimplerfunctions.Wecanthinkofeach\napplicationofadiﬀerentmathematical functionasprovidinganewrepresentation\noftheinput.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 35, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 363}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0036_d19f18b7", "text": "Theideaoflearningtherightrepresentationforthedataprovidesoneperspec-\ntiveondeeplearning.Anotherperspectiveondeeplearningisthatdepthallowsthe\ncomputertolearnamulti-stepcomputerprogram.Eachlayeroftherepresentation\ncanbethoughtofasthestateofthecomputer’smemoryafterexecutinganother\nsetofinstructionsinparallel.Networkswithgreaterdepthcanexecutemore\ninstructionsinsequence.Sequentialinstructionsoﬀergreatpowerbecauselater\n5\nCHAPTER1.INTRODUCTION\nVisible layer\n(input pixels)1st hidden layer\n(edges)2nd hidden layer\n(corners and\ncontours)3rd hidden layer\n(object parts)CARPERSONANIMALOutput\n(object identity)\nFigure1.2:Illustrationofadeeplearningmodel.Itisdiﬃcultforacomputertounderstand\nthemeaningofrawsensoryinputdata,suchasthisimagerepresentedasacollection\nofpixelvalues.Thefunctionmappingfromasetofpixelstoanobjectidentityisvery\ncomplicated.Learningorevaluatingthismappingseemsinsurmountableiftackleddirectly.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 36, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 908}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0037_5cf73b36", "text": "Deeplearningresolvesthisdiﬃcultybybreakingthedesiredcomplicatedmappingintoa\nseriesofnestedsimplemappings,eachdescribedbyadiﬀerentlayerofthemodel.The\ninputispresentedatthevisiblelayer,sonamedbecauseitcontainsthevariablesthat\nweareabletoobserve.Thenaseriesofhiddenlayersextractsincreasinglyabstract\nfeaturesfromtheimage.Theselayersarecalled“hidden”becausetheirvaluesarenotgiven\ninthedata;insteadthemodelmustdeterminewhichconceptsareusefulforexplaining\ntherelationshipsintheobserveddata.Theimagesherearevisualizationsofthekind\noffeaturerepresentedbyeachhiddenunit.Giventhepixels,theﬁrstlayercaneasily\nidentifyedges,bycomparingthebrightnessofneighboringpixels.Giventheﬁrsthidden\nlayer’sdescriptionoftheedges,thesecondhiddenlayercaneasilysearchforcornersand\nextendedcontours,whicharerecognizableascollectionsofedges.Giventhesecondhidden\nlayer’sdescriptionoftheimageintermsofcornersandcontours,thethirdhiddenlayer\ncandetectentirepartsofspeciﬁcobjects,byﬁndingspeciﬁccollectionsofcontoursand\ncorners.Finally,thisdescriptionoftheimageintermsoftheobjectpartsitcontainscan\nbeusedtorecognizetheobjectspresentintheimage.Imagesreproducedwithpermission\nfromZeilerandFergus2014().", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 37, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1165}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0038_69507d1b", "text": "6\nCHAPTER1.INTRODUCTION\nx 1 x 1σ\nw 1 w 1×\nx 2 x 2 w 2 w 2×+El e me n t\nS e t\n+\n×\nσ\nxx wwEl e me n t\nS e t\nL ogi s t i c\nR e gr e s s i onL ogi s t i c\nR e gr e s s i on\nFigure1.3:Illustrationofcomputationalgraphsmappinganinputtoanoutputwhere\neachnodeperformsanoperation.Depthisthelengthofthelongestpathfrominputto\noutputbutdependsonthedeﬁnitionofwhatconstitutesapossiblecomputationalstep. Thecomputationdepictedinthesegraphsistheoutputofalogisticregressionmodel,\nσ ( wTx ),whereσisthelogisticsigmoidfunction.Ifweuseaddition,multiplicationand\nlogisticsigmoidsastheelementsofourcomputerlanguage,thenthismodelhasdepth\nthree.Ifweviewlogisticregressionasanelementitself,thenthismodelhasdepthone. instructionscanreferbacktotheresultsofearlierinstructions.Accordingtothis\nviewofdeeplearning,notalloftheinformationinalayer’sactivationsnecessarily\nencodesfactorsofvariationthatexplaintheinput.Therepresentationalsostores\nstateinformationthathelpstoexecuteaprogramthatcanmakesenseoftheinput.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 38, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 981}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0039_7ff772d9", "text": "Thisstateinformationcouldbeanalogoustoacounterorpointerinatraditional\ncomputerprogram.Ithasnothingtodowiththecontentoftheinputspeciﬁcally,\nbutithelpsthemodeltoorganizeitsprocessing. Therearetwomainwaysofmeasuringthedepthofamodel.Theﬁrstviewis\nbasedonthenumberofsequentialinstructionsthatmustbeexecutedtoevaluate\nthearchitecture.Wecanthinkofthisasthelengthofthelongestpaththrough\naﬂowchartthatdescribeshowtocomputeeachofthemodel’soutputsgiven\nitsinputs.Justastwoequivalentcomputerprogramswillhavediﬀerentlengths\ndependingonwhichlanguagetheprogramiswrittenin,thesamefunctionmay\nbedrawnasaﬂowchartwithdiﬀerentdepthsdependingonwhichfunctionswe\nallowtobeusedasindividualstepsintheﬂowchart.Figureillustrateshowthis 1.3\nchoiceoflanguagecangivetwodiﬀerentmeasurementsforthesamearchitecture.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 39, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 782}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0040_70482da1", "text": "Anotherapproach,usedbydeepprobabilisticmodels,regardsthedepthofa\nmodelasbeingnotthedepthofthecomputational graphbutthedepthofthe\ngraphdescribinghowconceptsarerelatedtoeachother.Inthiscase,thedepth\n7\nCHAPTER1.INTRODUCTION\noftheﬂowchartofthecomputations neededtocomputetherepresentationof\neachconceptmaybemuchdeeperthanthegraphoftheconceptsthemselves. Thisisbecausethesystem’sunderstandingofthesimplerconceptscanbereﬁned\ngiveninformationaboutthemorecomplexconcepts.Forexample,anAIsystem\nobservinganimageofafacewithoneeyeinshadowmayinitiallyonlyseeoneeye. Afterdetectingthatafaceispresent,itcantheninferthatasecondeyeisprobably\npresentaswell. Inthiscase,thegraphofconceptsonlyincludestwolayers—a\nlayerforeyesandalayerforfaces—butthegraphofcomputations includes 2n\nlayersifwereﬁneourestimateofeachconceptgiventheothertimes.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 40, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 819}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0041_e365c8ca", "text": "Inthiscase,thegraphofconceptsonlyincludestwolayers—a\nlayerforeyesandalayerforfaces—butthegraphofcomputations includes 2n\nlayersifwereﬁneourestimateofeachconceptgiventheothertimes. n\nBecauseitisnotalwaysclearwhichofthesetwoviews—thedepthofthe\ncomputational graph,orthedepthoftheprobabilisticmodelinggraph—ismost\nrelevant,andbecausediﬀerentpeoplechoosediﬀerentsetsofsmallestelements\nfromwhichtoconstructtheirgraphs,thereisnosinglecorrectvalueforthe\ndepthofanarchitecture,justasthereisnosinglecorrectvalueforthelengthof\nacomputerprogram. Nor isthereaconsensusabouthowmuchdepthamodel\nrequirestoqualifyas“deep.”However,deeplearningcansafelyberegardedasthe\nstudyofmodelsthateitherinvolveagreateramountofcompositionoflearned\nfunctionsorlearnedconceptsthantraditionalmachinelearningdoes. Tosummarize,deeplearning,thesubjectofthisbook,isanapproachtoAI. Speciﬁcally,itisatypeofmachinelearning,atechniquethatallowscomputer\nsystemstoimprovewithexperienceanddata.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 41, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 950}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0042_7e247fdf", "text": "Tosummarize,deeplearning,thesubjectofthisbook,isanapproachtoAI. Speciﬁcally,itisatypeofmachinelearning,atechniquethatallowscomputer\nsystemstoimprovewithexperienceanddata. Accordingtotheauthorsofthis\nbook,machinelearningistheonlyviableapproachtobuildingAIsystemsthat\ncanoperateincomplicated,real-worldenvironments.Deeplearningisaparticular\nkindofmachinelearningthatachievesgreatpowerandﬂexibilitybylearningto\nrepresenttheworldasanestedhierarchyofconcepts,witheachconceptdeﬁnedin\nrelationtosimplerconcepts,andmoreabstractrepresentationscomputedinterms\noflessabstractones.Figureillustratestherelationshipbetweenthesediﬀerent 1.4\nAIdisciplines.Figuregivesahigh-levelschematicofhoweachworks. 1.5\n1. 1 Wh o S h ou l d R ead T h i s Bo ok ?", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 42, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 733}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0043_663faa5d", "text": "1.5\n1. 1 Wh o S h ou l d R ead T h i s Bo ok ? Thisbookcanbeusefulforavarietyofreaders,butwewroteitwithtwomain\ntargetaudiencesinmind.Oneofthesetargetaudiencesisuniversitystudents\n(undergraduate orgraduate)learningaboutmachinelearning,includingthosewho\narebeginningacareerindeeplearningandartiﬁcialintelligenceresearch.The\nothertargetaudienceissoftwareengineerswhodonothaveamachinelearning\norstatisticsbackground, butwanttorapidlyacquireoneandbeginusingdeep\nlearningintheirproductorplatform.Deeplearninghasalreadyprovenusefulin\n8\nCHAPTER1.INTRODUCTION\nAIMachine learningRepresentation learningDeep learning\nExample:\nKnowledge\nbasesExample:\nLogistic\nregressionExample:\nShallow\nautoencoders Example:\nMLPs\nFigure1.4:AVenndiagramshowinghowdeeplearningisakindofrepresentationlearning,\nwhichisinturnakindofmachinelearning,whichisusedformanybutnotallapproaches\ntoAI.EachsectionoftheVenndiagramincludesanexampleofanAItechnology.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 43, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 919}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0044_cec50d1d", "text": "9\nCHAPTER1.INTRODUCTION\nInputHand-\ndesigned \nprogramOutput\nInputHand-\ndesigned \nfeaturesMapping from \nfeaturesOutput\nInputFeaturesMapping from \nfeaturesOutput\nInputSimple \nfeaturesMapping from \nfeaturesOutput\nAdditional \nlayers of more \nabstract \nfeatures\nRule-based\nsystemsClassic\nmachine\nlearning Representation\nlearningDeep\nlearning\nFigure1.5: FlowchartsshowinghowthediﬀerentpartsofanAIsystemrelatetoeach\notherwithindiﬀerentAIdisciplines.Shadedboxesindicatecomponentsthatareableto\nlearnfromdata. 1 0\nCHAPTER1.INTRODUCTION\nmanysoftwaredisciplinesincludingcomputervision,speechandaudioprocessing,\nnaturallanguageprocessing,robotics,bioinformatics andchemistry,videogames,\nsearchengines,onlineadvertisingandﬁnance.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 44, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 714}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0045_7796580e", "text": "Thisbookhasbeenorganizedintothreepartsinordertobestaccommodatea\nvarietyofreaders.Partintroducesbasicmathematical toolsandmachinelearning I\nconcepts.Partdescribesthemostestablisheddeeplearningalgorithmsthatare II\nessentiallysolvedtechnologies.Partdescribesmorespeculativeideasthatare III\nwidelybelievedtobeimportantforfutureresearchindeeplearning. Readersshouldfeelfreetoskippartsthatarenotrelevantgiventheirinterests\norbackground. Readersfamiliarwithlinearalgebra,probability,andfundamental\nmachinelearningconceptscanskippart,forexample,whilereaderswhojustwant I\ntoimplementaworkingsystemneednotreadbeyondpart.Tohelpchoosewhich II\nchapterstoread,ﬁgureprovidesaﬂowchartshowingthehigh-levelorganization 1.6\nofthebook. Wedoassumethatallreaderscomefromacomputersciencebackground. We\nassumefamiliaritywithprogramming, abasicunderstandingofcomputational\nperformanceissues,complexitytheory,introductory levelcalculusandsomeofthe\nterminologyofgraphtheory. 1.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 45, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 950}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0046_04b9cfa0", "text": "We\nassumefamiliaritywithprogramming, abasicunderstandingofcomputational\nperformanceissues,complexitytheory,introductory levelcalculusandsomeofthe\nterminologyofgraphtheory. 1. 2 Hi s t or i c a l T ren d s i n D eep L earni n g\nItiseasiesttounderstanddeeplearningwithsomehistoricalcontext.Ratherthan\nprovidingadetailedhistoryofdeeplearning,weidentifyafewkeytrends:\n•Deeplearninghashadalongandrichhistory,buthasgonebymanynames\nreﬂectingdiﬀerentphilosophicalviewpoints,andhaswaxedandwanedin\npopularity. •Deeplearninghasbecomemoreusefulastheamountofavailabletraining\ndatahasincreased. •Deeplearningmodelshavegrowninsizeovertimeascomputerinfrastructure\n(bothhardwareandsoftware)fordeeplearninghasimproved. •Deeplearninghassolvedincreasinglycomplicatedapplicationswithincreasing\naccuracyovertime. 1 1\nCHAPTER1. INTRODUCTION\n1. Introduction\nPart I: Applied Math and Machine Learning Basics\n2. Linear Algebra3. Probability and\nInformation Theory\n4. Numerical\nComputation5.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 46, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 964}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0047_fcc81805", "text": "1 1\nCHAPTER1. INTRODUCTION\n1. Introduction\nPart I: Applied Math and Machine Learning Basics\n2. Linear Algebra3. Probability and\nInformation Theory\n4. Numerical\nComputation5. Machine Learning\nBasics\nPart II: Deep Networks: Modern Practices\n6. Deep Feedforwar d\nNetworks\n7. Regularization 8. Optimization 9. CNNs 10. RNNs\n11. Practical\nMethodology12. Applications\nPart III: Deep Learning Research\n13. Linear Factor\nModels14. Autoencoders15. Representation\nLearning\n16. Structured\nProbabilistic Models17. Monte Carlo\nMethods\n18. Partition\nFunction19. Inference\n20. Deep Generative\nModels\nFigure1.6: Thehigh-levelorganizationofthebook. Anarrowfromonechaptertoanother\nindicates that the former chapteris prerequisite material for understanding the latter. 12\nCHAPTER1.INTRODUCTION\n1 . 2 .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 47, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 783}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0048_09a780a4", "text": "Anarrowfromonechaptertoanother\nindicates that the former chapteris prerequisite material for understanding the latter. 12\nCHAPTER1.INTRODUCTION\n1 . 2 . 1 T h e Ma n y Na m es a n d Ch a n g i n g F o rt u n es o f Neu ra l Net -\nw o rks\nWeexpectthatmanyreadersofthisbookhaveheardofdeeplearningasan\nexcitingnewtechnology,andaresurprisedtoseeamentionof“history”inabook\naboutanemergingﬁeld.Infact,deeplearningdatesbacktothe1940s.Deep\nlearningonly a p p e a r stobenew,becauseitwasrelativelyunpopularforseveral\nyearsprecedingitscurrentpopularity,andbecauseithasgonethroughmany\ndiﬀerentnames,andhasonlyrecentlybecomecalled“deeplearning.”Theﬁeld\nhasbeenrebrandedmanytimes,reﬂectingtheinﬂuenceofdiﬀerentresearchers\nanddiﬀerentperspectives. Acomprehensivehistoryofdeeplearningisbeyondthescopeofthistextbook.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 48, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 799}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0049_e2c85df2", "text": "Acomprehensivehistoryofdeeplearningisbeyondthescopeofthistextbook. However,somebasiccontextisusefulforunderstandingdeeplearning.Broadly\nspeaking,therehavebeenthreewavesofdevelopmentofdeeplearning:deep\nlearning known as c y b e r net i c sin the 1940s–1960s, deep learning knownas\nc o nnec t i o n i s minthe1980s–1990s,andthecurrentresurgenceunderthename\ndeeplearningbeginningin2006.Thisisquantitativelyillustratedinﬁgure.1.7\nSomeoftheearliestlearningalgorithmswerecognizetodaywereintended\ntobecomputational modelsofbiologicallearning,i.e.modelsofhowlearning\nhappensorcouldhappeninthebrain. Asaresult,oneofthenamesthatdeep\nlearninghasgonebyis ar t i ﬁc i al neur al net w o r k s(ANNs).Thecorresponding\nperspectiveondeeplearningmodelsisthattheyareengineeredsystemsinspired\nbythebiologicalbrain(whetherthehumanbrainorthebrainofanotheranimal).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 49, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 841}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0050_fa2538fc", "text": "Whilethekindsofneuralnetworksusedformachinelearninghavesometimes\nbeenusedtounderstandbrainfunction( ,),theyare HintonandShallice1991\ngenerallynotdesignedtoberealisticmodelsofbiologicalfunction.Theneural\nperspectiveondeeplearningismotivatedbytwomainideas.Oneideaisthat\nthebrainprovidesaproofbyexamplethatintelligentbehaviorispossible,anda\nconceptuallystraightforwardpathtobuildingintelligenceistoreverseengineerthe\ncomputational principlesbehindthebrainandduplicateitsfunctionality.Another\nperspectiveisthatitwouldbedeeplyinterestingtounderstandthebrainandthe\nprinciplesthatunderliehumanintelligence,somachinelearningmodelsthatshed\nlightonthesebasicscientiﬁcquestionsareusefulapartfromtheirabilitytosolve\nengineeringapplications.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 50, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 728}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0051_3ace75be", "text": "Themodernterm“deeplearning”goesbeyondtheneuroscientiﬁcperspective\nonthecurrentbreedofmachinelearningmodels.Itappealstoamoregeneral\nprincipleoflearning m u l t i p l e l e v e l s o f c o m p o s i t i o n,whichcanbeappliedinmachine\nlearningframeworksthatarenotnecessarilyneurallyinspired.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 51, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 288}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0052_9e4b1fa7", "text": "1 3\nCHAPTER1.INTRODUCTION\n1940 1950 1960 1970 1980 1990 2000\nYear0.0000000.0000500.0001000.0001500.0002000.000250FrequencyofWordorPhrase\nc y b e r n e t i c s\n( c o n n e c t i o n i s m + n e u r a l n e t w o r k s )\nFigure1.7:Theﬁgureshowstwoofthethreehistoricalwavesofartiﬁcialneuralnets\nresearch,asmeasuredbythefrequencyofthephrases“cybernetics”and“connectionism”or\n“neuralnetworks”accordingtoGoogleBooks(thethirdwaveistoorecenttoappear).The\nﬁrstwavestartedwithcyberneticsinthe1940s–1960s, withthedevelopmentoftheories\nofbiologicallearning( ,;,)andimplementationsof McCullochandPitts1943Hebb1949\ntheﬁrstmodelssuchastheperceptron(Rosenblatt1958,)allowingthetrainingofasingle\nneuron.Thesecondwavestartedwiththeconnectionistapproachofthe1980–1995period,\nwithback-propagation( ,)totrainaneuralnetworkwithoneortwo Rumelhart e t a l .1986a\nhiddenlayers.Thecurrentandthirdwave,deeplearning,startedaround2006(Hinton\ne t a l . e t a l . e t a l .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 52, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 942}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0053_ad50c133", "text": "e t a l . e t a l . ,;2006Bengio,;2007Ranzato,),andisjustnowappearinginbook 2007a\nformasof2016.Theothertwowavessimilarlyappearedinbookformmuchlaterthan\nthecorrespondingscientiﬁcactivityoccurred. 1 4\nCHAPTER1.INTRODUCTION\nTheearliestpredecessorsofmoderndeeplearningweresimplelinearmodels\nmotivatedfromaneuroscientiﬁcperspective.Thesemodelsweredesignedto\ntakeasetofninputvalues x 1,...,x nandassociatethemwithanoutput y. Thesemodelswouldlearnasetofweightsw 1,...,w nandcomputetheiroutput\nf ( x w, ) =x 1w 1 + · · · +x nw n.Thisﬁrstwaveofneuralnetworksresearchwas\nknownascybernetics,asillustratedinﬁgure.1.7\nTheMcCulloch-PittsNeuron( ,)wasanearlymodel McCullochandPitts1943\nofbrainfunction.Thislinearmodelcouldrecognizetwodiﬀerentcategoriesof\ninputsbytestingwhether f ( x w, )ispositiveornegative.Ofcourse,forthemodel\ntocorrespondtothedesireddeﬁnitionofthecategories,theweightsneededtobe\nsetcorrectly.Theseweightscouldbesetbythehumanoperator.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 53, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 939}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0054_2e143365", "text": "Inthe1950s,\ntheperceptron(Rosenblatt19581962,,)becametheﬁrstmodelthatcouldlearn\ntheweightsdeﬁningthecategoriesgivenexamplesofinputsfromeachcategory. The adapt i v e l i near e l e m e n t(ADALINE),whichdatesfromaboutthesame\ntime,simplyreturnedthevalueoff ( x )itselftopredictarealnumber(Widrow\nandHoﬀ1960,),andcouldalsolearntopredictthesenumbersfromdata. Thesesimplelearningalgorithmsgreatlyaﬀectedthemodernlandscapeofma-\nchinelearning.ThetrainingalgorithmusedtoadapttheweightsoftheADALINE\nwasaspecialcaseofanalgorithmcalled st o c hast i c g r adi e n t desc e n t.Slightly\nmodiﬁedversionsofthestochasticgradientdescentalgorithmremainthedominant\ntrainingalgorithmsfordeeplearningmodelstoday. Modelsbasedonthef ( x w, )usedbytheperceptronandADALINEarecalled\nl i near m o del s.Thesemodelsremainsomeofthemostwidelyusedmachine\nlearningmodels,thoughinmanycasestheyare t r a i ne dindiﬀerentwaysthanthe\noriginalmodelsweretrained.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 54, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 925}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0055_36b0fbd1", "text": "Linearmodelshavemanylimitations.Mostfamously,theycannotlearnthe\nXORfunction,where f ( [ 0, 1], w ) = 1and f ( [ 1, 0], w ) = 1butf ( [ 1, 1], w ) = 0\nandf ( [ 0, 0], w ) = 0.Criticswhoobservedtheseﬂawsinlinearmodelscaused\nabacklashagainstbiologicallyinspiredlearningingeneral(MinskyandPapert,\n1969).Thiswastheﬁrstmajordipinthepopularityofneuralnetworks. Today,neuroscienceisregardedasanimportantsourceofinspirationfordeep\nlearningresearchers,butitisnolongerthepredominant guidefortheﬁeld. Themainreasonforthediminishedrole ofneuroscienceindeeplearning\nresearchtodayisthatwesimplydonothaveenoughinformationaboutthebrain\ntouseitasaguide.Toobtainadeepunderstandingoftheactualalgorithmsused\nbythebrain,wewouldneedtobeabletomonitortheactivityof(atthevery\nleast)thousandsofinterconnectedneuronssimultaneously.Becausewearenot\nabletodothis,wearefarfromunderstandingevensomeofthemostsimpleand\n1 5\nCHAPTER1.INTRODUCTION\nwell-studiedpartsofthebrain( ,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 55, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 942}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0056_2b4ad534", "text": "OlshausenandField2005\nNeurosciencehasgivenusareasontohopethatasingledeeplearningalgorithm\ncansolvemanydiﬀerenttasks.Neuroscientistshavefoundthatferretscanlearnto\n“see”withtheauditoryprocessingregionoftheirbrainiftheirbrainsarerewired\ntosendvisualsignalstothatarea(VonMelchner 2000 e t a l .,).Thissuggeststhat\nmuchofthemammalianbrainmightuseasinglealgorithmtosolvemostofthe\ndiﬀerenttasksthatthebrainsolves.Beforethishypothesis,machinelearning\nresearchwasmorefragmented,withdiﬀerentcommunitiesofresearchersstudying\nnaturallanguageprocessing,vision,motionplanningandspeechrecognition.Today,\ntheseapplicationcommunitiesarestillseparate,butitiscommonfordeeplearning\nresearchgroupstostudymanyorevenalloftheseapplicationareassimultaneously.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 56, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 734}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0057_ca2e70a3", "text": "Weareabletodrawsomeroughguidelinesfromneuroscience.Thebasicideaof\nhavingmanycomputational unitsthatbecomeintelligentonlyviatheirinteractions\nwitheachotherisinspiredbythebrain.TheNeocognitron(Fukushima1980,)\nintroducedapowerfulmodelarchitectureforprocessingimagesthatwasinspired\nbythestructureofthemammalianvisualsystemandlaterbecamethebasis\nforthemodernconvolutionalnetwork( ,),aswewillseein LeCun e t a l .1998b\nsection.Mostneuralnetworkstodayarebasedonamodelneuroncalled 9.10\nthe r e c t i ﬁed l i near uni t.TheoriginalCognitron(Fukushima1975,)introduced\namorecomplicatedversionthatwashighlyinspiredbyourknowledgeofbrain\nfunction.Thesimpliﬁedmodernversionwasdevelopedincorporatingideasfrom\nmanyviewpoints,with ()and ()citing NairandHinton2010Glorot e t a l .2011a\nneuroscienceasaninﬂuence,and ()citingmoreengineering- Jarrett e t a l .2009\norientedinﬂuences.Whileneuroscienceisanimportantsourceofinspiration,it\nneednotbetakenasarigidguide.Weknowthatactualneuronscomputevery\ndiﬀerentfunctionsthanmodernrectiﬁedlinearunits,butgreaterneuralrealism\nhasnotyetledtoanimprovementinmachinelearningperformance.Also,while\nneurosciencehassuccessfullyinspiredseveralneuralnetwork a r c h i t e c t u r e s,we\ndonotyetknowenoughaboutbiologicallearningforneurosciencetooﬀermuch\nguidanceforthe l e a r ning a l g o r i t h m sweusetotrainthesearchitectures.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 57, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1345}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0058_78c7ccfc", "text": "Mediaaccountsoftenemphasizethesimilarityofdeeplearningtothebrain. Whileitistruethatdeeplearningresearchersaremorelikelytocitethebrainasan\ninﬂuencethanresearchersworkinginothermachinelearningﬁeldssuchaskernel\nmachinesorBayesianstatistics,oneshouldnotviewdeeplearningasanattempt\ntosimulatethebrain.Moderndeeplearningdrawsinspirationfrommanyﬁelds,\nespeciallyappliedmathfundamentalslikelinearalgebra,probability,information\ntheory,andnumericaloptimization. Whilesomedeeplearningresearcherscite\nneuroscienceasanimportantsourceofinspiration,othersarenotconcernedwith\n1 6\nCHAPTER1.INTRODUCTION\nneuroscienceatall. Itis worth notingthat theeﬀorttounderstandhowthe brainworkson\nan algorithmic lev el is alive andwell.This endeavor is primarily knownas\n“computational neuroscience”andisaseparateﬁeldofstudyfromdeeplearning.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 58, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 812}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0059_663afe31", "text": "Itiscommonforresearcherstomovebackandforthbetweenbothﬁelds.The\nﬁeldofdeeplearningisprimarilyconcernedwithhowtobuildcomputersystems\nthatareabletosuccessfullysolvetasksrequiringintelligence,whiletheﬁeldof\ncomputational neuroscienceisprimarilyconcernedwithbuildingmoreaccurate\nmodelsofhowthebrainactuallyworks. Inthe1980s,thesecondwaveofneuralnetworkresearchemergedingreat\npartviaamovementcalled c o nnec t i o n i s mor par al l e l di st r i but e d pr o c e ss-\ni ng( ,; ,). Connectionism arosein Rumelhart e t a l .1986cMcClelland e t a l .1995\nthecontextofcognitivescience.Cognitivescienceisaninterdisciplinaryapproach\ntounderstandingthemind,combiningmultiplediﬀerentlevelsofanalysis.During\ntheearly1980s,mostcognitivescientistsstudiedmodelsofsymbolicreasoning.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 59, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 763}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0060_079f2f7c", "text": "Despitetheirpopularity,symbolicmodelswerediﬃculttoexplainintermsof\nhowthebraincouldactuallyimplementthemusingneurons.Theconnectionists\nbegantostudymodelsofcognitionthatcouldactuallybegroundedinneural\nimplementations(TouretzkyandMinton1985,),revivingmanyideasdatingback\ntotheworkofpsychologistDonaldHebbinthe1940s(,).Hebb1949\nThecentralideainconnectionism isthatalargenumberofsimplecomputational\nunitscanachieveintelligentbehaviorwhennetworkedtogether.Thisinsight\nappliesequallytoneuronsinbiologicalnervoussystemsandtohiddenunitsin\ncomputational models. Severalkeyconceptsaroseduringtheconnectionism movementofthe1980s\nthatremaincentraltotoday’sdeeplearning.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 60, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 657}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0061_e1cd4e9d", "text": "Severalkeyconceptsaroseduringtheconnectionism movementofthe1980s\nthatremaincentraltotoday’sdeeplearning. Oneoftheseconceptsisthatof di st r i but e d r e pr e se n t at i o n(Hinton e t a l .,\n1986).Thisistheideathateachinputtoasystemshouldberepresentedby\nmanyfeatures,andeachfeatureshouldbeinvolvedintherepresentationofmany\npossibleinputs.Forexample,supposewehaveavisionsystemthatcanrecognize\ncars,trucks,andbirdsandtheseobjectscaneachbered,green,orblue.Oneway\nofrepresentingtheseinputswouldbetohaveaseparateneuronorhiddenunit\nthatactivatesforeachoftheninepossiblecombinations:redtruck,redcar,red\nbird,greentruck,andsoon.Thisrequiresninediﬀerentneurons,andeachneuron\nmustindependentlylearntheconceptofcolorandobjectidentity.Onewayto\nimproveonthissituationistouseadistributedrepresentation,withthreeneurons\ndescribingthecolorandthreeneuronsdescribingtheobjectidentity.Thisrequires\nonlysixneuronstotalinsteadofnine,andtheneurondescribingrednessisableto\n1 7\nCHAPTER1.INTRODUCTION\nlearnaboutrednessfromimagesofcars,trucksandbirds,notonlyfromimages\nofonespeciﬁccategoryofobjects.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 61, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1075}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0062_babe69b2", "text": "Theconceptofdistributedrepresentationis\ncentraltothisbook,andwillbedescribedingreaterdetailinchapter.15\nAnothermajoraccomplishmentoftheconnectionistmovementwasthesuc-\ncessfuluseofback-propagation totraindeepneuralnetworkswithinternalrepre-\nsentationsandthepopularization oftheback-propagation algorithm(Rumelhart\ne t a l .,;,).Thisalgorithmhaswaxedandwanedinpopularity 1986aLeCun1987\nbutasofthiswritingiscurrentlythedominantapproachtotrainingdeepmodels. Duringthe1990s,researchersmadeimportantadvancesinmodelingsequences\nwithneuralnetworks.()and ()identiﬁedsomeof Hochreiter1991Bengio e t a l .1994\nthefundamentalmathematical diﬃcultiesinmodelinglongsequences,describedin\nsection.10.7HochreiterandSchmidhuber1997()introducedthelongshort-term\nmemoryorLSTMnetworktoresolvesomeofthesediﬃculties.Today,theLSTM\niswidelyusedformanysequencemodelingtasks,includingmanynaturallanguage\nprocessingtasksatGoogle.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 62, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 900}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0063_a2c12595", "text": "Thesecondwaveofneuralnetworksresearchlasteduntilthemid-1990s.Ven-\nturesbasedonneuralnetworksandotherAItechnologiesbegantomakeunrealisti-\ncallyambitiousclaimswhileseekinginvestments.WhenAIresearchdidnotfulﬁll\ntheseunreasonableexpectations,investorsweredisappointed.Simultaneously,\notherﬁeldsofmachinelearningmadeadvances.Kernelmachines(,Boser e t a l . 1992CortesandVapnik1995Schölkopf1999 Jor- ; ,; e t a l .,)andgraphicalmodels(\ndan1998,)bothachievedgoodresultsonmanyimportanttasks.Thesetwofactors\nledtoadeclineinthepopularityofneuralnetworksthatlasteduntil2007. Duringthistime,neuralnetworkscontinuedtoobtainimpressiveperformance\nonsometasks( ,; ,).TheCanadianInstitute LeCun e t a l .1998bBengio e t a l .2001\nforAdvancedResearch(CIFAR)helpedtokeepneuralnetworksresearchalive\nviaitsNeuralComputation andAdaptivePerception(NCAP)researchinitiative.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 63, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 849}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0064_ac1c1702", "text": "ThisprogramunitedmachinelearningresearchgroupsledbyGeoﬀreyHinton\natUniversityofToronto,YoshuaBengioatUniversityofMontreal,andYann\nLeCunatNewYorkUniversity.TheCIFARNCAPresearchinitiativehada\nmulti-disciplinarynaturethatalsoincludedneuroscientistsandexpertsinhuman\nandcomputervision. Atthispointintime,deepnetworksweregenerallybelievedtobeverydiﬃcult\ntotrain. Wenowknowthatalgorithmsthathaveexistedsincethe1980swork\nquitewell,butthiswasnotapparentcirca2006.Theissueisperhapssimplythat\nthesealgorithmsweretoocomputationally costlytoallowmuchexperimentation\nwiththehardwareavailableatthetime. Thethirdwaveofneuralnetworksresearchbeganwithabreakthrough in\n1 8\nCHAPTER1.INTRODUCTION\n2006.GeoﬀreyHintonshowedthatakindofneuralnetworkcalledadeepbelief\nnetworkcouldbeeﬃcientlytrainedusingastrategycalledgreedylayer-wisepre-\ntraining( ,),whichwillbedescribedinmoredetailinsection.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 64, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 869}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0065_62827e4a", "text": "Hinton e t a l .2006 15.1\nTheotherCIFAR-aﬃliatedresearchgroupsquicklyshowedthatthesamestrategy\ncouldbeusedtotrainmanyotherkindsofdeepnetworks( ,; Bengio e t a l .2007\nRanzato 2007a e t a l .,)andsystematicallyhelpedtoimprovegeneralization on\ntestexamples.Thiswaveofneuralnetworksresearchpopularizedtheuseofthe\nterm“deeplearning”toemphasizethatresearcherswerenowabletotraindeeper\nneuralnetworksthanhadbeenpossiblebefore,andtofocusattentiononthe\ntheoreticalimportanceofdepth( ,; , BengioandLeCun2007DelalleauandBengio\n2011Pascanu2014aMontufar2014 ; e t a l .,; e t a l .,).Atthistime,deepneural\nnetworksoutperformedcompetingAIsystemsbasedonothermachinelearning\ntechnologiesaswellashand-designedfunctionality.Thisthirdwaveofpopularity\nofneuralnetworkscontinuestothetimeofthiswriting,thoughthefocusofdeep\nlearningresearchhaschangeddramatically withinthetimeofthiswave.The\nthirdwavebeganwithafocusonnewunsupervisedlearningtechniquesandthe\nabilityofdeepmodelstogeneralizewellfromsmalldatasets,buttodaythereis\nmoreinterestinmucholdersupervisedlearningalgorithmsandtheabilityofdeep\nmodelstoleveragelargelabeleddatasets.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 65, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1111}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0066_c84fc0ca", "text": "1 . 2 . 2 In creasin g D a t a s et S i zes\nOnemaywonderwhydeeplearninghasonlyrecentlybecomerecognizedasa\ncrucialtechnologythoughtheﬁrstexperimentswithartiﬁcialneuralnetworkswere\nconductedinthe1950s.Deeplearninghasbeensuccessfullyusedincommercial\napplicationssincethe1990s,butwasoftenregardedasbeingmoreofanartthan\natechnologyandsomethingthatonlyanexpertcoulduse,untilrecently.Itistrue\nthatsomeskillisrequiredtogetgoodperformancefromadeeplearningalgorithm.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 66, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 456}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0067_ccac2aab", "text": "Fortunately,theamountofskillrequiredreducesastheamountoftrainingdata\nincreases.Thelearningalgorithmsreachinghumanperformanceoncomplextasks\ntodayarenearlyidenticaltothelearningalgorithmsthatstruggledtosolvetoy\nproblemsinthe1980s,thoughthemodelswetrainwiththesealgorithmshave\nundergonechangesthatsimplifythetrainingofverydeeparchitectures.Themost\nimportantnewdevelopmentisthattodaywecanprovidethesealgorithmswith\ntheresourcestheyneedtosucceed.Figureshowshowthesizeofbenchmark 1.8\ndatasetshasincreasedremarkablyovertime.Thistrendisdrivenbytheincreasing\ndigitizationofsociety.Asmoreandmoreofouractivitiestakeplaceoncomputers,\nmoreandmoreofwhatwedoisrecorded.Asourcomputersareincreasingly\nnetworkedtogether,itbecomeseasiertocentralizetheserecordsandcuratethem\n1 9\nCHAPTER1.INTRODUCTION\nintoadatasetappropriateformachinelearningapplications.Theageof“Big\nData”hasmademachinelearningmucheasierbecausethekeyburdenofstatistical\nestimation—generalizingwelltonewdataafterobservingonlyasmallamount\nofdata—hasbeenconsiderablylightened.Asof2016,aroughruleofthumb\nisthatasuperviseddeeplearningalgorithmwillgenerallyachieveacceptable\nperformancewitharound5,000labeledexamplespercategory,andwillmatchor\nexceedhumanperformancewhentrainedwithadatasetcontainingatleast10\nmillionlabeledexamples.Workingsuccessfullywithdatasetssmallerthanthisis\nanimportantresearcharea,focusinginparticularonhowwecantakeadvantage\noflargequantitiesofunlabeledexamples,withunsupervisedorsemi-supervised\nlearning.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 67, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1470}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0068_30209875", "text": "1 . 2 . 3 In creasin g Mo d el S i zes\nAnotherkeyreasonthatneuralnetworksarewildlysuccessfultodayafterenjoying\ncomparativelylittlesuccesssincethe1980sisthatwehavethecomputational\nresourcestorunmuchlargermodelstoday.Oneofthemaininsightsofconnection-\nismisthatanimalsbecomeintelligentwhenmanyoftheirneuronsworktogether. Anindividualneuronorsmallcollectionofneuronsisnotparticularlyuseful. Biologicalneuronsarenotespeciallydenselyconnected.Asseeninﬁgure,1.10\nourmachinelearningmodelshavehadanumberofconnectionsperneuronthat\nwaswithinanorderofmagnitudeofevenmammalianbrainsfordecades.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 68, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 580}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0069_431ef5e4", "text": "Biologicalneuronsarenotespeciallydenselyconnected.Asseeninﬁgure,1.10\nourmachinelearningmodelshavehadanumberofconnectionsperneuronthat\nwaswithinanorderofmagnitudeofevenmammalianbrainsfordecades. Intermsofthetotalnumberofneurons,neuralnetworkshavebeenastonishingly\nsmalluntilquiterecently,asshowninﬁgure.Sincetheintroductionofhidden 1.11\nunits,artiﬁcialneuralnetworkshavedoubledinsizeroughlyevery2.4years.This\ngrowthisdrivenbyfastercomputerswithlargermemoryandbytheavailability\noflargerdatasets.Largernetworksareabletoachievehigheraccuracyonmore\ncomplextasks.Thistrendlookssettocontinuefordecades.Unlessnewtechnologies\nallowfasterscaling,artiﬁcialneuralnetworkswillnothavethesamenumberof\nneuronsasthehumanbrainuntilatleastthe2050s.Biologicalneuronsmay\nrepresentmorecomplicatedfunctionsthancurrentartiﬁcialneurons,sobiological\nneuralnetworksmaybeevenlargerthanthisplotportrays.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 69, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 874}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0070_b2c9aa80", "text": "Inretrospect,itisnotparticularlysurprisingthatneuralnetworkswithfewer\nneuronsthanaleechwereunabletosolvesophisticatedartiﬁcialintelligenceprob-\nlems.Eventoday’snetworks,whichweconsiderquitelargefromacomputational\nsystemspointofview,aresmallerthanthenervoussystemofevenrelatively\nprimitivevertebrateanimalslikefrogs.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 70, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 315}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0071_ea88c576", "text": "Theincreaseinmodelsizeovertime,duetotheavailabilityoffasterCPUs,\n2 0\nCHAPTER1.INTRODUCTION\n1900 1950 198520002015\nYear100101102103104105106107108109Datasetsize(numberexamples)\nIrisMNISTPublicSVHN\nImageNet\nCIFAR-10ImageNet10k\nILSVRC 2014Sports-1M\nRotatedTvs.C Tvs.Gvs.FCriminalsCanadianHansard\nWMT\nFigure1.8:Datasetsizeshaveincreasedgreatlyovertime.Intheearly1900s,statisticians\nstudieddatasetsusinghundredsorthousandsofmanuallycompiledmeasurements(,Garson\n1900Gosset1908Anderson1935Fisher1936 ;,;,;,).Inthe1950sthrough1980s,thepioneers\nofbiologicallyinspiredmachinelearningoftenworkedwithsmall,syntheticdatasets,such\naslow-resolutionbitmapsofletters,thatweredesignedtoincurlowcomputationalcostand\ndemonstratethatneuralnetworkswereabletolearnspeciﬁckindsoffunctions(Widrow\nandHoﬀ1960Rumelhart1986b ,; e t a l .,).Inthe1980sand1990s,machinelearning\nbecamemorestatisticalinnatureandbegantoleveragelargerdatasetscontainingtens\nofthousandsofexamplessuchastheMNISTdataset(showninﬁgure)ofscans 1.9\nofhandwrittennumbers( ,).Intheﬁrstdecadeofthe2000s,more LeCun e t a l .1998b\nsophisticateddatasetsofthissamesize,suchastheCIFAR-10dataset(Krizhevskyand\nHinton2009,)continuedtobeproduced.Towardtheendofthatdecadeandthroughout\ntheﬁrsthalfofthe2010s,signiﬁcantlylargerdatasets,containinghundredsofthousands\ntotensofmillionsofexamples,completelychangedwhatwaspossiblewithdeeplearning.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 71, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1370}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0072_84019dc6", "text": "ThesedatasetsincludedthepublicStreetViewHouseNumbersdataset( , Netzer e t a l . 2011),variousversionsoftheImageNetdataset( ,,; Deng e t a l .20092010aRussakovsky\ne t a l . e t a l . ,),andtheSports-1Mdataset( 2014a Karpathy,).Atthetopofthe 2014\ngraph,weseethatdatasetsoftranslatedsentences,suchasIBM’sdatasetconstructed\nfromtheCanadianHansard( ,)andtheWMT2014EnglishtoFrench Brown e t a l .1990\ndataset(Schwenk2014,)aretypicallyfaraheadofotherdatasetsizes. 2 1\nCHAPTER1.INTRODUCTION\nFigure1.9:ExampleinputsfromtheMNISTdataset.The“NIST”standsforNational\nInstituteofStandardsandTechnology,theagencythatoriginallycollectedthisdata. The“M”standsfor“modiﬁed,”sincethedatahasbeenpreprocessedforeasierusewith\nmachinelearningalgorithms.TheMNISTdatasetconsistsofscansofhandwrittendigits\nandassociatedlabelsdescribingwhichdigit0–9iscontainedineachimage.Thissimple\nclassiﬁcationproblemisoneofthesimplestandmostwidelyusedtestsindeeplearning\nresearch.Itremainspopulardespitebeingquiteeasyformoderntechniquestosolve.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 72, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1002}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0073_e0f73a9e", "text": "GeoﬀreyHintonhasdescribeditas“the d r o s o p h i l aofmachinelearning,”meaningthat\nitallowsmachinelearningresearcherstostudytheiralgorithmsincontrolledlaboratory\nconditions,muchasbiologistsoftenstudyfruitﬂies. 2 2\nCHAPTER1.INTRODUCTION\ntheadventofgeneralpurposeGPUs(describedinsection),fasternetwork 12.1.2\nconnectivityandbettersoftwareinfrastructurefordistributedcomputing,isoneof\nthemostimportanttrendsinthehistoryofdeeplearning.Thistrendisgenerally\nexpectedtocontinuewellintothefuture. 1 . 2 . 4 In creasin g A ccu ra cy , Co m p l e xi t y a n d Rea l - W o rl d Im p a ct\nSincethe1980s,deeplearninghasconsistentlyimprovedinitsabilitytoprovide\naccuraterecognitionorprediction.Moreover,deeplearninghasconsistentlybeen\nappliedwithsuccesstobroaderandbroadersetsofapplications.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 73, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 778}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0074_639dbd28", "text": "Theearliestdeepmodelswereusedtorecognizeindividualobjectsintightly\ncropped,extremelysmallimages( ,).Sincethentherehas Rumelhart e t a l .1986a\nbeenagradualincreaseinthesizeofimagesneuralnetworkscouldprocess.Modern\nobjectrecognitionnetworksprocessrichhigh-resolutionphotographs anddonot\nhavearequirementthatthephotobecroppedneartheobjecttoberecognized\n( ,).Similarly,theearliestnetworkscouldonlyrecognize Krizhevsky e t a l .2012\ntwokindsofobjects(orinsomecases,theabsenceorpresenceofasinglekindof\nobject),whilethesemodernnetworkstypicallyrecognizeatleast1,000diﬀerent\ncategoriesofobjects.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 74, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 588}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0075_f4ea3f2c", "text": "ThelargestcontestinobjectrecognitionistheImageNet\nLargeScaleVisualRecognitionChallenge(ILSVRC)heldeachyear.Adramatic\nmomentinthemeteoricriseofdeeplearningcamewhenaconvolutionalnetwork\nwonthischallengefortheﬁrsttimeandbyawidemargin,bringingdownthe\nstate-of-the-art top-5errorratefrom26.1%to15.3%( ,), Krizhevsky e t a l .2012\nmeaningthattheconvolutionalnetworkproducesarankedlistofpossiblecategories\nforeachimageandthecorrectcategoryappearedintheﬁrstﬁveentriesofthis\nlistforallbut15.3%ofthetestexamples.Sincethen,thesecompetitionsare\nconsistentlywonbydeepconvolutionalnets,andasofthiswriting,advancesin\ndeeplearninghavebroughtthelatesttop-5errorrateinthiscontestdownto3.6%,\nasshowninﬁgure.1.12\nDeeplearninghasalsohadadramaticimpactonspeechrecognition.After\nimprovingthroughoutthe1990s,theerrorratesforspeechrecognitionstagnated\nstartinginabout2000.Theintroductionofdeeplearning(,; Dahl e t a l .2010Deng\ne t a l . e t a l . e t a l .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 75, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 932}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0076_79c8bf1c", "text": "e t a l . e t a l . ,;2010bSeide,;2011Hinton,)tospeechrecognitionresulted 2012a\ninasuddendropoferrorrates,withsomeerrorratescutinhalf.Wewillexplore\nthishistoryinmoredetailinsection.12.3\nDeepnetworkshavealsohadspectacularsuccessesforpedestriandetectionand\nimagesegmentation( ,; Sermanet e t a l .2013Farabet2013Couprie e t a l .,; e t a l .,\n2013)andyieldedsuperhumanperformanceintraﬃcsignclassiﬁcation(Ciresan\n2 3\nCHAPTER1.INTRODUCTION\n1 9 5 0 1 9 8 5 2 0 0 0 2 0 1 5\nY e a r1 011 021 031 04C o nne c t i o ns p e r ne ur o n\n12\n34\n567\n89\n1 0\nF r ui t ﬂyMo useC a tH um a n\nFigure1.10:Initially,thenumberofconnectionsbetweenneuronsinartiﬁcialneural\nnetworkswaslimitedbyhardwarecapabilities.Today,thenumberofconnectionsbetween\nneuronsismostlyadesignconsideration.Someartiﬁcialneuralnetworkshavenearlyas\nmanyconnectionsperneuronasacat,anditisquitecommonforotherneuralnetworks\ntohaveasmanyconnectionsperneuronassmallermammalslikemice.Eventhehuman\nbraindoesnothaveanexorbitantamountofconnectionsperneuron.Biologicalneural\nnetworksizesfrom ().", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 76, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1038}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0077_c3c3bf4b", "text": "Wikipedia2015\n1.Adaptivelinearelement( ,) WidrowandHoﬀ1960\n2.Neocognitron(Fukushima1980,)\n3.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al.2006\n4.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)\n5.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al.2009\n6.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al.2010\n7.Distributedautoencoder(,) Le e t al.2012\n8.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al.2012\n9.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al.2013\n10.GoogLeNet( ,) Szegedy e t al.2014a\n2 4\nCHAPTER1.INTRODUCTION\ne t a l .,).2012\nAtthesametimethatthescaleandaccuracyofdeepnetworkshasincreased,\nsohasthecomplexityofthetasksthattheycansolve.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 77, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 687}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0078_16242073", "text": "() Goodfellow e t a l .2014d\nshowedthatneuralnetworkscouldlearntooutputanentiresequenceofcharacters\ntranscribedfromanimage,ratherthanjustidentifyingasingleobject.Previously,\nitwaswidelybelievedthatthiskindoflearningrequiredlabelingoftheindividual\nelementsofthesequence( ,).Recurrentneuralnetworks, GülçehreandBengio2013\nsuchastheLSTMsequencemodelmentionedabove,arenowusedtomodel\nrelationshipsbetween s e q u e nc e s s e q u e nc e s andother ratherthanjustﬁxedinputs. Thissequence-to-sequencelearningseemstobeonthecuspofrevolutionizing\nanotherapplication:machinetranslation(Sutskever2014Bahdanau e t a l .,; e t a l .,\n2015).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 78, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 626}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0079_0fdbe61d", "text": "Thissequence-to-sequencelearningseemstobeonthecuspofrevolutionizing\nanotherapplication:machinetranslation(Sutskever2014Bahdanau e t a l .,; e t a l .,\n2015). Thistrendofincreasingcomplexityhasbeenpushedtoitslogicalconclusion\nwiththeintroductionofneuralTuringmachines(Graves2014a e t a l .,)thatlearn\ntoreadfrommemorycellsandwritearbitrarycontenttomemorycells.Such\nneuralnetworkscanlearnsimpleprogramsfromexamplesofdesiredbehavior.For\nexample,theycanlearntosortlistsofnumbersgivenexamplesofscrambledand\nsortedsequences.Thisself-programming technologyisinitsinfancy,butinthe\nfuturecouldinprinciplebeappliedtonearlyanytask.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 79, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 620}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0080_5661c76d", "text": "Anothercrowningachievementofdeeplearningisitsextensiontothedomainof\nr e i nf o r c e m e n t l e ar ni ng.Inthecontextofreinforcementlearning,anautonomous\nagentmustlearntoperformataskbytrialanderror,withoutanyguidancefrom\nthehumanoperator.DeepMinddemonstratedthatareinforcementlearningsystem\nbasedondeeplearningiscapableoflearningtoplayAtarivideogames,reaching\nhuman-levelperformanceonmanytasks(,).Deeplearninghas Mnih e t a l .2015\nalsosigniﬁcantlyimprovedtheperformanceofreinforcementlearningforrobotics\n(,). Finn e t a l .2015\nManyoftheseapplicationsofdeeplearningarehighlyproﬁtable.Deeplearning\nisnowused bymanytoptechnologycompanies includi ngGoogle, Microsoft,\nFacebook,IBM,Baidu,Apple,Adobe,Netﬂix,NVIDIAandNEC. Advancesindeeplearninghavealsodependedheavilyonadvancesinsoftware\ninfrastructure.SoftwarelibrariessuchasTheano( ,; Bergstra e t a l .2010Bastien\ne t a l . e t a l .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 80, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 883}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0081_9910893c", "text": "Advancesindeeplearninghavealsodependedheavilyonadvancesinsoftware\ninfrastructure.SoftwarelibrariessuchasTheano( ,; Bergstra e t a l .2010Bastien\ne t a l . e t a l . ,),PyLearn2( 2012 Goodfellow,),Torch( ,), 2013c Collobert e t a l .2011b\nDistBelief(,),Caﬀe(,),MXNet(,),and Dean e t a l .2012 Jia2013 Chen e t a l .2015\nTensorFlow(,)haveallsupportedimportantresearchprojectsor Abadi e t a l .2015\ncommercialproducts.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 81, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 415}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0082_3064d082", "text": "Deeplearninghasalsomadecontributionsbacktoothersciences.Modern\nconvolutionalnetworksforobjectrecognitionprovideamodelofvisualprocessing\n2 5\nCHAPTER1.INTRODUCTION\nthatneuroscientistscanstudy(,).Deeplearningalsoprovidesuseful DiCarlo2013\ntoolsforprocessingmassiveamountsofdataandmakingusefulpredictionsin\nscientiﬁcﬁelds.Ithasbeensuccessfullyusedtopredicthowmoleculeswillinteract\ninordertohelppharmaceutical companiesdesignnewdrugs(,), Dahl e t a l .2014\ntosearchforsubatomicparticles(,),andtoautomatically parse Baldi e t a l .2014\nmicroscopeimagesusedtoconstructa3-Dmapofthehumanbrain(Knowles-\nBarley2014 e t a l .,).Weexpectdeeplearningtoappearinmoreandmorescientiﬁc\nﬁeldsinthefuture.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 82, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 684}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0083_9b1181f6", "text": "Insummary,deeplearningisanapproachtomachinelearningthathasdrawn\nheavilyonourknowledgeofthehumanbrain,statisticsandappliedmathasit\ndevelopedoverthepastseveraldecades.Inrecentyears,ithasseentremendous\ngrowthinitspopularityandusefulness,dueinlargeparttomorepowerfulcom-\nputers,largerdatasetsandtechniquestotraindeepernetworks.Theyearsahead\narefullofchallengesandopportunitiestoimprovedeeplearningevenfurtherand\nbringittonewfrontiers. 2 6\nCHAPTER1.INTRODUCTION\n1950 198520002015 2056\nYear10− 210− 1100101102103104105106107108109101 0101 1Numberofneurons(logarithmicscale)\n123\n456\n78\n91011\n121314\n151617\n181920\nSpongeRoundwormLeechAntBeeFrogOctopusHuman\nFigure1.11:Sincetheintroductionofhiddenunits,artiﬁcialneuralnetworkshavedoubled\ninsizeroughlyevery2.4years.Biologicalneuralnetworksizesfrom ().", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 83, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 792}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0084_7a87edbd", "text": "Wikipedia2015\n1.Perceptron(,,) Rosenblatt19581962\n2.Adaptivelinearelement( ,) WidrowandHoﬀ1960\n3.Neocognitron(Fukushima1980,)\n4.Earlyback-propagationnetwork( ,) Rumelhart e t al.1986b\n5.Recurrentneuralnetworkforspeechrecognition(RobinsonandFallside1991,)\n6.Multilayerperceptronforspeechrecognition( ,) Bengio e t al.1991\n7.Meanﬁeldsigmoidbeliefnetwork(,) Saul e t al.1996\n8.LeNet-5( ,) LeCun e t al.1998b\n9.Echostatenetwork( ,) JaegerandHaas2004\n10.Deepbeliefnetwork( ,) Hinton e t al.2006\n11.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al.2006\n12.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)\n13.GPU-accelerateddeepbeliefnetwork(,) Raina e t al.2009\n14.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al.2009\n15.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al.2010\n16.OMP-1network( ,) CoatesandNg2011\n17.Distributedautoencoder(,) Le e t al.2012\n18.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al.2012\n19.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al.2013\n20.GoogLeNet( ,) Szegedy e t al.2014a\n2 7\nCHAPTER1.INTRODUCTION\n2010 2011 2012 2013 2014 2015\nYear000 .005 .010 .015 .020 .025 .030 .ILSVRC classiﬁcationerrorrate\nFigure1.12:SincedeepnetworksreachedthescalenecessarytocompeteintheImageNet\nLargeScaleVisualRecognitionChallenge,theyhaveconsistentlywonthecompetition\neveryyear,andyieldedlowerandlowererrorrateseachtime.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 84, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1362}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0085_e2154721", "text": "DatafromRussakovsky\ne t a l . e t a l . ()and2014b He().2015\n2 8\nP a rt I\nAppliedMathandMachine\nLearningBasics\n29\nThis part of t he b o ok in t r o duces t he bas ic mathematical c oncepts needed t o\nunders t an d deep learning. W e b e gin with general ideas f r om applied math t hat\nallo w us t o deﬁne f unctions of many v ariables , ﬁ nd t he highes t and low e s t p oints\non t hes e f unctions and q uantify degrees of b e lief. N e x t , w e des c r ib e t he f undamen t al goals of machine learning. W e des c r ibe how\nt o accomplis h t hes e goals b y s p e c ifying a mo del t hat r e pres e n t s c e r t ain b e liefs ,\ndes igning a c os t f unction t hat meas ures how well t hos e beliefs c orres p ond with\nr e alit y and us ing a t r aining algorithm t o minimize t hat c os t f unction. This e lementary f r amew ork is t he bas is f or a broad v ariety of mac hine learning\nalgorithms , including approac hes t o machine learning t hat are not deep.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 85, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 970}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0086_36f771b7", "text": "This e lementary f r amew ork is t he bas is f or a broad v ariety of mac hine learning\nalgorithms , including approac hes t o machine learning t hat are not deep. In t he\ns ubs e q uen t parts of t he bo ok, we develop deep learning algorithms within t his\nf r amew ork. 3 0\nC h a p t e r 2\nL i n e ar A l ge b ra\nLinearalgebraisabranchofmathematics thatiswidelyusedthroughoutscience\nandengineering.However,becauselinearalgebraisaformofcontinuousrather\nthandiscretemathematics,manycomputerscientistshavelittleexperiencewithit. Agoodunderstandingoflinearalgebraisessentialforunderstandingandworking\nwithmanymachinelearningalgorithms,especiallydeeplearningalgorithms.We\nthereforeprecedeourintroductiontodeeplearningwithafocusedpresentationof\nthekeylinearalgebraprerequisites.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 86, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 774}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0087_9b6b2f2d", "text": "Ifyouarealreadyfamiliarwithlinearalgebra,feelfreetoskipthischapter.If\nyouhavepreviousexperiencewiththeseconceptsbutneedadetailedreference\nsheettoreviewkeyformulas,werecommend TheMatrixCookbook(Petersenand\nPedersen2006,).Ifyouhavenoexposureatalltolinearalgebra,thischapter\nwillteachyouenoughtoreadthisbook,butwehighlyrecommendthatyoualso\nconsultanotherresourcefocusedexclusivelyonteachinglinearalgebra,suchas\nShilov1977().Thischapterwillcompletelyomitmanyimportantlinearalgebra\ntopicsthatarenotessentialforunderstandingdeeplearning. 2.1Scalars,Vectors,MatricesandTensors\nThestudyoflinearalgebrainvolvesseveraltypesofmathematical objects:\n•Scalars:Ascalarisjustasinglenumber,incontrasttomostoftheother\nobjectsstudiedinlinearalgebra,whichareusuallyarraysofmultiplenumbers. Wewritescalarsinitalics.Weusuallygivescalarslower-casevariablenames.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 87, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 838}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0088_0c4bf93c", "text": "Wewritescalarsinitalics.Weusuallygivescalarslower-casevariablenames. Whenweintroducethem,wespecifywhatkindofnumbertheyare.For\n31\nCHAPTER2.LINEARALGEBRA\nexample,wemightsay“Let s∈ Rbetheslopeoftheline,”whiledeﬁninga\nreal-valuedscalar,or“Let n∈ Nbethenumberofunits,”whiledeﬁninga\nnaturalnumberscalar. •Vectors: Avectorisanarrayofnumbers.Thenumbersarearrangedin\norder.Wecanidentifyeachindividualnumberbyitsindexinthatordering. Typicallywegivevectorslowercasenameswritteninboldtypeface,such\nasx.Theelementsofthevectorareidentiﬁedbywritingitsnameinitalic\ntypeface,withasubscript.Theﬁrstelementofxis x 1,thesecondelement\nis x 2andsoon.Wealsoneedtosaywhatkindofnumbersarestoredin\nthevector.Ifeachelementisin R,andthevectorhas nelements,thenthe\nvectorliesinthesetformedbytakingtheCartesianproductof R ntimes,\ndenotedas Rn.Whenweneedtoexplicitlyidentifytheelementsofavector,\nwewritethemasacolumnenclosedinsquarebrackets:\nx=\nx 1\nx 2\n... x n\n.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 88, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 941}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0089_f42e197c", "text": "x n\n. (2.1)\nWecanthinkofvectorsasidentifyingpointsinspace,witheachelement\ngivingthecoordinatealongadiﬀerentaxis. Sometimesweneedtoindexasetofelementsofavector.Inthiscase,we\ndeﬁneasetcontainingtheindicesandwritethesetasasubscript.For\nexample,toaccess x 1, x 3and x 6,wedeﬁnetheset S={1 ,3 ,6}andwrite\nx S.Weusethe−signtoindexthecomplementofaset.Forexamplex − 1is\nthevectorcontainingallelementsofxexceptfor x 1,andx − Sisthevector\ncontainingalloftheelementsofexceptforx x 1, x 3and x 6. •Matrices:Amatrixisa2-Darrayofnumbers,soeachelementisidentiﬁed\nbytwoindicesinsteadofjustone.Weusuallygivematricesupper-case\nvariablenameswithboldtypeface,suchasA.Ifareal-valuedmatrixAhas\naheightof mandawidthof n,thenwesaythatA∈ Rm n ×.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 89, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 725}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0090_7419e844", "text": "Weusually\nidentifytheelementsofamatrixusingitsnameinitalicbutnotboldfont,\nandtheindicesarelistedwithseparatingcommas.Forexample, A 1 1 ,isthe\nupperleftentryofAand A m , nisthebottomrightentry.Wecanidentifyall\nofthenumberswithverticalcoordinate ibywritinga“”forthehorizontal :\ncoordinate.Forexample,A i , :denotesthehorizontalcrosssectionofAwith\nverticalcoordinate i.Thisisknownasthe i-throwofA.Likewise,A : , iis\n3 2\nCHAPTER2.LINEARALGEBRA\nA =\nA 1 1 , A 1 2 ,\nA 2 1 , A 2 2 ,\nA 3 1 , A 3 2 ,\n ⇒ A=A 1 1 , A 2 1 , A 3 1 ,\nA 1 2 , A 2 2 , A 3 2 ,\nFigure2.1:Thetransposeofthematrixcanbethoughtofasamirrorimageacrossthe\nmaindiagonal. the-thof.Whenweneedtoexplicitlyidentifytheelementsof icolumnA\namatrix,wewritethemasanarrayenclosedinsquarebrackets:\nA 1 1 , A 1 2 ,\nA 2 1 , A 2 2 ,\n.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 90, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 788}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0091_0a257983", "text": "the-thof.Whenweneedtoexplicitlyidentifytheelementsof icolumnA\namatrix,wewritethemasanarrayenclosedinsquarebrackets:\nA 1 1 , A 1 2 ,\nA 2 1 , A 2 2 ,\n. (2.2)\nSometimeswemayneedtoindexmatrix-valuedexpressionsthatarenotjust\nasingleletter.Inthiscase,weusesubscriptsaftertheexpression,butdo\nnotconvertanythingtolowercase.Forexample, f(A) i , jgiveselement( i , j)\nofthematrixcomputedbyapplyingthefunctionto. fA\n•Tensors:Insomecaseswewillneedanarraywithmorethantwoaxes. Inthegeneralcase,anarrayofnumbersarrangedonaregulargridwitha\nvariablenumberofaxesisknownasatensor.Wedenoteatensornamed“A”\nwiththistypeface: A.Weidentifytheelementof Aatcoordinates ( i , j , k)\nbywriting A i , j , k. Oneimportantoperationonmatricesisthetranspose.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 91, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 727}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0092_cd141499", "text": "Oneimportantoperationonmatricesisthetranspose. Thetransposeofa\nmatrixisthemirrorimageofthematrixacrossadiagonalline,calledthemain\ndiagonal,runningdownandtotheright,startingfromitsupperleftcorner.See\nﬁgureforagraphicaldepictionofthisoperation.Wedenotethetransposeofa 2.1\nmatrixasAA,anditisdeﬁnedsuchthat\n(A) i , j= A j , i . (2.3)\nVectorscanbethoughtofasmatricesthatcontainonlyonecolumn.The\ntransposeofavectoristhereforeamatrixwithonlyonerow.Sometimeswe\n3 3\nCHAPTER2.LINEARALGEBRA\ndeﬁneavectorbywritingoutitselementsinthetextinlineasarowmatrix,\nthenusingthetransposeoperatortoturnitintoastandardcolumnvector,e.g.,\nx= [ x 1 , x 2 , x 3]. Ascalarcanbethoughtofasamatrixwithonlyasingleentry.Fromthis,we\ncanseethatascalarisitsowntranspose: a a= . Wecanaddmatricestoeachother,aslongastheyhavethesameshape,just\nbyaddingtheircorrespondingelements: whereCAB = + C i , j= A i , j+ B i , j .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 92, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 884}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0093_7f27bd51", "text": "Wecanaddmatricestoeachother,aslongastheyhavethesameshape,just\nbyaddingtheircorrespondingelements: whereCAB = + C i , j= A i , j+ B i , j . Wecanalsoaddascalartoamatrixormultiplyamatrixbyascalar,just\nbyperformingthatoperationoneachelementofamatrix:D= a·B+ cwhere\nD i , j= a B· i , j+ c. Inthecontextofdeeplearning,wealsousesomelessconventionalnotation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 93, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 351}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0094_7c82dabc", "text": "Inthecontextofdeeplearning,wealsousesomelessconventionalnotation. Weallowtheadditionofmatrixandavector,yieldinganothermatrix:C=A+b,\nwhere C i , j= A i , j+ b j.Inotherwords,thevectorbisaddedtoeachrowofthe\nmatrix.Thisshorthandeliminatestheneedtodeﬁneamatrixwithbcopiedinto\neachrowbeforedoingtheaddition.Thisimplicitcopyingofbtomanylocations\niscalled .broadcasting\n2.2MultiplyingMatricesandVectors\nOneofthemostimportantoperationsinvolvingmatricesismultiplication oftwo\nmatrices.ThematrixproductofmatricesAandBisathirdmatrixC.In\norderforthisproducttobedeﬁned,Amusthavethesamenumberofcolumnsas\nBhasrows.IfAisofshape m n×andBisofshape n p×,thenCisofshape\nm p×.Wecanwritethematrixproductjustbyplacingtwoormorematrices\ntogether,e.g. CAB= . (2.4)\nTheproductoperationisdeﬁnedby\nC i , j=\nkA i , k B k, j .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 94, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 796}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0095_91daf566", "text": "CAB= . (2.4)\nTheproductoperationisdeﬁnedby\nC i , j=\nkA i , k B k, j . (2.5)\nNotethatthestandardproductoftwomatricesisjustamatrixcontaining not\ntheproductoftheindividualelements.Suchanoperationexistsandiscalledthe\nelement-wiseproductHadamardproduct or ,andisdenotedas.AB\nThedotproductbetweentwovectorsxandyofthesamedimensionality\nisthematrixproductxy.WecanthinkofthematrixproductC=ABas\ncomputing C i , jasthedotproductbetweenrowofandcolumnof. iA jB\n3 4\nCHAPTER2.LINEARALGEBRA\nMatrixproductoperationshavemanyusefulpropertiesthatmakemathematical\nanalysis ofmatrices moreconvenient.For example, matrix m ultiplication is\ndistributive:\nABCABAC (+) = + . (2.6)\nItisalsoassociative:\nABCABC ( ) = ( ) . (2.7)\nMatrixmultiplication iscommutative(thecondition not AB=BAdoesnot\nalwayshold),unlikescalarmultiplication. However,thedotproductbetweentwo\nvectorsiscommutative:\nxyy= x . (2.8)\nThetransposeofamatrixproducthasasimpleform:\n( )AB= BA.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 95, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 937}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0096_908116db", "text": "However,thedotproductbetweentwo\nvectorsiscommutative:\nxyy= x . (2.8)\nThetransposeofamatrixproducthasasimpleform:\n( )AB= BA. (2.9)\nThisallowsustodemonstrateequation,byexploitingthefactthatthevalue 2.8\nofsuchaproductisascalarandthereforeequaltoitsowntranspose:\nxy=\nxy\n= yx . (2.10)\nSincethefocusofthistextbookisnotlinearalgebra,wedonotattemptto\ndevelopacomprehensivelistofusefulpropertiesofthematrixproducthere,but\nthereadershouldbeawarethatmanymoreexist. Wenowknowenoughlinearalgebranotationtowritedownasystemoflinear\nequations:\nAxb= (2.11)\nwhereA∈ Rm n ×isaknownmatrix,b∈ Rmisaknownvector,andx∈ Rnisa\nvectorofunknownvariableswewouldliketosolvefor.Eachelement x iofxisone\noftheseunknownvariables.EachrowofAandeachelementofbprovideanother\nconstraint.Wecanrewriteequationas:2.11\nA 1 : ,x= b 1 (2.12)\nA 2 : ,x= b 2 (2.13)\n. . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 96, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 834}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0097_e0171298", "text": ". . (2.14)\nA m , :x= b m (2.15)\nor,evenmoreexplicitly,as:\nA 1 1 , x 1+A 1 2 , x 2+ +···A 1 , n x n= b 1 (2.16)\n3 5\nCHAPTER2.LINEARALGEBRA\n\n100\n010\n001\n\nFigure2.2:Exampleidentitymatrix:ThisisI 3. A 2 1 , x 1+A 2 2 , x 2+ +···A 2 , n x n= b 2 (2.17)\n. . . (2.18)\nA m , 1 x 1+A m , 2 x 2+ +···A m , n x n= b m . (2.19)\nMatrix-vectorproductnotationprovidesamorecompactrepresentationfor\nequationsofthisform. 2.3IdentityandInverseMatrices\nLinearalgebraoﬀersapowerfultoolcalledmatrixinversionthatallowsusto\nanalyticallysolveequationformanyvaluesof. 2.11 A\nTodescribematrixinversion,weﬁrstneedtodeﬁnetheconceptofanidentity\nmatrix.Anidentitymatrixisamatrixthatdoesnotchangeanyvectorwhenwe\nmultiplythatvectorbythatmatrix.Wedenotetheidentitymatrixthatpreserves\nn-dimensionalvectorsasI n.Formally,I n∈ Rn n ×,and\n∀∈x Rn,I nxx= .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 97, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 820}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0098_43a2b97c", "text": "(2.20)\nThestructureoftheidentitymatrixissimple:alloftheentriesalongthemain\ndiagonalare1,whilealloftheotherentriesarezero.Seeﬁgureforanexample.2.2\nThematrixinverseofAisdenotedasA− 1,anditisdeﬁnedasthematrix\nsuchthat\nA− 1AI= n . (2.21)\nWecannowsolveequationbythefollowingsteps: 2.11\nAxb= (2.22)\nA− 1AxA= − 1b (2.23)\nI nxA= − 1b (2.24)\n3 6\nCHAPTER2.LINEARALGEBRA\nxA= − 1b . (2.25)\nOfcourse,thisprocessdependsonitbeingpossibletoﬁndA− 1.Wediscuss\ntheconditionsfortheexistenceofA− 1inthefollowingsection. WhenA− 1exists,severaldiﬀerentalgorithmsexistforﬁndingitinclosedform. Intheory,thesameinversematrixcanthenbeusedtosolvetheequationmany\ntimesfordiﬀerentvaluesofb.However,A− 1isprimarilyusefulasatheoretical\ntool,andshouldnotactuallybeusedinpracticeformostsoftwareapplications.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 98, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 773}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0099_38c8d39f", "text": "BecauseA− 1canberepresentedwithonlylimitedprecisiononadigitalcomputer,\nalgorithmsthatmakeuseofthevalueofbcanusuallyobtainmoreaccurate\nestimatesof.x\n2.4LinearDependenceandSpan\nInorderforA− 1toexist,equationmusthaveexactlyonesolutionforevery 2.11\nvalueofb.However,itisalsopossibleforthesystemofequationstohaveno\nsolutionsorinﬁnitelymanysolutionsforsomevaluesofb. Itisnotpossibleto\nhavemorethanonebutlessthaninﬁnitelymanysolutionsforaparticularb;if\nbothandaresolutionsthen xy\nzxy = α+(1 )− α (2.26)\nisalsoasolutionforanyreal. α\nToanalyzehowmanysolutionstheequationhas,wecanthinkofthecolumns\nofAasspecifyingdiﬀerentdirectionswecantravelfromtheorigin(thepoint\nspeciﬁedbythevectorofallzeros),anddeterminehowmanywaysthereareof\nreachingb.Inthisview,eachelementofxspeciﬁeshowfarweshouldtravelin\neachofthesedirections,with x ispecifyinghowfartomoveinthedirectionof\ncolumn: i\nAx=\nix iA : , i . (2.27)\nIngeneral,thiskindofoperationiscalledalinearcombination.Formally,a\nlinearcombinationofsomesetofvectors{v( 1 ), .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 99, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1003}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0100_f0dba8fd", "text": "(2.27)\nIngeneral,thiskindofoperationiscalledalinearcombination.Formally,a\nlinearcombinationofsomesetofvectors{v( 1 ), . . . ,v( ) n}isgivenbymultiplying\neachvectorv( ) ibyacorrespondingscalarcoeﬃcientandaddingtheresults:\n\nic iv( ) i. (2.28)\nThespanofasetofvectorsisthesetofallpointsobtainablebylinearcombination\noftheoriginalvectors. 3 7\nCHAPTER2.LINEARALGEBRA\nDeterminingwhetherAx=bhasasolutionthusamountstotestingwhetherb\nisinthespanofthecolumnsofA.Thisparticularspanisknownasthecolumn\nspacerangeortheof.A\nInorderforthesystemAx=btohaveasolutionforallvaluesofb∈ Rm,\nwethereforerequirethatthecolumnspaceofAbeallof Rm.Ifanypointin Rm\nisexcludedfromthecolumnspace,thatpointisapotentialvalueofbthathas\nnosolution.TherequirementthatthecolumnspaceofAbeallof Rmimplies\nimmediately thatAmusthaveatleast mcolumns,i.e., n m≥.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 100, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 817}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0101_4f4f419f", "text": "Otherwise, the\ndimensionalityofthecolumnspacewouldbelessthan m.Forexample,considera\n3×2matrix.Thetargetbis3-D,butxisonly2-D,somodifyingthevalueofx\natbestallowsustotraceouta2-Dplanewithin R3.Theequationhasasolution\nifandonlyifliesonthatplane.b\nHaving n m≥isonlyanecessaryconditionforeverypointtohaveasolution. Itisnotasuﬃcientcondition,becauseitispossibleforsomeofthecolumnsto\nberedundant.Considera2×2matrixwherebothofthecolumnsareidentical. Thishasthesamecolumnspaceasa2×1matrixcontainingonlyonecopyofthe\nreplicatedcolumn.Inotherwords,thecolumnspaceisstilljustaline,andfailsto\nencompassallof R2,eventhoughtherearetwocolumns. Formally,thiskindofredundancyisknownaslineardependence.Asetof\nvectorsislinearlyindependentifnovectorinthesetisalinearcombination\noftheothervectors.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 101, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 772}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0102_6e0faee0", "text": "Formally,thiskindofredundancyisknownaslineardependence.Asetof\nvectorsislinearlyindependentifnovectorinthesetisalinearcombination\noftheothervectors. Ifweaddavectortoasetthatisalinearcombinationof\ntheothervectorsintheset,thenewvectordoesnotaddanypointstotheset’s\nspan.Thismeansthatforthecolumnspaceofthematrixtoencompassallof Rm,\nthematrixmustcontainatleastonesetof mlinearlyindependentcolumns.This\nconditionisbothnecessaryandsuﬃcientforequationtohaveasolutionfor 2.11\neveryvalueofb.Notethattherequirementisforasettohaveexactly mlinear\nindependentcolumns,notatleast m.Nosetof m-dimensionalvectorscanhave\nmorethan mmutuallylinearlyindependentcolumns,butamatrixwithmorethan\nmcolumnsmayhavemorethanonesuchset. Inorderforthematrixtohaveaninverse,weadditionallyneedtoensurethat\nequationhasonesolutionforeachvalueof 2.11 atmost b.Todoso,weneedto\nensurethatthematrixhasatmost mcolumns.Otherwisethereismorethanone\nwayofparametrizing eachsolution.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 102, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 936}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0103_371b9270", "text": "Together,thismeansthatthematrixmustbesquare,thatis,werequirethat\nm= nandthatallofthecolumnsmustbelinearlyindependent.Asquarematrix\nwithlinearlydependentcolumnsisknownas.singular\nIfAisnotsquareorissquarebutsingular,itcanstillbepossibletosolvethe\nequation.However,wecannotusethemethodofmatrixinversiontoﬁndthe\n3 8\nCHAPTER2.LINEARALGEBRA\nsolution. Sofarwehavediscussedmatrixinversesasbeingmultipliedontheleft.Itis\nalsopossibletodeﬁneaninversethatismultipliedontheright:\nAA− 1= I . (2.29)\nForsquarematrices,theleftinverseandrightinverseareequal. 2.5Norms\nSometimesweneedtomeasurethesizeofavector.Inmachinelearning,weusually\nmeasurethesizeofvectorsusingafunctioncalledanorm.Formally,the Lpnorm\nisgivenby\n||||x p=\ni| x i|p 1\np\n(2.30)\nfor p , p .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 103, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 742}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0104_cb9ee84c", "text": "2.5Norms\nSometimesweneedtomeasurethesizeofavector.Inmachinelearning,weusually\nmeasurethesizeofvectorsusingafunctioncalledanorm.Formally,the Lpnorm\nisgivenby\n||||x p=\ni| x i|p 1\np\n(2.30)\nfor p , p . ∈ R≥1\nNorms,includingthe Lpnorm,arefunctionsmappingvectorstonon-negative\nvalues.Onanintuitivelevel,thenormofavectorxmeasuresthedistancefrom\ntheorigintothepointx.Morerigorously,anormisanyfunction fthatsatisﬁes\nthefollowingproperties:\n• ⇒ f() = 0 xx= 0\n• ≤ f(+) xy f f ()+x ()y(thetriangleinequality)\n•∀∈ || α R , f α(x) = α f()x\nThe L2norm,with p= 2,isknownastheEuclideannorm.Itissimplythe\nEuclideandistancefromtheorigintothepointidentiﬁedbyx.The L2normis\nusedsofrequentlyinmachinelearningthatitisoftendenotedsimplyas||||x,with\nthesubscriptomitted.Itisalsocommontomeasurethesizeofavectorusing 2\nthesquared L2norm,whichcanbecalculatedsimplyasxx.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 104, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 845}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0105_b46d5149", "text": "Thesquared L2normismoreconvenienttoworkwithmathematically and\ncomputationally thanthe L2normitself.Forexample,thederivativesofthe\nsquared L2normwithrespecttoeachelementofxeachdependonlyonthe\ncorrespondingelementofx,whileallofthederivativesofthe L2normdepend\nontheentirevector.Inmanycontexts,thesquared L2normmaybeundesirable\nbecauseitincreasesveryslowlyneartheorigin.Inseveralmachinelearning\n3 9\nCHAPTER2.LINEARALGEBRA\napplications,itisimportanttodiscriminatebetweenelementsthatareexactly\nzeroandelementsthataresmallbutnonzero.Inthesecases,weturntoafunction\nthatgrowsatthesamerateinalllocations,butretainsmathematical simplicity:\nthe L1norm.The L1normmaybesimpliﬁedto\n||||x 1=\ni| x i| . (2.31)\nThe L1normiscommonlyusedinmachinelearningwhenthediﬀerencebetween\nzeroandnonzeroelementsisveryimportant.Everytimeanelementofxmoves\nawayfrom0by,the  L1normincreasesby.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 105, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 861}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0106_fcf8511f", "text": "(2.31)\nThe L1normiscommonlyusedinmachinelearningwhenthediﬀerencebetween\nzeroandnonzeroelementsisveryimportant.Everytimeanelementofxmoves\nawayfrom0by,the  L1normincreasesby. \nWesometimesmeasurethesizeofthevectorbycountingitsnumberofnonzero\nelements.Someauthorsrefertothisfunctionasthe“ L0norm,”butthisisincorrect\nterminology.Thenumberofnon-zeroentriesinavectorisnotanorm,because\nscalingthevectorby αdoesnotchangethenumberofnonzeroentries. The L1\nnormisoftenusedasasubstituteforthenumberofnonzeroentries. Oneothernormthatcommonlyarisesinmachinelearningisthe L∞norm,\nalsoknownasthemaxnorm.Thisnormsimpliﬁestotheabsolutevalueofthe\nelementwiththelargestmagnitudeinthevector,\n||||x ∞= max\ni| x i| . (2.32)\nSometimeswemayalsowishtomeasurethesizeofamatrix.Inthecontext\nofdeeplearning,themostcommonwaytodothisiswiththeotherwiseobscure\nFrobeniusnorm:\n|||| A F=\ni , jA2\ni , j , (2.33)\nwhichisanalogoustothe L2normofavector.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 106, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 916}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0107_9ae1c252", "text": "Thedotproductoftwovectorscanberewrittenintermsofnorms.Speciﬁcally,\nxyx= |||| 2||||y 2cos θ (2.34)\nwhereistheanglebetweenand. θ xy\n2.6SpecialKindsofMatricesandVectors\nSomespecialkindsofmatricesandvectorsareparticularlyuseful. Diagonalmatricesconsistmostlyofzerosandhavenon-zeroentriesonlyalong\nthemaindiagonal. Formally,amatrixDisdiagonalifandonlyif D i , j=0for\n4 0\nCHAPTER2.LINEARALGEBRA\nall i= j. Wehavealreadyseenoneexampleofadiagonalmatrix: theidentity\nmatrix,whereallofthediagonalentriesare1.Wewritediag(v) todenoteasquare\ndiagonalmatrixwhosediagonalentriesaregivenbytheentriesofthevectorv. Diagonalmatricesareofinterestinpartbecausemultiplyingbyadiagonalmatrix\nisverycomputationally eﬃcient.Tocomputediag(v)x,weonlyneedtoscaleeach\nelement x iby v i.Inotherwords,diag(v)x=vx.Invertingasquarediagonal\nmatrixisalsoeﬃcient.Theinverseexistsonlyifeverydiagonalentryisnonzero,\nandinthatcase,diag(v)− 1=diag([1 /v 1 , . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 107, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 924}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0108_5b9af326", "text": ". . ,1 /v n]).Inmanycases,wemay\nderivesomeverygeneralmachinelearningalgorithmintermsofarbitrarymatrices,\nbutobtainalessexpensive(andlessdescriptive)algorithmbyrestrictingsome\nmatricestobediagonal. Notalldiagonalmatricesneedbesquare.Itispossibletoconstructarectangular\ndiagonalmatrix.Non-squarediagonalmatricesdonothaveinversesbutitisstill\npossibletomultiplybythemcheaply.Foranon-squarediagonalmatrixD,the\nproductDxwillinvolvescalingeachelementofx,andeitherconcatenating some\nzerostotheresultifDistallerthanitiswide,ordiscardingsomeofthelast\nelementsofthevectorifiswiderthanitistall. D\nA matrixisanymatrixthatisequaltoitsowntranspose: symmetric\nAA= . (2.35)\nSymmetricmatricesoftenarisewhentheentriesaregeneratedbysomefunctionof\ntwoargumentsthatdoesnotdependontheorderofthearguments.Forexample,\nifAisamatrixofdistancemeasurements,withA i , jgivingthedistancefrompoint\nitopoint,then jA i , j= A j , ibecausedistancefunctionsaresymmetric. A isavectorwith : unitvectorunitnorm\n||||x 2= 1 .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 108, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0109_68ac34d5", "text": "A isavectorwith : unitvectorunitnorm\n||||x 2= 1 . (2.36)\nAvectorxandavectoryareorthogonaltoeachotherifxy= 0.Ifboth\nvectorshavenonzeronorm,thismeansthattheyareata90degreeangletoeach\nother.In Rn,atmost nvectorsmaybemutuallyorthogonalwithnonzeronorm. Ifthevectorsarenotonlyorthogonalbutalsohaveunitnorm,wecallthem\northonormal. Anorthogonalmatrixisasquarematrixwhoserowsaremutuallyorthonor-\nmalandwhosecolumnsaremutuallyorthonormal:\nAAAA= = I . (2.37)\n4 1\nCHAPTER2.LINEARALGEBRA\nThisimpliesthat\nA− 1= A, (2.38)\nsoorthogonalmatricesareofinterestbecausetheirinverseisverycheaptocompute. Paycarefulattentiontothedeﬁnitionoforthogonalmatrices.Counterintuitively,\ntheirrowsarenotmerelyorthogonalbutfullyorthonormal. Thereisnospecial\ntermforamatrixwhoserowsorcolumnsareorthogonalbutnotorthonormal. 2.7Eigendecomposition\nManymathematical objectscanbeunderstoodbetterbybreakingtheminto\nconstituentparts,orﬁndingsomepropertiesofthemthatareuniversal,notcaused\nbythewaywechoosetorepresentthem.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 109, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 982}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0110_665da221", "text": "2.7Eigendecomposition\nManymathematical objectscanbeunderstoodbetterbybreakingtheminto\nconstituentparts,orﬁndingsomepropertiesofthemthatareuniversal,notcaused\nbythewaywechoosetorepresentthem. Forexample,integerscanbedecomposedintoprimefactors.Thewaywe\nrepresentthenumberwillchangedependingonwhetherwewriteitinbaseten 12\norinbinary,butitwillalwaysbetruethat12 = 2×2×3.Fromthisrepresentation\nwecanconcludeusefulproperties,suchasthatisnotdivisibleby,orthatany 12 5\nintegermultipleofwillbedivisibleby. 12 3\nMuchaswecandiscoversomethingaboutthetruenatureofanintegerby\ndecomposingitintoprimefactors,wecanalsodecomposematricesinwaysthat\nshowusinformationabouttheirfunctionalpropertiesthatisnotobviousfromthe\nrepresentationofthematrixasanarrayofelements. Oneofthemostwidelyusedkindsofmatrixdecompositioniscalledeigen-\ndecomposition,inwhichwedecomposeamatrixintoasetofeigenvectorsand\neigenvalues. AneigenvectorofasquarematrixAisanon-zerovectorvsuchthatmulti-\nplicationbyaltersonlythescaleof: A v\nAvv= λ .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 110, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 994}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0111_1d2f2dd5", "text": "AneigenvectorofasquarematrixAisanon-zerovectorvsuchthatmulti-\nplicationbyaltersonlythescaleof: A v\nAvv= λ . (2.39)\nThescalar λisknownastheeigenvaluecorrespondingtothiseigenvector.(One\ncanalsoﬁndalefteigenvectorsuchthatvA= λv, butweareusually\nconcernedwithrighteigenvectors). IfvisaneigenvectorofA,thensoisanyrescaledvector svfor s , s ∈ R= 0. Moreover, svstillhasthesameeigenvalue.Forthisreason,weusuallyonlylook\nforuniteigenvectors. SupposethatamatrixAhas nlinearlyindependenteigenvectors,{v( 1 ), . . . ,\nv( ) n},withcorrespondingeigenvalues { λ 1 , . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 111, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 560}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0112_4ac0147a", "text": "SupposethatamatrixAhas nlinearlyindependenteigenvectors,{v( 1 ), . . . ,\nv( ) n},withcorrespondingeigenvalues { λ 1 , . . . , λ n}.Wemayconcatenateallofthe\n4 2\nCHAPTER2.LINEARALGEBRA\n\u0000 \u0000 \u0000    \n\u0000\u0000\u0000 \n                     \n\u0000 \u0000 \u0000    \n\n\u0000\u0000\u0000\n   \n                                                     \nFigure2.3:Anexampleoftheeﬀectofeigenvectorsandeigenvalues.Here,wehave\namatrixAwithtwoorthonormaleigenvectors,v( 1 )witheigenvalue λ 1andv( 2 )with\neigenvalue λ 2. ( L e f t )Weplotthesetofallunitvectorsu∈ R2asaunitcircle. ( R i g h t )We\nplotthesetofallpointsAu.ByobservingthewaythatAdistortstheunitcircle,we\ncanseethatitscalesspaceindirectionv( ) iby λ i. eigenvectorstoformamatrixVwithoneeigenvectorpercolumn:V= [v( 1 ), . . . ,\nv( ) n].Likewise,wecanconcatenatetheeigenvaluestoformavectorλ= [ λ 1 , . . . ,\nλ n].The ofisthengivenby eigendecompositionA\nAVλV = diag()− 1.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 112, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1011}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0113_6ace75db", "text": ". . ,\nv( ) n].Likewise,wecanconcatenatetheeigenvaluestoformavectorλ= [ λ 1 , . . . ,\nλ n].The ofisthengivenby eigendecompositionA\nAVλV = diag()− 1. (2.40)\nWehaveseenthatconstructingmatriceswithspeciﬁceigenvaluesandeigenvec-\ntorsallowsustostretchspaceindesireddirections. Ho wever,weoftenwantto\ndecomposematricesintotheireigenvaluesandeigenvectors.Doingsocanhelp\nustoanalyzecertainpropertiesofthematrix,muchasdecomposinganinteger\nintoitsprimefactorscanhelpusunderstandthebehaviorofthatinteger. Noteverymatrixcanbedecomposedintoeigenvaluesandeigenvectors.Insome\n4 3\nCHAPTER2.LINEARALGEBRA\ncases,thedecompositionexists,butmayinvolvecomplexratherthanrealnumbers.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 113, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 659}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0114_34036c82", "text": "Noteverymatrixcanbedecomposedintoeigenvaluesandeigenvectors.Insome\n4 3\nCHAPTER2.LINEARALGEBRA\ncases,thedecompositionexists,butmayinvolvecomplexratherthanrealnumbers. Fortunately,inthisbook,weusuallyneedtodecomposeonlyaspeciﬁcclassof\nmatricesthathaveasimpledecomposition.Speciﬁcally,everyrealsymmetric\nmatrixcanbedecomposedintoanexpressionusingonlyreal-valuedeigenvectors\nandeigenvalues:\nAQQ = Λ, (2.41)\nwhereQisanorthogonalmatrixcomposedofeigenvectorsofA,and Λisa\ndiagonalmatrix.TheeigenvalueΛ i , iisassociatedwiththeeigenvectorincolumn i\nofQ,denotedasQ : , i.BecauseQisanorthogonalmatrix,wecanthinkofAas\nscalingspaceby λ iindirectionv( ) i.Seeﬁgureforanexample.2.3\nWhileanyrealsymmetricmatrixAisguaranteedtohaveaneigendecomposi-\ntion,theeigendecompositionmaynotbeunique.Ifanytwoormoreeigenvectors\nsharethesameeigenvalue,thenanysetoforthogonalvectorslyingintheirspan\narealsoeigenvectorswiththateigenvalue,andwecouldequivalentlychooseaQ\nusingthoseeigenvectorsinstead.Byconvention,weusuallysorttheentriesof Λ\nindescendingorder.Underthisconvention,theeigendecompositionisuniqueonly\nifalloftheeigenvaluesareunique.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 114, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1112}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0115_b4d643b8", "text": "Theeigendecompositionof amatrix tellsus many usefulfactsabout the\nmatrix.Thematrixissingularifandonlyifanyoftheeigenvaluesarezero. Theeigendecomposition ofarealsymmetricmatrixcanalsobeusedtooptimize\nquadraticexpressionsoftheform f(x) =xAxsubjectto||||x 2= 1.Wheneverx\nisequaltoaneigenvectorofA, ftakesonthevalueofthecorrespondingeigenvalue. Themaximumvalueof fwithintheconstraintregionisthemaximumeigenvalue\nanditsminimumvaluewithintheconstraintregionistheminimumeigenvalue. Amatrixwhoseeigenvaluesareallpositiveiscalledpositivedeﬁnite.A\nmatrixwhoseeigenvaluesareallpositiveorzero-valuediscalledpositivesemideﬁ-\nnite.Likewise,ifalleigenvaluesarenegative,thematrixisnegativedeﬁnite,and\nifalleigenvaluesarenegativeorzero-valued,itisnegativesemideﬁnite.Positive\nsemideﬁnitematricesareinterestingbecausetheyguaranteethat∀xx ,Ax≥0. PositivedeﬁnitematricesadditionallyguaranteethatxAxx = 0 ⇒ = 0. 2.8SingularValueDecomposition\nInsection,wesawhowtodecomposeamatrixintoeigenvectorsandeigenvalues.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 115, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 991}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0116_e6b19a86", "text": "PositivedeﬁnitematricesadditionallyguaranteethatxAxx = 0 ⇒ = 0. 2.8SingularValueDecomposition\nInsection,wesawhowtodecomposeamatrixintoeigenvectorsandeigenvalues. 2.7\nThesingularvaluedecomposition(SVD)providesanotherwaytofactorize\namatrix,intosingularvectorsandsingularvalues.TheSVDallowsusto\ndiscoversomeofthesamekindofinformationastheeigendecomposition.However,\n4 4\nCHAPTER2.LINEARALGEBRA\ntheSVDismoregenerallyapplicable.Everyrealmatrixhasasingularvalue\ndecomposition,butthesameisnottrueoftheeigenvaluedecomposition.For\nexample,ifamatrixisnotsquare,theeigendecompositionisnotdeﬁned,andwe\nmustuseasingularvaluedecompositioninstead. RecallthattheeigendecompositioninvolvesanalyzingamatrixAtodiscover\namatrixVofeigenvectorsandavectorofeigenvaluesλsuchthatwecanrewrite\nAas\nAVλV = diag()− 1. (2.42)\nThesingularvaluedecompositionissimilar,exceptthistimewewillwriteA\nasaproductofthreematrices:\nAUDV = . (2.43)\nSupposethatAisan m n×matrix.ThenUisdeﬁnedtobean m m×matrix,\nD V tobeanmatrix,and m n× tobeanmatrix.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 116, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1005}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0117_6856c640", "text": "(2.43)\nSupposethatAisan m n×matrix.ThenUisdeﬁnedtobean m m×matrix,\nD V tobeanmatrix,and m n× tobeanmatrix. n n×\nEachofthesematricesisdeﬁnedtohaveaspecialstructure.ThematricesU\nandVarebothdeﬁnedtobeorthogonalmatrices.ThematrixDisdeﬁnedtobe\nadiagonalmatrix.Notethatisnotnecessarilysquare. D\nTheelementsalongthediagonalofDareknownasthesingularvaluesof\nthematrixA.ThecolumnsofUareknownastheleft-singularvectors.The\ncolumnsofareknownasasthe V right-singularvectors. WecanactuallyinterpretthesingularvaluedecompositionofAintermsof\ntheeigendecomposition offunctionsofA.Theleft-singularvectorsofAarethe\neigenvectorsofAA.Theright-singularvectorsofAaretheeigenvectorsofAA. Thenon-zerosingularvaluesofAarethesquarerootsoftheeigenvaluesofAA. ThesameistrueforAA. PerhapsthemostusefulfeatureoftheSVDisthatwecanuseittopartially\ngeneralizematrixinversiontonon-squarematrices,aswewillseeinthenext\nsection.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 117, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 892}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0118_bfb975ee", "text": "ThesameistrueforAA. PerhapsthemostusefulfeatureoftheSVDisthatwecanuseittopartially\ngeneralizematrixinversiontonon-squarematrices,aswewillseeinthenext\nsection. 2.9TheMoore-PenrosePseudoinverse\nMatrixinversionisnotdeﬁnedformatricesthatarenotsquare.Supposewewant\ntomakealeft-inverseofamatrix,sothatwecansolvealinearequation BA\nAxy= (2.44)\n4 5\nCHAPTER2.LINEARALGEBRA\nbyleft-multiplyingeachsidetoobtain\nxBy= . (2.45)\nDependingonthestructureoftheproblem,itmaynotbepossibletodesigna\nuniquemappingfromto.AB\nIfAistallerthanitiswide, thenitispossibleforthisequationtohave\nnosolution.IfAiswiderthanitistall,thentherecouldbemultiplepossible\nsolutions. TheMoore-Penrosepseudoinverseallowsustomakesomeheadwayin\nthesecases.Thepseudoinverseofisdeﬁnedasamatrix A\nA+=lim\nα  0(AAI+ α)− 1A.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 118, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 774}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0119_a911d8af", "text": "TheMoore-Penrosepseudoinverseallowsustomakesomeheadwayin\nthesecases.Thepseudoinverseofisdeﬁnedasamatrix A\nA+=lim\nα  0(AAI+ α)− 1A. (2.46)\nPracticalalgorithmsforcomputingthepseudoinversearenotbasedonthisdeﬁni-\ntion,butrathertheformula\nA+= VD+U, (2.47)\nwhereU,DandVarethesingularvaluedecompositionofA,andthepseudoinverse\nD+ofadiagonalmatrixDisobtainedbytakingthereciprocalofitsnon-zero\nelementsthentakingthetransposeoftheresultingmatrix. WhenAhasmorecolumnsthanrows,thensolvingalinearequationusingthe\npseudoinverseprovidesoneofthemanypossiblesolutions.Speciﬁcally,itprovides\nthesolutionx=A+ywithminimalEuclideannorm ||||x 2amongallpossible\nsolutions. WhenAhasmorerowsthancolumns,itispossiblefortheretobenosolution. Inthiscase,usingthepseudoinversegivesusthexforwhichAxisascloseas\npossibletointermsofEuclideannorm y ||−||Axy 2. 2.10TheTraceOperator\nThetraceoperatorgivesthesumofallofthediagonalentriesofamatrix:\nTr() =A\niA i , i .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 119, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 932}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0120_3db33a7a", "text": "2.10TheTraceOperator\nThetraceoperatorgivesthesumofallofthediagonalentriesofamatrix:\nTr() =A\niA i , i . (2.48)\nThetraceoperatorisusefulforavarietyofreasons.Someoperationsthatare\ndiﬃculttospecifywithoutresortingtosummationnotationcanbespeciﬁedusing\n4 6\nCHAPTER2.LINEARALGEBRA\nmatrixproductsandthetraceoperator.Forexample,thetraceoperatorprovides\nanalternativewayofwritingtheFrobeniusnormofamatrix:\n|||| A F=\nTr(AA) . (2.49)\nWritinganexpressionintermsofthetraceoperatoropensupopportunitiesto\nmanipulatetheexpressionusingmanyusefulidentities. Forexample,thetrace\noperatorisinvarianttothetransposeoperator:\nTr() = Tr(AA) . (2.50)\nThetraceofasquarematrixcomposedofmanyfactorsisalsoinvariantto\nmovingthelastfactorintotheﬁrstposition,iftheshapesofthecorresponding\nmatricesallowtheresultingproducttobedeﬁned:\nTr( ) = Tr( ) = Tr( ) ABCCABBCA (2.51)\normoregenerally,\nTr(n\ni = 1F( ) i) = Tr(F( ) nn − 1\ni = 1F( ) i) .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 120, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 911}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0121_7eeeb52a", "text": "(2.52)\nThisinvariancetocyclicpermutationholdseveniftheresultingproducthasa\ndiﬀerentshape.Forexample,forA∈ Rm n ×andB∈ Rn m ×,wehave\nTr( ) = Tr( )ABBA (2.53)\neventhoughAB∈ Rm m ×andBA∈ Rn n ×. Anotherusefulfacttokeepinmindisthatascalarisitsowntrace: a=Tr( a). 2.11TheDeterminant\nThedeterminant ofa squarematrix, denoted det(A), isa functionmapping\nmatricesto realscalars.Thedeterminant isequal totheproductof allthe\neigenvaluesofthematrix.Theabsolutevalueofthedeterminantcanbethought\nofasameasureofhowmuchmultiplicationbythematrixexpandsorcontracts\nspace.Ifthedeterminantis0,thenspaceiscontractedcompletelyalongatleast\nonedimension,causingittoloseallofitsvolume.Ifthedeterminantis1,then\nthetransformationpreservesvolume. 4 7\nCHAPTER2.LINEARALGEBRA\n2.12Example:PrincipalComponentsAnalysis\nOnesimplemachinelearningalgorithm,principalcomponentsanalysisorPCA\ncanbederivedusingonlyknowledgeofbasiclinearalgebra. Supposewehaveacollectionof mpoints{x( 1 ), . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 121, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 954}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0122_ca113a79", "text": "Supposewehaveacollectionof mpoints{x( 1 ), . . . ,x( ) m}in Rn.Supposewe\nwouldliketoapplylossycompressiontothesepoints.Lossycompressionmeans\nstoringthepointsinawaythatrequireslessmemorybutmaylosesomeprecision. Wewouldliketoloseaslittleprecisionaspossible. Onewaywecanencodethesepointsistorepresentalower-dimensionalversion\nofthem.Foreachpointx( ) i∈ Rnwewillﬁndacorrespondingcodevectorc( ) i∈ Rl. If lissmallerthan n,itwilltakelessmemorytostorethecodepointsthanthe\noriginaldata.Wewillwanttoﬁndsomeencodingfunctionthatproducesthecode\nforaninput, f(x) =c,andadecodingfunctionthatproducesthereconstructed\ninputgivenitscode, .xx ≈ g f(())\nPCAisdeﬁnedbyourchoiceofthedecodingfunction.Speciﬁcally,tomakethe\ndecoderverysimple,wechoosetousematrixmultiplicationtomapthecodeback\ninto Rn.Let,where g() = cDcD∈ Rn l ×isthematrixdeﬁningthedecoding.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 122, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 835}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0123_ddb9d933", "text": "Computingtheoptimalcodeforthisdecodercouldbeadiﬃcultproblem.To\nkeeptheencodingproblemeasy,PCAconstrainsthecolumnsofDtobeorthogonal\ntoeachother.(NotethatDisstillnottechnically“anorthogonalmatrix”unless\nl n= )\nWiththeproblemasdescribedsofar,manysolutionsarepossible,becausewe\ncanincreasethescaleofD : , iifwedecrease c iproportionallyforallpoints.Togive\ntheproblemauniquesolution,weconstrainallofthecolumnsoftohaveunitD\nnorm. Inordertoturnthisbasicideaintoanalgorithmwecanimplement,theﬁrst\nthingweneedtodoisﬁgureouthowtogeneratetheoptimalcodepointc∗for\neachinputpointx.Onewaytodothisistominimizethedistancebetweenthe\ninputpointxanditsreconstruction, g(c∗).Wecanmeasurethisdistanceusinga\nnorm.Intheprincipalcomponentsalgorithm,weusethe L2norm:\nc∗= argmin\nc||− ||x g()c 2 .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 123, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 769}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0124_5a955bbb", "text": "(2.54)\nWecanswitchtothesquared L2norminsteadofthe L2normitself,because\nbothareminimizedbythesamevalueofc.Bothareminimizedbythesame\nvalueofcbecausethe L2normisnon-negative andthesquaringoperationis\n4 8\nCHAPTER2.LINEARALGEBRA\nmonotonically increasingfornon-negative arguments. c∗= argmin\nc||− ||x g()c2\n2 . (2.55)\nThefunctionbeingminimizedsimpliﬁesto\n( ())x− gc( ())x− gc (2.56)\n(bythedeﬁnitionofthe L2norm,equation)2.30\n= xxx−g g ()c−()cxc+( g)g()c (2.57)\n(bythedistributiveproperty)\n= xxx−2g g ()+c ()cg()c (2.58)\n(becausethescalar g()cxisequaltothetransposeofitself).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 124, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 577}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0125_1aef7d0d", "text": "Wecannowchangethefunctionbeingminimizedagain,toomittheﬁrstterm,\nsincethistermdoesnotdependon:c\nc∗= argmin\nc−2xg g ()+c ()cg .()c (2.59)\nTomakefurtherprogress,wemustsubstituteinthedeﬁnitionof: g()c\nc∗= argmin\nc−2xDcc+DDc (2.60)\n= argmin\nc−2xDcc+I lc (2.61)\n(bytheorthogonalityandunitnormconstraintson)D\n= argmin\nc−2xDcc+c (2.62)\nWecansolvethisoptimization problemusingvectorcalculus(seesectionif4.3\nyoudonotknowhowtodothis):\n∇ c(2−xDcc+c) = 0 (2.63)\n−2Dxc+2= 0 (2.64)\ncD= x . (2.65)\n4 9\nCHAPTER2.LINEARALGEBRA\nThismakesthealgorithmeﬃcient: wecanoptimallyencodexjustusinga\nmatrix-vectoroperation.Toencodeavector,weapplytheencoderfunction\nf() = xDx . (2.66)\nUsingafurthermatrixmultiplication, wecanalsodeﬁnethePCAreconstruction\noperation:\nr g f () = x (()) = xDDx .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 125, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 777}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0126_b2e9e577", "text": "(2.66)\nUsingafurthermatrixmultiplication, wecanalsodeﬁnethePCAreconstruction\noperation:\nr g f () = x (()) = xDDx . (2.67)\nNext,weneedtochoosetheencodingmatrixD.Todoso,werevisittheidea\nofminimizingthe L2distancebetweeninputsandreconstructions.Sincewewill\nusethesamematrixDtodecodeallofthepoints,wecannolongerconsiderthe\npointsinisolation.Instead,wemustminimizetheFrobeniusnormofthematrix\noferrorscomputedoveralldimensionsandallpoints:\nD∗= argmin\nD\ni , j\nx( ) i\nj− r(x( ) i) j2\nsubjecttoDDI= l(2.68)\nToderivethealgorithmforﬁndingD∗,wewillstartbyconsideringthecase\nwhere l= 1.Inthiscase,Disjustasinglevector,d.Substitutingequation2.67\nintoequationandsimplifyinginto,theproblemreducesto 2.68 Dd\nd∗= argmin\nd\ni||x( ) i−ddx( ) i||2\n2subjectto||||d 2= 1 .(2.69)\nTheaboveformulationisthemostdirectwayofperformingthesubstitution,\nbutisnotthemoststylisticallypleasingwaytowritetheequation.Itplacesthe\nscalarvaluedx( ) iontherightofthevectord.Itismoreconventionaltowrite\nscalarcoeﬃcientsontheleftofvectortheyoperateon.Wethereforeusuallywrite\nsuchaformulaas\nd∗= argmin\nd\ni||x( ) i−dx( ) id||2\n2subjectto||||d 2= 1 ,(2.70)\nor,exploitingthefactthatascalarisitsowntranspose,as\nd∗= argmin\nd\ni||x( ) i−x( ) i dd||2\n2subjectto||||d 2= 1 .(2.71)\nThereadershouldaimtobecomefamiliarwithsuchcosmeticrearrangements .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 126, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1306}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0127_9331b85c", "text": "5 0\nCHAPTER2.LINEARALGEBRA\nAtthispoint,itcanbehelpfultorewritetheproblemintermsofasingle\ndesignmatrixofexamples,ratherthanasasumoverseparateexamplevectors. Thiswillallowustousemorecompactnotation.LetX∈ Rm n ×bethematrix\ndeﬁnedbystackingallofthevectorsdescribingthepoints,suchthatX i , :=x( ) i.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 127, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 295}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0128_89a8fb08", "text": "Thiswillallowustousemorecompactnotation.LetX∈ Rm n ×bethematrix\ndeﬁnedbystackingallofthevectorsdescribingthepoints,suchthatX i , :=x( ) i. Wecannowrewritetheproblemas\nd∗= argmin\nd||−XXdd||2\nFsubjecttodd= 1 .(2.72)\nDisregardingtheconstraintforthemoment,wecansimplifytheFrobeniusnorm\nportionasfollows:\nargmin\nd||−XXdd||2\nF (2.73)\n= argmin\ndTr\nXXdd −\nXXdd −\n(2.74)\n(byequation)2.49\n= argmin\ndTr(XXX−Xdd−ddXXdd+XXdd)(2.75)\n= argmin\ndTr(XX)Tr(−XXdd)Tr(−ddXX)+Tr(ddXXdd)\n(2.76)\n= argmin\nd−Tr(XXdd)Tr(−ddXX)+Tr(ddXXdd)(2.77)\n(becausetermsnotinvolvingdonotaﬀectthe) d argmin\n= argmin\nd−2Tr(XXdd)+Tr(ddXXdd)(2.78)\n(becausewecancycletheorderofthematricesinsideatrace,equation)2.52\n= argmin\nd−2Tr(XXdd)+Tr(XXdddd)(2.79)\n(usingthesamepropertyagain)\nAtthispoint,were-introducetheconstraint:\nargmin\nd−2Tr(XXdd)+Tr(XXdddd)subjecttodd= 1(2.80)\n= argmin\nd−2Tr(XXdd)+Tr(XXdd)subjecttodd= 1(2.81)\n(duetotheconstraint)\n= argmin\nd−Tr(XXdd)subjecttodd= 1(2.82)\n5 1\nCHAPTER2.LINEARALGEBRA\n= argmax\ndTr(XXdd)subjecttodd= 1(2.83)\n= argmax\ndTr(dXXdd )subjecttod= 1(2.84)\nThisoptimizationproblemmaybesolvedusingeigendecomposition.Speciﬁcally,\ntheoptimaldisgivenbytheeigenvectorofXXcorrespondingtothelargest\neigenvalue.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 128, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1258}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0129_ded8af8b", "text": "Thisderivationisspeciﬁctothecaseof l=1andrecoversonlytheﬁrst\nprincipalcomponent.Moregenerally,whenwewishtorecoverabasisofprincipal\ncomponents,thematrixDisgivenbythe leigenvectorscorrespondingtothe\nlargesteigenvalues.Thismaybeshownusingproofbyinduction.Werecommend\nwritingthisproofasanexercise. Linearalgebraisoneofthefundamentalmathematical disciplinesthatis\nnecessarytounderstanddeeplearning.Anotherkeyareaofmathematics thatis\nubiquitousinmachinelearningisprobabilitytheory,presentednext. 5 2\nC h a p t e r 3\nProbabilityandInformation\nTheory\nInthischapter,wedescribeprobabilitytheoryandinformationtheory.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 129, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 605}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0130_b1fdc33c", "text": "5 2\nC h a p t e r 3\nProbabilityandInformation\nTheory\nInthischapter,wedescribeprobabilitytheoryandinformationtheory. Probabilitytheoryisamathematical frameworkforrepresentinguncertain\nstatements.Itprovidesameansofquantifyinguncertaintyandaxiomsforderiving\nnewuncertainstatements.Inartiﬁcialintelligenceapplications,weuseprobability\ntheoryintwomajorways.First,thelawsofprobabilitytellushowAIsystems\nshouldreason,sowedesignouralgorithmstocomputeorapproximate various\nexpressionsderivedusingprobabilitytheory.Second,wecanuseprobabilityand\nstatisticstotheoreticallyanalyzethebehaviorofproposedAIsystems. Probabilitytheoryisafundamentaltoolofmanydisciplinesofscienceand\nengineering.Weprovidethischaptertoensurethatreaderswhosebackgroundis\nprimarilyinsoftwareengineeringwithlimitedexposuretoprobabilitytheorycan\nunderstandthematerialinthisbook.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 130, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 837}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0131_84b82a39", "text": "Whileprobabilitytheoryallowsustomakeuncertainstatementsandreasonin\nthepresenceofuncertainty,informationtheoryallowsustoquantifytheamount\nofuncertaintyinaprobabilitydistribution. Ifyouarealreadyfamiliarwithprobabilitytheoryandinformationtheory,you\nmaywishtoskipallofthischapterexceptforsection,whichdescribesthe 3.14\ngraphsweusetodescribestructuredprobabilisticmodelsformachinelearning.If\nyouhaveabsolutelynopriorexperiencewiththesesubjects,thischaptershould\nbesuﬃcienttosuccessfullycarryoutdeeplearningresearchprojects,butwedo\nsuggestthatyouconsultanadditionalresource,suchasJaynes2003(). 53\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n3.1WhyProbability?", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 131, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 651}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0132_71fa471b", "text": "53\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n3.1WhyProbability? Manybranchesofcomputersciencedealmostlywithentitiesthatareentirely\ndeterministicandcertain.AprogrammercanusuallysafelyassumethataCPUwill\nexecuteeachmachineinstructionﬂawlessly.Errorsinhardwaredooccur,butare\nrareenoughthatmostsoftwareapplicationsdonotneedtobedesignedtoaccount\nforthem.Giventhatmanycomputerscientistsandsoftwareengineersworkina\nrelativelycleanandcertainenvironment,itcanbesurprisingthatmachinelearning\nmakesheavyuseofprobabilitytheory. Thisisbecausemachinelearningmustalwaysdealwithuncertainquantities,\nandsometimesmayalsoneedtodealwithstochastic(non-determinis tic)quantities. Uncertaintyandstochasticitycanarisefrommanysources.Researchershavemade\ncompellingargumentsforquantifyinguncertaintyusingprobabilitysinceatleast\nthe1980s.Manyoftheargumentspresentedherearesummarizedfromorinspired\nbyPearl1988(). Nearlyallactivitiesrequiresomeabilitytoreasoninthepresenceofuncertainty.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 132, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 955}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0133_7a486886", "text": "Nearlyallactivitiesrequiresomeabilitytoreasoninthepresenceofuncertainty. Infact,beyondmathematical statementsthataretruebydeﬁnition,itisdiﬃcult\ntothinkofanypropositionthatisabsolutelytrueoranyeventthatisabsolutely\nguaranteedtooccur. Therearethreepossiblesourcesofuncertainty:\n1.Inherentstochasticityinthesystembeingmodeled.Forexample,most\ninterpretationsofquantummechanicsdescribethedynamicsofsubatomic\nparticlesasbeingprobabilistic.Wecanalsocreatetheoreticalscenariosthat\nwepostulatetohaverandomdynamics,suchasahypothetical cardgame\nwhereweassumethatthecardsaretrulyshuﬄedintoarandomorder. 2.Incompleteobservability.Evendeterministicsystemscanappearstochastic\nwhenwecannotobserveallofthevariablesthatdrivethebehaviorofthe\nsystem.Forexample,intheMontyHallproblem,agameshowcontestantis\naskedtochoosebetweenthreedoorsandwinsaprizeheldbehindthechosen\ndoor.Twodoorsleadtoagoatwhileathirdleadstoacar.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 133, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 895}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0134_c8171c04", "text": "Theoutcome\ngiventhecontestant’schoiceisdeterministic,butfromthecontestant’spoint\nofview,theoutcomeisuncertain. 3.Incompletemodeling.Whenweuseamodelthatmustdiscardsomeof\nthe information wehave observed, the discarded i nformationresults in\nuncertaintyinthemodel’spredictions. Forexample,supposewebuilda\nrobotthatcanexactlyobservethelocationofeveryobjectaroundit.Ifthe\n54\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nrobotdiscretizesspacewhenpredictingthefuturelocationoftheseobjects,\nthenthediscretizationmakestherobotimmediatelybecomeuncertainabout\ntheprecisepositionofobjects: eachobjectcouldbeanywherewithinthe\ndiscretecellthatitwasobservedtooccupy.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 134, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 647}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0135_c425b179", "text": "Inmanycases,itismorepracticaltouseasimplebutuncertainrulerather\nthanacomplexbutcertainone,evenifthetrueruleisdeterministicandour\nmodelingsystemhastheﬁdelitytoaccommodateacomplexrule.Forexample,the\nsimplerule“Mostbirdsﬂy”ischeaptodevelopandisbroadlyuseful,whilearule\noftheform,“Birdsﬂy,exceptforveryyoungbirdsthathavenotyetlearnedto\nﬂy,sickorinjuredbirdsthathavelosttheabilitytoﬂy,ﬂightlessspeciesofbirds\nincludingthecassowary,ostrichandkiwi...” isexpensivetodevelop,maintainand\ncommunicate,andafterallofthiseﬀortisstillverybrittleandpronetofailure. Whileitshouldbeclearthatweneedameansofrepresentingandreasoning\naboutuncertainty,itisnotimmediatelyobviousthatprobabilitytheorycanprovide\nallofthetoolswewantforartiﬁcialintelligenceapplications.Probabilitytheory\nwasoriginallydevelopedtoanalyzethefrequenciesofevents.Itiseasytosee\nhowprobabilitytheorycanbeusedtostudyeventslikedrawingacertainhandof\ncardsinagameofpoker.Thesekindsofeventsareoftenrepeatable.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 135, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 953}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0136_bfdff937", "text": "Whenwe\nsaythatanoutcomehasaprobabilitypofoccurring,itmeansthatifwerepeated\ntheexperiment(e.g.,drawahandofcards)inﬁnitelymanytimes,thenproportion\npoftherepetitionswouldresultinthatoutcome.Thiskindofreasoningdoesnot\nseemimmediatelyapplicabletopropositionsthatarenotrepeatable.Ifadoctor\nanalyzesapatientandsaysthatthepatienthasa40%chanceofhavingtheﬂu,\nthismeanssomethingverydiﬀerent—wecannotmakeinﬁnitelymanyreplicasof\nthepatient,noristhereanyreasontobelievethatdiﬀerentreplicasofthepatient\nwouldpresentwiththesamesymptomsyethavevaryingunderlyingconditions.In\nthecaseofthedoctordiagnosingthepatient,weuseprobabilitytorepresenta\ndegr e e o f b e l i e f,with1indicatingabsolutecertaintythatthepatienthastheﬂu\nand0indicatingabsolutecertaintythatthepatientdoesnothavetheﬂu. The\nformerkindofprobability,relateddirectlytotheratesatwhicheventsoccur,is\nknownas f r e q uen t i st pr o babili t y,whilethelatter,relatedtoqualitativelevels\nofcertainty,isknownas B ay e si an pr o babili t y.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 136, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 979}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0137_fffc7734", "text": "Ifwelistseveralpropertiesthatweexpectcommonsensereasoningabout\nuncertaintytohave,thentheonlywaytosatisfythosepropertiesistotreat\nBayesianprobabilities asbehavingexactlythesameasfrequentistprobabilities. Forexample,ifwewanttocomputetheprobabilitythataplayerwillwinapoker\ngamegiventhatshehasacertainsetofcards,weuseexactlythesameformulas\naswhenwecomputetheprobabilitythatapatienthasadiseasegiventhatshe\n55\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nhascertainsymptoms.Formoredetailsaboutwhyasmallsetofcommonsense\nassumptionsimpliesthatthesameaxiomsmustcontrolbothkindsofprobability,\nsee(). Ramsey1926\nProbabilitycanbeseenastheextensionoflogictodealwithuncertainty.Logic\nprovidesasetofformalrulesfordeterminingwhatpropositionsareimpliedto\nbetrueorfalsegiventheassumptionthatsomeothersetofpropositionsistrue\norfalse.Probabilitytheoryprovidesasetofformalrulesfordeterminingthe\nlikelihoodofapropositionbeingtruegiventhelikelihoodofotherpropositions.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 137, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 941}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0138_360fbfce", "text": "3.2RandomVariables\nA r andom v ar i abl eisavariablethatcantakeondiﬀerentvaluesrandomly.We\ntypicallydenotetherandomvariableitselfwithalowercaseletterinplaintypeface,\nandthevaluesitcantakeonwithlowercasescriptletters.Forexample,x 1andx 2\narebothpossiblevaluesthattherandomvariablexcantakeon.Forvector-valued\nvariables,wewouldwritetherandomvariableas xandoneofitsvaluesas x.On\nitsown,arandomvariableisjustadescriptionofthestatesthatarepossible;it\nmustbecoupledwithaprobabilitydistributionthatspeciﬁeshowlikelyeachof\nthesestatesare. Randomvariablesmaybediscreteorcontinuous.Adiscreterandomvariable\nisonethathasaﬁniteorcountablyinﬁnitenumberofstates.Notethatthese\nstatesarenotnecessarilytheintegers;theycanalsojustbenamedstatesthat\narenotconsideredtohaveanynumericalvalue.Acontinuousrandomvariableis\nassociatedwitharealvalue.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 138, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 821}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0139_4fb921f8", "text": "3.3ProbabilityDistributions\nA pr o babili t y di st r i but i o nisadescriptionofhowlikelyarandomvariableor\nsetofrandomvariablesistotakeoneachofitspossiblestates.Thewaywe\ndescribeprobabilitydistributionsdependsonwhetherthevariablesarediscreteor\ncontinuous. 3.3.1DiscreteVariablesandProbabilityMassFunctions\nAprobabilitydistributionoverdiscretevariablesmaybedescribedusinga pr o ba-\nbi l i t y m ass f unc t i o n(PMF).Wetypicallydenoteprobabilitymassfunctionswith\nacapitalP.Oftenweassociateeachrandomvariablewithadiﬀerentprobability\n56\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nmassfunctionandthereadermustinferwhichprobabilitymassfunctiontouse\nbasedontheidentityoftherandomvariable,ratherthanthenameofthefunction;\nP P ()xisusuallynotthesameas()y.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 139, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 746}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0140_640a173f", "text": "Theprobabilitymassfunctionmapsfromastateofarandomvariableto\ntheprobabilityofthatrandomvariabletakingonthatstate.Theprobability\nthatx=xisdenotedasP(x),withaprobabilityof1indicatingthatx=xis\ncertainandaprobabilityof0indicatingthatx=xisimpossible.Sometimes\ntodisambiguatewhichPMFtouse,wewritethenameoftherandomvariable\nexplicitly:P(x=x).Sometimeswedeﬁneavariableﬁrst,thenuse∼notationto\nspecifywhichdistributionitfollowslater:xx. ∼P()\nProbabilitymassfunctionscanactonmanyvariablesatthesametime.Such\naprobabilitydistributionovermanyvariablesisknownasa j o i n t pr o babili t y\ndi st r i but i o n.P(x=x,y=y)denotestheprobabilitythatx=xandy=y\nsimultaneously.Wemayalsowrite forbrevity. Px,y()\nTobeaprobabilitymassfunctiononarandomvariablex,afunctionPmust\nsatisfythefollowingproperties:\n•Thedomainofmustbethesetofallpossiblestatesofx.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 140, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 827}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0141_41706168", "text": "Px,y()\nTobeaprobabilitymassfunctiononarandomvariablex,afunctionPmust\nsatisfythefollowingproperties:\n•Thedomainofmustbethesetofallpossiblestatesofx. P\n•∀∈xx,0≤P(x)≤1.Animpossibleeventhasprobabilityandnostatecan 0 \nbelessprobablethanthat.Likewise,aneventthatisguaranteedtohappen\nhasprobability,andnostatecanhaveagreaterchanceofoccurring. 1\n•\nx ∈ xP(x) = 1.Werefertothispropertyasbeing nor m al i z e d.Without\nthisproperty,wecouldobtainprobabilities greaterthanonebycomputing\ntheprobabilityofoneofmanyeventsoccurring. Forexample,considerasinglediscreterandomvariablexwithkdiﬀerent\nstates.Wecanplacea uni f o r m di st r i but i o nonx—thatis,makeeachofits\nstatesequallylikely—bysettingitsprobabilitymassfunctionto\nPx (= x i) =1\nk(3.1)\nforalli.Wecanseethatthisﬁtstherequirementsforaprobabilitymassfunction. Thevalue1\nkispositivebecauseisapositiveinteger.Wealsoseethat k\n\niPx (= x i) =\ni1\nk=k\nk= 1, (3.2)\nsothedistributionisproperlynormalized.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 141, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 942}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0142_5f091121", "text": "Thevalue1\nkispositivebecauseisapositiveinteger.Wealsoseethat k\n\niPx (= x i) =\ni1\nk=k\nk= 1, (3.2)\nsothedistributionisproperlynormalized. 57\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n3.3.2ContinuousVariablesandProbabilityDensityFunctions\nWhenworkingwithcontinuousrandomvariables,wedescribeprobabilitydistri-\nbutionsusinga pr o babili t y densit y f unc t i o n ( P D F)ratherthanaprobability\nmassfunction.Tobeaprobabilitydensityfunction,afunctionpmustsatisfythe\nfollowingproperties:\n•Thedomainofmustbethesetofallpossiblestatesofx. p\n•∀∈ ≥ ≤ xx,px() 0 () . p Notethatwedonotrequirex 1. •\npxdx()= 1. Aprobabilitydensityfunctionp(x)doesnotgivetheprobabilityofaspeciﬁc\nstatedirectly,insteadtheprobabilityoflandinginsideaninﬁnitesimalregionwith\nvolumeisgivenby.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 142, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 756}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0143_a30c9d62", "text": "p Notethatwedonotrequirex 1. •\npxdx()= 1. Aprobabilitydensityfunctionp(x)doesnotgivetheprobabilityofaspeciﬁc\nstatedirectly,insteadtheprobabilityoflandinginsideaninﬁnitesimalregionwith\nvolumeisgivenby. δx pxδx()\nWecanintegratethedensityfunctiontoﬁndtheactualprobabilitymassofa\nsetofpoints.Speciﬁcally,theprobabilitythatxliesinsomeset Sisgivenbythe\nintegralofp(x)overthatset.Intheunivariateexample,theprobabilitythatx\nliesintheintervalisgivenby []a,b\n[ ] a , bpxdx().", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 143, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 467}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0144_cba57fd6", "text": "Foranexampleofaprobabilitydensityfunctioncorrespondingtoaspeciﬁc\nprobabilitydensityoveracontinuousrandomvariable,considerauniformdistribu-\ntiononanintervaloftherealnumbers.Wecandothiswithafunctionu(x;a,b),\nwhereaandbaretheendpointsoftheinterval,withb>a.The“;”notationmeans\n“parametrized by”;weconsiderxtobetheargumentofthefunction,whileaand\nbareparametersthatdeﬁnethefunction.Toensurethatthereisnoprobability\nmassoutsidetheinterval,wesayu(x;a,b)=0forallx∈[a,b] [.Withina,b],\nuxa,b (;) =1\nb a −.Wecanseethatthisisnonnegativeeverywhere.Additionally,it\nintegratesto1.Weoftendenotethatxfollowstheuniformdistributionon[a,b]\nbywritingx. ∼Ua,b()\n3.4MarginalProbability\nSometimesweknowtheprobabilitydistributionoverasetofvariablesandwewant\ntoknowtheprobabilitydistributionoverjustasubsetofthem.Theprobability\ndistributionoverthesubsetisknownasthe distribution.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 144, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 853}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0145_ccf2d742", "text": "m ar g i nal pr o babili t y\nForexample,supposewehavediscreterandomvariablesxandy,andweknow\nP,(xy.Wecanﬁndxwiththe : ) P() sum r ul e\n∀∈xxx,P(= ) =x\nyPx,y. (= xy= ) (3.3)\n58\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nThename“marginalprobability”comesfromtheprocessofcomputingmarginal\nprobabilities onpaper.WhenthevaluesofP(xy,)arewritteninagridwith\ndiﬀerentvaluesofxinrowsanddiﬀerentvaluesofyincolumns,itisnaturalto\nsumacrossarowofthegrid,thenwriteP(x)inthemarginofthepaperjustto\ntherightoftherow. Forcontinuousvariables,weneedtouseintegrationinsteadofsummation:\npx() =\npx,ydy. () (3.4)\n3.5ConditionalProbability\nInmanycases,weareinterestedintheprobabilityofsomeevent,giventhatsome\nothereventhashappened.Thisiscalleda c o ndi t i o n a l pr o babili t y.Wedenote\ntheconditionalprobabilitythaty=ygivenx=xasP(y=y|x=x).This\nconditionalprobabilitycanbecomputedwiththeformula\nPyx (= y |x= ) =Py,x (= yx= )\nPx (= x ).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 145, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 911}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0146_887d2908", "text": "(3.5)\nTheconditionalprobabilityisonlydeﬁnedwhenP(x=x)>0.Wecannotcompute\ntheconditionalprobabilityconditionedonaneventthatneverhappens. Itisimportantnottoconfuseconditionalprobabilitywithcomputingwhat\nwouldhappenifsomeactionwereundertaken.Theconditionalprobabilitythat\napersonisfromGermanygiventhattheyspeakGermanisquitehigh,butif\narandomlyselectedpersonistaughttospeakGerman,theircountryoforigin\ndoesnotchange.Computingtheconsequencesofanactioniscalledmakingan\ni n t e r v e n t i o n q uer y.Interventionqueriesarethedomainof c ausal m o del i ng,\nwhichwedonotexploreinthisbook. 3.6TheChainRuleofConditionalProbabilities\nAnyjointprobabilitydistributionovermanyrandomvariablesmaybedecomposed\nintoconditionaldistributionsoveronlyonevariable:\nP(x( 1 ),...,x( ) n) = (Px( 1 ))Πn\ni = 2P(x( ) i|x( 1 ),...,x( 1 ) i −).(3.6)\nThisobservationisknownasthe c hai n r ul eor pr o duc t r ul eofprobability.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 146, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 895}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0147_4f798520", "text": "Itfollowsimmediatelyfromthedeﬁnitionofconditionalprobabilityinequation.3.5\n59\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nForexample,applyingthedeﬁnitiontwice,weget\nP,,P,P, (abc)= (ab|c)(bc)\nP,PP (bc)= ( )bc| ()c\nP,,P,PP. (abc)= (ab|c)( )bc| ()c\n3.7IndependenceandConditionalIndependence\nTworandomvariablesxandyare i ndep e nden tiftheirprobabilitydistribution\ncanbeexpressedasaproductoftwofactors,oneinvolvingonlyxandoneinvolving\nonlyy:\n∀∈ ∈xx,yyxyxy (3.7) ,p(= x,= ) = (yp= )(xp= )y. Tworandomvariablesxandyare c o ndi t i o n a l l y i ndep e nden tgivenarandom\nvariableziftheconditionalprobabilitydistributionoverxandyfactorizesinthis\nwayforeveryvalueofz:\n∀∈ ∈ ∈ | | | xx,yy,zzxy,p(= x,= yzx = ) = (zp= xzy = )(zp= yz= )z. (3.8)\nWe candenoteindependence andconditionalindependence with compact\nnotation:xy⊥meansthatxandyareindependent,whilexyz ⊥|meansthatx\nandyareconditionallyindependentgivenz.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 147, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 896}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0148_0ca09d15", "text": "(3.8)\nWe candenoteindependence andconditionalindependence with compact\nnotation:xy⊥meansthatxandyareindependent,whilexyz ⊥|meansthatx\nandyareconditionallyindependentgivenz. 3.8Expectation,VarianceandCovariance\nThe e x p e c t at i o nor e x p e c t e d v al ueofsomefunctionf(x)withrespecttoa\nprobabilitydistributionP(x)istheaverageormeanvaluethatftakesonwhenx\nisdrawnfrom.Fordiscretevariablesthiscanbecomputedwithasummation: P\nE x ∼ P[()] =fx\nxPxfx, ()() (3.9)\nwhileforcontinuousvariables,itiscomputedwithanintegral:\nE x ∼ p[()] =fx\npxfxdx. ()() (3.10)\n60\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nWhentheidentityofthedistributionisclearfromthecontext,wemaysimply\nwritethenameoftherandomvariablethattheexpectationisover,asin E x[f(x)]. Ifitisclearwhichrandomvariabletheexpectationisover,wemayomitthe\nsubscriptentirely,asin E[f(x)].Bydefault,wecanassumethat E[·]averagesover\nthevaluesofalltherandomvariablesinsidethebrackets.Likewise,whenthereis\nnoambiguity,wemayomitthesquarebrackets.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 148, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0149_48e4d6ed", "text": "Expectationsarelinear,forexample,\nE x[()+ ()] = αfxβgxα E x[()]+fxβ E x[()]gx, (3.11)\nwhenandarenotdependenton. αβ x\nThe v ar i anc egivesameasureofhowmuchthevaluesofafunctionofarandom\nvariablexvaryaswesamplediﬀerentvaluesofxfromitsprobabilitydistribution:\nVar(()) = fx E\n(() [()]) fx− Efx2\n. (3.12)\nWhenthevarianceislow,thevaluesoff(x)clusterneartheirexpectedvalue.The\nsquarerootofthevarianceisknownasthe .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 149, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 409}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0150_ec9b9e40", "text": "(3.12)\nWhenthevarianceislow,thevaluesoff(x)clusterneartheirexpectedvalue.The\nsquarerootofthevarianceisknownasthe . st andar d dev i at i o n\nThe c o v ar i anc egivessomesenseofhowmuchtwovaluesarelinearlyrelated\ntoeachother,aswellasthescaleofthesevariables:\nCov(()()) = [(() [()])(() [()])] fx,gy Efx− Efxgy− Egy.(3.13)\nHighabsolutevaluesofthecovariancemeanthatthevalueschangeverymuch\nandarebothfarfromtheirrespectivemeansatthesametime.Ifthesignofthe\ncovarianceispositive,thenbothvariablestendtotakeonrelativelyhighvalues\nsimultaneously.Ifthesignofthecovarianceisnegative,thenonevariabletendsto\ntakeonarelativelyhighvalueatthetimesthattheothertakesonarelatively\nlowvalueandviceversa.Othermeasuressuchas c o r r e l at i o nnormalizethe\ncontributionofeachvariableinordertomeasureonlyhowmuchthevariablesare\nrelated,ratherthanalsobeingaﬀectedbythescaleoftheseparatevariables.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 150, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 872}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0151_ebfca52c", "text": "Thenotionsofcovarianceanddependencearerelated,butareinfactdistinct\nconcepts.Theyarerelatedbecausetwovariablesthatareindependenthavezero\ncovariance,andtwovariablesthathavenon-zerocovariancearedependent.How-\never,independence isadistinctpropertyfromcovariance.Fortwovariablestohave\nzerocovariance,theremustbenolineardependencebetweenthem.Independence\nisastrongerrequirementthanzerocovariance,becauseindependencealsoexcludes\nnonlinearrelationships.Itispossiblefortwovariablestobedependentbuthave\nzerocovariance.Forexample,supposeweﬁrstsamplearealnumberxfroma\nuniformdistributionovertheinterval[−1,1].Wenextsamplearandomvariable\n61\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\ns.Withprobability1\n2,wechoosethevalueofstobe.Otherwise,wechoose 1\nthevalueofstobe−1.Wecanthengeneratearandomvariableybyassigning\ny=sx.Clearly,xandyarenotindependent,becausexcompletelydetermines\nthemagnitudeof.However,y Cov() = 0x,y.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 151, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 901}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0152_3bf753f9", "text": "The c o v ar i anc e m at r i xofarandomvector x∈ Rnisannn×matrix,such\nthat\nCov() x i , j= Cov(x i,x j). (3.14)\nThediagonalelementsofthecovariancegivethevariance:\nCov(x i,x i) = Var(x i). (3.15)\n3.9CommonProbabilityDistributions\nSeveralsimpleprobabilitydistributionsareusefulinmanycontextsinmachine\nlearning. 3.9.1BernoulliDistribution\nThe B e r noul l idistributionisadistributionoverasinglebinaryrandomvariable.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 152, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 413}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0153_62d7a1e0", "text": "3.9.1BernoulliDistribution\nThe B e r noul l idistributionisadistributionoverasinglebinaryrandomvariable. Itiscontrolledbyasingleparameterφ∈[0,1],whichgivestheprobabilityofthe\nrandomvariablebeingequalto1.Ithasthefollowingproperties:\nP φ (= 1) = x (3.16)\nP φ (= 0) = 1x − (3.17)\nPxφ (= x ) = x(1 )−φ1 − x(3.18)\nE x[] = xφ (3.19)\nVar x() = (1 )xφ−φ (3.20)\n3.9.2MultinoulliDistribution\nThe m ul t i noull ior c at e g o r i c a ldistributionisadistributionoverasinglediscrete\nvariablewithkdiﬀerentstates,wherekisﬁnite.1Themultinoullidistributionis\n1“Multinoulli”isatermthatwasrecentlycoinedbyGustavoLacerdoandpopularizedby\nMurphy2012().Themultinoullidistributionisaspecialcaseofthe m u lt in om ia ldistribution. Amultinomialdistributionisthedistributionovervectorsin{0,...,n}krepresentinghowmany\ntimeseachofthekcategoriesisvisitedwhennsamplesaredrawnfromamultinoullidistribution. Manytextsusetheterm“multinomial”torefertomultinoullidistributionswithoutclarifying\nthattheyreferonlytothecase.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 153, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 987}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0154_b83915cd", "text": "Manytextsusetheterm“multinomial”torefertomultinoullidistributionswithoutclarifying\nthattheyreferonlytothecase. n= 1\n62\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nparametrized byavector p∈[0,1]k − 1,wherep igivestheprobabilityofthei-th\nstate.Theﬁnal,k-thstate’sprobabilityisgivenby1− 1p.Notethatwemust\nconstrain 1p≤1.Multinoullidistributionsareoftenusedtorefertodistributions\novercategoriesofobjects,sowedonotusuallyassumethatstate1hasnumerical\nvalue1,etc.Forthisreason,wedonotusuallyneedtocomputetheexpectation\norvarianceofmultinoulli-dis tributedrandomvariables. TheBernoulliandmultinoullidistributionsaresuﬃcienttodescribeanydistri-\nbutionovertheirdomain.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 154, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 657}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0155_23f785da", "text": "TheBernoulliandmultinoullidistributionsaresuﬃcienttodescribeanydistri-\nbutionovertheirdomain. They areabletodescribeanydistributionovertheir\ndomainnotsomuchbecausetheyareparticularlypowerfulbutratherbecause\ntheirdomainissimple;theymodeldiscretevariablesforwhichitisfeasibleto\nenumerateallofthestates.Whendealingwithcontinuousvariables,thereare\nuncountablymanystates,soanydistributiondescribedbyasmallnumberof\nparametersmustimposestrictlimitsonthedistribution. 3.9.3GaussianDistribution\nThemostcommonlyuseddistributionoverrealnumbersisthe nor m al di st r i bu-\nt i o n,alsoknownasthe : G aussian di st r i but i o n\nN(;xµ,σ2) =\n1\n2πσ2exp\n−1\n2σ2( )xµ−2\n.(3.21)\nSeeﬁgureforaplotofthedensityfunction. 3.1\nThetwoparameters µ∈ Randσ∈(0,∞)controlthenormaldistribution. Theparameterµgivesthecoordinateofthecentralpeak.Thisisalsothemeanof\nthedistribution: E[x] =µ.Thestandarddeviationofthedistributionisgivenby\nσ,andthevariancebyσ2.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 155, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 927}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0156_5b0807c8", "text": "Theparameterµgivesthecoordinateofthecentralpeak.Thisisalsothemeanof\nthedistribution: E[x] =µ.Thestandarddeviationofthedistributionisgivenby\nσ,andthevariancebyσ2. WhenweevaluatethePDF,weneedtosquareandinvertσ.Whenweneedto\nfrequentlyevaluatethePDFwithdiﬀerentparametervalues,amoreeﬃcientway\nofparametrizing thedistributionistouseaparameterβ∈(0,∞)tocontrolthe\npr e c i si o norinversevarianceofthedistribution:\nN(;xµ,β− 1) =\nβ\n2πexp\n−1\n2βxµ (−)2\n. (3.22)\nNormaldistributionsareasensiblechoiceformanyapplications.Intheabsence\nofpriorknowledgeaboutwhatformadistributionovertherealnumbersshould\ntake,thenormaldistributionisagooddefaultchoicefortwomajorreasons. 63\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n− − − − 20 . 15 . 10 . 05 00 05 10 15 20 . . . . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 156, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 754}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0157_81cf5084", "text": "63\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n− − − − 20 . 15 . 10 . 05 00 05 10 15 20 . . . . . . x000 .005 .010 .015 .020 .025 .030 .035 .040 .p(x)Maximumat= x µ\nInﬂectionpointsat\nx µ σ = ±\nFigure3.1:Thenormaldistribution:ThenormaldistributionN(x;µ,σ2)exhibits\naclassic“bellcurve”shape,withthexcoordinateofitscentralpeakgivenbyµ,and\nthewidthofitspeakcontrolledbyσ.Inthisexample,wedepictthestandardnormal\ndistribution,withand. µ= 0σ= 1\nFirst,manydistributionswewishtomodelaretrulyclosetobeingnormal\ndistributions.The c e n t r al l i m i t t heor e mshowsthatthesumofmanyindepen-\ndentrandomvariablesisapproximatelynormallydistributed.Thismeansthat\ninpractice,manycomplicatedsystemscanbemodeledsuccessfullyasnormally\ndistributednoise,evenifthesystemcanbedecomposedintopartswithmore\nstructuredbehavior.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 157, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 799}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0158_f6065e1d", "text": "Second,outofallpossibleprobabilitydistributionswiththesamevariance,\nthenormaldistributionencodesthemaximumamountofuncertaintyoverthe\nrealnumbers.Wecanthusthinkofthenormaldistributionasbeingtheone\nthatinsertstheleastamountofpriorknowledgeintoamodel.Fullydeveloping\nandjustifyingthisidearequiresmoremathematical tools,andispostponedto\nsection.19.4.2\nThenormaldistributiongeneralizesto Rn,inwhichcaseitisknownasthe\nm ul t i v ar i at e nor m al di st r i but i o n.Itmaybeparametrized withapositive\ndeﬁnitesymmetricmatrix: Σ\nN(; ) = x µ, Σ\n1\n(2)πndet() Σexp\n−1\n2( ) x µ−Σ− 1( ) x µ−\n.(3.23)\n64\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nTheparameter µstillgivesthemeanofthedistribution,thoughnowitis\nvector-valued.Theparameter Σgivesthecovariancematrixofthedistribution.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 158, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 769}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0159_dedce1df", "text": "Asintheunivariatecase,whenwewishtoevaluatethePDFseveraltimesfor\nmanydiﬀerentvaluesoftheparameters,thecovarianceisnotacomputationally\neﬃcientwaytoparametrizethedistribution,sinceweneedtoinvert Σtoevaluate\nthePDF.Wecaninsteadusea : pr e c i si o n m at r i x β\nN(; x µ β,− 1) =\ndet() β\n(2)πnexp\n−1\n2( ) x µ−β x µ (−)\n.(3.24)\nWeoftenﬁxthecovariancematrixtobeadiagonalmatrix.Anevensimpler\nversionisthe i sot r o pi cGaussiandistribution,whosecovariancematrixisascalar\ntimestheidentitymatrix. 3.9.4ExponentialandLaplaceDistributions\nInthecontextofdeeplearning,weoftenwanttohaveaprobabilitydistribution\nwithasharppointatx=0.Toaccomplishthis,wecanusethe e x p o nen t i al\ndi st r i but i o n:\npxλλ (;) = 1 x ≥ 0exp( )−λx.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 159, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 719}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0160_fd05d0c0", "text": "(3.25)\nTheexponentialdistributionusestheindicatorfunction 1 x ≥ 0toassignprobability\nzerotoallnegativevaluesof.x\nAcloselyrelatedprobabilitydistributionthatallowsustoplaceasharppeak\nofprobabilitymassatanarbitrarypointistheµ L apl ac e di st r i but i o n\nLaplace(;) =xµ,γ1\n2γexp\n−|−|xµ\nγ\n. (3.26)\n3.9.5TheDiracDistributionandEmpiricalDistribution\nInsomecases,wewishtospecifythatallofthemassinaprobabilitydistribution\nclustersaroundasinglepoint.ThiscanbeaccomplishedbydeﬁningaPDFusing\ntheDiracdeltafunction,:δx()\npxδxµ.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 160, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 519}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0161_8f3d166e", "text": "() = (−) (3.27)\nTheDiracdeltafunctionisdeﬁnedsuchthatitiszero-valuedeverywhereexcept\n0,yetintegratesto1.TheDiracdeltafunctionisnotanordinaryfunctionthat\nassociateseachvaluexwithareal-valuedoutput,insteaditisadiﬀerentkindof\n65\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nmathematical objectcalleda g e ner al i z e d f unc t i o nthatisdeﬁnedintermsofits\npropertieswhenintegrated.WecanthinkoftheDiracdeltafunctionasbeingthe\nlimitpointofaseriesoffunctionsthatputlessandlessmassonallpointsother\nthanzero.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 161, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 498}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0162_94ac14d5", "text": "Bydeﬁningp(x)tobeδshiftedby−µweobtainaninﬁnitelynarrowand\ninﬁnitelyhighpeakofprobabilitymasswhere.xµ= \nAcommonuseoftheDiracdeltadistributionisasacomponentofan e m pi r i c a l\ndi st r i but i o n,\nˆp() = x1\nmm\ni = 1δ( x x−( ) i) (3.28)\nwhichputsprobabilitymass1\nmoneachofthempoints x( 1 ),..., x( ) mforminga\ngivendatasetorcollectionofsamples.TheDiracdeltadistributionisonlynecessary\ntodeﬁnetheempiricaldistributionovercontinuousvariables.Fordiscretevariables,\nthesituationissimpler:anempiricaldistributioncanbeconceptualized asa\nmultinoullidistribution,withaprobabilityassociatedtoeachpossibleinputvalue\nthatissimplyequaltothe e m pi r i c a l f r e q uenc yofthatvalueinthetrainingset. Wecanviewtheempiricaldistributionformedfromadatasetoftraining\nexamplesasspecifyingthedistributionthatwesamplefromwhenwetrainamodel\nonthisdataset.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 162, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 834}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0163_d965b85a", "text": "Wecanviewtheempiricaldistributionformedfromadatasetoftraining\nexamplesasspecifyingthedistributionthatwesamplefromwhenwetrainamodel\nonthisdataset. Anotherimportantperspectiveontheempiricaldistributionis\nthatitistheprobabilitydensitythatmaximizesthelikelihoodofthetrainingdata\n(seesection).5.5\n3.9.6MixturesofDistributions\nItisalsocommontodeﬁneprobabilitydistributionsbycombiningothersimpler\nprobabilitydistributions.Onecommon wayof combining distributionsis to\nconstructa m i x t ur e di st r i but i o n.Amixturedistributionismadeupofseveral\ncomponentdistributions.Oneachtrial,thechoiceofwhichcomponentdistribution\ngeneratesthesampleisdeterminedbysamplingacomponentidentityfroma\nmultinoullidistribution:\nP() =x\niPiPi (= c )( = xc| ) (3.29)\nwherecisthemultinoullidistributionovercomponentidentities. P()\nWehavealreadyseenoneexampleofamixturedistribution:theempirical\ndistributionoverreal-valuedvariablesisamixturedistributionwithoneDirac\ncomponentforeachtrainingexample.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 163, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 970}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0164_eac9852d", "text": "P()\nWehavealreadyseenoneexampleofamixturedistribution:theempirical\ndistributionoverreal-valuedvariablesisamixturedistributionwithoneDirac\ncomponentforeachtrainingexample. 66\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nThemixturemodelisonesimplestrategyforcombiningprobabilitydistributions\ntocreatearicherdistribution.Inchapter,weexploretheartofbuildingcomplex 16\nprobabilitydistributionsfromsimpleonesinmoredetail.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 164, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 411}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0165_0c3c098b", "text": "Themixturemodelallowsustobrieﬂyglimpseaconceptthatwillbeof\nparamountimportancelater—the l at e n t v ar i abl e.Alatentvariableisarandom\nvariablethatwecannotobservedirectly.Thecomponentidentityvariablecofthe\nmixturemodelprovidesanexample.Latentvariablesmayberelatedtoxthrough\nthejointdistribution,inthiscase,P(xc,) =P(xc|)P(c).ThedistributionP(c)\noverthelatentvariableandthedistributionP(xc|)relatingthelatentvariables\ntothevisiblevariablesdeterminestheshapeofthedistributionP(x)eventhough\nitispossibletodescribeP(x)withoutreferencetothelatentvariable.Latent\nvariablesarediscussedfurtherinsection.16.5\nAverypowerfulandcommontypeofmixturemodelisthe G aussian m i x t ur e\nmodel,inwhichthecomponentsp( x|c=i)areGaussians.Eachcomponenthas\naseparatelyparametrized mean µ( ) iandcovariance Σ( ) i.Somemixturescanhave\nmoreconstraints.Forexample,thecovariancescouldbesharedacrosscomponents\nviatheconstraint Σ( ) i= Σ,i∀.AswithasingleGaussiandistribution,themixture\nofGaussiansmightconstrainthecovariancematrixforeachcomponenttobe\ndiagonalorisotropic.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 165, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1043}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0166_1e792581", "text": "Inadditiontothemeansandcovariances,theparametersofaGaussianmixture\nspecifythe pr i o r pr o babili t yα i=P(c=i) giventoeachcomponenti.Theword\n“prior”indicatesthatitexpressesthemodel’sbeliefsaboutc b e f o r eithasobserved\nx.Bycomparison,P(c| x)isa p o st e r i o r pr o babili t y,becauseitiscomputed\na f t e robservationof x.AGaussianmixturemodelisa uni v e r sal appr o x i m a t o r\nofdensities,inthesensethatanysmoothdensitycanbeapproximatedwithany\nspeciﬁc,non-zeroamountoferrorbyaGaussianmixturemodelwithenough\ncomponents. FigureshowssamplesfromaGaussianmixturemodel. 3.2\n3.10UsefulPropertiesofCommonFunctions\nCertainfunctionsariseoftenwhileworkingwithprobabilitydistributions,especially\ntheprobabilitydistributionsusedindeeplearningmodels. Oneofthesefunctionsisthe : l o g i st i c si g m o i d\nσx() =1\n1+exp()−x.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 166, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 820}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0167_91e8c540", "text": "Oneofthesefunctionsisthe : l o g i st i c si g m o i d\nσx() =1\n1+exp()−x. (3.30)\nThelogisticsigmoidiscommonlyusedtoproducetheφparameterofaBernoulli\n67\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nx 1x 2\nFigure3.2: SamplesfromaGaussianmixturemodel.Inthisexample,therearethree\ncomponents.Fromlefttoright,theﬁrstcomponenthasanisotropiccovariancematrix,\nmeaningithasthesameamountofvarianceineachdirection.Thesecondhasadiagonal\ncovariancematrix,meaningitcancontrolthevarianceseparatelyalongeachaxis-aligned\ndirection.Thisexamplehasmorevariancealongthex 2axisthanalongthex 1axis.The\nthirdcomponenthasafull-rankcovariancematrix,allowingittocontrolthevariance\nseparatelyalonganarbitrarybasisofdirections. distributionbecauseitsrangeis(0,1),whichlieswithinthevalidrangeofvalues\nfortheφparameter.Seeﬁgureforagraphofthesigmoidfunction.The 3.3\nsigmoidfunction sat ur at e swhenitsargumentisverypositiveorverynegative,\nmeaningthatthefunctionbecomesveryﬂatandinsensitivetosmallchangesinits\ninput.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 167, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 978}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0168_f76efa86", "text": "Anothercommonlyencounteredfunctionisthe sof t pl usfunction(,Dugas e t a l . 2001):\nζx x. () = log(1+exp()) (3.31)\nThesoftplusfunctioncanbeusefulforproducingtheβorσparameterofanormal\ndistributionbecauseitsrangeis(0,∞).Italsoarisescommonlywhenmanipulating\nexpressionsinvolvingsigmoids.Thenameofthesoftplusfunctioncomesfromthe\nfactthatitisasmoothedor“softened”versionof\nx+= max(0),x. (3.32)\nSeeﬁgureforagraphofthesoftplusfunction. 3.4\nThefollowingpropertiesareallusefulenoughthatyoumaywishtomemorize\nthem:\n68\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n− − 1 0 5 0 5 1 0\nx0 0 .0 2 .0 4 .0 6 .0 8 .1 0 .σ x ( )\nFigure3.3:Thelogisticsigmoidfunction. − − 1 0 5 0 5 1 0\nx024681 0ζ x ( )\nFigure3.4:Thesoftplusfunction.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 168, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 708}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0169_ada7c5d0", "text": "− − 1 0 5 0 5 1 0\nx024681 0ζ x ( )\nFigure3.4:Thesoftplusfunction. 69\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nσx() =exp()x\nexp()+exp(0)x(3.33)\nd\ndxσxσxσx () = ()(1−()) (3.34)\n1 () = () −σxσ−x (3.35)\nlog() = () σx −ζ−x (3.36)\nd\ndxζxσx () = () (3.37)\n∀∈x(01),,σ− 1() = logxx\n1−x\n(3.38)\n∀x>,ζ0− 1() = log(exp()1) x x− (3.39)\nζx() =x\n− ∞σydy() (3.40)\nζxζxx ()−(−) = (3.41)\nThefunctionσ− 1(x)iscalledthe l o g i tinstatistics,butthistermismorerarely\nusedinmachinelearning. Equationprovidesextrajustiﬁcationforthename“softplus.”Thesoftplus 3.41\nfunctionisintendedasasmoothedversionofthe p o si t i v e par tfunction,x+=\nmax{0,x}.Thepositivepartfunctionisthecounterpartofthe negat i v e par t\nfunction,x−=max{0,x−}.Toobtainasmoothfunctionthatisanalogoustothe\nnegativepart,onecanuseζ(−x).Justasxcanberecoveredfromitspositivepart\nandnegativepartviatheidentityx+−x−=x,itisalsopossibletorecoverx\nusingthesamerelationshipbetweenand,asshowninequation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 169, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 941}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0170_9a63bc9c", "text": "ζx()ζx(−) 3.41\n3.11Bayes’Rule\nWeoftenﬁndourselvesinasituationwhereweknowP(yx|)andneedtoknow\nP(xy|).Fortunately,ifwealsoknowP(x),wecancomputethedesiredquantity\nusing B a y e s’ r ul e:\nP( ) =xy|PP()x( )yx|\nP()y. (3.42)\nNotethatwhileP(y)appearsintheformula,itisusuallyfeasibletocompute\nP() =y\nxPxPx P (y|)(),sowedonotneedtobeginwithknowledgeof()y. 70\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nBayes’ruleis straightforwardto derivefrom thedeﬁnitionofconditional\nprobability,butitisusefultoknowthenameofthisformulasincemanytexts\nrefertoitbyname.ItisnamedaftertheReverendThomasBayes,whoﬁrst\ndiscoveredaspecialcaseoftheformula.Thegeneralversionpresentedherewas\nindependentlydiscoveredbyPierre-SimonLaplace.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 170, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 700}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0171_1bd16cba", "text": "3.12TechnicalDetailsofContinuousVariables\nAproperformalunderstandingofcontinuousrandomvariablesandprobability\ndensityfunctionsrequiresdevelopingprobabilitytheoryintermsofabranchof\nmathematics knownas m e asur e t heor y.Measuretheoryisbeyondthescopeof\nthistextbook,butwecanbrieﬂysketchsomeoftheissuesthatmeasuretheoryis\nemployedtoresolve. Insection,wesawthattheprobabilityofacontinuousvector-valued 3.3.2 x\nlyinginsomeset Sisgivenbytheintegralofp( x)overtheset S.Somechoices\nofset Scanproduceparadoxes.Forexample,itispossibletoconstructtwosets\nS 1and S 2suchthatp( x∈ S 1) +p( x∈ S 2)>1but S 1∩ S 2=∅.Thesesets\naregenerallyconstructedmakingveryheavyuseoftheinﬁniteprecisionofreal\nnumbers,forexamplebymakingfractal-shapedsetsorsetsthataredeﬁnedby\ntransformingthesetofrationalnumbers.2Oneofthekeycontributionsofmeasure\ntheoryistoprovideacharacterization ofthesetofsetsthatwecancomputethe\nprobabilityofwithoutencounteringparadoxes.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 171, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 928}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0172_55fd5c5f", "text": "Inthisbook,weonlyintegrate\noversetswithrelativelysimpledescriptions,sothisaspectofmeasuretheorynever\nbecomesarelevantconcern. Forourpurposes,measuretheoryismoreusefulfordescribingtheoremsthat\napplytomostpointsin Rnbutdonotapplytosomecornercases.Measuretheory\nprovidesarigorouswayofdescribingthatasetofpointsisnegligiblysmall.Such\nasetissaidtohave m e asur e z e r o.Wedonotformallydeﬁnethisconceptinthis\ntextbook.Forourpurposes,itissuﬃcienttounderstandtheintuitionthataset\nofmeasurezerooccupiesnovolumeinthespacewearemeasuring.Forexample,\nwithin R2,alinehasmeasurezero,whileaﬁlledpolygonhaspositivemeasure. Likewise,anindividualpointhasmeasurezero.Anyunionofcountablymanysets\nthateachhavemeasurezeroalsohasmeasurezero(sothesetofalltherational\nnumbershasmeasurezero,forinstance). Anotherusefultermfrommeasuretheoryis al m o st e v e r y wher e.Aproperty\nthatholdsalmosteverywhereholdsthroughoutallofspaceexceptforonasetof\n2TheBanach-Tarskitheoremprovidesafunexampleofsuchsets.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 172, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 975}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0173_3d90893e", "text": "Anotherusefultermfrommeasuretheoryis al m o st e v e r y wher e.Aproperty\nthatholdsalmosteverywhereholdsthroughoutallofspaceexceptforonasetof\n2TheBanach-Tarskitheoremprovidesafunexampleofsuchsets. 71\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nmeasurezero.Becausetheexceptionsoccupyanegligibleamountofspace,they\ncanbesafelyignoredformanyapplications.Someimportantresultsinprobability\ntheoryholdforalldiscretevaluesbutonlyhold“almosteverywhere”forcontinuous\nvalues. Anothertechnicaldetailofcontinuousvariablesrelatestohandlingcontinuous\nrandomvariablesthataredeterministicfunctionsofoneanother.Supposewehave\ntworandomvariables, xand y,suchthat y=g( x),wheregisaninvertible,con-\ntinuous,diﬀerentiabletransformation.Onemightexpectthatp y( y) =p x(g− 1( y)). Thisisactuallynotthecase.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 173, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 777}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0174_8bf9c71e", "text": "Thisisactuallynotthecase. Asasimpleexample,supposewehavescalarrandomvariablesxandy.Suppose\ny=x\n2andx∼U(0,1).Ifweusetherulep y(y)=p x(2y)thenp ywillbe0\neverywhereexcepttheinterval[0,1\n2] 1 ,anditwillbeonthisinterval.Thismeans\n\np y()=ydy1\n2, (3.43)\nwhichviolatesthedeﬁnitionofaprobabilitydistribution.Thisisacommonmistake. Theproblemwiththisapproachisthatitfailstoaccountforthedistortionof\nspaceintroducedbythefunctiong.Recallthattheprobabilityof xlyinginan\ninﬁnitesimallysmallregionwithvolumeδ xisgivenbyp( x)δ x.Sincegcanexpand\norcontractspace,theinﬁnitesimalvolumesurrounding xin xspacemayhave\ndiﬀerentvolumeinspace. y\nToseehowtocorrecttheproblem,wereturntothescalarcase.Weneedto\npreservetheproperty\n|p y(())= gxdy||p x()xdx.| (3.44)\nSolvingfromthis,weobtain\np y() = yp x(g− 1())y∂x\n∂y(3.45)\norequivalently\np x() = xp y(())gx∂gx()\n∂x.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 174, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 852}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0175_a798b403", "text": "(3.46)\nInhigherdimensions,thederivativegeneralizestothedeterminantofthe J ac o bi an\nm at r i x—thematrixwithJ i , j=∂ x i\n∂ y j.Thus,forreal-valuedvectorsand, x y\np x() = xp y(())g xdet∂g() x\n∂ x .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 175, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 208}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0176_57e116ae", "text": "(3.47)\n72\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n3.13InformationTheory\nInformationtheory isa branchof appliedmathematics thatrevolvesaround\nquantifyinghowmuchinformationispresentinasignal.Itwasoriginallyinvented\ntostudysendingmessagesfromdiscretealphabetsoveranoisychannel,suchas\ncommunicationviaradiotransmission.Inthiscontext,informationtheorytellshow\ntodesignoptimalcodesandcalculatetheexpectedlengthofmessagessampledfrom\nspeciﬁcprobabilitydistributionsusingvariousencodingschemes.Inthecontextof\nmachinelearning,wecanalsoapplyinformationtheorytocontinuousvariables\nwheresomeofthesemessagelengthinterpretations donotapply.Thisﬁeldis\nfundamentaltomanyareasofelectricalengineeringandcomputerscience.Inthis\ntextbook,wemostlyuseafewkeyideasfrominformationtheorytocharacterize\nprobabilitydistributionsorquantifysimilaritybetweenprobabilitydistributions.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 176, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 852}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0177_57fa6a04", "text": "Formoredetailoninformationtheory,seeCoverandThomas2006MacKay ()or\n().2003\nThebasicintuitionbehindinformationtheoryisthatlearningthatanunlikely\neventhas occurredismoreinformativethanlearningthata likely eventhas\noccurred.Amessagesaying“thesunrosethismorning”issouninformative as\ntobeunnecessarytosend,butamessagesaying“therewasasolareclipsethis\nmorning”isveryinformative. Wewouldliketoquantifyinformationinawaythatformalizesthisintuition. Speciﬁcally,\n•Likelyeventsshouldhavelowinformationcontent,andintheextremecase,\neventsthatareguaranteedtohappenshouldhavenoinformationcontent\nwhatsoever. •Lesslikelyeventsshouldhavehigherinformationcontent. •Independenteventsshouldhaveadditiveinformation. Forexample,ﬁnding\noutthatatossedcoinhascomeupasheadstwiceshouldconveytwiceas\nmuchinformationasﬁndingoutthatatossedcoinhascomeupasheads\nonce. Inordertosatisfyallthreeoftheseproperties,wedeﬁnethe se l f - i nf o r m a t i o n\nofaneventxtobe = x\nIxPx.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 177, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 941}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0178_4a74f411", "text": "Inordertosatisfyallthreeoftheseproperties,wedeﬁnethe se l f - i nf o r m a t i o n\nofaneventxtobe = x\nIxPx. () = log− () (3.48)\nInthisbook,wealwaysuselogtomeanthenaturallogarithm,withbasee.Our\ndeﬁnitionofI(x)isthereforewritteninunitsof nat s.Onenatistheamountof\n73\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\ninformationgainedbyobservinganeventofprobability1\ne.Othertextsusebase-2\nlogarithmsandunitscalled bi t sor shannons;informationmeasuredinbitsis\njustarescalingofinformationmeasuredinnats. Whenxiscontinuous,weusethesamedeﬁnitionofinformationbyanalogy,\nbutsomeofthepropertiesfromthediscretecasearelost.Forexample,anevent\nwithunitdensitystillhaszeroinformation, despitenotbeinganeventthatis\nguaranteedtooccur. Self-information dealsonlywithasingleoutcome.Wecanquantifytheamount\nofuncertaintyinanentireprobabilitydistributionusingthe Shannon e nt r o p y:\nH() = x E x ∼ P[()] = Ix − E x ∼ P[log()]Px.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 178, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 900}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0179_5baaa20f", "text": "Self-information dealsonlywithasingleoutcome.Wecanquantifytheamount\nofuncertaintyinanentireprobabilitydistributionusingthe Shannon e nt r o p y:\nH() = x E x ∼ P[()] = Ix − E x ∼ P[log()]Px. (3.49)\nalsodenotedH(P).Inotherwords,theShannonentropyofadistributionisthe\nexpectedamountofinformationinaneventdrawnfromthatdistribution.Itgives\nalowerboundonthenumberofbits(ifthelogarithmisbase2,otherwisetheunits\narediﬀerent)neededonaveragetoencodesymbolsdrawnfromadistributionP. Distributionsthatarenearlydeterministic(wheretheoutcomeisnearlycertain)\nhavelowentropy;distributionsthatareclosertouniformhavehighentropy.See\nﬁgureforademonstration.When 3.5 xiscontinuous,theShannonentropyis\nknownasthe di ﬀ e r e n t i al e nt r o p y.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 179, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 722}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0180_07c1f1de", "text": "IfwehavetwoseparateprobabilitydistributionsP(x)andQ(x)overthesame\nrandomvariablex,wecanmeasurehowdiﬀerentthesetwodistributionsareusing\nthe K ul l bac k - L e i bl e r ( K L ) di v e r g e nc e:\nD K L( ) = PQ E x ∼ P\nlogPx()\nQx()\n= E x ∼ P[log()log()] Px−Qx.(3.50)\nInthecaseofdiscretevariables,itistheextraamountofinformation(measured\ninbitsifweusethebaselogarithm,butinmachinelearningweusuallyusenats 2\nandthenaturallogarithm)neededtosendamessagecontainingsymbolsdrawn\nfromprobabilitydistributionP,whenweuseacodethatwasdesignedtominimize\nthelengthofmessagesdrawnfromprobabilitydistribution.Q\nTheKLdivergencehasmanyusefulproperties,mostnotablythatitisnon-\nnegative.TheKLdivergenceis0ifandonlyifPandQarethesamedistributionin\nthecaseofdiscretevariables,orequal“almosteverywhere”inthecaseofcontinuous\nvariables.BecausetheKLdivergenceisnon-negativeandmeasuresthediﬀerence\nbetweentwodistributions,itisoftenconceptualized asmeasuringsomesortof\ndistancebetweenthesedistributions.However,itisnotatruedistancemeasure\nbecauseitisnotsymmetric:D K L(PQ)=D K L(QP)forsomePandQ.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 180, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1069}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0181_7a3d26c0", "text": "This\n74\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n0 0 0 2 0 4 0 6 0 8 1 0 . . . . . . p0 0 .0 1 .0 2 .0 3 .0 4 .0 5 .0 6 .0 7 .Sha nno n e ntr o p y i n na t s\nFigure3.5:Thisplotshowshowdistributionsthatareclosertodeterministichavelow\nShannonentropywhiledistributionsthatareclosetouniformhavehighShannonentropy. Onthehorizontalaxis,weplotp,theprobabilityofabinaryrandomvariablebeingequal\nto.Theentropyisgivenby 1 (p−1)log(1−p)−pplog.Whenpisnear0,thedistribution\nisnearlydeterministic,becausetherandomvariableisnearlyalways0.Whenpisnear1,\nthedistributionisnearlydeterministic,becausetherandomvariableisnearlyalways1. Whenp= 0.5,theentropyismaximal,becausethedistributionisuniformoverthetwo\noutcomes. asymmetrymeansthatthereareimportantconsequencestothechoiceofwhether\ntouseD K L( )PQorD K L( )QP.Seeﬁgureformoredetail.3.6\nAquantitythatiscloselyrelatedtotheKLdivergenceisthe c r o ss-en t r o p y\nH(P,Q) =H(P)+D K L(PQ),whichissimilartotheKLdivergencebutlacking\nthetermontheleft:\nHP,Q( ) = − E x ∼ Plog()Qx.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 181, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1007}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0182_c7cce9c8", "text": "(3.51)\nMinimizingthecross-entropywithrespecttoQisequivalenttominimizingthe\nKLdivergence,becausedoesnotparticipateintheomittedterm. Q\nWhencomputingmanyofthesequantities,itiscommontoencounterexpres-\nsionsoftheform0log0.Byconvention,inthecontextofinformationtheory,we\ntreattheseexpressionsaslim x → 0xxlog= 0. 3.14StructuredProbabilisticModels\nMachinelearningalgorithmsofteninvolveprobabilitydistributionsoveravery\nlargenumberofrandomvariables.Often,theseprobabilitydistributionsinvolve\ndirectinteractionsbetweenrelativelyfewvariables.Usingasinglefunctionto\n75\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nxProbability Densityq∗= argminq D K L() p q \np x()\nq∗() x\nxProbability Densityq∗= argminq D K L() q p \np() x\nq∗() x\nFigure3.6:TheKLdivergenceisasymmetric.Supposewehaveadistributionp(x)and\nwishtoapproximateitwithanotherdistributionq(x).Wehavethechoiceofminimizing\neitherD KL(pq)orD KL(qp).Weillustratetheeﬀectofthischoiceusingamixtureof\ntwoGaussiansforp,andasingleGaussianforq.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 182, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 979}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0183_94060db0", "text": "Thechoiceofwhichdirectionofthe\nKLdivergencetouseisproblem-dependent.Someapplicationsrequireanapproximation\nthatusuallyplaceshighprobabilityanywherethatthetruedistributionplaceshigh\nprobability,whileotherapplicationsrequireanapproximationthatrarelyplaceshigh\nprobabilityanywherethatthetruedistributionplaceslowprobability.Thechoiceofthe\ndirectionoftheKLdivergencereﬂectswhichoftheseconsiderationstakespriorityforeach\napplication. ( L e f t )TheeﬀectofminimizingD KL(pq).Inthiscase,weselectaqthathas\nhighprobabilitywherephashighprobability.Whenphasmultiplemodes,qchoosesto\nblurthemodestogether,inordertoputhighprobabilitymassonallofthem.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 183, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 636}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0184_7890d303", "text": "( R i g h t )The\neﬀectofminimizingD KL(qp).Inthiscase,weselectaqthathaslowprobabilitywhere\nphaslowprobability.Whenphasmultiplemodesthataresuﬃcientlywidelyseparated,\nasinthisﬁgure,theKLdivergenceisminimizedbychoosingasinglemode,inorderto\navoidputtingprobabilitymassinthelow-probabilityareasbetweenmodesofp.Here,we\nillustratetheoutcomewhenqischosentoemphasizetheleftmode.Wecouldalsohave\nachievedanequalvalueoftheKLdivergencebychoosingtherightmode.Ifthemodes\narenotseparatedbyasuﬃcientlystronglowprobabilityregion,thenthisdirectionofthe\nKLdivergencecanstillchoosetoblurthemodes. 76\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\ndescribetheentirejointprobabilitydistributioncanbeveryineﬃcient(both\ncomputationally andstatistically). Insteadofusingasinglefunctiontorepresentaprobabilitydistribution,we\ncansplitaprobabilitydistributionintomanyfactorsthatwemultiplytogether.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 184, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 863}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0185_9a0ce3f8", "text": "Insteadofusingasinglefunctiontorepresentaprobabilitydistribution,we\ncansplitaprobabilitydistributionintomanyfactorsthatwemultiplytogether. Forexample,supposewehavethreerandomvariables:a,bandc.Supposethat\nainﬂuencesthevalueofbandbinﬂuencesthevalueofc,butthataandcare\nindependentgivenb.Wecanrepresenttheprobabilitydistributionoverallthree\nvariablesasaproductofprobabilitydistributionsovertwovariables:\np,,ppp. (abc) = ()a( )ba|( )cb| (3.52)\nThesefactorizationscangreatlyreducethenumberofparametersneeded\ntodescribethedistribution.Eachfactorusesanumberofparametersthatis\nexponentialinthenumberofvariablesinthefactor.Thismeansthatwecangreatly\nreducethecostofrepresentingadistributionifweareabletoﬁndafactorization\nintodistributionsoverfewervariables.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 185, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 746}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0186_446c92a7", "text": "Wecandescribethesekindsoffactorizationsusinggraphs.Hereweusetheword\n“graph”inthesenseofgraphtheory:asetofverticesthatmaybeconnectedtoeach\notherwithedges.Whenwerepresentthefactorizationofaprobabilitydistribution\nwithagraph,wecallita st r uc t ur e d pr o babili s t i c m o delor g r aphic al m o del. Therearetwomainkindsofstructuredprobabilisticmodels:directedand\nundirected.Bothkindsofgraphicalmodelsuseagraph Ginwhicheachnode\ninthegraphcorrespondstoarandomvariable, and anedgeconnectingtwo\nrandomvariablesmeansthattheprobabilitydistributionisabletorepresentdirect\ninteractionsbetweenthosetworandomvariables. D i r e c t e dmodelsuse graphswithdirectededges, andtheyrepresentfac-\ntorizationsintoconditionalprobabilitydistributions,asintheexampleabove. Speciﬁcally,adirectedmodelcontainsonefactorforeveryrandomvariablex iin\nthedistribution,andthatfactorconsistsoftheconditionaldistributionoverx i\ngiventheparentsofx i,denotedPa G(x i):\np() = x\nip(x i|Pa G(x i)).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 186, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 964}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0187_2a93ca2b", "text": "(3.53)\nSeeﬁgureforanexampleofadirectedgraphandthefactorizationofprobability 3.7\ndistributionsitrepresents. U ndi r e c t e dmodelsusegraphswithundirectededges,andtheyrepresent\nfactorizationsintoasetoffunctions;unlikeinthedirectedcase,thesefunctions\n77\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\naa\nccbb\needd\nFigure3.7:Adirectedgraphicalmodeloverrandomvariablesa,b,c,dande.Thisgraph\ncorrespondstoprobabilitydistributionsthatcanbefactoredas\np,,,,ppp,pp. (abcde) = ()a( )ba|(ca|b)( )db|( )ec| (3.54)\nThisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,a\nandcinteractdirectly,butaandeinteractonlyindirectlyviac. areusuallynotprobabilitydistributionsofanykind.Anysetofnodesthatareall\nconnectedtoeachotherinGiscalledaclique.Eachclique C( ) iinanundirected\nmodelisassociatedwithafactorφ( ) i(C( ) i).Thesefactorsarejustfunctions,not\nprobabilitydistributions.Theoutputofeachfactormustbenon-negative, but\nthereisnoconstraintthatthefactormustsumorintegrateto1likeaprobability\ndistribution.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 187, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 997}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0188_cd5cbc70", "text": "Theprobabilityofaconﬁgurationofrandomvariablesis pr o p o r t i o naltothe\nproductofallofthesefactors—assignmentsthatresultinlargerfactorvaluesare\nmorelikely.Ofcourse,thereisnoguaranteethatthisproductwillsumto1.We\nthereforedividebyanormalizingconstantZ,deﬁnedtobethesumorintegral\noverallstatesoftheproductoftheφfunctions,inordertoobtainanormalized\nprobabilitydistribution:\np() = x1\nZ\niφ( ) i\nC( ) i\n. (3.55)\nSeeﬁgureforanexampleofanundirectedgraphandthefactorizationof 3.8\nprobabilitydistributionsitrepresents.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 188, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 513}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0189_53a9e55d", "text": "(3.55)\nSeeﬁgureforanexampleofanundirectedgraphandthefactorizationof 3.8\nprobabilitydistributionsitrepresents. Keep inmind thatthese graphicalrepresentationsof factorizations are a\nlanguagefordescribingprobabilitydistributions.Theyarenotmutuallyexclusive\nfamiliesofprobabilitydistributions.Beingdirectedorundirectedisnotaproperty\nofaprobabilitydistribution;itisapropertyofaparticular desc r i pti o nofa\n78\nCHAPTER3.PROBABILITYANDINFORMATIONTHEORY\naa\nccbb\needd\nFigure3.8:Anundirectedgraphicalmodeloverrandomvariablesa,b,c,dande.This\ngraphcorrespondstoprobabilitydistributionsthatcanbefactoredas\np,,,, (abcde) =1\nZφ( 1 )( )abc,,φ( 2 )()bd,φ( 3 )()ce,. (3.56)\nThisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,a\nandcinteractdirectly,butaandeinteractonlyindirectlyviac. probabilitydistribution,butanyprobabilitydistributionmaybedescribedinboth\nways.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 189, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 867}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0190_9c65773e", "text": "probabilitydistribution,butanyprobabilitydistributionmaybedescribedinboth\nways. Throughoutpartsandofthisbook,wewillusestructuredprobabilistic III\nmodelsmerelyasalanguagetodescribewhichdirectprobabilisticrelationships\ndiﬀerentmachinelearningalgorithmschoosetorepresent.Nofurtherunderstanding\nofstructuredprobabilisticmodelsisneededuntilthediscussionofresearchtopics,\ninpart,wherewewillexplorestructuredprobabilisticmodelsinmuchgreater III\ndetail. Thischapterhasreviewedthebasicconceptsofprobabilitytheorythatare\nmostrelevanttodeeplearning.Onemoresetoffundamentalmathematical tools\nremains:numericalmethods.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 190, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 605}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0191_b99b6fc8", "text": "Thischapterhasreviewedthebasicconceptsofprobabilitytheorythatare\nmostrelevanttodeeplearning.Onemoresetoffundamentalmathematical tools\nremains:numericalmethods. 79\nC h a p t e r 4\nNumericalComputation\nMachinelearningalgorithmsusuallyrequireahighamountofnumericalcompu-\ntation.Thistypicallyreferstoalgorithmsthatsolvemathematical problemsby\nmethodsthatupdateestimatesofthesolutionviaaniterativeprocess,ratherthan\nanalyticallyderivingaformulaprovidingasymbolicexpressionforthecorrectso-\nlution.Commonoperationsincludeoptimization (ﬁndingthevalueofanargument\nthatminimizesormaximizesafunction)andsolvingsystemsoflinearequations. Evenjustevaluatingamathematical functiononadigitalcomputercanbediﬃcult\nwhenthefunctioninvolvesrealnumbers,whichcannotberepresentedprecisely\nusingaﬁniteamountofmemory. 4.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 191, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 794}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0192_a3664729", "text": "Evenjustevaluatingamathematical functiononadigitalcomputercanbediﬃcult\nwhenthefunctioninvolvesrealnumbers,whichcannotberepresentedprecisely\nusingaﬁniteamountofmemory. 4. 1 O v erﬂ o w an d Un d erﬂ o w\nThefundamentaldiﬃcultyinperformingcontinuousmathonadigitalcomputer\nisthatweneedtorepresentinﬁnitelymanyrealnumberswithaﬁnitenumber\nofbitpatterns.Thismeansthatforalmostallrealnumbers, weincursome\napproximationerrorwhenwerepresentthenumberinthecomputer.Inmany\ncases,thisisjustroundingerror.Roundingerrorisproblematic, especiallywhen\nitcompoundsacrossmanyoperations,andcancausealgorithmsthatworkin\ntheorytofailinpracticeiftheyarenotdesignedtominimizetheaccumulationof\nroundingerror. Oneformofroundingerrorthatisparticularlydevastatingis under ﬂo w.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 192, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 747}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0193_3105efbc", "text": "Oneformofroundingerrorthatisparticularlydevastatingis under ﬂo w. Underﬂowoccurswhennumbersnearzeroareroundedtozero.Manyfunctions\nbehavequalitativelydiﬀerentlywhentheirargumentiszeroratherthanasmall\npositivenumber.Forexample,weusuallywanttoavoiddivisionbyzero(some\n80\nCHAPTER4.NUMERICALCOMPUTATION\nsoftwareenvironmentswillraiseexceptionswhenthisoccurs,otherswillreturna\nresultwithaplaceholdernot-a-numbervalue)ortakingthelogarithmofzero(this\nisusuallytreatedas−∞,whichthenbecomesnot-a-numberifitisusedformany\nfurtherarithmeticoperations). Anotherhighlydamagingformofnumericalerroris o v e r ﬂo w.Overﬂowoccurs\nwhennumberswithlargemagnitudeareapproximatedas∞or−∞.Further\narithmeticwillusuallychangetheseinﬁnitevaluesintonot-a-numbervalues. Oneexampleofafunctionthatmustbestabilizedagainstunderﬂowand\noverﬂowisthesoftmaxfunction.Thesoftmaxfunctionisoftenusedtopredictthe\nprobabilities associatedwithamultinoullidistribution.Thesoftmaxfunctionis\ndeﬁnedtobe\nsoftmax() x i=exp( x i)n\nj = 1exp( x j).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 193, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 995}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0194_b1b73fcf", "text": "(4.1)\nConsiderwhathappenswhenallofthe x iareequaltosomeconstant c.Analytically,\nwecanseethatalloftheoutputsshouldbeequalto1\nn.Numerically,thismay\nnotoccurwhen chaslargemagnitude.If cisverynegative,thenexp( c)will\nunderﬂow.Thismeansthedenominator ofthesoftmaxwillbecome0,sotheﬁnal\nresultisundeﬁned.When cisverylargeandpositive,exp( c)willoverﬂow,again\nresultingintheexpressionasawholebeingundeﬁned.Bothofthesediﬃculties\ncanberesolvedbyinsteadevaluating softmax( z)where z= x−max i x i.Simple\nalgebrashowsthatthevalueofthesoftmaxfunctionisnotchangedanalyticallyby\naddingorsubtractingascalarfromtheinputvector.Subtracting max i x iresults\ninthelargestargumenttoexpbeing0,whichrulesoutthepossibilityofoverﬂow. Likewise,atleastoneterminthedenominator hasavalueof1,whichrulesout\nthepossibilityofunderﬂowinthedenominator leadingtoadivisionbyzero.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 194, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 839}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0195_f5868ea0", "text": "Likewise,atleastoneterminthedenominator hasavalueof1,whichrulesout\nthepossibilityofunderﬂowinthedenominator leadingtoadivisionbyzero. Thereisstillonesmallproblem.Underﬂowinthenumeratorcanstillcause\ntheexpressionasawholetoevaluatetozero.Thismeansthatifweimplement\nlogsoftmax( x)byﬁrstrunningthesoftmaxsubroutinethenpassingtheresultto\nthelogfunction,wecoulderroneouslyobtain −∞.Instead,wemustimplement\naseparatefunctionthatcalculates logsoftmaxinanumericallystableway.The\nlogsoftmaxfunctioncanbestabilizedusingthesametrickasweusedtostabilize\nthefunction. softmax\nForthemostpart,wedonotexplicitlydetailallofthenumericalconsiderations\ninvolvedinimplementing thevariousalgorithmsdescribedinthisbook.Developers\noflow-levellibrariesshouldkeepnumericalissuesinmindwhenimplementing\ndeeplearningalgorithms.Mostreadersofthisbookcansimplyrelyonlow-\nlevellibrariesthatprovidestableimplementations .Insomecases,itispossible\ntoimplementanewalgorithmandhavethenewimplementation automatically\n8 1\nCHAPTER4.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 195, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 989}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0196_b903b992", "text": "NUMERICALCOMPUTATION\nstabilized. Theano ( , ; , ) is an example Bergstra et al.2010 Bastien et al.2012\nof asoftwarepackage that automatically detects and stabilizes many common\nnumericallyunstableexpressionsthatariseinthecontextofdeeplearning. 4.2 P o or Conditioning\nConditioningreferstohowrapidlyafunctionchangeswithrespecttosmallchanges\ninitsinputs. Functionsthatchangerapidlywhentheirinputsareperturbedslightly\ncanbeproblematicforscientiﬁccomputationbecauseroundingerrorsintheinputs\ncanresultinlargechangesintheoutput. Consider the function f( x) = A−1x. When A∈ Rn n ×has an eigenvalue\ndecomposition,its condition num ber is\nmax\ni,jλ i\nλ j. (4.2)\nThisistheratioofthemagnitudeofthelargestandsmallesteigenvalue. When\nthisnumberislarge,matrixinversionisparticularlysensitivetoerrorintheinput. This sensitivity is an intrinsic property of the matrix itself, not the result\nofroundingerrorduringmatrixinversion. Poorlyconditionedmatricesamplify\npre-existingerrorswhenwemultiplybythetruematrixinverse.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 196, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1008}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0197_71c75fbb", "text": "Poorlyconditionedmatricesamplify\npre-existingerrorswhenwemultiplybythetruematrixinverse. Inpractice,the\nerrorwillbecompoundedfurtherbynumericalerrorsintheinversionprocessitself. 4.3 Gradien t-Based Optimization\nMostdeeplearningalgorithmsinvolveoptimizationofsomesort. Optimization\nreferstothetaskofeitherminimizingormaximizingsomefunction f( x) byaltering\nx. Weusuallyphrasemostoptimizationproblemsintermsofminimizing f( x). Maximizationmaybeaccomplishedviaaminimizationalgorithmbyminimizing\n−f( ) x. Thefunctionwewanttominimizeormaximizeiscalledthe ob jectiv e func-\ntionor criterion. When we are minimizing it, we may also call it the cost\nfunction, loss function ,or error function . Inthisbook,weusetheseterms\ninterchangeably,thoughsomemachinelearningpublicationsassignspecialmeaning\ntosomeoftheseterms. We often denote the value that minimizes or maximizes a function with a\nsuperscript . Forexample,wemightsay ∗ x∗= arg min ( ) f x. 82\nCHAPTER4.NUMERICALCOMPUTATION\n− − − − 20. 15. 10.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 197, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 991}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0198_84738378", "text": "We often denote the value that minimizes or maximizes a function with a\nsuperscript . Forexample,wemightsay ∗ x∗= arg min ( ) f x. 82\nCHAPTER4.NUMERICALCOMPUTATION\n− − − − 20. 15. 10. 05 00 05 10 15 20 ...... x−20.−15.−10.−05.00.05.10.15.20. Globalminimumat= 0.x\nSincef() = 0,gradient x\ndescent haltshere. For 0,wehave x< f() 0,x<\nsowecandecreasebyf\nmoving rightward.For 0,wehave x> f() 0,x>\nsowecandecreasebyf\nmoving leftward. f x() =1\n2x2\nf() = x x\nFigure4.1:Anillustrationofhowthegradientdescentalgorithmusesthederivativesofa\nfunctioncanbeusedtofollowthefunctiondownhilltoaminimum. Weassumethereaderisalreadyfamiliarwithcalculus,butprovideabrief\nreviewofhowcalculusconceptsrelatetooptimization here. Supposewehaveafunction y= f( x),whereboth xand yarerealnumbers.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 198, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 770}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0199_c0d9ad55", "text": "Weassumethereaderisalreadyfamiliarwithcalculus,butprovideabrief\nreviewofhowcalculusconceptsrelatetooptimization here. Supposewehaveafunction y= f( x),whereboth xand yarerealnumbers. The der i v at i v eofthisfunctionisdenotedas f( x)orasd y\nd x.Thederivative f( x)\ngivestheslopeof f( x)atthepoint x.Inotherwords,itspeciﬁeshowtoscale\nasmallchangeintheinputinordertoobtainthecorrespondingchangeinthe\noutput: f x  f x  f (+) ≈()+() x. Thederivativeisthereforeusefulforminimizingafunctionbecauseittells\nushowtochange xinordertomakeasmallimprovementin y.Forexample,\nweknowthat f( x −sign( f( x)))islessthan f( x)forsmallenough .Wecan\nthusreduce f( x)bymoving xinsmallstepswithoppositesignofthederivative. Thistechniqueiscalled g r adi e n t desc e n t(Cauchy1847,).Seeﬁgureforan4.1\nexampleofthistechnique.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 199, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 808}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0200_dd6f42d9", "text": "Thistechniqueiscalled g r adi e n t desc e n t(Cauchy1847,).Seeﬁgureforan4.1\nexampleofthistechnique. When f( x) = 0,thederivativeprovidesnoinformationaboutwhichdirection\ntomove.Pointswhere f( x)=0areknownas c r i t i c al p o i nt sor st at i o na r y\np o i n t s.A l o c al m i ni m umisapointwhere f( x)islowerthanatallneighboring\npoints,soitisnolongerpossibletodecrease f( x)bymakinginﬁnitesimalsteps. A l o c al m ax i m u misapointwhere f( x)ishigherthanatallneighboringpoints,\n8 3\nCHAPTER4.NUMERICALCOMPUTATION\nMinimum Maximum Saddlepoint\nFigure4.2:Examplesofeachofthethreetypesofcriticalpointsin1-D.Acriticalpointis\napointwithzeroslope.Suchapointcaneitherbealocalminimum,whichislowerthan\ntheneighboringpoints,alocalmaximum,whichishigherthantheneighboringpoints,or\nasaddlepoint,whichhasneighborsthatarebothhigherandlowerthanthepointitself.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 200, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 847}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0201_15030be4", "text": "soitisnotpossibletoincrease f( x)bymakinginﬁnitesimalsteps.Somecritical\npointsareneithermaximanorminima.Theseareknownas saddle p o i nt s.See\nﬁgureforexamplesofeachtypeofcriticalpoint. 4.2\nApointthatobtainstheabsolutelowestvalueof f( x)isa g l o bal m i ni m um. Itispossiblefortheretobeonlyoneglobalminimumormultipleglobalminimaof\nthefunction.Itisalsopossiblefortheretobelocalminimathatarenotglobally\noptimal.Inthecontextofdeeplearning,weoptimizefunctionsthatmayhave\nmanylocalminimathatarenotoptimal,andmanysaddlepointssurroundedby\nveryﬂatregions.Allofthismakesoptimization verydiﬃcult,especiallywhenthe\ninputtothefunctionismultidimensional.Wethereforeusuallysettleforﬁndinga\nvalueof fthatisverylow,butnotnecessarilyminimalinanyformalsense.See\nﬁgureforanexample.4.3\nWeoftenminimizefunctionsthathavemultipleinputs: f: Rn→ R.Forthe\nconceptof“minimization” to makesense,theremuststillbeonlyone(scalar)\noutput.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 201, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 907}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0202_41e6d3c1", "text": "Forfunctionswithmultipleinputs,wemustmakeuseoftheconceptof par t i al\nder i v at i v e s.Thepartialderivative∂\n∂ x if( x)measureshow fchangesasonlythe\nvariable x iincreasesatpoint x.The g r adi e n tgeneralizesthenotionofderivative\ntothecasewherethederivativeiswithrespecttoavector:thegradientof fisthe\nvectorcontainingallofthepartialderivatives,denoted ∇ x f( x).Element iofthe\ngradientisthepartialderivativeof fwithrespectto x i.Inmultipledimensions,\n8 4\nCHAPTER4.NUMERICALCOMPUTATION\nxf x()\nIdeally,wewouldlike\ntoarriveattheglobal\nminimum, butthis\nmight notbepossible.Thislocalminimum\nperformsnearlyaswellas\ntheglobalone,\nsoitisanacceptable\nhaltingpoint. Thislocalminimumperforms\npoorlyandshouldbeavoided. Figure4.3:Optimizationalgorithmsmayfailtoﬁndaglobalminimumwhenthereare\nmultiplelocalminimaorplateauspresent.Inthecontextofdeeplearning,wegenerally\nacceptsuchsolutionseventhoughtheyarenottrulyminimal,solongastheycorrespond\ntosigniﬁcantlylowvaluesofthecostfunction.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 202, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 972}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0203_7885cf10", "text": "criticalpointsarepointswhereeveryelementofthegradientisequaltozero. The di r e c t i o n a l der i v at i v eindirection(aunitvector)istheslopeofthe u\nfunction findirection u.Inotherwords,thedirectionalderivativeisthederivative\nofthefunction f( x+ α u)withrespectto α,evaluatedat α= 0.Usingthechain\nrule,wecanseethat∂\n∂ αf α (+ x u)evaluatesto u∇ x f α () xwhen = 0. Tominimize f,wewouldliketoﬁndthedirectioninwhich fdecreasesthe\nfastest.Wecandothisusingthedirectionalderivative:\nmin\nu u , u = 1u∇ x f() x (4.3)\n=min\nu u , u = 1|||| u 2||∇ x f() x|| 2cos θ (4.4)\nwhere θistheanglebetween uandthegradient.Substitutingin|||| u 2= 1and\nignoringfactorsthatdonotdependon u,thissimpliﬁestomin ucos θ.Thisis\nminimizedwhen upointsintheoppositedirectionasthegradient.Inother\nwords,thegradientpointsdirectlyuphill,andthenegativegradientpointsdirectly\ndownhill.Wecandecrease fbymovinginthedirectionofthenegativegradient. Thisisknownasthe or .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 203, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 935}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0204_8eb645a9", "text": "Thisisknownasthe or . m e t ho d o f st e e p e st desc e nt g r adi e nt desc e nt\nSteepestdescentproposesanewpoint\nx= x−∇  x f() x (4.5)\n8 5\nCHAPTER4.NUMERICALCOMPUTATION\nwhere isthe l e ar ni ng r at e,apositivescalardeterminingthesizeofthestep. Wecanchoose inseveraldiﬀerentways.Apopularapproachistoset toasmall\nconstant.Sometimes,wecansolveforthestepsizethatmakesthedirectional\nderivativevanish.Anotherapproachistoevaluate f  ( x−∇ x f()) xforseveral\nvaluesof andchoosetheonethatresultsinthesmallestobjectivefunctionvalue. Thislaststrategyiscalleda l i ne se ar c h. Steepestdescentconvergeswheneveryelementofthegradientiszero(or,in\npractice,veryclosetozero).Insomecases,wemaybeabletoavoidrunningthis\niterativealgorithm,andjustjumpdirectlytothecriticalpointbysolvingthe\nequation ∇ x f() = 0 xfor.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 204, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 808}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0205_f6263e23", "text": "x\nAlthoughgradientdescentislimitedtooptimization incontinuousspaces,the\ngeneralconceptofrepeatedlymakingasmallmove(thatisapproximately thebest\nsmallmove)towardsbetterconﬁgurations canbegeneralizedtodiscretespaces. Ascendinganobjectivefunctionofdiscreteparametersiscalled hi l l c l i m bi ng\n( ,). RusselandNorvig2003\n4 . 3 . 1 B ey o n d t h e G ra d i en t : Ja co b i a n a n d Hessi a n Ma t ri ces\nSometimesweneedtoﬁndallofthepartialderivativesofafunctionwhoseinput\nandoutputarebothvectors.Thematrixcontainingallsuchpartialderivativesis\nknownasa J ac o bi an m at r i x.Speciﬁcally,ifwehaveafunction f: Rm→ Rn,\nthentheJacobianmatrix J∈ Rn m ×ofisdeﬁnedsuchthat f J i , j=∂\n∂ x jf() x i. Wearealsosometimesinterestedinaderivativeofaderivative.Thisisknown\nasa se c o nd der i v at i v e.Forexample,forafunction f: Rn→ R,thederivative\nwithrespectto x iofthederivativeof fwithrespectto x jisdenotedas∂2\n∂ x i ∂ x jf.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 205, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 917}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0206_e6fbc85d", "text": "Inasingledimension,wecandenoted2\nd x2 fby f ( x).Thesecondderivativetells\nushowtheﬁrstderivativewillchangeaswevarytheinput.Thisisimportant\nbecauseittellsuswhetheragradientstepwillcauseasmuchofanimprovement\naswewouldexpectbasedonthegradientalone.Wecanthinkofthesecond\nderivativeasmeasuring c ur v at ur e.Supposewehaveaquadraticfunction(many\nfunctionsthatariseinpracticearenotquadraticbutcanbeapproximated well\nasquadratic,atleastlocally).Ifsuchafunctionhasasecondderivativeofzero,\nthenthereisnocurvature.Itisaperfectlyﬂatline,anditsvaluecanbepredicted\nusingonlythegradient.Ifthegradientis,thenwecanmakeastepofsize 1 \nalongthenegativegradient,andthecostfunctionwilldecreaseby .Ifthesecond\nderivativeisnegative,thefunctioncurvesdownward,sothecostfunctionwill\nactuallydecreasebymorethan .Finally,ifthesecondderivativeispositive,the\nfunctioncurvesupward,sothecostfunctioncandecreasebylessthan .See\n8 6\nCHAPTER4.NUMERICALCOMPUTATION\nxf x()N e g a t i v e c u r v a t u r e\nxf x()N o c u r v a t u r e\nxf x()P o s i t i v e c u r v a t u r e\nFigure4.4:Thesecondderivativedeterminesthecurvatureofafunction.Hereweshow\nquadraticfunctionswithvariouscurvature.Thedashedlineindicatesthevalueofthecost\nfunctionwewouldexpectbasedonthegradientinformationaloneaswemakeagradient\nstepdownhill.Inthecaseofnegativecurvature,thecostfunctionactuallydecreasesfaster\nthanthegradientpredicts.Inthecaseofnocurvature,thegradientpredictsthedecrease\ncorrectly.Inthecaseofpositivecurvature,thefunctiondecreasesslowerthanexpected\nandeventuallybeginstoincrease,sostepsthataretoolargecanactuallyincreasethe\nfunctioninadvertently.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 206, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1602}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0207_669a8ab3", "text": "ﬁguretoseehowdiﬀerentformsofcurvatureaﬀecttherelationshipbetween 4.4\nthevalueofthecostfunctionpredictedbythegradientandthetruevalue. Whenourfunctionhasmultipleinputdimensions,therearemanysecond\nderivatives.Thesederivativescanbecollectedtogetherintoamatrixcalledthe\nHessian m at r i x.TheHessianmatrix isdeﬁnedsuchthat H x()( f)\nH x()( f) i , j=∂2\n∂ x i ∂ x jf .() x (4.6)\nEquivalently,theHessianistheJacobianofthegradient. Anywherethatthesecondpartialderivativesarecontinuous,thediﬀerential\noperatorsarecommutative,i.e.theirordercanbeswapped:\n∂2\n∂ x i ∂ x jf() = x∂2\n∂ x j ∂ x if .() x (4.7)\nThisimpliesthat H i , j= H j , i,sotheHessianmatrixissymmetricatsuchpoints. Mostofthefunctionsweencounterinthecontextofdeeplearninghaveasymmetric\nHessianalmosteverywhere.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 207, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 762}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0208_77368cd4", "text": "Mostofthefunctionsweencounterinthecontextofdeeplearninghaveasymmetric\nHessianalmosteverywhere. Because theHessianmatrixisrealandsymmetric,\nwecandecomposeitintoasetofrealeigenvaluesandanorthogonalbasisof\n8 7\nCHAPTER4.NUMERICALCOMPUTATION\neigenvectors.Thesecondderivativeinaspeciﬁcdirectionrepresentedbyaunit\nvector disgivenby dH d.When disaneigenvectorof H,thesecondderivative\ninthatdirectionisgivenbythecorrespondingeigenvalue.Forotherdirectionsof\nd,thedirectionalsecondderivativeisaweightedaverageofalloftheeigenvalues,\nwithweightsbetween0and1,andeigenvectorsthathavesmalleranglewith d\nreceivingmoreweight.Themaximumeigenvaluedeterminesthemaximumsecond\nderivativeandtheminimumeigenvaluedeterminestheminimumsecondderivative.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 208, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 725}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0209_52821929", "text": "The(directional)secondderivativetellsushowwellwecanexpectagradient\ndescentsteptoperform.Wecanmakeasecond-orderTaylorseriesapproximation\ntothefunction aroundthecurrentpoint f() x x( 0 ):\nf f () x≈( x( 0 ))+( x x−( 0 ))g+1\n2( x x−( 0 ))H x x (−( 0 )) .(4.8)\nwhere gisthegradientand HistheHessianat x( 0 ). Ifweusealearningrate\nof ,thenthenewpoint xwillbegivenby x( 0 )−  g.Substitutingthisintoour\napproximation,weobtain\nf( x( 0 )− ≈  g) f( x( 0 ))−  gg+1\n22gH g .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 209, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 470}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0210_695e76f4", "text": "Ifweusealearningrate\nof ,thenthenewpoint xwillbegivenby x( 0 )−  g.Substitutingthisintoour\napproximation,weobtain\nf( x( 0 )− ≈  g) f( x( 0 ))−  gg+1\n22gH g . (4.9)\nTherearethree termshere:theoriginalvalue ofthefunction, the expected\nimprovementduetotheslopeofthefunction,andthecorrectionwemustapply\ntoaccountforthecurvatureofthefunction.Whenthislasttermistoolarge,the\ngradientdescentstepcanactuallymoveuphill.When gH giszeroornegative,\ntheTaylorseriesapproximationpredictsthatincreasing foreverwilldecrease f\nforever.Inpractice,theTaylorseriesisunlikelytoremainaccurateforlarge ,so\nonemustresorttomoreheuristicchoicesof inthiscase.When gH gispositive,\nsolvingfortheoptimalstepsizethatdecreasestheTaylorseriesapproximation of\nthefunctionthemostyields\n∗=gg\ngH g.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 210, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 775}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0211_fca5ab86", "text": "(4.10)\nIntheworstcase,when galignswiththeeigenvectorof Hcorrespondingtothe\nmaximaleigenvalue λ m a x,thenthisoptimalstepsizeisgivenby1\nλmax.Tothe\nextentthatthefunctionweminimizecanbeapproximatedwellbyaquadratic\nfunction,theeigenvaluesoftheHessianthusdeterminethescaleofthelearning\nrate. Thesecondderivativecanbeusedtodeterminewhetheracriticalpointis\nalocalmaximum,alocalminimum,orsaddlepoint.Recallthatonacritical\npoint, f( x) = 0.Whenthesecondderivative f ( x) >0,theﬁrstderivative f( x)\nincreasesaswemovetotherightanddecreasesaswemovetotheleft.Thismeans\n8 8\nCHAPTER4.NUMERICALCOMPUTATION\nf( x −) <0and f( x+ ) >0forsmallenough .Inotherwords,aswemove\nright,theslopebeginstopointuphilltotheright,andaswemoveleft,theslope\nbeginstopointuphilltotheleft.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 211, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 759}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0212_25c27167", "text": "Thus,when f( x)=0and f ( x) >0,wecan\nconcludethat xisalocalminimum.Similarly,when f( x) = 0and f ( x) <0,we\ncanconcludethat xisalocalmaximum.Thisisknownasthe se c o nd der i v at i v e\nt e st.Unfortunately,when f ( x) = 0,thetestisinconclusive.Inthiscase xmay\nbeasaddlepoint,orapartofaﬂatregion.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 212, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 303}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0213_f1449aaf", "text": "Inmultipledimensions,weneedtoexamineallofthesecondderivativesofthe\nfunction.UsingtheeigendecompositionoftheHessianmatrix,wecangeneralize\nthesecondderivativetesttomultipledimensions.Atacriticalpoint,where\n∇ x f( x) = 0,wecanexaminetheeigenvaluesoftheHessiantodeterminewhether\nthecriticalpointisalocalmaximum,localminimum,orsaddlepoint.Whenthe\nHessianispositivedeﬁnite(allitseigenvaluesarepositive),thepointisalocal\nminimum.Thiscanbeseenbyobservingthatthedirectionalsecondderivative\ninanydirectionmustbepositive,andmakingreferencetotheunivariatesecond\nderivativetest.Likewise,whentheHessianisnegativedeﬁnite(allitseigenvalues\narenegative),thepointisalocalmaximum.Inmultipledimensions,itisactually\npossibletoﬁndpositiveevidenceofsaddlepointsinsomecases.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 213, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 750}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0214_a5c3ffff", "text": "Whenatleast\noneeigenvalueispositiveandatleastoneeigenvalueisnegative,weknowthat\nxisalocalmaximumononecrosssectionof fbutalocalminimumonanother\ncrosssection.Seeﬁgureforanexample.Finally,themultidimensionalsecond 4.5\nderivativetestcanbeinconclusive,justliketheunivariateversion.Thetestis\ninconclusivewheneverallofthenon-zeroeigenvalueshavethesamesign,butat\nleastoneeigenvalueiszero.Thisisbecausetheunivariatesecondderivativetestis\ninconclusiveinthecrosssectioncorrespondingtothezeroeigenvalue.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 214, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 491}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0215_b115d7b1", "text": "Inmultipledimensions,thereisadiﬀerentsecondderivativeforeachdirection\natasinglepoint.TheconditionnumberoftheHessianatthispointmeasures\nhowmuchthesecondderivativesdiﬀerfromeachother.WhentheHessianhasa\npoorconditionnumber,gradientdescentperformspoorly.Thisisbecauseinone\ndirection,thederivativeincreasesrapidly,whileinanotherdirection,itincreases\nslowly.Gradientdescentisunawareofthischangeinthederivativesoitdoesnot\nknowthatitneedstoexplorepreferentially inthedirectionwherethederivative\nremainsnegativeforlonger.Italsomakesitdiﬃculttochooseagoodstepsize.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 215, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 554}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0216_f4188072", "text": "Thestepsizemustbesmallenoughtoavoidovershootingtheminimumandgoing\nuphillindirectionswithstrongpositivecurvature.Thisusuallymeansthatthe\nstepsizeistoosmalltomakesigniﬁcantprogressinotherdirectionswithless\ncurvature.Seeﬁgureforanexample.4.6\nThisissuecanberesolvedbyusinginformationfromtheHessianmatrixtoguide\n8 9\nCHAPTER4.NUMERICALCOMPUTATION\n\u0000   \u0000   \n\u0000    \nFigure4.5:Asaddlepointcontainingbothpositiveandnegativecurvature.Thefunction\ninthisexampleis f( x)= x2\n1− x2\n2.Alongtheaxiscorrespondingto x 1,thefunction\ncurvesupward.ThisaxisisaneigenvectoroftheHessianandhasapositiveeigenvalue.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 216, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 614}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0217_64c17bea", "text": "Alongtheaxiscorrespondingto x 2,thefunctioncurvesdownward.Thisdirectionisan\neigenvectoroftheHessianwithnegativeeigenvalue.Thename“saddlepoint”derivesfrom\nthesaddle-likeshapeofthisfunction.Thisisthequintessentialexampleofafunction\nwithasaddlepoint.Inmorethanonedimension,itisnotnecessarytohaveaneigenvalue\nof0inordertogetasaddlepoint:itisonlynecessarytohavebothpositiveandnegative\neigenvalues.Wecanthinkofasaddlepointwithbothsignsofeigenvaluesasbeingalocal\nmaximumwithinonecrosssectionandalocalminimumwithinanothercrosssection.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 217, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 526}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0218_2119d185", "text": "9 0\nCHAPTER4.NUMERICALCOMPUTATION\n− − − 3 0 2 0 1 0 0 1 0 2 0\nx 1− 3 0− 2 0− 1 001 02 0x 2\nFigure4.6:Gradientdescentfailstoexploitthecurvatureinformationcontainedinthe\nHessianmatrix.Hereweusegradientdescenttominimizeaquadraticfunction f( x) whose\nHessianmatrixhasconditionnumber5.Thismeansthatthedirectionofmostcurvature\nhasﬁvetimesmorecurvaturethanthedirectionofleastcurvature.Inthiscase,themost\ncurvatureisinthedirection[1 ,1]andtheleastcurvatureisinthedirection[1 ,−1].The\nredlinesindicatethepathfollowedbygradientdescent.Thisveryelongatedquadratic\nfunctionresemblesalongcanyon.Gradientdescentwastestimerepeatedlydescending\ncanyonwalls,becausetheyarethesteepestfeature.Becausethestepsizeissomewhat\ntoolarge,ithasatendencytoovershootthebottomofthefunctionandthusneedsto\ndescendtheoppositecanyonwallonthenextiteration.Thelargepositiveeigenvalue\noftheHessiancorrespondingtotheeigenvectorpointedinthisdirectionindicatesthat\nthisdirectionalderivativeisrapidlyincreasing,soanoptimizationalgorithmbasedon\ntheHessiancouldpredictthatthesteepestdirectionisnotactuallyapromisingsearch\ndirectioninthiscontext.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 218, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1102}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0219_cf547c11", "text": "9 1\nCHAPTER4.NUMERICALCOMPUTATION\nthesearch.Thesimplestmethodfordoingsoisknownas Newt o n’ s m e t ho d. Newton’smethodisbasedonusingasecond-orderTaylorseriesexpansionto\napproximatenearsomepoint f() x x( 0 ):\nf f () x≈( x( 0 ))+( x x−( 0 ))∇ x f( x( 0 ))+1\n2( x x−( 0 ))H x()( f( 0 ))( x x−( 0 )) .(4.11)\nIfwethensolveforthecriticalpointofthisfunction,weobtain:\nx∗= x( 0 )− H x()( f( 0 ))− 1∇ x f( x( 0 )) . (4.12)\nWhen fisapositivedeﬁnitequadraticfunction,Newton’smethodconsistsof\napplyingequationoncetojumptotheminimumofthefunctiondirectly. 4.12\nWhen fisnottrulyquadraticbutcanbelocallyapproximatedasapositive\ndeﬁnitequadratic,Newton’smethodconsistsofapplyingequationmultiple4.12\ntimes.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 219, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 690}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0220_0e24e8b9", "text": "4.12\nWhen fisnottrulyquadraticbutcanbelocallyapproximatedasapositive\ndeﬁnitequadratic,Newton’smethodconsistsofapplyingequationmultiple4.12\ntimes. Iterativelyupdatingtheapproximation andjumpingtotheminimumof\ntheapproximation canreachthecriticalpointmuchfasterthangradientdescent\nwould.Thisisausefulpropertynearalocalminimum,butitcanbeaharmful\npropertynearasaddlepoint.Asdiscussedinsection,Newton’smethodis 8.2.3\nonlyappropriatewhenthenearbycriticalpointisaminimum(alltheeigenvalues\noftheHessianarepositive),whereasgradientdescentisnotattractedtosaddle\npointsunlessthegradientpointstowardthem. Optimization algorithmsthatuseonlythegradient,suchasgradientdescent,\narecalled ﬁr st - o r d e r o pt i m i z a t i o n al g o r i t hms.Optimization algorithmsthat\nalsousetheHessianmatrix,suchasNewton’smethod,arecalled se c o nd-or d e r\no pt i m i z a t i o n al g o r i t hms(NocedalandWright2006,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 220, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 894}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0221_90b96a6d", "text": "The optimization algorithms employedin mostcontextsin this book are\napplicabletoawidevarietyoffunctions,butcomewithalmostnoguarantees. Deeplearningalgorithmstendtolackguaranteesbecausethefamilyoffunctions\nusedindeeplearningisquitecomplicated.Inmanyotherﬁelds,thedominant\napproachtooptimization istodesignoptimization algorithmsforalimitedfamily\noffunctions. Inthecontextofdeeplearning,wesometimesgainsomeguaranteesbyrestrict-\ningourselvestofunctionsthatareeither L i psc hi t z c o n t i n uousorhaveLipschitz\ncontinuousderivatives.ALipschitzcontinuousfunctionisafunction fwhoserate\nofchangeisboundedbya L i psc hi t z c o nst antL:\n∀∀| − |≤L||−|| x , y , f() x f() y x y 2 .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 221, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 675}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0222_128cd58c", "text": "(4.13)\nThispropertyisusefulbecauseitallowsustoquantifyourassumptionthata\nsmallchangeintheinputmadebyanalgorithmsuchasgradientdescentwillhave\n9 2\nCHAPTER4.NUMERICALCOMPUTATION\nasmallchangeintheoutput.Lipschitzcontinuityisalsoafairlyweakconstraint,\nandmanyoptimizationproblemsindeeplearningcanbemadeLipschitzcontinuous\nwithrelativelyminormodiﬁcations. Perhapsthemostsuccessfulﬁeldofspecializedoptimization is c o n v e x o p-\nt i m i z at i o n.Convexoptimization algorithmsareabletoprovidemanymore\nguaranteesbymakingstrongerrestrictions.Convexoptimization algorithmsare\napplicableonlytoconvexfunctions—functionsforwhichtheHessianispositive\nsemideﬁniteeverywhere.Suchfunctionsarewell-behavedbecausetheylacksaddle\npointsandalloftheirlocalminimaarenecessarilyglobalminima.However,most\nproblemsindeeplearningarediﬃculttoexpressintermsofconvexoptimization. Convexoptimization isusedonlyasasubroutineofsomedeeplearningalgorithms.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 222, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 922}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0223_7ded96f6", "text": "Convexoptimization isusedonlyasasubroutineofsomedeeplearningalgorithms. Ideasfromtheanalysisofconvexoptimization algorithmscanbeusefulforproving\ntheconvergenceofdeeplearningalgorithms.However,ingeneral,theimportance\nofconvexoptimization isgreatlydiminishedinthecontextofdeeplearning.For\nmoreinformationaboutconvexoptimization, seeBoydandVandenberghe2004()\norRockafellar1997(). 4. 4 C on s t ra i n ed O p t i m i z a t i o n\nSometimeswewishnotonlytomaximizeorminimizeafunction f( x)overall\npossible values of x.Insteadwemay wishto ﬁnd themaximal or minimal\nvalue of f( x)for valuesof xinsome set S.Thisis known as c o nst r ai n e d\no pt i m i z a t i o n.Points xthatliewithintheset Sarecalled f e asi bl epointsin\nconstrainedoptimization terminology. Weoftenwishtoﬁndasolutionthatissmallinsomesense.Acommon\napproachinsuchsituationsistoimposeanormconstraint,suchas.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 223, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 866}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0224_31a06858", "text": "Weoftenwishtoﬁndasolutionthatissmallinsomesense.Acommon\napproachinsuchsituationsistoimposeanormconstraint,suchas. ||||≤ x 1\nOnesimpleapproachtoconstrainedoptimization issimplytomodifygradient\ndescenttakingtheconstraintintoaccount.Ifweuseasmallconstantstepsize ,\nwecanmakegradientdescentsteps,thenprojecttheresultbackinto S.Ifweuse\nalinesearch,wecansearchonlyoverstepsizes thatyieldnew xpointsthatare\nfeasible,orwecanprojecteachpointonthelinebackintotheconstraintregion.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 224, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 471}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0225_4e81bf24", "text": "Whenpossible,thismethodcanbemademoreeﬃcientbyprojectingthegradient\nintothetangentspaceofthefeasibleregionbeforetakingthesteporbeginning\nthelinesearch(,).Rosen1960\nAmoresophisticatedapproachistodesignadiﬀerent,unconstrainedopti-\nmizationproblemwhosesolutioncanbeconvertedintoasolutiontotheoriginal,\nconstrainedoptimization problem.Forexample,ifwewanttominimize f( x)for\n9 3\nCHAPTER4.NUMERICALCOMPUTATION\nx∈ R2with xconstrainedtohaveexactlyunit L2norm,wecaninsteadminimize\ng( θ) = f([cossin θ , θ])withrespectto θ,thenreturn[cossin θ , θ]asthesolution\ntotheoriginalproblem.Thisapproachrequirescreativity;thetransformation\nbetweenoptimization problemsmustbedesignedspeciﬁcallyforeachcasewe\nencounter. The K ar ush– K u h n – T uc k e r(KKT)approach1providesaverygeneralso-\nlutiontoconstrainedoptimization. WiththeKKTapproach,weintroducea\nnewfunctioncalledthe g e ner al i z e d L agr angi a nor g e ner al i z e d L agr ange\nf unc t i o n.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 225, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 937}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0226_cab441c2", "text": "WiththeKKTapproach,weintroducea\nnewfunctioncalledthe g e ner al i z e d L agr angi a nor g e ner al i z e d L agr ange\nf unc t i o n. TodeﬁnetheLagrangian,weﬁrstneedtodescribe Sintermsofequations\nandinequalities. W ewantadescriptionof Sintermsof mfunctions g( ) iand n\nfunctions h( ) jsothat S={|∀ x i , g( ) i( x) = 0and∀ j , h( ) j( x)≤0}.Theequations\ninvolving g( ) iarecalledthe e q ual i t y c o nst r ai n t sandtheinequalitiesinvolving\nh( ) jarecalled . i neq ual i t y c o nst r ai n t s\nWeintroducenewvariables λ iand α jforeachconstraint,thesearecalledthe\nKKTmultipliers.ThegeneralizedLagrangianisthendeﬁnedas\nL , , f ( x λ α) = ()+ x\niλ i g( ) i()+ x\njα j h( ) j() x .(4.14)\nWecannowsolveaconstrainedminimization problemusingunconstrained\noptimization ofthegeneralizedLagrangian.Observethat,solongasatleastone\nfeasiblepointexistsandisnotpermittedtohavevalue,then f() x ∞\nmin\nxmax\nλmax\nα α , ≥ 0L , , .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 226, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 914}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0227_124f9ed0", "text": "( x λ α) (4.15)\nhasthesameoptimalobjectivefunctionvalueandsetofoptimalpointsas x\nmin\nx ∈ Sf .() x (4.16)\nThisfollowsbecauseanytimetheconstraintsaresatisﬁed,\nmax\nλmax\nα α , ≥ 0L , , f , ( x λ α) = () x (4.17)\nwhileanytimeaconstraintisviolated,\nmax\nλmax\nα α , ≥ 0L , , . ( x λ α) = ∞ (4.18)\n1Th e K K T a p p ro a c h g e n e ra l i z e s t h e m e t h o d o f La gra n ge m u lt ip lie r s wh i c h a l l o ws e q u a l i t y\nc o n s t ra i n t s b u t n o t i n e q u a l i t y c o n s t ra i n t s . 9 4\nCHAPTER4.NUMERICALCOMPUTATION\nThesepropertiesguaranteethatnoinfeasiblepointcanbeoptimal,andthatthe\noptimumwithinthefeasiblepointsisunchanged.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 227, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 646}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0228_0b42eb47", "text": "9 4\nCHAPTER4.NUMERICALCOMPUTATION\nThesepropertiesguaranteethatnoinfeasiblepointcanbeoptimal,andthatthe\noptimumwithinthefeasiblepointsisunchanged. Toperformconstrainedmaximization, wecanconstructthegeneralizedLa-\ngrangefunctionof,whichleadstothisoptimization problem: − f() x\nmin\nxmax\nλmax\nα α , ≥ 0− f()+ x\niλ i g( ) i()+ x\njα j h( ) j() x .(4.19)\nWemayalsoconvertthistoaproblemwithmaximization intheouterloop:\nmax\nxmin\nλmin\nα α , ≥ 0f()+ x\niλ i g( ) i() x−\njα j h( ) j() x .(4.20)\nThesignofthetermfortheequalityconstraintsdoesnotmatter;wemaydeﬁneit\nwithadditionorsubtractionaswewish,becausetheoptimization isfreetochoose\nanysignforeach λ i.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 228, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 645}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0229_ed9a69cb", "text": "Theinequalityconstraintsareparticularlyinteresting.Wesaythataconstraint\nh( ) i( x)is ac t i v eif h( ) i( x∗) = 0.Ifaconstraintisnotactive,thenthesolutionto\ntheproblemfoundusingthatconstraintwouldremainatleastalocalsolutionif\nthatconstraintwereremoved.Itispossiblethataninactiveconstraintexcludes\nothersolutions.Forexample,aconvexproblemwithanentireregionofglobally\noptimalpoints(awide,ﬂat,regionofequalcost)couldhaveasubsetofthis\nregioneliminatedbyconstraints,oranon-convexproblemcouldhavebetterlocal\nstationarypointsexcludedbyaconstraintthatisinactiveatconvergence.However,\nthepointfoundatconvergenceremainsastationarypointwhetherornotthe\ninactiveconstraintsareincluded.Becauseaninactive h( ) ihasnegativevalue,then\nthesolutiontomin xmax λmax α α , ≥ 0 L( x λ α , ,)willhave α i=0.Wecanthus\nobservethatatthesolution, α h( x)= 0.Inotherwords,forall i,weknow\nthatatleastoneoftheconstraints α i≥0and h( ) i( x)≤0mustbeactiveatthe\nsolution.Togainsomeintuitionforthisidea,wecansaythateitherthesolution\nisontheboundaryimposedbytheinequalityandwemustuseitsKKTmultiplier\ntoinﬂuencethesolutionto x,ortheinequalityhasnoinﬂuenceonthesolution\nandwerepresentthisbyzeroingoutitsKKTmultiplier.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 229, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1181}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0230_c3847991", "text": "Asimplesetofpropertiesdescribetheoptimalpointsofconstrainedopti-\nmizationproblems.ThesepropertiesarecalledtheKarush-Kuhn-Tucker(KKT)\nconditions(,;Karush1939KuhnandTucker1951,).Theyarenecessaryconditions,\nbutnotalwayssuﬃcientconditions,forapointtobeoptimal.Theconditionsare:\n•ThegradientofthegeneralizedLagrangianiszero. •AllconstraintsonbothandtheKKTmultipliersaresatisﬁed. x\n9 5\nCHAPTER4.NUMERICALCOMPUTATION\n•Theinequalityconstraintsexhibit“complementary slackness”: α h( x) = 0. FormoreinformationabouttheKKTapproach,seeNocedalandWright2006(). 4. 5 E x am p l e: L i n ear L eas t S q u are s\nSupposewewanttoﬁndthevalueofthatminimizes x\nf() = x1\n2||−|| A x b2\n2 . (4.21)\nTherearespecializedlinearalgebraalgorithmsthatcansolvethisproblemeﬃciently. However,wecanalsoexplorehowtosolveitusinggradient-basedoptimization as\nasimpleexampleofhowthesetechniqueswork. First,weneedtoobtainthegradient:\n∇ x f() = x A( ) = A x b− AA x A−b .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 230, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 934}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0231_74d721d9", "text": "However,wecanalsoexplorehowtosolveitusinggradient-basedoptimization as\nasimpleexampleofhowthesetechniqueswork. First,weneedtoobtainthegradient:\n∇ x f() = x A( ) = A x b− AA x A−b . (4.22)\nWecanthenfollowthisgradientdownhill,takingsmallsteps.Seealgorithm4.1\nfordetails. Al g o r i t hm 4 . 1Analgorithmtominimize f( x) =1\n2||−|| A x b2\n2withrespectto x\nusinggradientdescent,startingfromanarbitraryvalueof. x\nSetthestepsize()andtolerance()tosmall,positivenumbers.  δ\nwhi l e|| AA x A−b|| 2 > δ do\nx x← − \nAA x A−b\ne nd whi l e\nOnecanalsosolvethisproblemusingNewton’smethod.Inthiscase,because\nthetruefunctionisquadratic,thequadraticapproximation employedbyNewton’s\nmethodisexact,andthealgorithmconvergestotheglobalminimuminasingle\nstep. Nowsuppose we wishto minimizethesame function,butsubjectto the\nconstraint xx≤1.Todoso,weintroducetheLagrangian\nL , λ f λ ( x) = ()+ x\nxx−1\n. (4.23)\nWecannowsolvetheproblem\nmin\nxmax\nλ , λ ≥ 0L , λ .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 231, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 946}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0232_1ce39043", "text": "Nowsuppose we wishto minimizethesame function,butsubjectto the\nconstraint xx≤1.Todoso,weintroducetheLagrangian\nL , λ f λ ( x) = ()+ x\nxx−1\n. (4.23)\nWecannowsolvetheproblem\nmin\nxmax\nλ , λ ≥ 0L , λ . ( x) (4.24)\n9 6\nCHAPTER4.NUMERICALCOMPUTATION\nThesmallest-normsolutiontotheunconstrainedleastsquaresproblemmaybe\nfoundusingtheMoore-Penrosepseudoinverse: x= A+b.Ifthispointisfeasible,\nthenitisthesolutiontotheconstrainedproblem.Otherwise,wemustﬁnda\nsolutionwheretheconstraintisactive.Bydiﬀerentiating theLagrangianwith\nrespectto,weobtaintheequation x\nAA x A−b x+2 λ= 0 . (4.25)\nThistellsusthatthesolutionwilltaketheform\nx A= (A I+2 λ)− 1Ab . (4.26)\nThemagnitudeof λmustbechosensuchthattheresultobeystheconstraint.We\ncanﬁndthisvaluebyperforminggradientascenton.Todoso,observe λ\n∂\n∂ λL , λ( x) = xx−1 .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 232, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 806}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0233_9a55d93c", "text": "(4.26)\nThemagnitudeof λmustbechosensuchthattheresultobeystheconstraint.We\ncanﬁndthisvaluebyperforminggradientascenton.Todoso,observe λ\n∂\n∂ λL , λ( x) = xx−1 . (4.27)\nWhenthenormof xexceeds1,thisderivativeispositive,sotofollowthederivative\nuphillandincreasetheLagrangianwithrespectto λ,weincrease λ.Becausethe\ncoeﬃcientonthe xxpenaltyhasincreased,solvingthelinearequationfor xwill\nnowyieldasolutionwithsmallernorm.Theprocessofsolvingthelinearequation\nandadjusting λcontinuesuntil xhasthecorrectnormandthederivativeon λis\n0. Thisconcludesthemathematical preliminaries thatweusetodevelopmachine\nlearningalgorithms.Wearenowreadytobuildandanalyzesomefull-ﬂedged\nlearningsystems.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 233, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 675}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0234_d5be9e79", "text": "Thisconcludesthemathematical preliminaries thatweusetodevelopmachine\nlearningalgorithms.Wearenowreadytobuildandanalyzesomefull-ﬂedged\nlearningsystems. 9 7\nC h a p t e r 5\nMac h i n e L e ar n i n g B asics\nDeeplearningisaspeciﬁckindofmachinelearning.Inordertounderstand\ndeeplearningwell,onemusthaveasolidunderstandingofthebasicprinciplesof\nmachinelearning.Thischapterprovidesabriefcourseinthemostimportantgeneral\nprinciplesthatwillbeappliedthroughouttherestofthebook.Novicereadersor\nthosewhowantawiderperspectiveareencouragedtoconsidermachinelearning\ntextbookswithamorecomprehensivecoverageofthefundamentals,suchasMurphy\n()or().Ifyouarealreadyfamiliarwithmachinelearningbasics, 2012Bishop2006\nfeelfreetoskipaheadtosection.Thatsectioncoverssomeperspectives 5.11\non traditional machinelearning techniques thathavestrongly inﬂuenced the\ndevelopmentofdeeplearningalgorithms. Webeginwithadeﬁnitionofwhatalearningalgorithmis,andpresentan\nexample:thelinearregressionalgorithm.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 234, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 969}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0235_2772bbae", "text": "Webeginwithadeﬁnitionofwhatalearningalgorithmis,andpresentan\nexample:thelinearregressionalgorithm. W ethenproceedtodescribehowthe\nchallengeofﬁttingthetrainingdatadiﬀersfromthechallengeofﬁndingpatterns\nthatgeneralizetonewdata.Mostmachinelearningalgorithmshavesettings\ncalledhyperparametersthatmustbedeterminedexternaltothelearningalgorithm\nitself;wediscusshowtosettheseusingadditionaldata.Machinelearningis\nessentiallyaformofappliedstatisticswithincreasedemphasisontheuseof\ncomputerstostatisticallyestimatecomplicatedfunctionsandadecreasedemphasis\nonprovingconﬁdenceintervalsaroundthesefunctions;wethereforepresentthe\ntwocentralapproachestostatistics:frequentistestimatorsandBayesianinference. Mostmachinelearningalgorithmscanbedividedintothecategoriesofsupervised\nlearningandunsupervisedlearning;wedescribethesecategoriesandgivesome\nexamplesofsimplelearningalgorithmsfromeachcategory.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 235, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 884}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0236_728455d3", "text": "Mostmachinelearningalgorithmscanbedividedintothecategoriesofsupervised\nlearningandunsupervisedlearning;wedescribethesecategoriesandgivesome\nexamplesofsimplelearningalgorithmsfromeachcategory. Mostdeeplearning\nalgorithmsare basedonan optimization algorithmcalled stochasticgradient\ndescent.Wedescribehowtocombinevariousalgorithmcomponentssuchas\n98\nCHAPTER5.MACHINELEARNINGBASICS\nanoptimization algorithm,acostfunction,amodel,andadatasettobuilda\nmachinelearningalgorithm.Finally,insection,wedescribesomeofthe 5.11\nfactorsthathavelimitedtheabilityoftraditionalmachinelearningtogeneralize. Thesechallengeshavemotivatedthedevelopmentofdeeplearningalgorithmsthat\novercometheseobstacles.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 236, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 680}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0237_9f1ac504", "text": "Thesechallengeshavemotivatedthedevelopmentofdeeplearningalgorithmsthat\novercometheseobstacles. 5.1LearningAlgorithms\nAmachinelearningalgorithmisanalgorithmthatisabletolearnfromdata.But\nwhatdowemeanbylearning?Mitchell1997()providesthedeﬁnition“Acomputer\nprogramissaidtolearnfromexperienceEwithrespecttosomeclassoftasksT\nandperformancemeasureP,ifitsperformanceattasksinT,asmeasuredbyP,\nimproveswithexperienceE.”Onecanimagineaverywidevarietyofexperiences\nE,tasksT,andperformancemeasuresP,andwedonotmakeanyattemptinthis\nbooktoprovideaformaldeﬁnitionofwhatmaybeusedforeachoftheseentities. Instead,thefollowingsectionsprovideintuitivedescriptionsandexamplesofthe\ndiﬀerentkindsoftasks,performance measuresandexperiencesthatcanbeused\ntoconstructmachinelearningalgorithms.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 237, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 763}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0238_fc075756", "text": "Instead,thefollowingsectionsprovideintuitivedescriptionsandexamplesofthe\ndiﬀerentkindsoftasks,performance measuresandexperiencesthatcanbeused\ntoconstructmachinelearningalgorithms. 5.1.1TheTask, T\nMachinelearningallowsustotackletasksthataretoodiﬃculttosolvewith\nﬁxedprogramswrittenanddesignedbyhumanbeings.Fromascientiﬁcand\nphilosophicalpointofview,machinelearningisinterestingbecausedevelopingour\nunderstandingofmachinelearningentailsdevelopingourunderstandingofthe\nprinciplesthatunderlieintelligence. Inthisrelativelyformaldeﬁnitionoftheword“task,”theprocessoflearning\nitselfisnotthetask.Learningisourmeansofattainingtheabilitytoperformthe\ntask.Forexample,ifwewantarobottobeabletowalk,thenwalkingisthetask. Wecouldprogramtherobottolearntowalk,orwecouldattempttodirectlywrite\naprogramthatspeciﬁeshowtowalkmanually.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 238, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 814}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0239_c6c009f9", "text": "Wecouldprogramtherobottolearntowalk,orwecouldattempttodirectlywrite\naprogramthatspeciﬁeshowtowalkmanually. Machinelearningtasksareusuallydescribedintermsofhowthemachine\nlearningsystemshouldprocessanexample.Anexampleisacollectionoffeatures\nthathavebeenquantitativelymeasuredfromsomeobjectoreventthatwewant\nthemachinelearningsystemtoprocess.Wetypicallyrepresentanexampleasa\nvectorx∈ Rnwhereeachentryx iofthevectorisanotherfeature.Forexample,\nthefeaturesofanimageareusuallythevaluesofthepixelsintheimage.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 239, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 501}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0240_4eff87c5", "text": "9 9\nCHAPTER5.MACHINELEARNINGBASICS\nManykindsoftaskscanbesolvedwithmachinelearning.Someofthemost\ncommonmachinelearningtasksincludethefollowing:\n•Classiﬁcation:Inthistypeoftask,thecomputerprogramisaskedtospecify\nwhichofkcategoriessomeinputbelongsto.Tosolvethistask,thelearning\nalgorithmisusuallyaskedtoproduceafunctionf: Rn→{1,...,k}.When\ny=f(x),themodelassignsaninputdescribedbyvectorxtoacategory\nidentiﬁedbynumericcodey.Thereareothervariantsoftheclassiﬁcation\ntask,forexample,wherefoutputsaprobabilitydistributionoverclasses.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 240, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 525}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0241_30d681ca", "text": "Anexampleofaclassiﬁcationtaskisobjectrecognition,wheretheinput\nisanimage(usuallydescribedasasetofpixelbrightnessvalues),andthe\noutputisanumericcodeidentifyingtheobjectintheimage.Forexample,\ntheWillowGaragePR2robotisabletoactasawaiterthatcanrecognize\ndiﬀerentkindsofdrinksanddeliverthemtopeopleoncommand(Good-\nfellow2010etal.,).Modernobjectrecognitionisbestaccomplishedwith\ndeeplearning( ,; ,).Object Krizhevskyetal.2012IoﬀeandSzegedy2015\nrecognitionisthesamebasictechnologythatallowscomputerstorecognize\nfaces(Taigman 2014etal.,),whichcanbeusedtoautomatically tagpeople\ninphotocollectionsandallowcomputerstointeractmorenaturallywith\ntheirusers.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 241, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 644}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0242_48f797ba", "text": "•Classiﬁcationwithmissinginputs:Classiﬁcationbecomesmorechal-\nlengingifthecomputerprogramisnotguaranteedthateverymeasurement\ninitsinputvectorwillalwaysbeprovided.Inordertosolvetheclassiﬁcation\ntask,thelearningalgorithmonlyhastodeﬁneafunctionmapping single\nfromavectorinputtoacategoricaloutput.Whensomeoftheinputsmay\nbemissing,ratherthanprovidingasingleclassiﬁcationfunction,thelearning\nalgorithmmustlearnaoffunctions.Eachfunctioncorrespondstoclassi- set\nfyingxwithadiﬀerentsubsetofitsinputsmissing.Thiskindofsituation\narisesfrequentlyinmedicaldiagnosis,becausemanykindsofmedicaltests\nareexpensiveorinvasive.Onewaytoeﬃcientlydeﬁnesuchalargeset\noffunctionsistolearnaprobabilitydistributionoveralloftherelevant\nvariables,thensolvetheclassiﬁcationtaskbymarginalizing outthemissing\nvariables.Withninputvariables,wecannowobtainall2ndiﬀerentclassiﬁ-\ncationfunctionsneededforeachpossiblesetofmissinginputs,butweonly\nneedtolearnasinglefunctiondescribingthejointprobabilitydistribution.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 242, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 976}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0243_a4ee00f1", "text": "SeeGoodfellow2013betal.()foranexampleofadeepprobabilisticmodel\nappliedtosuchataskinthisway.Manyoftheothertasksdescribedinthis\nsectioncanalsobegeneralizedtoworkwithmissinginputs;classiﬁcation\nwithmissinginputsisjustoneexampleofwhatmachinelearningcando. 1 0 0\nCHAPTER5.MACHINELEARNINGBASICS\n•Regression:Inthistypeoftask,thecomputerprogramisaskedtopredicta\nnumericalvaluegivensomeinput.Tosolvethistask,thelearningalgorithm\nisaskedtooutputafunctionf: Rn→ R.Thistypeoftaskissimilarto\nclassiﬁcation,exceptthattheformatofoutputisdiﬀerent.Anexampleof\naregressiontaskisthepredictionoftheexpectedclaimamountthatan\ninsuredpersonwillmake(usedtosetinsurancepremiums),ortheprediction\noffuturepricesofsecurities.Thesekindsofpredictionsarealsousedfor\nalgorithmictrading.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 243, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 754}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0244_5d49877d", "text": "•Transcription:Inthistypeoftask,themachinelearningsystemisasked\ntoobservearelativelyunstructuredrepresentationofsomekindofdataand\ntranscribeitintodiscrete,textualform.Forexample,inopticalcharacter\nrecognition,thecomputerprogramisshownaphotographcontainingan\nimageoftextandisaskedtoreturnthistextintheformofasequence\nofcharacters(e.g.,inASCIIorUnicodeformat).GoogleStreetViewuses\ndeeplearningtoprocessaddressnumbersinthisway( , Goodfellow etal. 2014d).Anotherexampleisspeechrecognition,wherethecomputerprogram\nisprovidedanaudiowaveformandemitsasequenceofcharactersorword\nIDcodesdescribingthewordsthatwerespokenintheaudiorecording.Deep\nlearningisacrucialcomponentofmodernspeechrecognitionsystemsused\natmajorcompaniesincludingMicrosoft,IBMandGoogle( ,Hintonetal. 2012b).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 244, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 767}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0245_821aaf3c", "text": "2012b). •Machinetranslation:Inamachinetranslationtask,theinputalready\nconsistsofasequenceofsymbolsinsomelanguage,andthecomputerprogram\nmustconvertthisintoasequenceofsymbolsinanotherlanguage.Thisis\ncommonlyappliedtonaturallanguages,suchastranslatingfromEnglishto\nFrench.Deeplearninghasrecentlybeguntohaveanimportantimpacton\nthiskindoftask(Sutskever2014Bahdanau 2015 etal.,; etal.,). •Structuredoutput:Structuredoutputtasksinvolveanytaskwherethe\noutputisavector(orotherdatastructurecontainingmultiplevalues)with\nimportantrelationshipsbetweenthediﬀerentelements.Thisisabroad\ncategory,andsubsumesthetranscriptionandtranslationtasksdescribed\nabove,butalsomanyothertasks.Oneexampleisparsing—mappinga\nnaturallanguagesentenceintoatreethatdescribesitsgrammaticalstructure\nandtaggingnodesofthetreesasbeingverbs,nouns,oradverbs,andsoon.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 245, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 825}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0246_85d23fb0", "text": "See ()foranexampleofdeeplearningappliedtoaparsing Collobert2011\ntask.Anotherexampleispixel-wisesegmentationofimages, wherethe\ncomputerprogramassignseverypixelinanimagetoaspeciﬁccategory.For\n1 0 1\nCHAPTER5.MACHINELEARNINGBASICS\nexample,deeplearningcanbeusedtoannotatethelocationsofroadsin\naerialphotographs(MnihandHinton2010,).Theoutputneednothaveits\nformmirrorthestructureoftheinputascloselyasintheseannotation-style\ntasks.Forexample,inimagecaptioning,thecomputerprogramobservesan\nimageandoutputsanaturallanguagesentencedescribingtheimage(Kiros\netal. etal. ,,;2014abMao,;2015Vinyals2015bDonahue2014 etal.,; etal.,;\nKarpathyandLi2015Fang2015Xu2015 ,;etal.,;etal.,).Thesetasksare\ncalledstructuredoutputtasksbecausetheprogrammustoutputseveral\nvaluesthatarealltightlyinter-related.Forexample,thewordsproducedby\nanimagecaptioningprogrammustformavalidsentence.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 246, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 854}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0247_f2b762ba", "text": "•Anomalydetection:Inthistypeoftask,thecomputerprogramsifts\nthroughasetofeventsorobjects,andﬂagssomeofthemasbeingunusual\noratypical.Anexampleofananomalydetectiontaskiscreditcardfraud\ndetection.Bymodelingyourpurchasinghabits,acreditcardcompanycan\ndetectmisuseofyourcards.Ifathiefstealsyourcreditcardorcreditcard\ninformation,thethief’spurchaseswilloftencomefromadiﬀerentprobability\ndistributionoverpurchasetypesthanyourown.Thecreditcardcompany\ncanpreventfraudbyplacingaholdonanaccountassoonasthatcardhas\nbeenusedforanuncharacteris ticpurchase.See ()fora Chandola etal.2009\nsurveyofanomalydetectionmethods. •Synthesisandsampling:Inthistypeoftask,themachinelearningal-\ngorithmisaskedtogeneratenewexamplesthataresimilartothoseinthe\ntrainingdata.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 247, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 739}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0248_6dd16ddc", "text": "•Synthesisandsampling:Inthistypeoftask,themachinelearningal-\ngorithmisaskedtogeneratenewexamplesthataresimilartothoseinthe\ntrainingdata. Synthesisandsamplingviamachinelearningcanbeuseful\nformediaapplicationswhereitcanbeexpensiveorboringforanartistto\ngeneratelargevolumesofcontentbyhand.Forexample,videogamescan\nautomatically generatetexturesforlargeobjectsorlandscapes,ratherthan\nrequiringanartisttomanuallylabeleachpixel(,).Insome Luoetal.2013\ncases,wewantthesamplingorsynthesisproceduretogeneratesomespeciﬁc\nkindofoutputgiventheinput.Forexample,inaspeechsynthesistask,we\nprovideawrittensentenceandasktheprogramtoemitanaudiowaveform\ncontainingaspokenversionofthatsentence. Thisisakindofstructured\noutputtask,butwiththeaddedqualiﬁcationthatthereisnosinglecorrect\noutputforeachinput,andweexplicitlydesirealargeamountofvariationin\ntheoutput,inorderfortheoutputtoseemmorenaturalandrealistic.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 248, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 888}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0249_8a5c7f49", "text": "•Imputationofmissingvalues:Inthistypeoftask,themachinelearning\nalgorithmisgivenanewexamplex∈ Rn,butwithsomeentriesx iofx\nmissing.Thealgorithmmustprovideapredictionofthevaluesofthemissing\nentries. 1 0 2\nCHAPTER5.MACHINELEARNINGBASICS\n•Denoising:Inthistypeoftask,themachinelearningalgorithmisgivenin\ninputacorruptedexample˜x∈ Rnobtainedbyanunknowncorruptionprocess\nfromacleanexamplex∈ Rn.Thelearnermustpredictthecleanexample\nxfromitscorruptedversion˜x,ormoregenerallypredicttheconditional\nprobabilitydistributionp(x|˜x). •Densityestimationorprobabilitymassfunctionestimation:In\nthedensityestimationproblem,themachinelearningalgorithmisasked\ntolearnafunctionpmodel: Rn→ R,wherepmodel(x)canbeinterpreted\nasaprobabilitydensityfunction(if xiscontinuous)oraprobabilitymass\nfunction(if xisdiscrete)onthespacethattheexamplesweredrawnfrom.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 249, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 829}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0250_9cbbe5a8", "text": "Todosuchataskwell(wewillspecifyexactlywhatthatmeanswhenwe\ndiscussperformancemeasuresP),thealgorithmneedstolearnthestructure\nofthedataithasseen.Itmustknowwhereexamplesclustertightlyand\nwheretheyareunlikelytooccur.Mostofthetasksdescribedaboverequire\nthelearningalgorithmtoatleastimplicitlycapturethestructureofthe\nprobabilitydistribution.Densityestimationallowsustoexplicitlycapture\nthatdistribution.Inprinciple,wecanthenperformcomputations onthat\ndistributioninordertosolvetheothertasksaswell.Forexample,ifwe\nhaveperformeddensityestimationtoobtainaprobabilitydistributionp(x),\nwecanusethatdistributiontosolvethemissingvalueimputationtask.If\navaluex iismissingandalloftheothervalues,denotedx − i,aregiven,\nthenweknowthedistributionoveritisgivenbyp(x i|x − i).Inpractice,\ndensityestimationdoesnotalwaysallowustosolvealloftheserelatedtasks,\nbecauseinmanycasestherequiredoperationsonp(x)arecomputationally\nintractable.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 250, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 913}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0251_9a0ea6a0", "text": "Ofcourse,manyothertasksandtypesoftasksarepossible.Thetypesoftasks\nwelisthereareintendedonlytoprovideexamplesofwhatmachinelearningcan\ndo,nottodeﬁnearigidtaxonomyoftasks. 5.1.2ThePerformanceMeasure, P\nInordertoevaluatetheabilitiesofamachinelearningalgorithm,wemustdesign\naquantitativemeasureofitsperformance.UsuallythisperformancemeasurePis\nspeciﬁctothetaskbeingcarriedoutbythesystem.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 251, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 382}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0252_975e25f2", "text": "T\nFortaskssuchasclassiﬁcation,classiﬁcationwithmissinginputs,andtran-\nscription,weoftenmeasuretheaccuracyofthemodel.Accuracyisjustthe\nproportionofexamplesforwhichthemodelproducesthecorrectoutput.Wecan\n1 0 3\nCHAPTER5.MACHINELEARNINGBASICS\nalsoobtainequivalentinformationbymeasuringtheerrorrate,theproportion\nofexamplesforwhichthemodelproducesanincorrectoutput.Weoftenreferto\ntheerrorrateastheexpected0-1loss.The0-1lossonaparticularexampleis0\nifitiscorrectlyclassiﬁedand1ifitisnot.Fortaskssuchasdensityestimation,\nitdoesnotmakesensetomeasureaccuracy,errorrate,oranyotherkindof0-1\nloss.Instead,wemustuseadiﬀerentperformancemetricthatgivesthemodel\nacontinuous-valuedscoreforeachexample.Themostcommonapproachisto\nreporttheaveragelog-probabilit ythemodelassignstosomeexamples.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 252, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 770}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0253_e2c7886a", "text": "Usuallyweareinterestedinhowwellthemachinelearningalgorithmperforms\nondatathatithasnotseenbefore,sincethisdetermineshowwellitwillworkwhen\ndeployedintherealworld.Wethereforeevaluatetheseperformancemeasuresusing\natestsetofdatathatisseparatefromthedatausedfortrainingthemachine\nlearningsystem. Thechoiceofperformancemeasuremayseemstraightforwardandobjective,\nbutitisoftendiﬃculttochooseaperformancemeasurethatcorrespondswellto\nthedesiredbehaviorofthesystem. Insomecases,thisisbecauseitisdiﬃculttodecidewhatshouldbemeasured. Forexample,whenperformingatranscriptiontask,shouldwemeasuretheaccuracy\nofthesystemattranscribingentiresequences,orshouldweuseamoreﬁne-grained\nperformancemeasurethatgivespartialcreditforgettingsomeelementsofthe\nsequencecorrect?Whenperformingaregressiontask,shouldwepenalizethe\nsystemmoreifitfrequentlymakesmedium-sizedmistakesorifitrarelymakes\nverylargemistakes?Thesekindsofdesignchoicesdependontheapplication.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 253, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 929}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0254_185c06a5", "text": "Inothercases,weknowwhatquantitywewouldideallyliketomeasure,but\nmeasuringitisimpractical.Forexample,thisarisesfrequentlyinthecontextof\ndensityestimation.Manyofthebestprobabilisticmodelsrepresentprobability\ndistributionsonlyimplicitly.Computingtheactualprobabilityvalueassignedto\naspeciﬁcpointinspaceinmanysuchmodelsisintractable.Inthesecases,one\nmustdesignanalternativecriterionthatstillcorrespondstothedesignobjectives,\nordesignagoodapproximationtothedesiredcriterion. 5.1.3TheExperience, E\nMachinelearningalgorithmscanbebroadlycategorizedasunsupervisedor\nsupervisedbywhatkindofexperiencetheyareallowedtohaveduringthe\nlearningprocess. Mostofthelearningalgorithmsinthisbookcanbeunderstoodasbeingallowed\ntoexperienceanentiredataset.Adatasetisacollectionofmanyexamples,as\n1 0 4\nCHAPTER5.MACHINELEARNINGBASICS\ndeﬁnedinsection.Sometimeswewillalsocallexamples .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 254, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 855}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0255_91d848ee", "text": "5.1.1 datapoints\nOneoftheoldestdatasetsstudiedbystatisticiansandmachinelearningre-\nsearchersistheIrisdataset(,).Itisacollectionofmeasurementsof Fisher1936\ndiﬀerentpartsof150irisplants.Eachindividualplantcorrespondstooneexample. Thefeatureswithineachexamplearethemeasurementsofeachofthepartsofthe\nplant:thesepallength,sepalwidth,petallengthandpetalwidth.Thedataset\nalsorecordswhichspecieseachplantbelongedto.Threediﬀerentspeciesare\nrepresentedinthedataset. Unsupervisedlearningalgorithmsexperienceadatasetcontainingmany\nfeatures,thenlearnusefulpropertiesofthestructureofthisdataset.Inthecontext\nofdeeplearning,weusuallywanttolearntheentireprobabilitydistributionthat\ngeneratedadataset,whetherexplicitlyasindensityestimationorimplicitlyfor\ntaskslikesynthesisordenoising.Someotherunsupervisedlearningalgorithms\nperformotherroles,likeclustering,whichconsistsofdividingthedatasetinto\nclustersofsimilarexamples.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 255, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 905}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0256_43082701", "text": "Supervisedlearningalgorithmsexperienceadatasetcontainingfeatures,\nbuteachexampleisalsoassociatedwithalabelortarget.Forexample,theIris\ndatasetisannotatedwiththespeciesofeachirisplant.Asupervisedlearning\nalgorithmcanstudytheIrisdatasetandlearntoclassifyirisplantsintothree\ndiﬀerentspeciesbasedontheirmeasurements. Roughlyspeaking,unsupervisedlearninginvolvesobservingseveralexamples\nofarandomvector x,andattemptingtoimplicitlyorexplicitlylearntheproba-\nbilitydistributionp( x),orsomeinterestingpropertiesofthatdistribution,while\nsupervisedlearninginvolvesobservingseveralexamplesofarandomvector xand\nanassociatedvalueorvector y,andlearningtopredict yfrom x,usuallyby\nestimatingp( y x|).Thetermsupervisedlearningoriginatesfromtheviewof\nthetarget ybeingprovidedbyaninstructororteacherwhoshowsthemachine\nlearningsystemwhattodo.Inunsupervisedlearning,thereisnoinstructoror\nteacher,andthealgorithmmustlearntomakesenseofthedatawithoutthisguide.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 256, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 936}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0257_42db2dda", "text": "Unsupervisedlearningandsupervisedlearningarenotformallydeﬁnedterms. Thelinesbetweenthemareoftenblurred.Manymachinelearningtechnologiescan\nbeusedtoperformbothtasks.Forexample,thechainruleofprobabilitystates\nthatforavector x∈ Rn,thejointdistributioncanbedecomposedas\np() = xn\ni=1p(x i|x1,...,x i −1). (5.1)\nThisdecompositionmeansthatwecansolvetheostensiblyunsupervisedproblemof\nmodelingp( x) bysplittingitintonsupervisedlearningproblems.Alternatively,we\n1 0 5\nCHAPTER5.MACHINELEARNINGBASICS\ncansolvethesupervisedlearningproblemoflearningp(y| x)byusingtraditional\nunsupervised learningtechnologiesto learn thejointdistributionp( x,y)and\ninferring\npy(| x) =p,y( x)\nyp,y( x).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 257, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 674}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0258_923d6fd2", "text": "(5.2)\nThoughunsupervisedlearningandsupervisedlearningarenotcompletelyformalor\ndistinctconcepts,theydohelptoroughlycategorizesomeofthethingswedowith\nmachinelearningalgorithms.Traditionally,peoplerefertoregression,classiﬁcation\nandstructuredoutputproblemsassupervisedlearning.Densityestimationin\nsupportofothertasksisusuallyconsideredunsupervisedlearning. Othervariantsofthelearningparadigmarepossible.Forexample,insemi-\nsupervisedlearning,someexamplesincludeasupervisiontargetbutothersdo\nnot.Inmulti-instancelearning,anentirecollectionofexamplesislabeledas\ncontainingornotcontaininganexampleofaclass,buttheindividualmembers\nofthecollectionarenotlabeled.Forarecentexampleofmulti-instancelearning\nwithdeepmodels,seeKotzias 2015etal.(). Somemachinelearningalgorithmsdonotjustexperienceaﬁxeddataset.For\nexample,reinforcementlearningalgorithmsinteractwithanenvironment,so\nthereisafeedbackloopbetweenthelearningsystemanditsexperiences.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 258, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 928}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0259_f3b83a17", "text": "Somemachinelearningalgorithmsdonotjustexperienceaﬁxeddataset.For\nexample,reinforcementlearningalgorithmsinteractwithanenvironment,so\nthereisafeedbackloopbetweenthelearningsystemanditsexperiences. Such\nalgorithmsarebeyondthescopeofthisbook.Pleasesee () SuttonandBarto1998\norBertsekasandTsitsiklis1996()forinformationaboutreinforcementlearning,\nand ()forthedeeplearningapproachtoreinforcementlearning. Mnihetal.2013\nMostmachinelearningalgorithmssimplyexperienceadataset.Adatasetcan\nbedescribedinmanyways.Inallcases,adatasetisacollectionofexamples,\nwhichareinturncollectionsoffeatures.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 259, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 582}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0260_d9bf6fc5", "text": "Mnihetal.2013\nMostmachinelearningalgorithmssimplyexperienceadataset.Adatasetcan\nbedescribedinmanyways.Inallcases,adatasetisacollectionofexamples,\nwhichareinturncollectionsoffeatures. Onecommonwayofdescribingadatasetiswitha .Adesign designmatrix\nmatrixisamatrixcontainingadiﬀerentexampleineachrow.Eachcolumnofthe\nmatrixcorrespondstoadiﬀerentfeature.Forinstance,theIrisdatasetcontains\n150exampleswithfourfeaturesforeachexample.Thismeanswecanrepresent\nthedatasetwithadesignmatrixX∈ R1504 ×,whereX i ,1isthesepallengthof\nplanti,X i ,2isthesepalwidthofplanti,etc.Wewilldescribemostofthelearning\nalgorithmsinthisbookintermsofhowtheyoperateondesignmatrixdatasets. Ofcourse,todescribeadatasetasadesignmatrix,itmustbepossibleto\ndescribeeachexampleasavector,andeachofthesevectorsmustbethesamesize.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 260, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 787}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0261_7da548c4", "text": "Ofcourse,todescribeadatasetasadesignmatrix,itmustbepossibleto\ndescribeeachexampleasavector,andeachofthesevectorsmustbethesamesize. Thisisnotalwayspossible.Forexample,ifyouhaveacollectionofphotographs\nwithdiﬀerentwidthsandheights,thendiﬀerentphotographswillcontaindiﬀerent\nnumbersofpixels,sonotallofthephotographs maybedescribedwiththesame\nlengthofvector.Sectionandchapterdescribehowtohandlediﬀerent 9.7 10\n1 0 6\nCHAPTER5.MACHINELEARNINGBASICS\ntypesofsuchheterogeneous data.Incaseslikethese,ratherthandescribingthe\ndatasetasamatrixwithmrows,wewilldescribeitasasetcontainingmelements:\n{x(1),x(2),...,x() m}.Thisnotationdoesnotimplythatanytwoexamplevectors\nx() iandx() jhavethesamesize.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 261, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 683}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0262_c2984f67", "text": "Inthecaseofsupervisedlearning,theexamplecontainsalabelortargetas\nwellasacollectionoffeatures.Forexample,ifwewanttousealearningalgorithm\ntoperformobjectrecognitionfromphotographs, weneedtospecifywhichobject\nappearsineachofthephotos.Wemightdothiswithanumericcode,with0\nsignifyingaperson,1signifyingacar,2signifyingacat,etc.Oftenwhenworking\nwithadatasetcontainingadesignmatrixoffeatureobservationsX,wealso\nprovideavectoroflabels,withyy iprovidingthelabelforexample.i\nOfcourse,sometimesthelabelmaybemorethanjustasinglenumber.For\nexample,ifwewanttotrainaspeechrecognitionsystemtotranscribeentire\nsentences,thenthelabelforeachexamplesentenceisasequenceofwords. Justasthereisnoformaldeﬁnitionofsupervisedandunsupervisedlearning,\nthereisnorigidtaxonomyofdatasetsorexperiences.Thestructuresdescribedhere\ncovermostcases,butitisalwayspossibletodesignnewonesfornewapplications.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 262, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 865}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0263_57b4d1e3", "text": "5.1.4Example:LinearRegression\nOurdeﬁnitionofamachinelearningalgorithmasanalgorithmthatiscapable\nofimprovingacomputerprogram’sperformanceatsometaskviaexperienceis\nsomewhatabstract.Tomakethismoreconcrete,wepresentanexampleofa\nsimplemachinelearningalgorithm:linearregression.Wewillreturntothis\nexamplerepeatedlyasweintroducemoremachinelearningconceptsthathelpto\nunderstanditsbehavior. Asthenameimplies,linearregressionsolvesaregressionproblem. Inother\nwords,thegoalistobuildasystemthatcantakeavectorx∈ Rnasinputand\npredictthevalueofascalary∈ Rasitsoutput.Inthecaseoflinearregression,\ntheoutputisalinearfunctionoftheinput.Letˆybethevaluethatourmodel\npredictsshouldtakeon.Wedeﬁnetheoutputtobe y\nˆy= wx (5.3)\nwherew∈ Rnisavectorof .parameters\nParametersarevaluesthatcontrolthebehaviorofthesystem.Inthiscase,w iis\nthecoeﬃcientthatwemultiplybyfeaturex ibeforesummingupthecontributions\nfromallthefeatures.Wecanthinkofwasasetofweightsthatdeterminehow\neachfeatureaﬀectstheprediction.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 263, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 973}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0264_8c836f09", "text": "If afeaturex ireceivesapositiveweightw i,\n1 0 7\nCHAPTER5.MACHINELEARNINGBASICS\nthenincreasingthevalueofthatfeatureincreasesthevalueofourprediction ˆy. Ifafeaturereceivesanegativeweight,thenincreasingthevalueofthatfeature\ndecreasesthevalueofourprediction.Ifafeature’sweightislargeinmagnitude,\nthenithasalargeeﬀectontheprediction.Ifafeature’sweightiszero,ithasno\neﬀectontheprediction. WethushaveadeﬁnitionofourtaskT: topredictyfromxbyoutputting\nˆy= wx.Nextweneedadeﬁnitionofourperformancemeasure,.P\nSupposethatwehaveadesignmatrixofmexampleinputsthatwewillnot\nusefortraining,onlyforevaluatinghowwellthemodelperforms.Wealsohave\navectorofregressiontargetsprovidingthecorrectvalueofyforeachofthese\nexamples.Becausethisdatasetwillonlybeusedforevaluation,wecallitthetest\nset.WerefertothedesignmatrixofinputsasX()testandthevectorofregression\ntargetsasy()test.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 264, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 851}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0265_ffd7276c", "text": "Onewayofmeasuringtheperformanceofthemodelistocomputethemean\nsquarederrorofthemodelonthetestset.Ifˆy()testgivesthepredictionsofthe\nmodelonthetestset,thenthemeansquarederrorisgivenby\nMSEtest=1\nm\ni(ˆy()test−y()test)2\ni. (5.4)\nIntuitively,onecanseethatthiserrormeasuredecreasesto0when ˆy()test=y()test. Wecanalsoseethat\nMSEtest=1\nm||ˆy()test−y()test||2\n2, (5.5)\nsotheerrorincreaseswhenevertheEuclideandistancebetweenthepredictions\nandthetargetsincreases. Tomakeamachinelearningalgorithm,weneedtodesignanalgorithmthat\nwillimprovetheweightswinawaythatreducesMSEtestwhenthealgorithm\nisallowedtogainexperiencebyobservingatrainingset(X()train,y()train).One\nintuitivewayofdoingthis(whichwewilljustifylater,insection)isjustto 5.5.1\nminimizethemeansquarederroronthetrainingset,MSEtrain. TominimizeMSEtrain,wecansimplysolveforwhereitsgradientis: 0\n∇ wMSEtrain= 0 (5.6)\n⇒∇ w1\nm||ˆy()train−y()train||2\n2= 0 (5.7)\n⇒1\nm∇ w||X()trainwy−()train||2\n2= 0 (5.8)\n1 0 8\nCHAPTER5.MACHINELEARNINGBASICS\n− − 1 0 . 0 5 0 0 0 5 1 0 .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 265, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1005}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0266_54d6ba68", "text": "0 5 0 0 0 5 1 0 . . . . x1− 3− 2− 10123yL i n ea r r eg r es s i o n ex a m p l e\n0 5 1 0 1 5 . . . w10 2 0 .0 2 5 .0 3 0 .0 3 5 .0 4 0 .0 4 5 .0 5 0 .0 5 5 .MSE(train)O p t i m i za t i o n o f w\nFigure5.1:Alinearregressionproblem,withatrainingsetconsistingoftendatapoints,\neachcontainingonefeature.Becausethereisonlyonefeature,theweightvectorw\ncontainsonlyasingleparametertolearn,w 1. ( L e f t )Observethatlinearregressionlearns\ntosetw 1suchthattheliney=w 1xcomesascloseaspossibletopassingthroughallthe\ntrainingpoints.Theplottedpointindicatesthevalueof ( R i g h t ) w 1foundbythenormal\nequations,whichwecanseeminimizesthemeansquarederroronthetrainingset.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 266, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 658}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0267_fad60451", "text": "⇒∇ w\nX()trainwy−()train\nX()trainwy−()train\n= 0(5.9)\n⇒∇ w\nwX()train X()trainww−2X()train y()train+y()train y()train\n= 0\n(5.10)\n⇒2X()train X()trainwX−2()train y()train= 0(5.11)\n⇒w=\nX()train X()train−1\nX()train y()train(5.12)\nThesystemofequationswhosesolutionisgivenbyequationisknownas 5.12\nthenormalequations.Evaluatingequationconstitutesasimplelearning 5.12\nalgorithm.Foranexampleofthelinearregressionlearningalgorithminaction,\nseeﬁgure.5.1\nItisworthnotingthatthetermlinearregressionisoftenusedtoreferto\naslightlymoresophisticatedmodelwithoneadditionalparameter—an intercept\nterm.Inthismodelb\nˆy= wx+b (5.13)\nsothemappingfromparameterstopredictionsisstillalinearfunctionbutthe\nmappingfromfeaturestopredictionsisnowanaﬃnefunction.Thisextensionto\naﬃnefunctionsmeansthattheplotofthemodel’spredictionsstilllookslikea\nline,butitneednotpassthroughtheorigin.Insteadofaddingthebiasparameter\n1 0 9\nCHAPTER5.MACHINELEARNINGBASICS\nb,onecancontinuetousethemodelwithonlyweightsbutaugmentxwithan\nextraentrythatisalwayssetto.Theweightcorrespondingtotheextraentry 1 1\nplaystheroleofthebiasparameter.Wewillfrequentlyusetheterm“linear”when\nreferringtoaﬃnefunctionsthroughoutthisbook.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 267, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1183}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0268_0dfdc7a1", "text": "Theintercepttermbisoftencalledthebiasparameteroftheaﬃnetransfor-\nmation.Thisterminologyderivesfromthepointofviewthattheoutputofthe\ntransformationisbiasedtowardbeingbintheabsenceofanyinput.Thisterm\nisdiﬀerentfromtheideaofastatisticalbias,inwhichastatisticalestimation\nalgorithm’sexpectedestimateofaquantityisnotequaltothetruequantity. Linearregressionisofcourseanextremelysimpleandlimitedlearningalgorithm,\nbutitprovidesanexampleofhowalearningalgorithmcanwork.Inthesubsequent\nsectionswewilldescribesomeofthebasicprinciplesunderlyinglearningalgorithm\ndesignanddemonstratehowtheseprinciplescanbeusedtobuildmorecomplicated\nlearningalgorithms. 5.2Capacity,OverﬁttingandUnderﬁtting\nThecentralchallengeinmachinelearningisthatwemustperformwellonnew,\npreviouslyunseeninputs—notjustthoseonwhichourmodelwastrained. The\nabilitytoperformwellonpreviouslyunobservedinputsiscalledgeneralization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 268, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 879}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0269_90a77b24", "text": "The\nabilitytoperformwellonpreviouslyunobservedinputsiscalledgeneralization. Typically,whentrainingamachinelearningmodel,wehaveaccesstoatraining\nset,wecancomputesomeerrormeasureonthetrainingsetcalledthetraining\nerror,andwereducethistrainingerror.Sofar,whatwehavedescribedissimply\nanoptimization problem.Whatseparatesmachinelearningfromoptimization is\nthatwewantthegeneralizationerror,alsocalledthetesterror,tobelowas\nwell. Thegeneralization errorisdeﬁnedastheexpectedvalueoftheerrorona\nnewinput.Heretheexpectationistakenacrossdiﬀerentpossibleinputs,drawn\nfromthedistributionofinputsweexpectthesystemtoencounterinpractice. Wetypicallyestimatethegeneralization errorofamachinelearningmodelby\nmeasuringitsperformanceonatestsetofexamplesthatwerecollectedseparately\nfromthetrainingset. Inourlinearregressionexample,wetrainedthemodelbyminimizingthe\ntrainingerror,\n1\nm()train||X()trainwy−()train||2\n2, (5.14)\nbutweactuallycareaboutthetesterror,1\nm()test||X()testwy−()test||2\n2.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 269, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 969}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0270_c397def8", "text": "Inourlinearregressionexample,wetrainedthemodelbyminimizingthe\ntrainingerror,\n1\nm()train||X()trainwy−()train||2\n2, (5.14)\nbutweactuallycareaboutthetesterror,1\nm()test||X()testwy−()test||2\n2. Howcanweaﬀectperformanceonthetestsetwhenwegettoobserveonlythe\n1 1 0\nCHAPTER5. MACHINELEARNINGBASICS\ntrainingset? Theﬁeldof statisticallearningtheory providessomeanswers. If\nthetrainingandthetestsetarecollectedarbitrarily,thereisindeedlittlewecan\ndo. Ifweareallowedtomakesomeassumptionsabouthowthetrainingandtest\nsetarecollected,thenwecanmakesomeprogress. Thetrainandtestdataaregeneratedbyaprobabilitydistributionoverdatasets\ncalledthedatageneratingprocess . Wetypicallymakeasetofassumptions\nknowncollectivelyasthe i.i.d. assumptions . Theseassumptionsarethatthe\nexamplesineachdatasetare independent fromeachother,andthatthetrain\nsetandtestsetare identicallydistributed ,drawnfromthesameprobability\ndistributionaseachother.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 270, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 912}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0271_d017926c", "text": "assumptions . Theseassumptionsarethatthe\nexamplesineachdatasetare independent fromeachother,andthatthetrain\nsetandtestsetare identicallydistributed ,drawnfromthesameprobability\ndistributionaseachother. Thisassumptionallowsustodescribethedatagen-\neratingprocesswithaprobabilitydistributionoverasingleexample. Thesame\ndistributionisthenusedtogenerateeverytrainexampleandeverytestexample. Wecallthatsharedunderlyingdistributionthe datageneratingdistribution ,\ndenotedpdata. Thisprobabilisticframeworkandthei.i.d. assumptionsallowusto\nmathematicallystudytherelationshipbetweentrainingerrorandtesterror. Oneimmediateconnectionwecanobservebetweenthetrainingandtesterror\nisthattheexpectedtrainingerrorofarandomlyselectedmodelisequaltothe\nexpected test error of that model. Suppose wehavea probabilitydistributionp(x,y)andwesamplefromitrepeatedlytogeneratethetrainsetandthetest\nset.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 271, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 874}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0272_1f03f1ed", "text": "Suppose wehavea probabilitydistributionp(x,y)andwesamplefromitrepeatedlytogeneratethetrainsetandthetest\nset. Forsomeﬁxedvalue w,theexpectedtrainingseterrorisexactlythesameas\ntheexpectedtestseterror,becausebothexpectationsareformedusingthesame\ndatasetsamplingprocess. Theonlydiﬀerencebetweenthetwoconditionsisthe\nnameweassigntothedatasetwesample. Of course, when we use a machine learning algorithm, we do not ﬁx the\nparametersaheadoftime,thensamplebothdatasets. Wesamplethetrainingset,\nthenuseittochoosetheparameterstoreducetrainingseterror,thensamplethe\ntestset. Underthisprocess,theexpectedtesterrorisgreaterthanorequalto\ntheexpectedvalueoftrainingerror. Thefactorsdetermininghowwellamachine\nlearningalgorithmwillperformareitsabilityto:\n1. Makethetrainingerrorsmall. 2. Makethegapbetweentrainingandtesterrorsmall. Thesetwofactorscorrespondtothetwocentralchallengesinmachinelearning:\nunderﬁtting andoverﬁtting. Underﬁttingoccurswhenthemodelisnotableto\nobtainasuﬃcientlylowerrorvalueonthetrainingset.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 272, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1000}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0273_10dfe743", "text": "Thesetwofactorscorrespondtothetwocentralchallengesinmachinelearning:\nunderﬁtting andoverﬁtting. Underﬁttingoccurswhenthemodelisnotableto\nobtainasuﬃcientlylowerrorvalueonthetrainingset. Overﬁttingoccurswhen\nthegapbetweenthetrainingerrorandtesterroristoolarge. Wecancontrolwhetheramodelismorelikelytooverﬁtorunderﬁtbyaltering\nitscapacity. Informally,amodel’scapacityisitsabilitytoﬁtawidevarietyof\n111\nCHAPTER5.MACHINELEARNINGBASICS\nfunctions.Modelswithlowcapacitymaystruggletoﬁtthetrainingset.Models\nwithhighcapacitycanoverﬁtbymemorizingpropertiesofthetrainingsetthatdo\nnotservethemwellonthetestset. Onewaytocontrolthecapacityofalearningalgorithmisbychoosingits\nhypothesisspace,thesetoffunctionsthatthelearningalgorithmisallowedto\nselectasbeingthesolution.Forexample,thelinearregressionalgorithmhasthe\nsetofalllinearfunctionsofitsinputasitshypothesisspace.Wecangeneralize\nlinearregressiontoincludepolynomials,ratherthanjustlinearfunctions,inits\nhypothesisspace.Doingsoincreasesthemodel’scapacity.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 273, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 994}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0274_8a1fb476", "text": "Apolynomialofdegreeonegivesusthelinearregressionmodelwithwhichwe\narealreadyfamiliar,withprediction\nˆybwx. = + (5.15)\nByintroducingx2asanotherfeatureprovidedtothelinearregressionmodel,we\ncanlearnamodelthatisquadraticasafunctionof:x\nˆybw = +1xw+2x2. (5.16)\nThoughthismodelimplementsaquadraticfunctionofits,theoutputis input\nstillalinearfunctionoftheparameters,sowecanstillusethenormalequations\ntotrainthemodelinclosedform.Wecancontinuetoaddmorepowersofxas\nadditionalfeatures,forexampletoobtainapolynomialofdegree9:\nˆyb= +9\ni=1w ixi. (5.17)\nMachinelearningalgorithmswillgenerallyperformbestwhentheircapacity\nisappropriateforthetruecomplexityofthetasktheyneedtoperformandthe\namountoftrainingdatatheyareprovidedwith.Modelswithinsuﬃcientcapacity\nareunabletosolvecomplextasks.Modelswithhighcapacitycansolvecomplex\ntasks,butwhentheircapacityishigherthanneededtosolvethepresenttaskthey\nmayoverﬁt.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 274, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 888}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0275_ba3dd6e4", "text": "Figureshowsthisprincipleinaction.Wecomparealinear,quadratic 5.2\nanddegree-9predictorattemptingtoﬁtaproblemwherethetrueunderlying\nfunctionisquadratic. Thelinearfunctionisunabletocapturethecurvaturein\nthetrueunderlyingproblem,soitunderﬁts.Thedegree-9predictoriscapableof\nrepresentingthecorrectfunction,butitisalsocapableofrepresentinginﬁnitely\nmanyotherfunctionsthatpassexactlythroughthetrainingpoints,becausewe\n1 1 2\nCHAPTER5.MACHINELEARNINGBASICS\nhavemoreparametersthantrainingexamples.Wehavelittlechanceofchoosing\nasolutionthatgeneralizeswellwhensomanywildlydiﬀerentsolutionsexist.In\nthisexample,thequadraticmodelisperfectlymatchedtothetruestructureof\nthetasksoitgeneralizeswelltonewdata.           \n                  \n          \nFigure5.2:Weﬁtthreemodelstothisexampletrainingset.Thetrainingdatawas\ngeneratedsynthetically,byrandomlysamplingxvaluesandchoosingydeterministically\nbyevaluatingaquadraticfunction.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 275, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 961}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0276_708a7058", "text": "( L e f t )Alinearfunctionﬁttothedatasuﬀersfrom\nunderﬁtting—itcannotcapturethecurvaturethatispresentinthedata. A ( C e n t e r )\nquadraticfunctionﬁttothedatageneralizeswelltounseenpoints.Itdoesnotsuﬀerfrom\nasigniﬁcantamountofoverﬁttingorunderﬁtting.Apolynomialofdegree9ﬁtto ( R i g h t )\nthedatasuﬀersfromoverﬁtting.HereweusedtheMoore-Penrosepseudoinversetosolve\ntheunderdeterminednormalequations.Thesolutionpassesthroughallofthetraining\npointsexactly,butwehavenotbeenluckyenoughforittoextractthecorrectstructure. Itnowhasadeepvalleyinbetweentwotrainingpointsthatdoesnotappearinthetrue\nunderlyingfunction.Italsoincreasessharplyontheleftsideofthedata,whilethetrue\nfunctiondecreasesinthisarea.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 276, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 691}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0277_5e207897", "text": "Itnowhasadeepvalleyinbetweentwotrainingpointsthatdoesnotappearinthetrue\nunderlyingfunction.Italsoincreasessharplyontheleftsideofthedata,whilethetrue\nfunctiondecreasesinthisarea. Sofarwehavedescribedonlyonewayofchangingamodel’scapacity:by\nchangingthenumberofinputfeaturesithas,andsimultaneouslyaddingnew\nparametersassociatedwiththosefeatures.Thereareinfactmanywaysofchanging\namodel’scapacity.Capacityisnotdeterminedonlybythechoiceofmodel.The\nmodelspeciﬁeswhichfamilyoffunctionsthelearningalgorithmcanchoosefrom\nwhenvaryingtheparametersinordertoreduceatrainingobjective.Thisiscalled\ntherepresentationalcapacityofthemodel.Inmanycases,ﬁndingthebest\nfunctionwithinthisfamilyisaverydiﬃcultoptimization problem.Inpractice,\nthelearningalgorithmdoesnotactuallyﬁndthebestfunction,butmerelyone\nthatsigniﬁcantlyreducesthetrainingerror.Theseadditionallimitations,suchas\n1 1 3\nCHAPTER5.MACHINELEARNINGBASICS\ntheimperfectionoftheoptimization algorithm,meanthatthelearningalgorithm’s\neﬀectivecapacitymaybelessthantherepresentationalcapacityofthemodel\nfamily.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 277, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1042}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0278_c6b7fd2e", "text": "Ourmodernideasaboutimprovingthegeneralization ofmachinelearning\nmodelsarereﬁnementsofthoughtdatingbacktophilosophersatleastasearly\nasPtolemy.Manyearlyscholarsinvokeaprincipleofparsimonythatisnow\nmostwidelyknownasOccam’srazor(c.1287-1347).Thisprinciplestatesthat\namongcompetinghypothesesthatexplainknownobservationsequallywell,one\nshouldchoosethe“simplest”one.Thisideawasformalizedandmademoreprecise\ninthe20thcenturybythefoundersofstatisticallearningtheory(Vapnikand\nChervonenkis1971Vapnik1982Blumer1989Vapnik1995 ,;,; etal.,;,). Statisticallearningtheoryprovidesvariousmeansofquantifyingmodelcapacity. Amongthese,themostwell-knownistheVapnik-Chervonenkisdimension,or\nVCdimension.TheVCdimensionmeasuresthecapacityofabinaryclassiﬁer.The\nVCdimensionisdeﬁnedasbeingthelargestpossiblevalueofmforwhichthere\nexistsatrainingsetofmdiﬀerentxpointsthattheclassiﬁercanlabelarbitrarily.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 278, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 873}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0279_cdbc6785", "text": "Quantifyingthecapacityofthemodelallowsstatisticallearningtheoryto\nmakequantitativepredictions.Themostimportantresultsinstatisticallearning\ntheoryshowthatthediscrepancybetweentrainingerrorandgeneralization error\nisboundedfromabovebyaquantitythatgrowsasthemodelcapacitygrowsbut\nshrinksasthenumberoftrainingexamplesincreases(VapnikandChervonenkis,\n1971Vapnik1982Blumer 1989Vapnik1995 ;,; etal.,;,).Theseboundsprovide\nintellectualjustiﬁcationthatmachinelearningalgorithmscanwork,buttheyare\nrarelyusedinpracticewhenworkingwithdeeplearningalgorithms.Thisisin\npartbecausetheboundsareoftenquitelooseandinpartbecauseitcanbequite\ndiﬃculttodeterminethecapacityofdeeplearningalgorithms. Theproblemof\ndeterminingthecapacityofadeeplearningmodelisespeciallydiﬃcultbecausethe\neﬀectivecapacityislimitedbythecapabilitiesoftheoptimization algorithm,and\nwehavelittletheoreticalunderstandingoftheverygeneralnon-convexoptimization\nproblemsinvolvedindeeplearning.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 279, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 940}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0280_cf130d74", "text": "Wemustrememberthatwhilesimplerfunctionsaremorelikelytogeneralize\n(tohaveasmallgapbetweentrainingandtesterror)wemuststillchoosea\nsuﬃcientlycomplexhypothesistoachievelowtrainingerror.Typically,training\nerrordecreasesuntilitasymptotestotheminimumpossibleerrorvalueasmodel\ncapacityincreases(assumingtheerrormeasurehasaminimumvalue).Typically,\ngeneralization errorhasaU-shapedcurveasafunctionofmodelcapacity.Thisis\nillustratedinﬁgure.5.3\nToreachthemostextremecaseofarbitrarilyhighcapacity,weintroduce\n1 1 4\nCHAPTER5.MACHINELEARNINGBASICS\n0 O pti m a l C a pa c i t y\nC a pa c i t yE r r o rU nde r ﬁtti ng z o ne O v e r ﬁtti ng z o ne\nG e ne r a l i z a t i o n g a pT r a i n i n g e r r o r\nG e n e r a l i z a t i o n e r r o r\nFigure5.3:Typicalrelationshipbetweencapacityanderror.Trainingandtesterror\nbehavediﬀerently.Attheleftendofthegraph,trainingerrorandgeneralizationerror\narebothhigh.Thisistheunderﬁttingregime.Asweincreasecapacity,trainingerror\ndecreases,butthegapbetweentrainingandgeneralizationerrorincreases.Eventually,\nthesizeofthisgapoutweighsthedecreaseintrainingerror,andweentertheoverﬁtting\nregime,wherecapacityistoolarge,abovetheoptimalcapacity.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 280, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1160}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0281_cdbc9e13", "text": "theconceptofnon-parametricmodels.Sofar,wehaveseenonlyparametric\nmodels,suchaslinearregression.Parametricmodelslearnafunctiondescribed\nbyaparametervectorwhosesizeisﬁniteandﬁxedbeforeanydataisobserved. Non-parametric modelshavenosuchlimitation. Sometimes,non-parametric modelsarejusttheoreticalabstractions(suchas\nanalgorithmthatsearchesoverallpossibleprobabilitydistributions)thatcannot\nbeimplemented inpractice.However,wecanalsodesignpracticalnon-parametric\nmodelsbymakingtheircomplexityafunctionofthetrainingsetsize.Oneexample\nofsuchanalgorithmisnearestneighborregression.Unlikelinearregression,\nwhichhasaﬁxed-lengthvectorofweights,thenearestneighborregressionmodel\nsimplystorestheXandyfromthetrainingset. Whenaskedtoclassifyatest\npointx,themodellooksupthenearestentryinthetrainingsetandreturnsthe\nassociatedregressiontarget.Inotherwords,ˆy=y iwherei=argmin||X i ,:−||x2\n2.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 281, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 874}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0282_8d86f070", "text": "Whenaskedtoclassifyatest\npointx,themodellooksupthenearestentryinthetrainingsetandreturnsthe\nassociatedregressiontarget.Inotherwords,ˆy=y iwherei=argmin||X i ,:−||x2\n2. ThealgorithmcanalsobegeneralizedtodistancemetricsotherthantheL2norm,\nsuchaslearneddistancemetrics( ,).Ifthealgorithmis Goldbergeretal.2005\nallowedtobreaktiesbyaveragingthey ivaluesforallX i ,:thataretiedfornearest,\nthenthisalgorithmisabletoachievetheminimumpossibletrainingerror(which\nmightbegreaterthanzero,iftwoidenticalinputsareassociatedwithdiﬀerent\noutputs)onanyregressiondataset. Finally,wecanalsocreateanon-parametric learningalgorithmbywrappinga\n1 1 5\nCHAPTER5.MACHINELEARNINGBASICS\nparametriclearningalgorithminsideanotheralgorithmthatincreasesthenumber\nofparametersasneeded.Forexample,wecouldimagineanouterloopoflearning\nthatchangesthedegreeofthepolynomiallearnedbylinearregressionontopofa\npolynomialexpansionoftheinput. Theidealmodelisanoraclethatsimplyknowsthetrueprobabilitydistribution\nthatgeneratesthedata.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 282, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 989}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0283_e6febdee", "text": "Theidealmodelisanoraclethatsimplyknowsthetrueprobabilitydistribution\nthatgeneratesthedata. Evensuchamodelwillstillincursomeerroronmany\nproblems,becausetheremaystillbesomenoiseinthedistribution.Inthecase\nofsupervisedlearning,themappingfromxtoymaybeinherentlystochastic,\norymaybeadeterministicfunctionthatinvolvesothervariablesbesidesthose\nincludedinx.Theerrorincurredbyanoraclemakingpredictionsfromthetrue\ndistributioniscalledthe p,y(x)Bayeserror. Trainingandgeneralization errorvaryasthesizeofthetrainingsetvaries. Expectedgeneralization errorcanneverincreaseasthenumberoftrainingexamples\nincreases.Fornon-parametric models,moredatayieldsbettergeneralization until\nthebestpossibleerrorisachieved.Anyﬁxedparametricmodelwithlessthan\noptimalcapacitywillasymptotetoanerrorvaluethatexceedstheBayeserror.See\nﬁgureforanillustration.Notethatitispossibleforthemodeltohaveoptimal 5.4\ncapacityandyetstillhavealargegapbetweentrainingandgeneralization error.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 283, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 945}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0284_a2033172", "text": "Inthissituation,wemaybeabletoreducethisgapbygatheringmoretraining\nexamples. 5.2.1TheNoFreeLunchTheorem\nLearningtheoryclaimsthatamachinelearningalgorithmcangeneralizewellfrom\naﬁnitetrainingsetofexamples.Thisseemstocontradictsomebasicprinciplesof\nlogic.Inductivereasoning,orinferringgeneralrulesfromalimitedsetofexamples,\nisnotlogicallyvalid. Tologicallyinferaruledescribingeverymemberofaset,\nonemusthaveinformationabouteverymemberofthatset. Inpart,machinelearningavoidsthisproblembyoﬀeringonlyprobabilisticrules,\nratherthantheentirelycertainrulesusedinpurelylogicalreasoning. Machine\nlearningpromisestoﬁndrulesthatareprobably most correctaboutmembersof\nthesettheyconcern.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 284, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 670}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0285_ac0f1b17", "text": "Machine\nlearningpromisestoﬁndrulesthatareprobably most correctaboutmembersof\nthesettheyconcern. Unfortunately,eventhisdoesnotresolvetheentireproblem.Thenofree\nlunchtheoremformachinelearning(Wolpert1996,)statesthat,averagedover\nallpossibledatageneratingdistributions,everyclassiﬁcationalgorithmhasthe\nsameerrorratewhenclassifyingpreviouslyunobservedpoints.Inotherwords,\ninsomesense,nomachinelearningalgorithmisuniversallyanybetterthanany\nother.Themostsophisticatedalgorithmwecanconceiveofhasthesameaverage\n1 1 6\nCHAPTER5.MACHINELEARNINGBASICS\n      \n                                                    \n                \n              \n                     \n                       \n      \n                                                     \nFigure5.4:Theeﬀectofthetrainingdatasetsizeonthetrainandtesterror,aswellas\nontheoptimalmodelcapacity.Weconstructedasyntheticregressionproblembasedon\naddingamoderateamountofnoisetoadegree-5polynomial,generatedasingletestset,\nandthengeneratedseveraldiﬀerentsizesoftrainingset.Foreachsize,wegenerated40\ndiﬀerenttrainingsetsinordertoploterrorbarsshowing95percentconﬁdenceintervals.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 285, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1377}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0286_458f3d01", "text": "( T o p )TheMSEonthetrainingandtestsetfortwodiﬀerentmodels:aquadraticmodel,\nandamodelwithdegreechosentominimizethetesterror.Bothareﬁtinclosedform.For\nthequadraticmodel,thetrainingerrorincreasesasthesizeofthetrainingsetincreases. Thisisbecauselargerdatasetsarehardertoﬁt.Simultaneously,thetesterrordecreases,\nbecausefewerincorrecthypothesesareconsistentwiththetrainingdata.Thequadratic\nmodeldoesnothaveenoughcapacitytosolvethetask,soitstesterrorasymptotesto\nahighvalue.ThetesterroratoptimalcapacityasymptotestotheBayeserror.The\ntrainingerrorcanfallbelowtheBayeserror,duetotheabilityofthetrainingalgorithm\ntomemorizespeciﬁcinstancesofthetrainingset.Asthetrainingsizeincreasestoinﬁnity,\nthetrainingerrorofanyﬁxed-capacitymodel(here,thequadraticmodel)mustrisetoat\nleasttheBayeserror. Asthetrainingsetsizeincreases,theoptimalcapacity ( Bottom )\n(shownhereasthedegreeoftheoptimalpolynomialregressor)increases. Theoptimal\ncapacityplateausafterreachingsuﬃcientcomplexitytosolvethetask.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 286, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 977}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0287_af137405", "text": "Asthetrainingsetsizeincreases,theoptimalcapacity ( Bottom )\n(shownhereasthedegreeoftheoptimalpolynomialregressor)increases. Theoptimal\ncapacityplateausafterreachingsuﬃcientcomplexitytosolvethetask. 1 1 7\nCHAPTER5.MACHINELEARNINGBASICS\nperformance(overallpossibletasks)asmerelypredictingthateverypointbelongs\ntothesameclass. Fortunately,theseresultsholdonlywhenweaverageoverpossibledata all\ngeneratingdistributions.Ifwemakeassumptionsaboutthekindsofprobability\ndistributionsweencounterinreal-worldapplications,thenwecandesignlearning\nalgorithmsthatperformwellonthesedistributions. Thismeansthatthegoalofmachinelearningresearchisnottoseekauniversal\nlearningalgorithmortheabsolutebestlearningalgorithm.Instead,ourgoalisto\nunderstandwhatkindsofdistributionsarerelevanttothe“realworld”thatanAI\nagentexperiences,andwhatkindsofmachinelearningalgorithmsperformwellon\ndatadrawnfromthekindsofdatageneratingdistributionswecareabout.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 287, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 921}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0288_d4313521", "text": "5.2.2Regularization\nThenofreelunchtheoremimpliesthatwemustdesignourmachinelearning\nalgorithmstoperformwellonaspeciﬁctask.Wedosobybuildingasetof\npreferencesintothelearningalgorithm.Whenthesepreferencesarealignedwith\nthelearningproblemsweaskthealgorithmtosolve,itperformsbetter. Sofar,theonlymethodofmodifyingalearningalgorithmthatwehavediscussed\nconcretelyistoincreaseordecreasethemodel’srepresentationalcapacitybyadding\norremovingfunctionsfromthehypothesisspaceofsolutionsthelearningalgorithm\nisabletochoose.Wegavethespeciﬁcexampleofincreasingordecreasingthe\ndegreeofapolynomialforaregressionproblem.Theviewwehavedescribedso\nfarisoversimpliﬁed.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 288, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 644}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0289_c4655426", "text": "Thebehaviorofouralgorithmisstronglyaﬀectednotjustbyhowlargewe\nmakethesetoffunctionsallowedinitshypothesisspace,butbythespeciﬁcidentity\nofthosefunctions.Thelearningalgorithmwehavestudiedsofar,linearregression,\nhasahypothesisspaceconsistingofthesetoflinearfunctionsofitsinput.These\nlinearfunctionscanbeveryusefulforproblemswheretherelationshipbetween\ninputsandoutputstrulyisclosetolinear.Theyarelessusefulforproblems\nthatbehaveinaverynonlinearfashion.Forexample,linearregressionwould\nnotperformverywellifwetriedtouseittopredict sin(x)fromx.Wecanthus\ncontroltheperformanceofouralgorithmsbychoosingwhatkindoffunctionswe\nallowthemtodrawsolutionsfrom,aswellasbycontrollingtheamountofthese\nfunctions. Wecanalsogivealearningalgorithmapreferenceforonesolutioninits\nhypothesisspacetoanother.Thismeansthatbothfunctionsareeligible,butone\nispreferred.Theunpreferredsolutionwillbechosenonlyifitﬁtsthetraining\n1 1 8\nCHAPTER5.MACHINELEARNINGBASICS\ndatasigniﬁcantlybetterthanthepreferredsolution.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 289, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 979}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0290_ace533d7", "text": "Forexample,wecanmodifythetrainingcriterionforlinearregressiontoinclude\nweightdecay.Toperformlinearregressionwithweightdecay,weminimizeasum\ncomprisingboththemeansquarederroronthetrainingandacriterionJ(w)that\nexpressesapreferencefortheweightstohavesmallersquaredL2norm.Speciﬁcally,\nJ() = wMSEtrain+λww, (5.18)\nwhereλisavaluechosenaheadoftimethatcontrolsthestrengthofourpreference\nforsmallerweights.Whenλ= 0,weimposenopreference,andlargerλforcesthe\nweightstobecomesmaller. MinimizingJ(w)resultsinachoiceofweightsthat\nmakeatradeoﬀbetweenﬁttingthetrainingdataandbeingsmall.Thisgivesus\nsolutionsthathaveasmallerslope,orputweightonfewerofthefeatures.Asan\nexampleofhowwecancontrolamodel’stendencytooverﬁtorunderﬁtviaweight\ndecay,wecantrainahigh-degreepolynomialregressionmodelwithdiﬀerentvalues\nof.Seeﬁgurefortheresults.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 290, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 813}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0291_53850377", "text": "λ 5.5\n           \n         \n                       \n         \n          \n    \nFigure5.5:Weﬁtahigh-degreepolynomialregressionmodeltoourexampletrainingset\nfromﬁgure.Thetruefunctionisquadratic,buthereweuseonlymodelswithdegree9. 5.2\nWevarytheamountofweightdecaytopreventthesehigh-degreemodelsfromoverﬁtting. ( L e f t )Withverylargeλ,wecanforcethemodeltolearnafunctionwithnoslopeat\nall.Thisunderﬁtsbecauseitcanonlyrepresentaconstantfunction.Witha ( C e n t e r )\nmediumvalueof,thelearningalgorithmrecoversacurvewiththerightgeneralshape.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 291, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 616}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0292_2adf4ce4", "text": "λ\nEventhoughthemodeliscapableofrepresentingfunctionswithmuchmorecomplicated\nshape,weightdecayhasencouragedittouseasimplerfunctiondescribedbysmaller\ncoeﬃcients.Withweightdecayapproachingzero(i.e.,usingtheMoore-Penrose ( R i g h t )\npseudoinversetosolvetheunderdeterminedproblemwithminimalregularization),the\ndegree-9polynomialoverﬁtssigniﬁcantly,aswesawinﬁgure.5.2\n1 1 9\nCHAPTER5.MACHINELEARNINGBASICS\nMoregenerally,wecanregularizeamodelthatlearnsafunctionf(x;θ)by\naddingapenaltycalledaregularizertothecostfunction.Inthecaseofweight\ndecay,theregularizerisΩ(w) =ww.Inchapter,wewillseethatmanyother 7\nregularizersarepossible. Expressingpreferencesforonefunctionoveranotherisamoregeneralway\nofcontrollingamodel’scapacitythanincludingorexcludingmembersfromthe\nhypothesisspace.Wecanthinkofexcludingafunctionfromahypothesisspaceas\nexpressinganinﬁnitelystrongpreferenceagainstthatfunction.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 292, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 882}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0293_d4dd44c2", "text": "Inourweightdecayexample,weexpressedourpreferenceforlinearfunctions\ndeﬁnedwithsmallerweightsexplicitly, viaanextraterminthecriterionwe\nminimize.Thereare many otherwaysof expressing preferencesfor diﬀerent\nsolutions,bothimplicitlyandexplicitly.Together,thesediﬀerentapproaches\nareknownasregularization. Regularizationisanymodiﬁcationwemaketoa\nlearningalgorithmthatisintendedtoreduceitsgeneralizationerrorbutnotits\ntrainingerror.Regularizationisoneofthecentralconcernsoftheﬁeldofmachine\nlearning,rivaledinitsimportanceonlybyoptimization. Thenofreelunchtheoremhasmadeitclearthatthereisnobestmachine\nlearningalgorithm,and,inparticular,nobestformofregularization. Instead\nwemustchooseaformofregularizationthatiswell-suitedtotheparticulartask\nwewanttosolve.Thephilosophyofdeeplearningingeneralandthisbookin\nparticularisthataverywiderangeoftasks(suchasalloftheintellectualtasks\nthatpeoplecando)mayallbesolvedeﬀectivelyusingverygeneral-purposeforms\nofregularization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 293, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 957}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0294_7d7e7693", "text": "5.3HyperparametersandValidationSets\nMostmachinelearningalgorithmshaveseveralsettingsthatwecanusetocontrol\nthebehaviorofthelearningalgorithm.Thesesettingsarecalledhyperparame-\nters.Thevaluesofhyperparameters arenotadaptedbythelearningalgorithm\nitself(thoughwecan designa nestedlearning procedure where one learning\nalgorithmlearnsthebesthyperparametersforanotherlearningalgorithm). Inthepolynomialregressionexamplewesawinﬁgure,thereisasingle 5.2\nhyperparameter:thedegreeofthepolynomial,whichactsasacapacityhyper-\nparameter.Theλvalueusedtocontrolthestrengthofweightdecayisanother\nexampleofahyperparameter.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 294, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 603}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0295_9172ae53", "text": "Sometimesasettingischosentobeahyperparameter thatthelearningal-\ngorithmdoesnotlearnbecauseitisdiﬃculttooptimize.Morefrequently,the\n1 2 0\nCHAPTER5.MACHINELEARNINGBASICS\nsettingmustbeahyperparameter becauseitisnotappropriatetolearnthat\nhyperparameteronthetrainingset.Thisappliestoallhyperparameters that\ncontrolmodelcapacity.Iflearnedonthetrainingset,suchhyperparameters would\nalwayschoosethemaximumpossiblemodelcapacity,resultinginoverﬁtting(refer\ntoﬁgure).Forexample,wecanalwaysﬁtthetrainingsetbetterwithahigher 5.3\ndegreepolynomialandaweightdecaysettingofλ= 0thanwecouldwithalower\ndegreepolynomialandapositiveweightdecaysetting. Tosolvethisproblem,weneedavalidationsetofexamplesthatthetraining\nalgorithmdoesnotobserve.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 295, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 719}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0296_db8955b3", "text": "Tosolvethisproblem,weneedavalidationsetofexamplesthatthetraining\nalgorithmdoesnotobserve. Earlierwediscussedhowaheld-outtestset,composedofexamplescomingfrom\nthesamedistributionasthetrainingset,canbeusedtoestimatethegeneralization\nerrorofalearner,afterthelearningprocesshascompleted.Itisimportantthatthe\ntestexamplesarenotusedinanywaytomakechoicesaboutthemodel,including\nitshyperparameters . Forthisreason,noexamplefromthetestsetcanbeused\ninthevalidationset.Therefore,wealwaysconstructthevalidationsetfromthe\ntrainingdata.Speciﬁcally,wesplitthetrainingdataintotwodisjointsubsets.One\nofthesesubsetsisusedtolearntheparameters.Theothersubsetisourvalidation\nset,usedtoestimatethegeneralization errorduringoraftertraining,allowing\nforthehyperparameterstobeupdatedaccordingly.Thesubsetofdatausedto\nlearntheparametersisstilltypicallycalledthetrainingset,eventhoughthis\nmaybeconfusedwiththelargerpoolofdatausedfortheentiretrainingprocess.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 296, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 929}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0297_501f71f5", "text": "Thesubsetofdatausedtoguidetheselectionofhyperparameters iscalledthe\nvalidationset.Typically,oneusesabout80%ofthetrainingdatafortrainingand\n20%forvalidation.Sincethevalidationsetisusedto“train”thehyperparameters ,\nthevalidationseterrorwillunderestimatethegeneralization error,thoughtypically\nbyasmalleramountthanthetrainingerror.Afterallhyperparameter optimization\niscomplete,thegeneralization errormaybeestimatedusingthetestset. Inpractice, when thesametestsethasbeenusedrepeatedlytoevaluate\nperformanceofdiﬀerentalgorithmsovermanyyears,andespeciallyifweconsider\nalltheattemptsfromthescientiﬁccommunityatbeatingthereportedstate-of-\nthe-artperformanceonthattestset,weenduphavingoptimisticevaluationswith\nthetestsetaswell.Benchmarkscanthusbecomestaleandthendonotreﬂectthe\ntrueﬁeldperformance ofatrainedsystem.Thankfully,thecommunitytendsto\nmoveontonew(andusuallymoreambitiousandlarger)benchmarkdatasets.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 297, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 901}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0298_1aec32fb", "text": "1 2 1\nCHAPTER5.MACHINELEARNINGBASICS\n5.3.1Cross-Validation\nDividingthedatasetintoaﬁxedtrainingsetandaﬁxedtestsetcanbeproblematic\nifitresultsinthetestsetbeingsmall.Asmalltestsetimpliesstatisticaluncertainty\naroundtheestimatedaveragetesterror,makingitdiﬃculttoclaimthatalgorithm\nAworksbetterthanalgorithmonthegiventask.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 298, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 317}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0299_cb50f84c", "text": "B\nWhenthedatasethashundredsofthousandsofexamplesormore,thisisnota\nseriousissue.Whenthedatasetistoosmall,arealternativeproceduresenableone\ntousealloftheexamplesintheestimationofthemeantesterror,atthepriceof\nincreasedcomputational cost.Theseproceduresarebasedontheideaofrepeating\nthetrainingandtestingcomputationondiﬀerentrandomlychosensubsetsorsplits\noftheoriginaldataset.Themostcommonoftheseisthek-foldcross-validation\nprocedure,showninalgorithm ,inwhichapartitionofthedatasetisformedby 5.1\nsplittingitintoknon-overlappingsubsets.Thetesterrormaythenbeestimated\nbytakingtheaveragetesterroracrossktrials.Ontriali,thei-thsubsetofthe\ndataisusedasthetestsetandtherestofthedataisusedasthetrainingset.One\nproblemisthatthereexistnounbiasedestimatorsofthevarianceofsuchaverage\nerrorestimators(BengioandGrandvalet2004,),butapproximationsaretypically\nused.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 299, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 845}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0300_3e5b8969", "text": "5.4Estimators,BiasandVariance\nTheﬁeldofstatisticsgivesusmanytoolsthatcanbeusedtoachievethemachine\nlearninggoalofsolvingatasknotonlyonthetrainingsetbutalsotogeneralize. Foundationalconceptssuchasparameterestimation,biasandvarianceareuseful\ntoformallycharacterizenotionsofgeneralization, underﬁttingandoverﬁtting. 5.4.1PointEstimation\nPointestimationistheattempttoprovidethesingle“best”predictionofsome\nquantityofinterest.Ingeneralthequantityofinterestcanbeasingleparameter\noravectorofparametersinsomeparametricmodel,suchastheweightsinour\nlinearregressionexampleinsection,butitcanalsobeawholefunction. 5.1.4\nInordertodistinguishestimatesofparametersfromtheirtruevalue,our\nconventionwillbetodenoteapointestimateofaparameterbyθ ˆθ.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 300, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 727}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0301_ae8fa328", "text": "5.1.4\nInordertodistinguishestimatesofparametersfromtheirtruevalue,our\nconventionwillbetodenoteapointestimateofaparameterbyθ ˆθ. Let{x(1),...,x() m}beasetofmindependentandidenticallydistributed\n1 2 2\nCHAPTER5.MACHINELEARNINGBASICS\nAlgorithm5.1Thek-foldcross-validationalgorithm.Itcanbeusedtoestimate\ngeneralization errorofalearningalgorithmAwhenthegivendataset Distoo\nsmallforasimpletrain/testortrain/validsplittoyieldaccurateestimationof\ngeneralization error,becausethemeanofalossLonasmalltestsetmayhavetoo\nhighvariance.Thedataset Dcontainsaselementstheabstractexamplesz() i(for\nthei-thexample),whichcouldstandforan(input,target) pairz() i= (x() i,y() i)\ninthecaseofsupervisedlearning,orforjustaninputz() i=x() iinthecase\nofunsupervisedlearning. The algorithmreturnsthevectoroferrorseforeach\nexamplein D,whosemeanistheestimatedgeneralization error. Theerrorson\nindividualexamplescanbeusedtocomputeaconﬁdenceintervalaroundthemean\n(equation).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 301, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 940}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0302_074a04a8", "text": "The algorithmreturnsthevectoroferrorseforeach\nexamplein D,whosemeanistheestimatedgeneralization error. Theerrorson\nindividualexamplescanbeusedtocomputeaconﬁdenceintervalaroundthemean\n(equation). Whiletheseconﬁdenceintervalsarenotwell-justiﬁedafterthe 5.47\nuseofcross-validation,itisstillcommonpracticetousethemtodeclarethat\nalgorithmAisbetterthanalgorithmBonlyiftheconﬁdenceintervaloftheerror\nofalgorithmAliesbelowanddoesnotintersecttheconﬁdenceintervalofalgorithm\nB. DeﬁneKFoldXV(): D,A,L,k\nRequire: D,thegivendataset,withelementsz() i\nRequire:A,thelearningalgorithm,seenasafunctionthattakesadatasetas\ninputandoutputsalearnedfunction\nRequire:L,thelossfunction,seenasafunctionfromalearnedfunctionfand\nanexamplez() i∈ ∈ Dtoascalar R\nRequire:k,thenumberoffolds\nSplitintomutuallyexclusivesubsets Dk D i,whoseunionis.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 302, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 813}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0303_dae83ae7", "text": "D\nfordoikfromto1\nf i= (A D D\\ i)\nforz() jin D ido\ne j= (Lf i,z() j)\nendfor\nendfor\nReturne\n1 2 3\nCHAPTER5.MACHINELEARNINGBASICS\n(i.i.d.)datapoints.A orisanyfunctionofthedata: pointestimatorstatistic\nˆθ m= (gx(1),...,x() m). (5.19)\nThedeﬁnitiondoesnotrequirethatgreturnavaluethatisclosetothetrue\nθoreventhattherangeofgisthesameasthesetofallowablevaluesofθ. Thisdeﬁnitionofapointestimatorisverygeneralandallowsthedesignerofan\nestimatorgreatﬂexibility.Whilealmostanyfunctionthusqualiﬁesasanestimator,\nagoodestimatorisafunctionwhoseoutputisclosetothetrueunderlyingθthat\ngeneratedthetrainingdata. Fornow,wetakethefrequentistperspectiveonstatistics.Thatis,weassume\nthatthetrueparametervalueθisﬁxedbutunknown,whilethepointestimate\nˆθisafunctionofthedata.Sincethedataisdrawnfromarandomprocess,any\nfunctionofthedataisrandom.Therefore ˆθisarandomvariable. Pointestimationcanalsorefertotheestimationoftherelationshipbetween\ninputandtargetvariables.Werefertothesetypesofpointestimatesasfunction\nestimators.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 303, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 993}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0304_803f5fa1", "text": "Pointestimationcanalsorefertotheestimationoftherelationshipbetween\ninputandtargetvariables.Werefertothesetypesofpointestimatesasfunction\nestimators. FunctionEstimationAswementionedabove,sometimesweareinterestedin\nperformingfunctionestimation(orfunctionapproximation).Herewearetryingto\npredictavariableygivenaninputvectorx.Weassumethatthereisafunction\nf(x)thatdescribestheapproximate relationshipbetweenyandx.Forexample,\nwemayassumethaty=f(x)+,wherestandsforthepartofythatisnot\npredictablefromx. Infunctionestimation,weareinterestedinapproximating\nfwithamodelorestimate ˆf.Functionestimationisreallyjustthesameas\nestimatingaparameterθ;thefunctionestimator ˆfissimplyapointestimatorin\nfunctionspace.Thelinearregressionexample(discussedaboveinsection)and5.1.4\nthepolynomialregressionexample(discussedinsection)arebothexamplesof 5.2\nscenariosthatmaybeinterpretedeitherasestimatingaparameterworestimating\nafunction ˆf y mappingfromtox.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 304, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 932}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0305_e6903b86", "text": "Wenowreviewthemostcommonlystudiedpropertiesofpointestimatorsand\ndiscusswhattheytellusabouttheseestimators. 5.4.2Bias\nThebiasofanestimatorisdeﬁnedas:\nbias(ˆθ m) = ( Eˆθ m)−θ (5.20)\n1 2 4\nCHAPTER5.MACHINELEARNINGBASICS\nwheretheexpectationisoverthedata(seenassamplesfromarandomvariable)\nandθisthetrueunderlyingvalueofθusedtodeﬁnethedatageneratingdistri-\nbution.Anestimator ˆθ missaidtobeunbiasedifbias(ˆθ m) = 0,whichimplies\nthat E(ˆθ m)=θ.Anestimator ˆθ missaidtobeasymptoticallyunbiasedif\nlim m → ∞bias(ˆθ m) = 0,whichimpliesthatlim m → ∞ E(ˆθ m) = θ. Example:BernoulliDistributionConsiderasetofsamples {x(1),...,x() m}\nthatareindependentlyandidenticallydistributedaccordingtoaBernoullidistri-\nbutionwithmean:θ\nPx(() i;) = θθx() i(1 )−θ(1 − x() i). (5.21)\nAcommonestimatorfortheθparameterofthisdistributionisthemeanofthe\ntrainingsamples:\nˆθ m=1\nmm\ni=1x() i.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 305, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 857}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0306_a4ace735", "text": "(5.21)\nAcommonestimatorfortheθparameterofthisdistributionisthemeanofthe\ntrainingsamples:\nˆθ m=1\nmm\ni=1x() i. (5.22)\nTodeterminewhetherthisestimatorisbiased,wecansubstituteequation5.22\nintoequation:5.20\nbias(ˆθ m) = [ Eˆθ m]−θ (5.23)\n= E\n1\nmm\ni=1x() i\n−θ (5.24)\n=1\nmm\ni=1E\nx() i\n−θ (5.25)\n=1\nmm\ni=11\nx() i=0\nx() iθx() i(1 )−θ(1 − x() i)\n−θ(5.26)\n=1\nmm\ni=1()θ−θ (5.27)\n= = 0θθ− (5.28)\nSince bias(ˆθ) = 0,wesaythatourestimator ˆθisunbiased. Example:GaussianDistributionEstimatoroftheMeanNow,consider\nasetofsamples {x(1),...,x() m}thatareindependentlyandidenticallydistributed\naccordingtoaGaussiandistributionp(x() i) =N(x() i;µ,σ2),wherei∈{1,...,m}.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 306, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 658}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0307_9ea60e76", "text": "1 2 5\nCHAPTER5.MACHINELEARNINGBASICS\nRecallthattheGaussianprobabilitydensityfunctionisgivenby\npx(() i;µ,σ2) =1√\n2πσ2exp\n−1\n2(x() i−µ)2\nσ2\n.(5.29)\nAcommonestimatoroftheGaussianmeanparameterisknownasthesample\nmean:\nˆµ m=1\nmm\ni=1x() i(5.30)\nTodeterminethebiasofthesamplemean,weareagaininterestedincalculating\nitsexpectation:\nbias(ˆµ m) = [ˆ Eµ m]−µ (5.31)\n= E\n1\nmm\ni=1x() i\n−µ (5.32)\n=\n1\nmm\ni=1E\nx() i\n−µ (5.33)\n=\n1\nmm\ni=1µ\n−µ (5.34)\n= = 0µµ− (5.35)\nThusweﬁndthatthesamplemeanisanunbiasedestimatorofGaussianmean\nparameter. Example:EstimatorsoftheVarianceofaGaussianDistributionAsan\nexample,wecomparetwodiﬀerentestimatorsofthevarianceparameterσ2ofa\nGaussiandistribution.Weareinterestedinknowingifeitherestimatorisbiased.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 307, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 730}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0308_e89ab991", "text": "Example:EstimatorsoftheVarianceofaGaussianDistributionAsan\nexample,wecomparetwodiﬀerentestimatorsofthevarianceparameterσ2ofa\nGaussiandistribution.Weareinterestedinknowingifeitherestimatorisbiased. Theﬁrstestimatorofσ2weconsiderisknownasthesamplevariance:\nˆσ2\nm=1\nmm\ni=1\nx() i−ˆµ m2\n, (5.36)\nwhere ˆµ misthesamplemean,deﬁnedabove.Moreformally,weareinterestedin\ncomputing\nbias(ˆσ2\nm) = [ˆ Eσ2\nm]−σ2(5.37)\n1 2 6\nCHAPTER5.MACHINELEARNINGBASICS\nWebeginbyevaluatingtheterm E[ˆσ2\nm]:\nE[ˆσ2\nm] = E\n1\nmm\ni=1\nx() i−ˆµ m2\n(5.38)\n=m−1\nmσ2(5.39)\nReturningtoequation,weconcludethatthebiasof 5.37 ˆσ2\nmis−σ2/m.Therefore,\nthesamplevarianceisabiasedestimator. Theunbiasedsamplevarianceestimator\n˜σ2\nm=1\nm−1m\ni=1\nx() i−ˆµ m2\n(5.40)\nprovidesanalternativeapproach.Asthenamesuggeststhisestimatorisunbiased. Thatis,weﬁndthat E[˜σ2\nm] = σ2:\nE[˜σ2\nm] = E\n1\nm−1m\ni=1\nx() i−ˆµ m2\n(5.41)\n=m\nm−1E[ˆσ2\nm] (5.42)\n=m\nm−1m−1\nmσ2\n(5.43)\n= σ2.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 308, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 927}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0309_cabefe70", "text": "Thatis,weﬁndthat E[˜σ2\nm] = σ2:\nE[˜σ2\nm] = E\n1\nm−1m\ni=1\nx() i−ˆµ m2\n(5.41)\n=m\nm−1E[ˆσ2\nm] (5.42)\n=m\nm−1m−1\nmσ2\n(5.43)\n= σ2. (5.44)\nWehavetwoestimators:oneisbiasedandtheotherisnot.Whileunbiased\nestimatorsareclearlydesirable,theyarenotalwaysthe“best”estimators.Aswe\nwillseeweoftenusebiasedestimatorsthatpossessotherimportantproperties. 5.4.3VarianceandStandardError\nAnotherpropertyoftheestimatorthatwemightwanttoconsiderishowmuch\nweexpectittovaryasafunctionofthedatasample.Justaswecomputedthe\nexpectationoftheestimatortodetermineitsbias,wecancomputeitsvariance. Thevarianceofanestimatorissimplythevariance\nVar(ˆθ) (5.45)\nwheretherandomvariableisthetrainingset.Alternately,thesquarerootofthe\nvarianceiscalledthe ,denotedstandarderror SE(ˆθ).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 309, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 745}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0310_e12cce6f", "text": "Thevarianceofanestimatorissimplythevariance\nVar(ˆθ) (5.45)\nwheretherandomvariableisthetrainingset.Alternately,thesquarerootofthe\nvarianceiscalledthe ,denotedstandarderror SE(ˆθ). 1 2 7\nCHAPTER5.MACHINELEARNINGBASICS\nThevarianceorthestandarderrorofanestimatorprovidesameasureofhow\nwewouldexpecttheestimatewecomputefromdatatovaryasweindependently\nresamplethedatasetfromtheunderlyingdatageneratingprocess.Justaswe\nmightlikeanestimatortoexhibitlowbiaswewouldalsolikeittohaverelatively\nlowvariance. Whenwecomputeanystatisticusingaﬁnitenumberofsamples,ourestimate\nofthetrueunderlyingparameterisuncertain,inthesensethatwecouldhave\nobtainedothersamplesfromthesamedistributionandtheirstatisticswouldhave\nbeendiﬀerent.Theexpecteddegreeofvariationinanyestimatorisasourceof\nerrorthatwewanttoquantify.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 310, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 788}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0311_d2272c30", "text": "Thestandarderrorofthemeanisgivenby\nSE(ˆµ m) =Var\n1\nmm\ni=1x() i\n=σ√m, (5.46)\nwhereσ2isthetruevarianceofthesamplesxi.Thestandarderrorisoften\nestimatedbyusinganestimateofσ.Unfortunately,neitherthesquarerootof\nthesamplevariancenorthesquarerootoftheunbiasedestimatorofthevariance\nprovideanunbiasedestimateofthestandarddeviation.Bothapproachestend\ntounderestimatethetruestandarddeviation,butarestillusedinpractice.The\nsquarerootoftheunbiasedestimatorofthevarianceislessofanunderestimate. Forlarge,theapproximation isquitereasonable. m\nThestandarderrorofthemeanisveryusefulinmachinelearningexperiments.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 311, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 602}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0312_630b62ae", "text": "Forlarge,theapproximation isquitereasonable. m\nThestandarderrorofthemeanisveryusefulinmachinelearningexperiments. Weoftenestimatethegeneralization errorbycomputingthesamplemeanofthe\nerroronthetestset.Thenumberofexamplesinthetestsetdeterminesthe\naccuracyofthisestimate.Takingadvantageofthecentrallimittheorem,which\ntellsusthatthemeanwillbeapproximatelydistributedwithanormaldistribution,\nwecanusethestandarderrortocomputetheprobabilitythatthetrueexpectation\nfallsinanychoseninterval.Forexample,the95%conﬁdenceintervalcenteredon\nthemean ˆµ mis\n(ˆµ m−196SE( ˆ.µ m)ˆ,µ m+196SE( ˆ.µ m)), (5.47)\nunderthenormaldistributionwithmean ˆµ mandvariance SE(ˆµ m)2.Inmachine\nlearningexperiments,itiscommontosaythatalgorithmAisbetterthanalgorithm\nBiftheupperboundofthe95%conﬁdenceintervalfortheerrorofalgorithmAis\nlessthanthelowerboundofthe95%conﬁdenceintervalfortheerrorofalgorithm\nB.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 312, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 870}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0313_57a99016", "text": "1 2 8\nCHAPTER5.MACHINELEARNINGBASICS\nExample: BernoulliDistributionWeonceagainconsiderasetofsamples\n{x(1),...,x() m}drawnindependentlyandidenticallyfromaBernoullidistribution\n(recallP(x() i;θ) =θx() i(1−θ)(1 − x() i)).Thistimeweareinterestedincomputing\nthevarianceoftheestimator ˆθ m=1\nmm\ni=1x() i. Var\nˆθ m\n= Var\n1\nmm\ni=1x() i\n(5.48)\n=1\nm2m\ni=1Var\nx() i\n(5.49)\n=1\nm2m\ni=1θθ (1−) (5.50)\n=1\nm2mθθ (1−) (5.51)\n=1\nmθθ (1−) (5.52)\nThevarianceoftheestimatordecreasesasafunctionofm,thenumberofexamples\ninthedataset.Thisisacommonpropertyofpopularestimatorsthatwewill\nreturntowhenwediscussconsistency(seesection).5.4.5\n5.4.4TradingoﬀBiasandVariancetoMinimizeMeanSquared\nError\nBiasandvariancemeasuretwodiﬀerentsourcesoferrorinanestimator.Bias\nmeasurestheexpecteddeviationfromthetruevalueofthefunctionorparameter. Varianceontheotherhand,providesameasureofthedeviationfromtheexpected\nestimatorvaluethatanyparticularsamplingofthedataislikelytocause.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 313, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 947}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0314_df8ea9f8", "text": "Varianceontheotherhand,providesameasureofthedeviationfromtheexpected\nestimatorvaluethatanyparticularsamplingofthedataislikelytocause. Whathappenswhenwearegivenachoicebetweentwoestimators,onewith\nmorebiasandonewithmorevariance?Howdowechoosebetweenthem?For\nexample,imaginethatweareinterestedinapproximating thefunctionshownin\nﬁgureandweareonlyoﬀeredthechoicebetweenamodelwithlargebiasand 5.2\nonethatsuﬀersfromlargevariance.Howdowechoosebetweenthem? Themostcommonwaytonegotiatethistrade-oﬀistousecross-validation. Empirically,cross-validationishighlysuccessfulonmanyreal-worldtasks.Alter-\nnatively,wecanalsocomparethemeansquarederror(MSE)oftheestimates:\nMSE = [( Eˆθ m−θ)2] (5.53)\n= Bias(ˆθ m)2+Var(ˆθ m) (5.54)\n1 2 9\nCHAPTER5.MACHINELEARNINGBASICS\nTheMSEmeasurestheoverallexpecteddeviation—in asquarederrorsense—\nbetweentheestimatorandthetruevalueoftheparameterθ.Asisclearfrom\nequation,evaluatingtheMSEincorporatesboththebiasandthevariance.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 314, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 938}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0315_34854038", "text": "5.54\nDesirableestimatorsarethosewithsmallMSEandtheseareestimatorsthat\nmanagetokeepboththeirbiasandvariancesomewhatincheck. C apac i t yB i as Ge ne r al i z at i on\ne r r orV ar i anc e\nO pt i m al\nc apac i t yO v e r ﬁt t i ng z o n e U nde r ﬁt t i ng z o n e\nFigure5.6:Ascapacityincreases(x-axis),bias(dotted)tendstodecreaseandvariance\n(dashed)tendstoincrease,yieldinganotherU-shapedcurveforgeneralizationerror(bold\ncurve).Ifwevarycapacityalongoneaxis,thereisanoptimalcapacity,withunderﬁtting\nwhenthecapacityisbelowthisoptimumandoverﬁttingwhenitisabove.Thisrelationship\nissimilartotherelationshipbetweencapacity,underﬁtting,andoverﬁtting,discussedin\nsectionandﬁgure.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 315, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 669}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0316_9ce26dd6", "text": "5.2 5.3\nTherelationshipbetweenbiasandvarianceistightlylinkedtothemachine\nlearningconceptsofcapacity,underﬁttingandoverﬁtting.Inthecasewheregen-\neralizationerrorismeasuredbytheMSE(wherebiasandvariancearemeaningful\ncomponentsofgeneralization error),increasingcapacitytendstoincreasevariance\nanddecreasebias.Thisisillustratedinﬁgure,whereweseeagaintheU-shaped 5.6\ncurveofgeneralization errorasafunctionofcapacity. 5.4.5Consistency\nSofarwehavediscussedthepropertiesofvariousestimatorsforatrainingsetof\nﬁxedsize.Usually,wearealsoconcernedwiththebehaviorofanestimatorasthe\namountoftrainingdatagrows.Inparticular,weusuallywishthat,asthenumber\nofdatapointsminourdatasetincreases,ourpointestimatesconvergetothetrue\n1 3 0\nCHAPTER5.MACHINELEARNINGBASICS\nvalueofthecorrespondingparameters.Moreformally,wewouldlikethat\nplimm → ∞ˆθ m= θ.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 316, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 823}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0317_3aaf81fb", "text": "(5.55)\nThesymbolplimindicatesconvergenceinprobability,meaningthatforany>0,\nP(|ˆθ m−|θ>)→0asm→∞.Theconditiondescribedbyequationis5.55\nknownasconsistency.Itissometimesreferredtoasweakconsistency,with\nstrongconsistencyreferringtothealmostsureconvergenceofˆθtoθ.Almost\nsureconvergenceofasequenceofrandomvariables x(1), x(2),...toavaluex\noccurswhenp(lim m → ∞ x() m= ) = 1x. Consistencyensuresthatthebiasinducedbytheestimatordiminishesasthe\nnumberofdataexamplesgrows.However,thereverseisnottrue—asymptotic\nunbiasednessdoesnotimplyconsistency. Forexample,considerestimatingthe\nmeanparameterµofanormaldistributionN(x;µ,σ2),withadatasetconsisting\nofmsamples:{x(1),...,x() m}.Wecouldusetheﬁrstsamplex(1)ofthedataset\nasanunbiasedestimator:ˆθ=x(1).Inthatcase, E(ˆθ m)=θsotheestimator\nisunbiasednomatterhowmanydatapointsareseen.This,ofcourse,implies\nthattheestimateisasymptoticallyunbiased.However,thisisnotaconsistent\nestimatorasitisthecasethat not ˆθ m→ →∞θmas.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 317, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 953}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0318_c64a8f4e", "text": "5.5MaximumLikelihoodEstimation\nPreviously,wehaveseensomedeﬁnitionsofcommonestimatorsandanalyzed\ntheirproperties.Butwheredidtheseestimatorscomefrom?Ratherthanguessing\nthatsomefunctionmightmakeagoodestimatorandthenanalyzingitsbiasand\nvariance,wewouldliketohavesomeprinciplefromwhichwecanderivespeciﬁc\nfunctionsthataregoodestimatorsfordiﬀerentmodels. Themostcommonsuchprincipleisthemaximumlikelihoodprinciple. Considerasetofmexamples X={x(1),...,x() m}drawnindependentlyfrom\nthetruebutunknowndatageneratingdistributionpdata() x. Letpmodel( x;θ)beaparametricfamilyofprobabilitydistributionsoverthe\nsamespaceindexedbyθ.Inotherwords,pmodel(x;θ)mapsanyconﬁgurationx\ntoarealnumberestimatingthetrueprobabilitypdata()x. Themaximumlikelihoodestimatorforisthendeﬁnedas θ\nθML= argmax\nθpmodel(;) Xθ (5.56)\n= argmax\nθm\ni=1pmodel(x() i;)θ (5.57)\n1 3 1\nCHAPTER5.MACHINELEARNINGBASICS\nThisproductovermanyprobabilitiescanbeinconvenientforavarietyofreasons.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 318, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 938}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0319_46cb5e92", "text": "Forexample,itispronetonumericalunderﬂow.Toobtainamoreconvenient\nbutequivalentoptimization problem,weobservethattakingthelogarithmofthe\nlikelihooddoesnotchangeitsargmaxbutdoesconvenientlytransformaproduct\nintoasum:\nθML= argmax\nθm\ni=1logpmodel(x() i;)θ. (5.58)\nBecausetheargmaxdoesnotchangewhenwerescalethecostfunction,wecan\ndividebymtoobtainaversionofthecriterionthatisexpressedasanexpectation\nwithrespecttotheempiricaldistributionˆpdatadeﬁnedbythetrainingdata:\nθML= argmax\nθE x ∼ˆ pdatalogpmodel(;)xθ.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 319, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 502}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0320_52c2c921", "text": "(5.59)\nOnewaytointerpretmaximumlikelihoodestimationistoviewitasminimizing\nthedissimilaritybetweentheempiricaldistributionˆpdatadeﬁnedbythetraining\nsetandthemodeldistribution,withthedegreeofdissimilaritybetweenthetwo\nmeasuredbytheKLdivergence.TheKLdivergenceisgivenby\nDKL(ˆpdatapmodel) = E x ∼ˆ pdata[log ˆpdata()logx−pmodel()]x.(5.60)\nThetermontheleftisafunctiononlyofthedatageneratingprocess,notthe\nmodel.ThismeanswhenwetrainthemodeltominimizetheKLdivergence,we\nneedonlyminimize\n− E x ∼ˆ pdata[logpmodel()]x (5.61)\nwhichisofcoursethesameasthemaximization inequation.5.59\nMinimizingthisKLdivergencecorrespondsexactlytominimizingthecross-\nentropybetweenthedistributions.Manyauthorsusetheterm“cross-entropy”to\nidentifyspeciﬁcallythenegativelog-likelihoodofaBernoulliorsoftmaxdistribution,\nbutthatisamisnomer.Anylossconsistingofanegativelog-likelihoodisacross-\nentropybetweentheempiricaldistributiondeﬁnedbythetrainingsetandthe\nprobabilitydistributiondeﬁnedbymodel.Forexample,meansquarederroristhe\ncross-entropybetweentheempiricaldistributionandaGaussianmodel.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 320, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1058}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0321_9648a222", "text": "Wecanthusseemaximumlikelihoodasanattempttomakethemodeldis-\ntributionmatchtheempiricaldistributionˆpdata.Ideally,wewouldliketomatch\nthetruedatageneratingdistributionpdata,butwehavenodirectaccesstothis\ndistribution. Whiletheoptimalθisthesameregardlessofwhetherwearemaximizingthe\nlikelihoodorminimizingtheKLdivergence,thevaluesoftheobjectivefunctions\n1 3 2\nCHAPTER5.MACHINELEARNINGBASICS\narediﬀerent.Insoftware,weoftenphrasebothasminimizingacostfunction.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 321, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 451}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0322_a01dd54d", "text": "Maximumlikelihoodthusbecomesminimization ofthenegativelog-likelihood\n(NLL),orequivalently,minimization ofthecrossentropy.Theperspectiveof\nmaximumlikelihoodasminimumKLdivergencebecomeshelpfulinthiscase\nbecausetheKLdivergencehasaknownminimumvalueofzero.Thenegative\nlog-likelihoodcanactuallybecomenegativewhenisreal-valued.x\n5.5.1ConditionalLog-LikelihoodandMeanSquaredError\nThemaximumlikelihoodestimatorcanreadilybegeneralizedtothecasewhere\nourgoalistoestimateaconditionalprobabilityP( y x|;θ)inordertopredict y\ngiven x.Thisisactuallythemostcommonsituationbecauseitformsthebasisfor\nmostsupervisedlearning.IfXrepresentsallourinputsandYallourobserved\ntargets,thentheconditionalmaximumlikelihoodestimatoris\nθML= argmax\nθP. ( ;)YX|θ (5.62)\nIftheexamplesareassumedtobei.i.d.,thenthiscanbedecomposedinto\nθML= argmax\nθm\ni=1log(Py() i|x() i;)θ.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 322, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 835}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0323_5c9cc55a", "text": "( ;)YX|θ (5.62)\nIftheexamplesareassumedtobei.i.d.,thenthiscanbedecomposedinto\nθML= argmax\nθm\ni=1log(Py() i|x() i;)θ. (5.63)\nExample:LinearRegressionasMaximumLikelihoodLinearregression,\nintroducedearlierinsection,maybejustiﬁedasamaximumlikelihood 5.1.4\nprocedure.Previously,wemotivatedlinearregressionasanalgorithmthatlearns\ntotakeaninputxandproduceanoutputvalue ˆy.Themappingfromxtoˆyis\nchosentominimizemeansquarederror,acriterionthatweintroducedmoreorless\narbitrarily.Wenowrevisitlinearregressionfromthepointofviewofmaximum\nlikelihoodestimation.Insteadofproducingasingleprediction ˆy,wenowthink\nofthemodelasproducingaconditionaldistributionp(y|x).Wecanimagine\nthatwithaninﬁnitelylargetrainingset,wemightseeseveraltrainingexamples\nwiththesameinputvaluexbutdiﬀerentvaluesofy.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 323, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 775}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0324_aa01b109", "text": "Thegoalofthelearning\nalgorithmisnowtoﬁtthedistributionp(y|x)toallofthosediﬀerentyvalues\nthatareallcompatiblewithx.Toderivethesamelinearregressionalgorithm\nweobtainedbefore,wedeﬁnep(y|x) =N(y;ˆy(x;w),σ2).Thefunction ˆy(x;w)\ngivesthepredictionofthemeanoftheGaussian.Inthisexample,weassumethat\nthevarianceisﬁxedtosomeconstantσ2chosenbytheuser.Wewillseethatthis\nchoiceofthefunctionalformofp(y|x)causesthemaximumlikelihoodestimation\nproceduretoyieldthesamelearningalgorithmaswedevelopedbefore.Sincethe\n1 3 3\nCHAPTER5.MACHINELEARNINGBASICS\nexamplesareassumedtobei.i.d.,theconditionallog-likelihood(equation)is5.63\ngivenby\nm\ni=1log(py() i|x() i;)θ (5.64)\n= log −mσ−m\n2log(2)π−m\ni=1ˆy() i−y() i2\n2σ2,(5.65)\nwhere ˆy() iistheoutputofthelinearregressiononthei-thinputx() iandmisthe\nnumberofthetrainingexamples.Comparingthelog-likelihoodwiththemean\nsquarederror,\nMSEtrain=1\nmm\ni=1||ˆy() i−y() i||2, (5.66)\nweimmediately seethatmaximizingthelog-likelihoodwithrespecttowyields\nthesameestimateoftheparameterswasdoesminimizingthemeansquarederror.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 324, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1038}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0325_d138d225", "text": "Thetwocriteriahavediﬀerentvaluesbutthesamelocationoftheoptimum.This\njustiﬁestheuseoftheMSEasamaximumlikelihoodestimationprocedure.Aswe\nwillsee,themaximumlikelihoodestimatorhasseveraldesirableproperties. 5.5.2PropertiesofMaximumLikelihood\nThemainappealofthemaximumlikelihoodestimatoristhatitcanbeshownto\nbethebestestimatorasymptotically,asthenumberofexamplesm→∞,interms\nofitsrateofconvergenceasincreases.m\nUnderappropriate conditions, the maximumlikelihood estimatorhas the\npropertyofconsistency(seesectionabove),meaningthatasthenumber 5.4.5\noftrainingexamplesapproachesinﬁnity,themaximumlikelihoodestimateofa\nparameterconvergestothetruevalueoftheparameter.Theseconditionsare:\n•Thetruedistributionpdatamustliewithinthemodelfamilypmodel(·;θ). Otherwise,noestimatorcanrecoverpdata. •Thetruedistributionpdatamustcorrespondtoexactlyonevalueofθ.Other-\nwise,maximumlikelihoodcanrecoverthecorrectpdata,butwillnotbeable\ntodeterminewhichvalueofwasusedbythedatageneratingprocessing.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 325, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 971}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0326_5d30fd67", "text": "•Thetruedistributionpdatamustcorrespondtoexactlyonevalueofθ.Other-\nwise,maximumlikelihoodcanrecoverthecorrectpdata,butwillnotbeable\ntodeterminewhichvalueofwasusedbythedatageneratingprocessing. θ\nThereareotherinductiveprinciplesbesidesthemaximumlikelihoodestima-\ntor,manyofwhichsharethepropertyofbeingconsistentestimators. However,\n1 3 4\nCHAPTER5.MACHINELEARNINGBASICS\nconsistentestimatorscandiﬀerintheirstatisticeﬃciency,meaningthatone\nconsistentestimatormayobtainlowergeneralization errorforaﬁxednumberof\nsamplesm,orequivalently,mayrequirefewerexamplestoobtainaﬁxedlevelof\ngeneralization error.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 326, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 595}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0327_c80e6494", "text": "Statisticaleﬃciencyistypicallystudiedintheparametriccase(likeinlinear\nregression)whereourgoalistoestimatethevalueofaparameter(andassuming\nitispossibletoidentifythetrueparameter),notthevalueofafunction.Awayto\nmeasurehowclosewearetothetrueparameterisbytheexpectedmeansquared\nerror,computingthesquareddiﬀerencebetweentheestimatedandtrueparameter\nvalues,wheretheexpectationisovermtrainingsamplesfromthedatagenerating\ndistribution.Thatparametricmeansquarederrordecreasesasmincreases,and\nformlarge,theCramér-Raolowerbound(,;,)showsthatno Rao1945Cramér1946\nconsistentestimatorhasalowermeansquarederrorthanthemaximumlikelihood\nestimator. Forthesereasons(consistencyandeﬃciency),maximumlikelihoodisoften\nconsideredthepreferredestimatortouseformachinelearning.Whenthenumber\nofexamplesissmallenoughtoyieldoverﬁttingbehavior,regularizationstrategies\nsuchasweightdecaymaybeusedtoobtainabiasedversionofmaximumlikelihood\nthathaslessvariancewhentrainingdataislimited.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 327, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 951}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0328_f04c5eb4", "text": "5.6BayesianStatistics\nSofarwehavediscussedfrequentiststatisticsandapproachesbasedonestimat-\ningasinglevalueofθ,thenmakingallpredictionsthereafterbasedonthatone\nestimate.Anotherapproachistoconsiderallpossiblevaluesofθwhenmakinga\nprediction.ThelatteristhedomainofBayesianstatistics. Asdiscussed insection , the frequen tist perspective isthat thetrue 5.4.1\nparametervalueθisﬁxedbutunknown,whilethepointestimate ˆθisarandom\nvariableonaccountofitbeingafunctionofthedataset(whichisseenasrandom). TheBayesianperspectiveonstatisticsisquitediﬀerent. The Bayesianuses\nprobabilitytoreﬂectdegreesofcertaintyofstatesofknowledge.Thedatasetis\ndirectlyobservedandsoisnotrandom.Ontheotherhand,thetrueparameterθ\nisunknownoruncertainandthusisrepresentedasarandomvariable. Beforeobservingthedata,werepresentourknowledgeofθusingtheprior\nprobabilitydistribution,p(θ)(sometimesreferredtoassimply“theprior”).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 328, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 885}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0329_c3cd3562", "text": "Beforeobservingthedata,werepresentourknowledgeofθusingtheprior\nprobabilitydistribution,p(θ)(sometimesreferredtoassimply“theprior”). Generally,themachinelearningpractitionerselectsapriordistributionthatis\nquitebroad(i.e.withhighentropy)toreﬂectahighdegreeofuncertaintyinthe\n1 3 5\nCHAPTER5.MACHINELEARNINGBASICS\nvalueofθbeforeobservinganydata.Forexample,onemightassume that apriori\nθliesinsomeﬁniterangeorvolume,withauniformdistribution. Manypriors\ninsteadreﬂectapreferencefor“simpler” solutions(suchassmallermagnitude\ncoeﬃcients,orafunctionthatisclosertobeingconstant).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 329, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 568}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0330_d7f984f0", "text": "Manypriors\ninsteadreﬂectapreferencefor“simpler” solutions(suchassmallermagnitude\ncoeﬃcients,orafunctionthatisclosertobeingconstant). Nowconsiderthatwehaveasetofdatasamples {x(1),...,x() m}.Wecan\nrecovertheeﬀectofdataonourbeliefaboutθbycombiningthedatalikelihood\npx((1),...,x() m|θ)withthepriorviaBayes’rule:\npx(θ|(1),...,x() m) =px((1),...,x() m|θθ)(p)\npx((1),...,x() m)(5.67)\nInthescenarioswhereBayesianestimationistypicallyused,thepriorbeginsasa\nrelativelyuniformorGaussiandistributionwithhighentropy,andtheobservation\nofthedatausuallycausestheposteriortoloseentropyandconcentratearounda\nfewhighlylikelyvaluesoftheparameters.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 330, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 627}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0331_5729a125", "text": "Relativetomaximumlikelihoodestimation,Bayesianestimationoﬀerstwo\nimportantdiﬀerences.First,unlikethemaximumlikelihoodapproachthatmakes\npredictionsusingapointestimateofθ,theBayesianapproachistomakepredictions\nusingafulldistributionoverθ.Forexample,afterobservingmexamples,the\npredicteddistributionoverthenextdatasample,x(+1) m,isgivenby\npx((+1) m|x(1),...,x() m) =\npx((+1) m| |θθ)(px(1),...,x() m)d.θ(5.68)\nHereeachvalueofθwithpositiveprobabilitydensitycontributestotheprediction\nofthenextexample,withthecontributionweightedbytheposteriordensityitself. Afterhavingobserved{x(1),...,x() m},ifwearestillquiteuncertainaboutthe\nvalueofθ,thenthisuncertaintyisincorporated directlyintoanypredictionswe\nmightmake.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 331, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 706}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0332_06f7d634", "text": "Afterhavingobserved{x(1),...,x() m},ifwearestillquiteuncertainaboutthe\nvalueofθ,thenthisuncertaintyisincorporated directlyintoanypredictionswe\nmightmake. Insection,wediscussedhowthefrequentistapproachaddressestheuncer- 5.4\ntaintyinagivenpointestimateofθbyevaluatingitsvariance.Thevarianceof\ntheestimatorisanassessmentofhowtheestimatemightchangewithalternative\nsamplingsoftheobserveddata.TheBayesiananswertothequestionofhowtodeal\nwiththeuncertaintyintheestimatoristosimplyintegrateoverit,whichtendsto\nprotectwellagainstoverﬁtting. Thisintegralisofcoursejustanapplicationof\nthelawsofprobability,makingtheBayesianapproachsimpletojustify,whilethe\nfrequentistmachineryforconstructinganestimatorisbasedontheratheradhoc\ndecisiontosummarizeallknowledgecontainedinthedatasetwithasinglepoint\nestimate.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 332, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 791}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0333_a3ee772f", "text": "ThesecondimportantdiﬀerencebetweentheBayesianapproachtoestimation\nandthemaximumlikelihoodapproachisduetothecontributionoftheBayesian\n1 3 6\nCHAPTER5.MACHINELEARNINGBASICS\npriordistribution.Thepriorhasaninﬂuencebyshiftingprobabilitymassdensity\ntowardsregionsoftheparameterspacethatarepreferred .Inpractice, apriori\ntheprioroftenexpressesapreferenceformodelsthataresimplerormoresmooth. CriticsoftheBayesianapproachidentifythepriorasasourceofsubjectivehuman\njudgmentimpactingthepredictions. Bayesianmethodstypicallygeneralizemuchbetterwhenlimitedtrainingdata\nisavailable,buttypicallysuﬀerfromhighcomputational costwhenthenumberof\ntrainingexamplesislarge. Example:BayesianLinearRegressionHereweconsidertheBayesianesti-\nmationapproachtolearningthelinearregressionparameters.Inlinearregression,\nwelearnalinearmappingfromaninputvectorx∈ Rntopredictthevalueofa\nscalar.Thepredictionisparametrized bythevector y∈ R w∈ Rn:\nˆy= wx.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 333, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 919}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0334_cefdcbaf", "text": "(5.69)\nGivenasetofmtrainingsamples (X()train,y()train),wecanexpresstheprediction\nofovertheentiretrainingsetas: y\nˆy()train= X()trainw. (5.70)\nExpressedasaGaussianconditionaldistributionony()train,wehave\np(y()train|X()train,wy ) = (N()train;X()trainwI,) (5.71)\n∝exp\n−1\n2(y()train−X()trainw)(y()train−X()trainw)\n,\n(5.72)\nwherewefollowthestandardMSEformulationinassumingthattheGaussian\nvarianceonyisone.Inwhatfollows,toreducethenotationalburden,wereferto\n(X()train,y()train) ( ) assimplyXy,. Todeterminetheposteriordistributionoverthemodelparametervectorw,we\nﬁrstneedtospecifyapriordistribution.Thepriorshouldreﬂectournaivebelief\naboutthevalueoftheseparameters.Whileitissometimesdiﬃcultorunnatural\ntoexpressourpriorbeliefsintermsoftheparametersofthemodel,inpracticewe\ntypicallyassumeafairlybroaddistributionexpressingahighdegreeofuncertainty\naboutθ.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 334, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 849}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0335_49627135", "text": "Forreal-valuedparametersitiscommontouseaGaussianasaprior\ndistribution:\np() = (;w Nwµ0, Λ0) exp∝\n−1\n2(wµ−0)Λ−1\n0(wµ−0)\n,(5.73)\n1 3 7\nCHAPTER5.MACHINELEARNINGBASICS\nwhereµ0and Λ0arethepriordistributionmeanvectorandcovariancematrix\nrespectively.1\nWiththepriorthusspeciﬁed,wecannowproceedindeterminingtheposterior\ndistributionoverthemodelparameters. p,p,p (wX|y) ∝(yX|w)()w (5.74)\n∝exp\n−1\n2( )yXw−( )yXw−\nexp\n−1\n2(wµ−0)Λ−1\n0(wµ−0)\n(5.75)\n∝exp\n−1\n2\n−2yXww+XXww+Λ−1\n0wµ−2\n0 Λ−1\n0w\n. (5.76)\nWenowdeﬁne Λ m=\nXX+ Λ−1\n0 −1andµ m= Λ m\nXy+ Λ−1\n0µ0\n.Using\nthesenewvariables,weﬁndthattheposteriormayberewrittenasaGaussian\ndistribution:\np, (wX|y) exp∝\n−1\n2(wµ− m)Λ−1\nm(wµ− m)+1\n2µ\nm Λ−1\nmµ m\n(5.77)\n∝exp\n−1\n2(wµ− m)Λ−1\nm(wµ− m)\n. (5.78)\nAlltermsthatdonotincludetheparametervectorwhavebeenomitted;they\nareimpliedbythefactthatthedistributionmustbenormalizedtointegrateto.1\nEquationshowshowtonormalizeamultivariateGaussiandistribution.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 335, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 948}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0336_ce93cbaf", "text": "3.23\nExaminingthisposteriordistributionallowsustogainsomeintuitionforthe\neﬀectofBayesianinference.Inmostsituations,wesetµ0to 0.Ifweset Λ0=1\nαI,\nthenµ mgivesthesameestimateofwasdoesfrequentistlinearregressionwitha\nweightdecaypenaltyofαww.OnediﬀerenceisthattheBayesianestimateis\nundeﬁnedifαissettozero—-wearenotallowedtobegintheBayesianlearning\nprocesswithaninﬁnitelywideprioronw.Themoreimportantdiﬀerenceisthat\ntheBayesianestimateprovidesacovariancematrix,showinghowlikelyallthe\ndiﬀerentvaluesofare,ratherthanprovidingonlytheestimate w µ m. 5.6.1Maximum (MAP)Estimation A P o s t e ri o ri\nWhilethemostprincipledapproachistomakepredictionsusingthefullBayesian\nposteriordistributionovertheparameterθ,itisstilloftendesirabletohavea\n1Un l e s s t h e re i s a re a s o n t o a s s u m e a p a rtic u l a r c o v a ria n c e s t ru c t u re , we t y p i c a l l y a s s u m e a\nd i a g o n a l c o v a ria n c e m a t rix Λ0= diag( λ0) . 1 3 8\nCHAPTER5.MACHINELEARNINGBASICS\nsinglepointestimate.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 336, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 991}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0337_ec324412", "text": "1 3 8\nCHAPTER5.MACHINELEARNINGBASICS\nsinglepointestimate. Onecommonreasonfordesiringapointestimateisthat\nmostoperationsinvolvingtheBayesianposteriorformostinterestingmodelsare\nintractable,andapointestimateoﬀersatractableapproximation.Ratherthan\nsimplyreturningtothemaximumlikelihoodestimate,wecanstillgainsomeof\nthebeneﬁtoftheBayesianapproachbyallowingthepriortoinﬂuencethechoice\nofthepointestimate.Onerationalwaytodothisistochoosethemaximum\naposteriori(MAP)pointestimate.TheMAPestimatechoosesthepointof\nmaximalposteriorprobability(ormaximalprobabilitydensityinthemorecommon\ncaseofcontinuous):θ\nθMAP= argmax\nθp( ) = argmaxθx|\nθlog( )+log() pxθ|pθ.(5.79)\nWerecognize,aboveontherighthandside,logp(xθ|),i.e.thestandardlog-\nlikelihoodterm,and,correspondingtothepriordistribution.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 337, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 775}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0338_072ce47a", "text": "log()pθ\nAsanexample,consideralinearregressionmodelwithaGaussianprioron\ntheweightsw.IfthispriorisgivenbyN(w; 0,1\nλI2),thenthelog-priortermin\nequationisproportional tothefamiliar 5.79 λwwweightdecaypenalty,plusa\ntermthatdoesnotdependonwanddoesnotaﬀectthelearningprocess.MAP\nBayesianinferencewithaGaussianpriorontheweightsthuscorrespondstoweight\ndecay. AswithfullBayesianinference,MAPBayesianinferencehastheadvantageof\nleveraginginformationthatisbroughtbythepriorandcannotbefoundinthe\ntrainingdata.Thisadditionalinformationhelpstoreducethevarianceinthe\nMAPpointestimate(incomparisontotheMLestimate).However,itdoessoat\nthepriceofincreasedbias.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 338, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 640}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0339_0b92fff2", "text": "Manyregularizedestimationstrategies,suchasmaximumlikelihoodlearning\nregularizedwithweightdecay,canbeinterpretedasmakingtheMAPapproxima-\ntiontoBayesianinference.Thisviewapplieswhentheregularizationconsistsof\naddinganextratermtotheobjectivefunctionthatcorrespondstologp(θ).Not\nallregularizationpenaltiescorrespondtoMAPBayesianinference.Forexample,\nsomeregularizertermsmaynotbethelogarithmofaprobabilitydistribution. Otherregularizationtermsdependonthedata,whichofcourseapriorprobability\ndistributionisnotallowedtodo. MAPBayesianinferenceprovidesastraightforwardwaytodesigncomplicated\nyetinterpretableregularizationterms.Forexample,amorecomplicatedpenalty\ntermcanbederivedbyusingamixtureofGaussians,ratherthanasingleGaussian\ndistribution,astheprior(NowlanandHinton1992,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 339, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 768}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0340_8098b223", "text": "1 3 9\nCHAPTER5.MACHINELEARNINGBASICS\n5.7SupervisedLearningAlgorithms\nRecallfromsectionthatsupervisedlearningalgorithmsare,roughlyspeaking, 5.1.3\nlearningalgorithmsthatlearntoassociatesomeinputwithsomeoutput,givena\ntrainingsetofexamplesofinputsxandoutputsy. Inmanycasestheoutputs\nymaybediﬃculttocollectautomatically andmustbeprovidedbyahuman\n“supervisor,”butthetermstillappliesevenwhenthetrainingsettargetswere\ncollectedautomatically . 5.7.1ProbabilisticSupervisedLearning\nMost supervised learning algorithms inthis book are based on estimating a\nprobabilitydistributionp(y|x).Wecandothissimplybyusingmaximum\nlikelihoodestimationtoﬁndthebestparametervectorθforaparametricfamily\nofdistributions .py(|xθ;)\nWehavealreadyseenthatlinearregressioncorrespondstothefamily\npyy (| Nxθ;) = (;θxI,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 340, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 787}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0341_05b6264b", "text": "(5.80)\nWecangeneralizelinearregressiontotheclassiﬁcationscenariobydeﬁninga\ndiﬀerentfamilyofprobabilitydistributions.Ifwehavetwoclasses,class0and\nclass1,thenweneedonlyspecifytheprobabilityofoneoftheseclasses.The\nprobabilityofclass1determinestheprobabilityofclass0,becausethesetwovalues\nmustaddupto1. Thenormaldistributionoverreal-valuednumbersthatweusedforlinear\nregressionisparametrized intermsofamean.Anyvaluewesupplyforthismean\nisvalid.Adistributionoverabinaryvariableisslightlymorecomplicated,because\nitsmeanmustalwaysbebetween0and1.Onewaytosolvethisproblemistouse\nthelogisticsigmoidfunctiontosquashtheoutputofthelinearfunctionintothe\ninterval(0,1)andinterpretthatvalueasaprobability:\npy σ (= 1 ;) = |xθ (θx). (5.81)\nThisapproachisknownaslogisticregression(asomewhatstrangenamesince\nweusethemodelforclassiﬁcationratherthanregression).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 341, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 838}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0342_4eef6032", "text": "(5.81)\nThisapproachisknownaslogisticregression(asomewhatstrangenamesince\nweusethemodelforclassiﬁcationratherthanregression). Inthecaseoflinearregression,wewereabletoﬁndtheoptimalweightsby\nsolvingthenormalequations.Logisticregressionissomewhatmorediﬃcult.There\nisnoclosed-formsolutionforitsoptimalweights.Instead,wemustsearchfor\nthembymaximizingthelog-likelihood.Wecandothisbyminimizingthenegative\nlog-likelihood(NLL)usinggradientdescent. 1 4 0\nCHAPTER5.MACHINELEARNINGBASICS\nThissamestrategycanbeappliedtoessentiallyanysupervisedlearningproblem,\nbywritingdownaparametricfamilyofconditionalprobabilitydistributionsover\ntherightkindofinputandoutputvariables.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 342, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 656}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0343_cf3b6e84", "text": "5.7.2SupportVectorMachines\nOneofthemostinﬂuentialapproachestosupervisedlearningisthesupportvector\nmachine(,; Boseretal.1992CortesandVapnik1995,).Thismodelissimilarto\nlogisticregressioninthatitisdrivenbyalinearfunctionwx+b.Unlikelogistic\nregression,thesupportvectormachinedoesnotprovideprobabilities, butonly\noutputsaclassidentity.TheSVMpredictsthatthepositiveclassispresentwhen\nwx+bispositive.Likewise,itpredictsthatthenegativeclassispresentwhen\nwx+bisnegative.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 343, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 464}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0344_ea0041ce", "text": "Onekeyinnovationassociatedwithsupportvectormachinesisthekernel\ntrick.Thekerneltrickconsistsofobservingthatmanymachinelearningalgorithms\ncanbewrittenexclusivelyintermsofdotproductsbetweenexamples.Forexample,\nitcanbeshownthatthelinearfunctionusedbythesupportvectormachinecan\nbere-writtenas\nwx+= +bbm\ni=1α ixx() i(5.82)\nwherex() iisatrainingexampleandαisavectorofcoeﬃcients.Rewritingthe\nlearningalgorithmthiswayallowsustoreplacexbytheoutputofagivenfeature\nfunctionφ(x) andthedotproductwithafunctionk(xx,() i) =φ(x)·φ(x() i) called\nakernel.The ·operatorrepresentsaninnerproductanalogoustoφ(x)φ(x() i). Forsomefeaturespaces,wemaynotuseliterallythevectorinnerproduct.In\nsomeinﬁnitedimensionalspaces,weneedtouseotherkindsofinnerproducts,for\nexample,innerproductsbasedonintegrationratherthansummation.Acomplete\ndevelopmentofthesekindsofinnerproductsisbeyondthescopeofthisbook. Afterreplacingdotproductswithkernelevaluations,wecanmakepredictions\nusingthefunction\nfb () = x +\niα ik,(xx() i).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 344, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0345_db3c0496", "text": "Afterreplacingdotproductswithkernelevaluations,wecanmakepredictions\nusingthefunction\nfb () = x +\niα ik,(xx() i). (5.83)\nThisfunctionisnonlinearwithrespecttox,buttherelationshipbetweenφ(x)\nandf(x)islinear.Also,therelationshipbetweenαandf(x)islinear.The\nkernel-basedfunctionisexactlyequivalenttopreprocessingthedatabyapplying\nφ()xtoallinputs,thenlearningalinearmodelinthenewtransformedspace. Thekerneltrickispowerfulfortworeasons.First,itallowsustolearnmodels\nthatarenonlinearasafunctionofxusingconvexoptimization techniquesthatare\n1 4 1\nCHAPTER5.MACHINELEARNINGBASICS\nguaranteedtoconvergeeﬃciently.Thisispossiblebecauseweconsiderφﬁxedand\noptimizeonlyα,i.e.,theoptimization algorithmcanviewthedecisionfunction\nasbeinglinearinadiﬀerentspace.Second,thekernelfunctionkoftenadmits\nanimplementationthatissigniﬁcantlymorecomputational eﬃcientthannaively\nconstructingtwovectorsandexplicitlytakingtheirdotproduct.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 345, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 904}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0346_13ccbbe4", "text": "φ()x\nInsomecases,φ(x)canevenbeinﬁnitedimensional,whichwouldresultin\naninﬁnitecomputational costforthenaive,explicitapproach.Inmanycases,\nk(xx,)isanonlinear,tractablefunctionofxevenwhenφ(x)isintractable.As\nanexampleofaninﬁnite-dimens ionalfeaturespacewithatractablekernel,we\nconstructafeaturemappingφ(x)overthenon-negativeintegersx.Supposethat\nthismappingreturnsavectorcontainingxonesfollowedbyinﬁnitelymanyzeros. Wecanwriteakernelfunctionk(x,x() i) =min(x,x() i)thatisexactlyequivalent\ntothecorrespondinginﬁnite-dimens ionaldotproduct. ThemostcommonlyusedkernelistheGaussiankernel\nk, ,σ (uvuv ) = (N −;02I) (5.84)\nwhere N(x;µ, Σ)isthestandardnormaldensity.Thiskernelisalsoknownas\ntheradialbasisfunction(RBF)kernel,becauseitsvaluedecreasesalonglines\ninvspaceradiatingoutwardfromu.TheGaussiankernelcorrespondstoadot\nproductinaninﬁnite-dimens ionalspace,butthederivationofthisspaceisless\nstraightforwardthaninourexampleofthekernelovertheintegers.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 346, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 944}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0347_5f49070d", "text": "min\nWecanthinkoftheGaussiankernelasperformingakindoftemplatematch-\ning.Atrainingexamplexassociatedwithtraininglabelybecomesatemplate\nforclassy.WhenatestpointxisnearxaccordingtoEuclideandistance,the\nGaussiankernelhasalargeresponse,indicatingthatxisverysimilartothex\ntemplate.Themodelthenputsalargeweightontheassociatedtraininglabely. Overall,thepredictionwillcombinemanysuchtraininglabelsweightedbythe\nsimilarityofthecorrespondingtrainingexamples. Supportvectormachinesarenottheonlyalgorithmthatcanbeenhanced\nusingthekerneltrick.Manyotherlinearmodelscanbeenhancedinthisway.The\ncategoryofalgorithmsthatemploythekerneltrickisknownaskernelmachines\norkernelmethods( ,; WilliamsandRasmussen1996Schölkopf1999etal.,). Amajordrawbacktokernelmachinesisthatthecostofevaluatingthedecision\nfunctionislinearinthenumberoftrainingexamples,becausethei-thexample\ncontributesatermα ik(xx,() i)tothedecisionfunction.Supportvectormachines\nareabletomitigatethisbylearninganαvectorthatcontainsmostlyzeros.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 347, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 984}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0348_6b8d4106", "text": "Classifyinganewexamplethenrequiresevaluatingthekernelfunctiononlyfor\nthetrainingexamplesthathavenon-zeroα i.Thesetrainingexamplesareknown\n1 4 2\nCHAPTER5.MACHINELEARNINGBASICS\nassupportvectors. Kernelmachinesalsosuﬀerfromahighcomputational costoftrainingwhen\nthedatasetislarge.Wewillrevisitthisideainsection.Kernelmachineswith 5.9\ngenerickernelsstruggletogeneralizewell.Wewillexplainwhyinsection.The5.11\nmodernincarnationofdeeplearningwasdesignedtoovercometheselimitationsof\nkernelmachines.ThecurrentdeeplearningrenaissancebeganwhenHintonetal. ()demonstratedthataneuralnetworkcouldoutperformtheRBFkernelSVM 2006\nontheMNISTbenchmark.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 348, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 631}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0349_05294a26", "text": "()demonstratedthataneuralnetworkcouldoutperformtheRBFkernelSVM 2006\nontheMNISTbenchmark. 5.7.3OtherSimpleSupervisedLearningAlgorithms\nWehavealreadybrieﬂyencounteredanothernon-probabilis ticsupervisedlearning\nalgorithm,nearestneighborregression.Moregenerally,k-nearestneighborsis\nafamilyoftechniquesthatcanbeusedforclassiﬁcationorregression.Asa\nnon-parametric learningalgorithm,k-nearestneighborsisnotrestrictedtoaﬁxed\nnumberofparameters.Weusuallythinkofthek-nearestneighborsalgorithm\nasnothavinganyparameters,butratherimplementingasimplefunctionofthe\ntrainingdata.Infact,thereisnotevenreallyatrainingstageorlearningprocess.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 349, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 623}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0350_39177a5a", "text": "Instead,attesttime,whenwewanttoproduceanoutputyforanewtestinputx,\nweﬁndthek-nearestneighborstoxinthetrainingdataX.Wethenreturnthe\naverageofthecorrespondingyvaluesinthetrainingset.Thisworksforessentially\nanykindofsupervisedlearningwherewecandeﬁneanaverageoveryvalues.In\nthecaseofclassiﬁcation,wecanaverageoverone-hotcodevectorscwithc y= 1\nandc i= 0forallothervaluesofi.Wecantheninterprettheaverageoverthese\none-hotcodesasgivingaprobabilitydistributionoverclasses.Asanon-parametric\nlearningalgorithm,k-nearestneighborcanachieveveryhighcapacity.Forexample,\nsupposewehaveamulticlassclassiﬁcationtaskandmeasureperformancewith0-1\nloss.Inthissetting,-nearestneighborconvergestodoubletheBayeserrorasthe 1\nnumberoftrainingexamplesapproachesinﬁnity.TheerrorinexcessoftheBayes\nerrorresultsfromchoosingasingleneighborbybreakingtiesbetweenequally\ndistantneighborsrandomly.Whenthereisinﬁnitetrainingdata,alltestpointsx\nwillhaveinﬁnitelymanytrainingsetneighborsatdistancezero.Ifweallowthe\nalgorithmtousealloftheseneighborstovote,ratherthanrandomlychoosingone\nofthem,theprocedureconvergestotheBayeserrorrate.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 350, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1092}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0351_8b28733f", "text": "Thehighcapacityof\nk-nearestneighborsallowsittoobtainhighaccuracygivenalargetrainingset. However,itdoessoathighcomputational cost,anditmaygeneralizeverybadly\ngivenasmall,ﬁnitetrainingset.Oneweaknessofk-nearestneighborsisthatit\ncannotlearnthatonefeatureismorediscriminativethananother.Forexample,\nimaginewehavearegressiontaskwithx∈ R100drawnfromanisotropicGaussian\n1 4 3\nCHAPTER5.MACHINELEARNINGBASICS\ndistribution,butonlyasinglevariablex1isrelevanttotheoutput.Suppose\nfurtherthatthisfeaturesimplyencodestheoutputdirectly,i.e.thaty=x1inall\ncases.Nearestneighborregressionwillnotbeabletodetectthissimplepattern. Thenearestneighborofmostpointsxwillbedeterminedbythelargenumberof\nfeaturesx2throughx100,notbythelonefeaturex1. Thustheoutputonsmall\ntrainingsetswillessentiallyberandom. 1 4 4\nCHAPTER5.MACHINELEARNINGBASICS\n0\n101\n1110 1\n011\n1111 1110110\n10010\n001110 11111101001 00\n010 01111\n111\n11\nFigure5.7:Diagramsdescribinghowadecisiontreeworks.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 351, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 940}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0352_1c3f403e", "text": "1 4 4\nCHAPTER5.MACHINELEARNINGBASICS\n0\n101\n1110 1\n011\n1111 1110110\n10010\n001110 11111101001 00\n010 01111\n111\n11\nFigure5.7:Diagramsdescribinghowadecisiontreeworks. ( T o p )Eachnodeofthetree\nchoosestosendtheinputexampletothechildnodeontheleft(0)ororthechildnodeon\ntheright(1).Internalnodesaredrawnascirclesandleafnodesassquares.Eachnodeis\ndisplayedwithabinarystringidentiﬁercorrespondingtoitspositioninthetree,obtained\nbyappendingabittoitsparentidentiﬁer(0=chooseleftortop,1=chooserightorbottom). ( Bottom )Thetreedividesspaceintoregions.The2Dplaneshowshowadecisiontree\nmightdivide R2.Thenodesofthetreeareplottedinthisplane,witheachinternalnode\ndrawnalongthedividinglineitusestocategorizeexamples,andleafnodesdrawninthe\ncenteroftheregionofexamplestheyreceive.Theresultisapiecewise-constantfunction,\nwithonepieceperleaf.Eachleafrequiresatleastonetrainingexampletodeﬁne,soitis\nnotpossibleforthedecisiontreetolearnafunctionthathasmorelocalmaximathanthe\nnumberoftrainingexamples.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 352, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 974}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0353_a98257d3", "text": "1 4 5\nCHAPTER5.MACHINELEARNINGBASICS\nAnothertypeoflearningalgorithmthatalsobreakstheinputspaceintoregions\nandhasseparateparametersforeachregionisthedecisiontree( , Breimanetal. 1984)anditsmanyvariants.Asshowninﬁgure,eachnodeofthedecision 5.7\ntreeisassociatedwitharegionintheinputspace,andinternalnodesbreakthat\nregionintoonesub-regionforeachchildofthenode(typicallyusinganaxis-aligned\ncut).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 353, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 390}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0354_22389a9b", "text": "Spaceisthussub-dividedintonon-overlappingregions,withaone-to-one\ncorrespondencebetweenleafnodesandinputregions.Eachleafnodeusuallymaps\neverypointinitsinputregiontothesameoutput.Decisiontreesareusually\ntrainedwithspecializedalgorithmsthatarebeyondthescopeofthisbook.The\nlearningalgorithmcanbeconsiderednon-parametric ifitisallowedtolearnatree\nofarbitrarysize,thoughdecisiontreesareusuallyregularizedwithsizeconstraints\nthatturnthemintoparametricmodelsinpractice.Decisiontreesastheyare\ntypicallyused,withaxis-alignedsplitsandconstantoutputswithineachnode,\nstruggletosolvesomeproblemsthatareeasyevenforlogisticregression.For\nexample,ifwehaveatwo-classproblemandthepositiveclassoccurswherever\nx2>x1,thedecisionboundaryisnotaxis-aligned.Thedecisiontreewillthus\nneedtoapproximatethedecisionboundarywithmanynodes,implementingastep\nfunctionthatconstantlywalksbackandforthacrossthetruedecisionfunction\nwithaxis-alignedsteps.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 354, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 915}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0355_859454d8", "text": "Aswehaveseen,nearestneighborpredictorsanddecisiontreeshavemany\nlimitations.Nonetheless,theyareusefullearningalgorithmswhencomputational\nresourcesareconstrained.Wecanalsobuildintuitionformoresophisticated\nlearningalgorithmsbythinkingaboutthesimilaritiesanddiﬀerencesbetween\nsophisticatedalgorithmsand-NNordecisiontreebaselines. k\nSee (), (),  ()orothermachine Murphy2012Bishop2006Hastieetal.2001\nlearningtextbooksformorematerialontraditionalsupervisedlearningalgorithms.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 355, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 469}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0356_e9baf0fc", "text": "k\nSee (), (),  ()orothermachine Murphy2012Bishop2006Hastieetal.2001\nlearningtextbooksformorematerialontraditionalsupervisedlearningalgorithms. 5.8UnsupervisedLearningAlgorithms\nRecallfromsectionthatunsupervisedalgorithmsarethosethatexperience 5.1.3\nonly“features”butnotasupervisionsignal.Thedistinctionbetweensupervised\nandunsupervisedalgorithmsisnotformallyandrigidlydeﬁnedbecausethereisno\nobjectivetestfordistinguishingwhetheravalueisafeatureoratargetprovidedby\nasupervisor.Informally,unsupervisedlearningreferstomostattemptstoextract\ninformationfromadistributionthatdonotrequirehumanlabortoannotate\nexamples.Thetermisusuallyassociatedwithdensityestimation,learningto\ndrawsamplesfromadistribution,learningtodenoisedatafromsomedistribution,\nﬁndingamanifoldthatthedataliesnear,orclusteringthedataintogroupsof\n1 4 6\nCHAPTER5.MACHINELEARNINGBASICS\nrelatedexamples.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 356, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 862}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0357_dec14968", "text": "Aclassicunsupervisedlearningtaskistoﬁndthe“best”representationofthe\ndata.By‘best’wecanmeandiﬀerentthings,butgenerallyspeakingwearelooking\nforarepresentationthatpreservesasmuchinformationaboutxaspossiblewhile\nobeyingsomepenaltyorconstraintaimedatkeepingtherepresentation or simpler\nmoreaccessiblethanitself.x\nTherearemultiplewaysofdeﬁningarepresentation.Threeofthe simpler \nmostcommonincludelowerdimensionalrepresentations,sparserepresentations\nandindependentrepresentations.Low-dimensionalrepresentationsattemptto\ncompressasmuchinformationaboutxaspossibleinasmallerrepresentation. Sparserepresentations(,; ,; Barlow1989OlshausenandField1996Hintonand\nGhahramani1997,)embedthedatasetintoarepresentationwhoseentriesare\nmostlyzeroesformostinputs.Theuseofsparserepresentationstypicallyrequires\nincreasingthedimensionalityoftherepresentation,sothattherepresentation\nbecomingmostlyzeroesdoesnotdiscardtoomuchinformation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 357, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 913}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0358_902f13ba", "text": "Thisresultsinan\noverallstructureoftherepresentationthattendstodistributedataalongtheaxes\noftherepresentationspace.Independentrepresentationsattempttodisentangle\nthesourcesofvariationunderlyingthedatadistributionsuchthatthedimensions\noftherepresentationarestatisticallyindependent. Of coursethese three criteriaare certainly notmutuallyexclusive.Low-\ndimensionalrepresentationsoftenyieldelementsthathavefewerorweakerde-\npendenciesthantheoriginalhigh-dimensionaldata.Thisisbecauseonewayto\nreducethesizeofarepresentationistoﬁndandremoveredundancies.Identifying\nandremovingmoreredundancyallowsthedimensionalityreductionalgorithmto\nachievemorecompressionwhilediscardinglessinformation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 358, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 680}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0359_da2911fa", "text": "Thenotionofrepresentationisoneofthecentralthemesofdeeplearningand\nthereforeoneofthecentralthemesinthisbook.Inthissection,wedevelopsome\nsimpleexamplesofrepresentationlearningalgorithms.Together,theseexample\nalgorithmsshowhowtooperationalizeallthreeofthecriteriaabove.Mostofthe\nremainingchaptersintroduceadditionalrepresentationlearningalgorithmsthat\ndevelopthesecriteriaindiﬀerentwaysorintroduceothercriteria. 5.8.1PrincipalComponentsAnalysis\nInsection,wesawthattheprincipalcomponentsanalysisalgorithmprovides 2.12\nameansofcompressingdata.WecanalsoviewPCAasanunsupervisedlearning\nalgorithmthatlearnsarepresentationofdata.Thisrepresentationisbasedon\ntwoofthecriteriaforasimplerepresentationdescribedabove.PCAlearnsa\n1 4 7\nCHAPTER5.MACHINELEARNINGBASICS\n− − 2 0 1 0 0 1 0 2 0\nx 1− 2 0− 1 001 02 0x 2\n− − 2 0 1 0 0 1 0 2 0\nz 1− 2 0− 1 001 02 0z 2\nFigure5.8:PCAlearnsalinearprojectionthatalignsthedirectionofgreatestvariance\nwiththeaxesofthenewspace.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 359, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 945}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0360_375d7128", "text": "( L e f t )Theoriginaldataconsistsofsamplesofx.Inthis\nspace,thevariancemightoccuralongdirectionsthatarenotaxis-aligned. ( R i g h t )The\ntransformeddataz=xWnowvariesmostalongtheaxisz 1.Thedirectionofsecond\nmostvarianceisnowalongz 2. representationthathaslowerdimensionalitythantheoriginalinput.Italsolearns\narepresentationwhoseelementshavenolinearcorrelationwitheachother.This\nisaﬁrststeptowardthecriterionoflearningrepresentationswhoseelementsare\nstatisticallyindependent.Toachievefullindependence,arepresentationlearning\nalgorithmmustalsoremovethenonlinearrelationshipsbetweenvariables.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 360, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 589}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0361_675330b6", "text": "PCAlearnsanorthogonal,lineartransformationofthedatathatprojectsan\ninputxtoarepresentationzasshowninﬁgure.Insection,wesawthat 5.8 2.12\nwecouldlearnaone-dimensional representationthatbestreconstructstheoriginal\ndata(inthesenseofmeansquarederror)andthatthisrepresentationactually\ncorrespondstotheﬁrstprincipalcomponentofthedata.ThuswecanusePCA\nasasimpleandeﬀectivedimensionalityreductionmethodthatpreservesasmuch\noftheinformationinthedataaspossible(again,asmeasuredbyleast-squares\nreconstructionerror).Inthefollowing,wewillstudyhowthePCArepresentation\ndecorrelatestheoriginaldatarepresentation.X\nLetusconsiderthemn×-dimensionaldesignmatrixX.Wewillassumethat\nthedatahasameanofzero, E[x] = 0.Ifthisisnotthecase,thedatacaneasily\nbecenteredbysubtractingthemeanfromallexamplesinapreprocessingstep. Theunbiasedsamplecovariancematrixassociatedwithisgivenby:X\nVar[] =x1\nm−1XX. (5.85)\n1 4 8\nCHAPTER5.MACHINELEARNINGBASICS\nPCAﬁndsarepresentation(throughlineartransformation)z=xWwhere\nVar[]zisdiagonal.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 361, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 990}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0362_b5f8f9b4", "text": "Theunbiasedsamplecovariancematrixassociatedwithisgivenby:X\nVar[] =x1\nm−1XX. (5.85)\n1 4 8\nCHAPTER5.MACHINELEARNINGBASICS\nPCAﬁndsarepresentation(throughlineartransformation)z=xWwhere\nVar[]zisdiagonal. Insection,wesawthattheprincipalcomponentsofadesignmatrix 2.12 X\naregivenbytheeigenvectorsofXX.Fromthisview,\nXXWW = Λ. (5.86)\nInthissection,weexploitanalternativederivationoftheprincipalcomponents.The\nprincipalcomponentsmayalsobeobtainedviathesingularvaluedecomposition. Speciﬁcally,theyaretherightsingularvectorsofX.Toseethis,letWbethe\nrightsingularvectorsinthedecompositionX=UW Σ.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 362, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 586}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0363_bbf7959e", "text": "Speciﬁcally,theyaretherightsingularvectorsofX.Toseethis,letWbethe\nrightsingularvectorsinthedecompositionX=UW Σ. Wethenrecoverthe\noriginaleigenvectorequationwithastheeigenvectorbasis: W\nXX=\nUW Σ\nUW Σ= W Σ2W.(5.87)\nTheSVDishelpfultoshowthatPCAresultsinadiagonal Var[z].Usingthe\nSVDof,wecanexpressthevarianceofas: X X\nVar[] =x1\nm−1XX (5.88)\n=1\nm−1(UW Σ)UW Σ(5.89)\n=1\nm−1W ΣUUW Σ(5.90)\n=1\nm−1W Σ2W, (5.91)\nwhereweusethefactthatUU=IbecausetheUmatrixofthesingularvalue\ndecompositionisdeﬁnedtobeorthogonal.Thisshowsthatifwetakez=xW,\nwecanensurethatthecovarianceofisdiagonalasrequired: z\nVar[] =z1\nm−1ZZ (5.92)\n=1\nm−1WXXW (5.93)\n=1\nm−1WW Σ2WW (5.94)\n=1\nm−1Σ2, (5.95)\nwherethistimeweusethefactthatWW=I,againfromthedeﬁnitionofthe\nSVD.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 363, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 748}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0364_78b127b4", "text": "1 4 9\nCHAPTER5.MACHINELEARNINGBASICS\nTheaboveanalysisshowsthatwhenweprojectthedataxtoz,viathelinear\ntransformationW,theresultingrepresentationhasadiagonalcovariancematrix\n(asgivenby Σ2)whichimmediatelyimpliesthattheindividualelementsofzare\nmutuallyuncorrelated. ThisabilityofPCAtotransformdataintoarepresentationwheretheelements\naremutuallyuncorrelated isaveryimportantpropertyofPCA.Itisasimple\nexampleofarepresentationthatattemptstodisentangletheunknownfactorsof\nvariationunderlyingthedata. InthecaseofPCA,thisdisentanglingtakesthe\nformofﬁndingarotationoftheinputspace(describedbyW)thatalignsthe\nprincipalaxesofvariancewiththebasisofthenewrepresentationspaceassociated\nwith.z\nWhilecorrelationisanimportantcategoryofdependencybetweenelementsof\nthedata,wearealsointerestedinlearningrepresentationsthatdisentanglemore\ncomplicatedformsoffeaturedependencies.Forthis,wewillneedmorethanwhat\ncanbedonewithasimplelineartransformation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 364, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 926}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0365_21a58c6c", "text": "5.8.2-meansClustering k\nAnotherexampleofasimplerepresentationlearningalgorithmisk-meansclustering. Thek-meansclusteringalgorithmdividesthetrainingsetintokdiﬀerentclusters\nofexamplesthatareneareachother.Wecanthusthinkofthealgorithmas\nprovidingak-dimensionalone-hotcodevectorhrepresentinganinputx.Ifx\nbelongstoclusteri,thenh i= 1andallotherentriesoftherepresentationhare\nzero. Theone-hotcodeprovidedbyk-meansclusteringisanexampleofasparse\nrepresentation,becausethemajorityofitsentriesarezeroforeveryinput.Later,\nwewilldevelopotheralgorithmsthatlearnmoreﬂexiblesparserepresentations,\nwheremorethanoneentrycanbenon-zeroforeachinputx.One-hotcodes\nareanextremeexampleofsparserepresentationsthatlosemanyofthebeneﬁts\nofadistributedrepresentation.Theone-hotcodestillconferssomestatistical\nadvantages(itnaturallyconveystheideathatallexamplesinthesameclusterare\nsimilartoeachother)anditconfersthecomputational advantagethattheentire\nrepresentationmaybecapturedbyasingleinteger.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 365, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 966}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0366_5eb01479", "text": "Thek-meansalgorithmworksbyinitializingkdiﬀerentcentroids{µ(1),...,µ() k}\ntodiﬀerentvalues,thenalternatingbetweentwodiﬀerentstepsuntilconvergence.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 366, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 145}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0367_8f251574", "text": "Inonestep,eachtrainingexampleisassignedtoclusteri,whereiistheindexof\nthenearestcentroidµ() i.Intheotherstep,eachcentroidµ() iisupdatedtothe\nmeanofalltrainingexamplesx() jassignedtocluster.i\n1 5 0\nCHAPTER5.MACHINELEARNINGBASICS\nOnediﬃcultypertainingtoclusteringisthattheclusteringproblemisinherently\nill-posed,inthesensethatthereisnosinglecriterionthatmeasureshowwella\nclusteringofthedatacorrespondstotherealworld.Wecanmeasurepropertiesof\ntheclusteringsuchastheaverageEuclideandistancefromaclustercentroidtothe\nmembersofthecluster.Thisallowsustotellhowwellweareabletoreconstruct\nthetrainingdatafromtheclusterassignments.Wedonotknowhowwellthe\nclusterassignmentscorrespondtopropertiesoftherealworld.Moreover,there\nmaybemanydiﬀerentclusteringsthatallcorrespondwelltosomepropertyof\ntherealworld.Wemayhopetoﬁndaclusteringthatrelatestoonefeaturebut\nobtainadiﬀerent,equallyvalidclusteringthatisnotrelevanttoourtask.For\nexample,supposethatweruntwoclusteringalgorithmsonadatasetconsistingof\nimagesofredtrucks,imagesofredcars,imagesofgraytrucks,andimagesofgray\ncars.Ifweaskeachclusteringalgorithmtoﬁndtwoclusters,onealgorithmmay\nﬁndaclusterofcarsandaclusteroftrucks,whileanothermayﬁndaclusterof\nredvehiclesandaclusterofgrayvehicles.Supposewealsorunathirdclustering\nalgorithm,whichisallowedtodeterminethenumberofclusters.Thismayassign\ntheexamplestofourclusters,redcars,redtrucks,graycars,andgraytrucks.This\nnewclusteringnowatleastcapturesinformationaboutbothattributes,butithas\nlostinformationaboutsimilarity.Redcarsareinadiﬀerentclusterfromgray\ncars,justastheyareinadiﬀerentclusterfromgraytrucks.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 367, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1585}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0368_29f84605", "text": "Theoutputofthe\nclusteringalgorithmdoesnottellusthatredcarsaremoresimilartograycars\nthantheyaretograytrucks.Theyarediﬀerentfromboththings,andthatisall\nweknow. Theseissuesillustratesomeofthereasonsthatwemaypreferadistributed\nrepresentationtoaone-hotrepresentation.Adistributedrepresentationcouldhave\ntwoattributesforeachvehicle—onerepresentingitscolorandonerepresenting\nwhetheritisacaroratruck.Itisstillnotentirelyclearwhattheoptimal\ndistributedrepresentationis(howcanthelearningalgorithmknowwhetherthe\ntwoattributesweareinterestedinarecolorandcar-versus-truckratherthan\nmanufacturerandage?)buthavingmanyattributesreducestheburdenonthe\nalgorithmtoguesswhichsingleattributewecareabout,andallowsustomeasure\nsimilaritybetweenobjectsinaﬁne-grainedwaybycomparingmanyattributes\ninsteadofjusttestingwhetheroneattributematches.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 368, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 817}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0369_e2e34e86", "text": "5.9StochasticGradientDescent\nNearlyallofdeeplearningispoweredbyoneveryimportantalgorithm:stochastic\ngradientdescentorSGD.Stochasticgradientdescentisanextensionofthe\n1 5 1\nCHAPTER5.MACHINELEARNINGBASICS\ngradientdescentalgorithmintroducedinsection.4.3\nArecurringprobleminmachinelearningisthatlargetrainingsetsarenecessary\nforgoodgeneralization, butlargetrainingsetsarealsomorecomputationally\nexpensive. Thecostfunctionusedbyamachinelearningalgorithmoftendecomposesasa\nsumovertrainingexamplesofsomeper-examplelossfunction.Forexample,the\nnegativeconditionallog-likelihoodofthetrainingdatacanbewrittenas\nJ() = θ E x ,y ∼ˆ pdataL,y,(xθ) =1\nmm\ni=1L(x() i,y() i,θ)(5.96)\nwhereistheper-exampleloss L L,y,py. (xθ) = log− (|xθ;)\nFortheseadditivecostfunctions,gradientdescentrequirescomputing\n∇ θJ() =θ1\nmm\ni=1∇ θL(x() i,y() i,.θ) (5.97)\nThecomputational costofthisoperationisO(m).Asthetrainingsetsizegrowsto\nbillionsofexamples,thetimetotakeasinglegradientstepbecomesprohibitively\nlong.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 369, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 976}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0370_c4c0a642", "text": "Theinsightofstochasticgradientdescentisthatthegradientisanexpectation. Theexpectationmaybeapproximately estimatedusingasmallsetofsamples. Speciﬁcally,oneachstepofthealgorithm,wecansampleaminibatchofexamples\nB={x(1),...,x( m)}drawnuniformlyfromthetrainingset.Theminibatchsize\nmistypicallychosentobearelativelysmallnumberofexamples,rangingfrom\n1toafewhundred.Crucially,misusuallyheldﬁxedasthetrainingsetsizem\ngrows.Wemayﬁtatrainingsetwithbillionsofexamplesusingupdatescomputed\nononlyahundredexamples. Theestimateofthegradientisformedas\ng=1\nm∇ θm\ni=1L(x() i,y() i,.θ) (5.98)\nusingexamplesfromtheminibatch.Thestochasticgradientdescentalgorithm B\nthenfollowstheestimatedgradientdownhill:\nθθg ← −, (5.99)\nwhereisthelearningrate. \n1 5 2\nCHAPTER5.MACHINELEARNINGBASICS\nGradientdescentingeneralhasoftenbeenregardedassloworunreliable.In\nthepast,theapplicationofgradientdescenttonon-convexoptimization problems\nwasregardedasfoolhardyorunprincipled.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 370, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 945}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0371_4738d79b", "text": "Today,weknowthatthemachine\nlearningmodelsdescribedinpartworkverywellwhentrainedwithgradient II\ndescent.Theoptimization algorithmmaynotbeguaranteedtoarriveatevena\nlocalminimuminareasonableamountoftime,butitoftenﬁndsaverylowvalue\nofthecostfunctionquicklyenoughtobeuseful. Stochasticgradientdescenthasmanyimportantusesoutsidethecontextof\ndeeplearning.Itisthemainwaytotrainlargelinearmodelsonverylarge\ndatasets.Foraﬁxedmodelsize,thecostperSGDupdatedoesnotdependonthe\ntrainingsetsizem.Inpractice,weoftenusealargermodelasthetrainingsetsize\nincreases,butwearenotforcedtodoso.Thenumberofupdatesrequiredtoreach\nconvergenceusuallyincreaseswithtrainingsetsize. However,asmapproaches\ninﬁnity,themodelwilleventuallyconvergetoitsbestpossibletesterrorbefore\nSGDhassampledeveryexampleinthetrainingset.Increasingmfurtherwillnot\nextendtheamountoftrainingtimeneededtoreachthemodel’sbestpossibletest\nerror.Fromthispointofview,onecanarguethattheasymptoticcostoftraining\namodelwithSGDisasafunctionof.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 371, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 978}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0372_07e4ce96", "text": "O(1) m\nPriortotheadventofdeeplearning,themainwaytolearnnonlinearmodels\nwastousethekerneltrickincombinationwithalinearmodel.Manykernellearning\nalgorithmsrequireconstructinganmm×matrixG i , j=k(x() i,x() j).Constructing\nthismatrixhascomputational costO(m2),whichisclearlyundesirablefordatasets\nwith billions of examples. In academia, starting in2006,deep learning was\ninitiallyinterestingbecauseitwasabletogeneralizetonewexamplesbetter\nthancompetingalgorithmswhentrainedonmedium-sizeddatasetswithtensof\nthousandsofexamples.Soonafter,deeplearninggarneredadditionalinterestin\nindustry,becauseitprovidedascalablewayoftrainingnonlinearmodelsonlarge\ndatasets. Stochasticgradientdescentandmanyenhancements toitaredescribedfurther\ninchapter.8\n5.10BuildingaMachineLearningAlgorithm\nNearlyalldeeplearningalgorithmscanbedescribedasparticularinstancesof\nafairlysimplerecipe:combineaspeciﬁcationofadataset,acostfunction,an\noptimization procedureandamodel.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 372, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 941}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0373_e9c933e0", "text": "Forexample,thelinearregressionalgorithmcombinesadatasetconsistingof\n1 5 3\nCHAPTER5.MACHINELEARNINGBASICS\nXyand,thecostfunction\nJ,b(w) = − E x ,y ∼ˆ pdatalogpmodel( )y|x, (5.100)\nthemodelspeciﬁcationpmodel(y|x) =N(y;xw+b,1),and,inmostcases,the\noptimization algorithmdeﬁnedbysolvingforwherethegradientofthecostiszero\nusingthenormalequations. Byrealizingthatwecanreplaceanyofthesecomponentsmostlyindependently\nfromtheothers,wecanobtainaverywidevarietyofalgorithms. Thecostfunctiontypicallyincludesatleastonetermthatcausesthelearning\nprocesstoperformstatisticalestimation.Themostcommoncostfunctionisthe\nnegativelog-likelihood,sothatminimizingthecostfunctioncausesmaximum\nlikelihoodestimation. Thecostfunctionmayalsoincludeadditionalterms,suchasregularization\nterms.Forexample,wecanaddweightdecaytothelinearregressioncostfunction\ntoobtain\nJ,bλ (w) = ||||w2\n2− E x ,y ∼ˆ pdatalogpmodel( )y|x.(5.101)\nThisstillallowsclosed-formoptimization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 373, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 934}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0374_74222214", "text": "Ifwechangethemodeltobenonlinear,thenmostcostfunctionscannolonger\nbeoptimizedinclosedform.Thisrequiresustochooseaniterativenumerical\noptimization procedure,suchasgradientdescent. Therecipeforconstructingalearningalgorithmbycombiningmodels,costs,and\noptimization algorithmssupportsbothsupervisedandunsupervisedlearning.The\nlinearregressionexampleshowshowtosupportsupervisedlearning.Unsupervised\nlearningcanbesupportedbydeﬁningadatasetthatcontainsonlyXandproviding\nanappropriateunsupervisedcostandmodel.Forexample,wecanobtaintheﬁrst\nPCAvectorbyspecifyingthatourlossfunctionis\nJ() = w E x ∼ˆ pdata||− ||xr(;)xw2\n2 (5.102)\nwhileourmodelisdeﬁnedtohavewwithnormoneandreconstructionfunction\nr() = xwxw. Insomecases,thecostfunctionmaybeafunctionthatwecannotactually\nevaluate,forcomputational reasons.Inthesecases,wecanstillapproximately\nminimizeitusingiterativenumericaloptimization solongaswehavesomewayof\napproximatingitsgradients.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 374, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 925}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0375_7a0b5e2a", "text": "Mostmachinelearningalgorithmsmakeuseofthisrecipe,thoughitmaynot\nimmediatelybeobvious.Ifamachinelearningalgorithmseemsespeciallyuniqueor\n1 5 4\nCHAPTER5.MACHINELEARNINGBASICS\nhand-designed,itcanusuallybeunderstoodasusingaspecial-caseoptimizer.Some\nmodelssuchasdecisiontreesork-meansrequirespecial-caseoptimizersbecause\ntheircostfunctionshaveﬂatregionsthatmaketheminappropriate forminimization\nbygradient-basedoptimizers.Recognizingthatmostmachinelearningalgorithms\ncanbedescribedusingthisrecipehelpstoseethediﬀerentalgorithmsaspartofa\ntaxonomyofmethodsfordoingrelatedtasksthatworkforsimilarreasons,rather\nthanasalonglistofalgorithmsthateachhaveseparatejustiﬁcations. 5.11ChallengesMotivatingDeepLearning\nThesimplemachinelearningalgorithmsdescribedinthischapterworkverywellon\nawidevarietyofimportantproblems.However,theyhavenotsucceededinsolving\nthecentralproblemsinAI,suchasrecognizingspeechorrecognizingobjects.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 375, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 910}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0376_6cb3a191", "text": "Thedevelopmentofdeeplearningwasmotivatedinpartbythefailureof\ntraditionalalgorithmstogeneralizewellonsuchAItasks. Thissectionisabouthowthechallengeofgeneralizingtonewexamplesbecomes\nexponentiallymorediﬃcultwhenworkingwithhigh-dimensionaldata,andhow\nthemechanismsusedtoachievegeneralization intraditionalmachinelearning\nareinsuﬃcienttolearncomplicatedfunctionsinhigh-dimensionalspaces.Such\nspacesalsooftenimposehighcomputational costs.Deeplearningwasdesignedto\novercometheseandotherobstacles. 5.11.1TheCurseofDimensionality\nManymachinelearningproblemsbecomeexceedinglydiﬃcultwhenthenumber\nofdimensionsinthedataishigh.Thisphenomenon isknownasthecurseof\ndimensionality.Ofparticularconcernisthatthenumberofpossibledistinct\nconﬁgurations ofasetofvariablesincreasesexponentiallyasthenumberofvariables\nincreases. 1 5 5\nCHAPTER5.MACHINELEARNINGBASICS\nFigure5.9:Asthenumberofrelevantdimensionsofthedataincreases(fromleftto\nright),thenumberofconﬁgurationsofinterestmaygrowexponentially.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 376, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 975}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0377_240b21a2", "text": "1 5 5\nCHAPTER5.MACHINELEARNINGBASICS\nFigure5.9:Asthenumberofrelevantdimensionsofthedataincreases(fromleftto\nright),thenumberofconﬁgurationsofinterestmaygrowexponentially. ( L e f t )Inthis\none-dimensionalexample,wehaveonevariableforwhichweonlycaretodistinguish10\nregionsofinterest.Withenoughexamplesfallingwithineachoftheseregions(eachregion\ncorrespondstoacellintheillustration),learningalgorithmscaneasilygeneralizecorrectly. Astraightforwardwaytogeneralizeistoestimatethevalueofthetargetfunctionwithin\neachregion(andpossiblyinterpolatebetweenneighboringregions).With2 ( C e n t e r )\ndimensionsitismorediﬃculttodistinguish10diﬀerentvaluesofeachvariable. Weneed\ntokeeptrackofupto10×10=100regions,andweneedatleastthatmanyexamplesto\ncoverallthoseregions.With3dimensionsthisgrowsto ( R i g h t ) 103= 1000regionsandat\nleastthatmanyexamples.Forddimensionsandvvaluestobedistinguishedalongeach\naxis,weseemtoneedO(vd)regionsandexamples.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 377, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 930}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0378_1e180e6f", "text": "Thisisaninstanceofthecurseof\ndimensionality.FiguregraciouslyprovidedbyNicolasChapados. Thecurseofdimensionalityarisesinmanyplacesincomputerscience,and\nespeciallysoinmachinelearning. Onechallengeposedbythecurseofdimensionalityisastatisticalchallenge. Asillustratedinﬁgure,astatisticalchallengearisesbecausethenumberof 5.9\npossibleconﬁgurations ofxismuchlargerthanthenumberoftrainingexamples. Tounderstandtheissue,letusconsiderthattheinputspaceisorganizedintoa\ngrid,likeintheﬁgure.Wecandescribelow-dimensional spacewithalownumber\nofgridcellsthataremostlyoccupiedbythedata.Whengeneralizingtoanewdata\npoint,wecanusuallytellwhattodosimplybyinspectingthetrainingexamples\nthatlieinthesamecellasthenewinput.Forexample,ifestimatingtheprobability\ndensityatsomepointx,wecanjustreturnthenumberoftrainingexamplesin\nthesameunitvolumecellasx,dividedbythetotalnumberoftrainingexamples. Ifwewishtoclassifyanexample,wecanreturnthemostcommonclassoftraining\nexamplesinthesamecell.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 378, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 960}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0379_91c8c7a5", "text": "Ifwewishtoclassifyanexample,wecanreturnthemostcommonclassoftraining\nexamplesinthesamecell. Ifwearedoingregressionwecanaveragethetarget\nvaluesobservedovertheexamplesinthatcell.Butwhataboutthecellsforwhich\nwehaveseennoexample?Becauseinhigh-dimensionalspacesthenumberof\nconﬁgurations ishuge,muchlargerthanournumberofexamples,atypicalgridcell\nhasnotrainingexampleassociatedwithit.Howcouldwepossiblysaysomething\n1 5 6\nCHAPTER5.MACHINELEARNINGBASICS\nmeaningfulaboutthesenewconﬁgurations? Manytraditionalmachinelearning\nalgorithmssimplyassumethattheoutputatanewpointshouldbeapproximately\nthesameastheoutputatthenearesttrainingpoint.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 379, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 625}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0380_d358f74e", "text": "Manytraditionalmachinelearning\nalgorithmssimplyassumethattheoutputatanewpointshouldbeapproximately\nthesameastheoutputatthenearesttrainingpoint. 5.11.2LocalConstancyandSmoothnessRegularization\nInordertogeneralizewell,machinelearningalgorithmsneedtobeguidedbyprior\nbeliefsaboutwhatkindoffunctiontheyshouldlearn.Previously,wehaveseen\nthesepriorsincorporatedasexplicitbeliefsintheformofprobabilitydistributions\noverparametersofthemodel.Moreinformally,wemayalsodiscusspriorbeliefsas\ndirectlyinﬂuencingtheitselfandonlyindirectlyactingontheparameters function\nviatheireﬀectonthefunction.Additionally,weinformallydiscusspriorbeliefsas\nbeingexpressedimplicitly,bychoosingalgorithmsthatarebiasedtowardchoosing\nsomeclassoffunctionsoveranother,eventhoughthesebiasesmaynotbeexpressed\n(orevenpossibletoexpress)intermsofaprobabilitydistributionrepresentingour\ndegreeofbeliefinvariousfunctions.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 380, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 878}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0381_cb87026b", "text": "Amongthemostwidelyusedoftheseimplicit“priors” isthesmoothness\npriororlocalconstancyprior.Thispriorstatesthatthefunctionwelearn\nshouldnotchangeverymuchwithinasmallregion. Manysimpleralgorithmsrelyexclusivelyonthispriortogeneralizewell,and\nasaresulttheyfailtoscaletothestatisticalchallengesinvolvedinsolvingAI-\nleveltasks.Throughoutthisbook,wewilldescribehowdeeplearningintroduces\nadditional(explicit andimplicit)priorsinorder toreducethegeneralization\nerroronsophisticatedtasks.Here,weexplainwhythesmoothnessprioraloneis\ninsuﬃcientforthesetasks.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 381, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 544}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0382_101c888d", "text": "Therearemanydiﬀerentwaystoimplicitlyorexplicitlyexpressapriorbelief\nthatthelearnedfunctionshouldbesmoothorlocallyconstant.Allofthesediﬀerent\nmethodsaredesignedtoencouragethelearningprocesstolearnafunctionf∗that\nsatisﬁesthecondition\nf∗() x≈f∗(+)x (5.103)\nformostconﬁgurationsxandsmallchange.Inotherwords,ifweknowagood\nanswerforaninputx(forexample,ifxisalabeledtrainingexample)thenthat\nanswerisprobablygoodintheneighborhoodofx.Ifwehaveseveralgoodanswers\ninsomeneighborhoodwewouldcombinethem(bysomeformofaveragingor\ninterpolation)toproduceananswerthatagreeswithasmanyofthemasmuchas\npossible. Anextremeexampleofthelocalconstancyapproachisthek-nearestneighbors\nfamilyoflearningalgorithms.Thesepredictorsareliterallyconstantovereach\n1 5 7\nCHAPTER5.MACHINELEARNINGBASICS\nregioncontainingallthepointsxthathavethesamesetofknearestneighborsin\nthetrainingset.Fork= 1,thenumberofdistinguishableregionscannotbemore\nthanthenumberoftrainingexamples.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 382, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 936}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0383_fca604c4", "text": "Whilethek-nearestneighborsalgorithmcopiestheoutputfromnearbytraining\nexamples,mostkernelmachinesinterpolatebetweentrainingsetoutputsassociated\nwithnearbytrainingexamples.Animportantclassofkernelsisthefamilyoflocal\nkernelswherek(uv,)islargewhenu=vanddecreasesasuandvgrowfarther\napartfromeachother.Alocalkernelcanbethoughtofasasimilarityfunction\nthatperformstemplatematching,bymeasuringhowcloselyatestexamplex\nresembleseachtrainingexamplex() i. Muchofthemodernmotivationfordeep\nlearningisderivedfromstudyingthelimitationsoflocaltemplatematchingand\nhowdeepmodelsareabletosucceedincaseswherelocaltemplatematchingfails\n( ,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 383, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 619}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0384_18639970", "text": "Muchofthemodernmotivationfordeep\nlearningisderivedfromstudyingthelimitationsoflocaltemplatematchingand\nhowdeepmodelsareabletosucceedincaseswherelocaltemplatematchingfails\n( ,). Bengioetal.2006b\nDecisiontreesalsosuﬀerfromthelimitationsofexclusivelysmoothness-based\nlearningbecausetheybreaktheinputspaceintoasmanyregionsasthereare\nleavesanduseaseparateparameter(orsometimesmanyparametersforextensions\nofdecisiontrees)ineachregion.Ifthetargetfunctionrequiresatreewithat\nleastnleavestoberepresentedaccurately,thenatleastntrainingexamplesare\nrequiredtoﬁtthetree.Amultipleofnisneededtoachievesomelevelofstatistical\nconﬁdenceinthepredictedoutput.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 384, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 639}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0385_6bcf2deb", "text": "Ingeneral,todistinguishO(k)regionsininputspace,allofthesemethods\nrequireO(k) examples.TypicallythereareO(k) parameters,withO(1) parameters\nassociatedwitheachoftheO(k)regions.Thecaseofanearestneighborscenario,\nwhereeachtrainingexamplecanbeusedtodeﬁneatmostoneregion,isillustrated\ninﬁgure.5.10\nIsthereawaytorepresentacomplexfunctionthathasmanymoreregions\ntobedistinguishedthanthenumberoftrainingexamples?Clearly,assuming\nonlysmoothnessoftheunderlyingfunctionwillnotallowalearnertodothat. For example, imagine that thetargetfunctionis akind ofcheckerboard.A\ncheckerboardcontainsmanyvariationsbutthereisasimplestructuretothem.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 385, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 622}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0386_657982e0", "text": "For example, imagine that thetargetfunctionis akind ofcheckerboard.A\ncheckerboardcontainsmanyvariationsbutthereisasimplestructuretothem. Imaginewhathappenswhenthenumberoftrainingexamplesissubstantially\nsmallerthanthenumberofblackandwhitesquaresonthecheckerboard.Based\nononlylocalgeneralization andthesmoothnessorlocalconstancyprior,wewould\nbeguaranteedtocorrectlyguessthecolorofanewpointifitlieswithinthesame\ncheckerboardsquareasatrainingexample.Thereisnoguaranteethatthelearner\ncouldcorrectlyextendthecheckerboardpatterntopointslyinginsquaresthatdo\nnotcontaintrainingexamples.Withthisprioralone,theonlyinformationthatan\nexampletellsusisthecolorofitssquare,andtheonlywaytogetthecolorsofthe\n1 5 8\nCHAPTER5.MACHINELEARNINGBASICS\nFigure5.10: Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspace\nintoregions.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 386, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 817}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0387_0f469485", "text": "Anexample(representedherebyacircle)withineachregiondeﬁnesthe\nregionboundary(representedherebythelines).Theyvalueassociatedwitheachexample\ndeﬁneswhattheoutputshouldbeforallpointswithinthecorrespondingregion. The\nregionsdeﬁnedbynearestneighbormatchingformageometricpatterncalledaVoronoi\ndiagram.Thenumberofthesecontiguousregionscannotgrowfasterthanthenumber\noftrainingexamples.Whilethisﬁgureillustratesthebehaviorofthenearestneighbor\nalgorithmspeciﬁcally,othermachinelearningalgorithmsthatrelyexclusivelyonthe\nlocalsmoothnesspriorforgeneralizationexhibitsimilarbehaviors:eachtrainingexample\nonlyinformsthelearnerabouthowtogeneralizeinsomeneighborhoodimmediately\nsurroundingthatexample. 1 5 9\nCHAPTER5.MACHINELEARNINGBASICS\nentirecheckerboardrightistocovereachofitscellswithatleastoneexample.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 387, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 789}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0388_40ee75f4", "text": "1 5 9\nCHAPTER5.MACHINELEARNINGBASICS\nentirecheckerboardrightistocovereachofitscellswithatleastoneexample. Thesmoothnessassumptionandtheassociatednon-parametric learningalgo-\nrithmsworkextremelywellsolongasthereareenoughexamplesforthelearning\nalgorithmtoobservehighpointsonmostpeaksandlowpointsonmostvalleys\nofthetrueunderlyingfunctiontobelearned.Thisisgenerallytruewhenthe\nfunctiontobelearnedissmoothenoughandvariesinfewenoughdimensions. Inhighdimensions,evenaverysmoothfunctioncanchangesmoothlybutina\ndiﬀerentwayalongeachdimension.Ifthefunctionadditionallybehavesdiﬀerently\nindiﬀerentregions,itcanbecomeextremelycomplicatedtodescribewithasetof\ntrainingexamples.Ifthefunctioniscomplicated(wewanttodistinguishahuge\nnumberofregionscomparedtothenumberofexamples),isthereanyhopeto\ngeneralizewell?", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 388, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 792}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0389_d25c3677", "text": "Theanswertobothofthesequestions—whetheritispossibletorepresent\nacomplicatedfunctioneﬃciently,andwhetheritispossiblefortheestimated\nfunctiontogeneralizewelltonewinputs—isyes.Thekeyinsightisthatavery\nlargenumberofregions,e.g.,O(2k),canbedeﬁnedwithO(k)examples,solong\nasweintroducesomedependenciesbetweentheregionsviaadditionalassumptions\nabouttheunderlyingdatageneratingdistribution.Inthisway,wecanactually\ngeneralizenon-locally( ,; ,).Many BengioandMonperrus2005Bengioetal.2006c\ndiﬀerentdeeplearningalgorithmsprovideimplicitorexplicitassumptionsthatare\nreasonableforabroadrangeofAItasksinordertocapturetheseadvantages.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 389, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 617}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0390_765c3dc5", "text": "Otherapproachestomachinelearningoftenmakestronger,task-speciﬁcas-\nsumptions.Forexample,wecouldeasilysolvethecheckerboardtaskbyproviding\ntheassumptionthatthetargetfunctionisperiodic.Usuallywedonotincludesuch\nstrong,task-speciﬁcassumptionsintoneuralnetworkssothattheycangeneralize\ntoamuchwidervarietyofstructures.AItaskshavestructurethatismuchtoo\ncomplextobelimitedtosimple,manuallyspeciﬁedpropertiessuchasperiodicity,\nsowewantlearningalgorithmsthatembodymoregeneral-purpos eassumptions. Thecoreideaindeeplearningisthatweassumethatthedatawasgeneratedby\nthecompositionoffactorsorfeatures,potentiallyatmultiplelevelsinahierarchy. Manyothersimilarlygenericassumptionscanfurtherimprovedeeplearningal-\ngorithms.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 390, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 704}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0391_6d474e23", "text": "Manyothersimilarlygenericassumptionscanfurtherimprovedeeplearningal-\ngorithms. Theseapparentlymildassumptionsallowanexponentialgaininthe\nrelationshipbetweenthenumberofexamplesandthenumberofregionsthatcan\nbedistinguished.Theseexponentialgainsaredescribedmorepreciselyinsections\n6.4.115.415.5,and.Theexponentialadvantagesconferredbytheuseofdeep,\ndistributedrepresentationscountertheexponentialchallengesposedbythecurse\nofdimensionality. 1 6 0\nCHAPTER5.MACHINELEARNINGBASICS\n5.11.3ManifoldLearning\nAnimportantconceptunderlyingmanyideasinmachinelearningisthatofa\nmanifold. Amanifoldisaconnected region. Mathematically , it isasetofpoints,\nassociatedwithaneighborhoodaroundeachpoint.Fromanygivenpoint,the\nmanifoldlocallyappearstobeaEuclideanspace.Ineverydaylife,weexperience\nthesurfaceoftheworldasa2-Dplane,butitisinfactasphericalmanifoldin\n3-Dspace.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 391, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 845}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0392_9bed0cba", "text": "Thedeﬁnitionofaneighborhoodsurroundingeachpointimpliestheexistence\noftransformationsthatcanbeappliedtomoveonthemanifoldfromoneposition\ntoaneighboringone.Intheexampleoftheworld’ssurfaceasamanifold,onecan\nwalknorth,south,east,orwest. Althoughthereisaformalmathematical meaningtotheterm“manifold,”in\nmachinelearningittendstobeusedmorelooselytodesignateaconnectedset\nofpointsthatcanbeapproximatedwellbyconsideringonlyasmallnumberof\ndegreesoffreedom,ordimensions,embeddedinahigher-dimens ionalspace.Each\ndimensioncorrespondstoalocaldirectionofvariation.Seeﬁgureforan5.11\nexampleoftrainingdatalyingnearaone-dimensional manifoldembeddedintwo-\ndimensionalspace.Inthecontextofmachinelearning,weallowthedimensionality\nofthemanifoldtovaryfromonepointtoanother. This oftenhappenswhena\nmanifoldintersectsitself.Forexample,aﬁgureeightisamanifoldthathasasingle\ndimensioninmostplacesbuttwodimensionsattheintersectionatthecenter. 0 5 1 0 1 5 2 0 2 5 3 0 3 5 4 0 . . . . . . . .− 1 0 .− 0 5 .0 0 .0 5 .1 0 .1 5 .2 0 .2 5 .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 392, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1004}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0393_73efd275", "text": "0 5 1 0 1 5 2 0 2 5 3 0 3 5 4 0 . . . . . . . .− 1 0 .− 0 5 .0 0 .0 5 .1 0 .1 5 .2 0 .2 5 . Figure5.11:Datasampledfromadistributioninatwo-dimensionalspacethatisactually\nconcentratednearaone-dimensionalmanifold,likeatwistedstring.Thesolidlineindicates\ntheunderlyingmanifoldthatthelearnershouldinfer. 1 6 1\nCHAPTER5.MACHINELEARNINGBASICS\nManymachinelearningproblemsseemhopelessifweexpectthemachine\nlearningalgorithmtolearnfunctionswithinterestingvariationsacrossallof Rn.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 393, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 469}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0394_77f64cd2", "text": "1 6 1\nCHAPTER5.MACHINELEARNINGBASICS\nManymachinelearningproblemsseemhopelessifweexpectthemachine\nlearningalgorithmtolearnfunctionswithinterestingvariationsacrossallof Rn. Manifoldlearningalgorithmssurmountthisobstaclebyassumingthatmost\nof Rnconsistsofinvalidinputs, andthatinterestinginputsoccuronlyalong\nacollectionofmanifoldscontainingasmallsubsetofpoints,withinteresting\nvariationsintheoutputofthelearnedfunctionoccurringonlyalongdirections\nthatlieonthemanifold,orwithinterestingvariationshappeningonlywhenwe\nmovefromonemanifoldtoanother.Manifoldlearningwasintroducedinthecase\nofcontinuous-valueddataandtheunsupervisedlearningsetting,althoughthis\nprobabilityconcentrationideacanbegeneralizedtobothdiscretedataandthe\nsupervisedlearningsetting:thekeyassumptionremainsthatprobabilitymassis\nhighlyconcentrated.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 394, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 809}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0395_a3b59cbd", "text": "Theassumptionthatthedataliesalongalow-dimensional manifoldmaynot\nalwaysbecorrectoruseful.WearguethatinthecontextofAItasks,suchas\nthosethatinvolveprocessingimages,sounds,ortext,themanifoldassumptionis\natleastapproximatelycorrect.Theevidenceinfavorofthisassumptionconsists\noftwocategoriesofobservations. Theﬁrstobservationinfavorofthemanifoldhypothesisisthattheproba-\nbilitydistributionoverimages,textstrings,andsoundsthatoccurinreallifeis\nhighlyconcentrated.Uniformnoiseessentiallyneverresemblesstructuredinputs\nfromthesedomains. Figureshowshow,instead,uniformlysampledpoints 5.12\nlooklikethepatternsofstaticthatappearonanalogtelevisionsetswhennosignal\nisavailable.Similarly,ifyougenerateadocumentbypickinglettersuniformlyat\nrandom,whatistheprobabilitythatyouwillgetameaningfulEnglish-language\ntext?Almostzero,again,becausemostofthelongsequencesoflettersdonot\ncorrespondtoanaturallanguagesequence:thedistributionofnaturallanguage\nsequencesoccupiesaverysmallvolumeinthetotalspaceofsequencesofletters.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 395, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 998}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0396_4574eced", "text": "1 6 2\nCHAPTER5.MACHINELEARNINGBASICS\nFigure5.12:Samplingimagesuniformlyatrandom(byrandomlypickingeachpixel\naccordingtoauniformdistribution)givesrisetonoisyimages.Althoughthereisanon-\nzeroprobabilitytogenerateanimageofafaceoranyotherobjectfrequentlyencountered\ninAIapplications,weneveractuallyobservethishappeninginpractice.Thissuggests\nthattheimagesencounteredinAIapplicationsoccupyanegligibleproportionofthe\nvolumeofimagespace.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 396, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 428}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0397_049b53f4", "text": "Ofcourse,concentratedprobabilitydistributionsarenotsuﬃcienttoshow\nthatthedataliesonareasonablysmallnumberofmanifolds.Wemustalso\nestablishthattheexamplesweencounterareconnectedtoeachotherbyother\n1 6 3\nCHAPTER5.MACHINELEARNINGBASICS\nexamples,witheachexamplesurroundedbyotherhighlysimilarexamplesthat\nmaybereachedbyapplyingtransformationstotraversethemanifold.Thesecond\nargumentinfavorofthemanifoldhypothesisisthatwecanalsoimaginesuch\nneighborhoodsandtransformations,atleastinformally.Inthecaseofimages,we\ncancertainlythinkofmanypossibletransformationsthatallowustotraceouta\nmanifoldinimagespace:wecangraduallydimorbrightenthelights,gradually\nmoveorrotateobjectsintheimage,graduallyalterthecolorsonthesurfacesof\nobjects,etc.Itremainslikelythattherearemultiplemanifoldsinvolvedinmost\napplications.Forexample,themanifoldofimagesofhumanfacesmaynotbe\nconnectedtothemanifoldofimagesofcatfaces.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 397, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 885}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0398_3e158a80", "text": "Thesethoughtexperimentssupportingthemanifoldhypothesesconveysomein-\ntuitivereasonssupportingit.Morerigorousexperiments (Cayton2005Narayanan,;\nandMitter2010Schölkopf1998RoweisandSaul2000Tenenbaum ,; etal.,; ,; etal.,\n2000Brand2003BelkinandNiyogi2003DonohoandGrimes2003Weinberger ;,; ,; ,;\nandSaul2004,)clearlysupportthehypothesisforalargeclassofdatasetsof\ninterestinAI.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 398, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 368}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0399_a180951b", "text": "Whenthedataliesonalow-dimensional manifold,itcanbemostnatural\nformachinelearningalgorithmstorepresentthedataintermsofcoordinateson\nthemanifold,ratherthanintermsofcoordinatesin Rn.Ineverydaylife,wecan\nthinkofroadsas1-Dmanifoldsembeddedin3-Dspace.Wegivedirectionsto\nspeciﬁcaddressesintermsofaddressnumbersalongthese1-Droads,notinterms\nofcoordinatesin3-Dspace.Extractingthesemanifoldcoordinatesischallenging,\nbutholdsthepromisetoimprovemanymachinelearningalgorithms.Thisgeneral\nprincipleisappliedinmanycontexts.Figureshowsthemanifoldstructureof 5.13\nadatasetconsistingoffaces.Bytheendofthisbook,wewillhavedevelopedthe\nmethodsnecessarytolearnsuchamanifoldstructure.Inﬁgure,wewillsee 20.6\nhowamachinelearningalgorithmcansuccessfullyaccomplishthisgoal. Thisconcludespart,whichhasprovidedthebasicconceptsinmathematics I\nandmachinelearningwhichareemployedthroughouttheremainingpartsofthe\nbook.Youarenowpreparedtoembarkuponyourstudyofdeeplearning.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 399, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 938}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0400_cacc29ff", "text": "Thisconcludespart,whichhasprovidedthebasicconceptsinmathematics I\nandmachinelearningwhichareemployedthroughouttheremainingpartsofthe\nbook.Youarenowpreparedtoembarkuponyourstudyofdeeplearning. 1 6 4\nCHAPTER5.MACHINELEARNINGBASICS\nFigure5.13:TrainingexamplesfromtheQMULMultiviewFaceDataset( ,) Gong e t a l .2000\nforwhichthesubjectswereaskedtomoveinsuchawayastocoverthetwo-dimensional\nmanifoldcorrespondingtotwoanglesofrotation.Wewouldlikelearningalgorithmstobe\nabletodiscoveranddisentanglesuchmanifoldcoordinates.Figureillustratessucha 20.6\nfeat. 1 6 5\nP a rt I I\nD e e p N e t w orks: Mo d e rn\nPractices\n166\nThispartofthebooksummarizesthestateofmoderndeeplearningasitis\nusedtosolvepracticalapplications. Deeplearninghasalonghistoryandmanyaspirations.Severalapproaches\nhavebeenproposedthathaveyettoentirelybearfruit.Severalambitiousgoals\nhaveyettoberealized.Theseless-developedbranchesofdeeplearningappearin\ntheﬁnalpartofthebook.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 400, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 929}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0401_295fbebe", "text": "Thispartfocusesonlyonthoseapproachesthatareessentiallyworkingtech-\nnologiesthatarealreadyusedheavilyinindustry. Modern deeplearning provides avery powerful framework forsupervised\nlearning.Byaddingmorelayersandmoreunitswithinalayer,adeepnetworkcan\nrepresentfunctionsofincreasingcomplexity.Mosttasksthatconsistofmappingan\ninputvectortoanoutputvector,andthatareeasyforapersontodorapidly,can\nbeaccomplishedviadeeplearning,givensuﬃcientlylargemodelsandsuﬃciently\nlargedatasetsoflabeledtrainingexamples.Othertasks,thatcannotbedescribed\nasassociatingonevectortoanother,orthatarediﬃcultenoughthataperson\nwouldrequiretimetothinkandreﬂectinordertoaccomplishthetask,remain\nbeyondthescopeofdeeplearningfornow. Thispartofthebookdescribesthecoreparametricfunctionapproximation\ntechnologythatisbehindnearlyallmodernpracticalapplicationsofdeeplearning.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 401, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 837}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0402_829d7ed2", "text": "Thispartofthebookdescribesthecoreparametricfunctionapproximation\ntechnologythatisbehindnearlyallmodernpracticalapplicationsofdeeplearning. We begin by describingthe feedforward deepnetworkmodelthatisusedto\nrepresentthesefunctions.Next,wepresentadvancedtechniquesforregularization\nandoptimization ofsuchmodels.Scalingthesemodelstolargeinputssuchashigh\nresolutionimagesorlongtemporalsequencesrequiresspecialization.Weintroduce\ntheconvolutionalnetworkforscalingtolargeimagesandtherecurrentneural\nnetworkforprocessingtemporalsequences.Finally,wepresentgeneralguidelines\nforthepracticalmethodologyinvolvedindesigning,building,andconﬁguringan\napplicationinvolvingdeeplearning,andreviewsomeoftheapplicationsofdeep\nlearning. Thesechaptersarethemostimportantforapractitioner—someone whowants\ntobeginimplementingandusingdeeplearningalgorithmstosolvereal-world\nproblemstoday.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 402, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 864}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0403_05d8a3a2", "text": "Thesechaptersarethemostimportantforapractitioner—someone whowants\ntobeginimplementingandusingdeeplearningalgorithmstosolvereal-world\nproblemstoday. 1 6 7\nC h a p t e r 6\nD e e p F e e d f orw ard N e t w orks\nDeepfeedforwardnetworks,alsooftencalledfeedforwardneuralnetworks,\normultilayerperceptrons(MLPs),arethequintessentialdeeplearningmodels. Thegoalofafeedforwardnetworkistoapproximatesomefunction f∗.Forexample,\nforaclassiﬁer, y= f∗(x)mapsaninputxtoacategory y.Afeedforwardnetwork\ndeﬁnesamappingy= f(x;θ)andlearnsthevalueoftheparametersθthatresult\ninthebestfunctionapproximation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 403, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 583}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0404_8b734adb", "text": "Thesemodelsarecalledfeedforwardbecauseinformationﬂowsthroughthe\nfunctionbeingevaluatedfromx,throughtheintermediate computations usedto\ndeﬁne f,andﬁnallytotheoutputy.Therearenofeedbackconnectionsinwhich\noutputsofthemodelarefedbackintoitself.Whenfeedforwardneuralnetworks\nareextendedtoincludefeedbackconnections,theyarecalledrecurrentneural\nnetworks,presentedinchapter.10\nFeedforwardnetworksareofextremeimportancetomachinelearningpracti-\ntioners.Theyformthebasisofmanyimportantcommercialapplications.For\nexample,theconvolutionalnetworksusedforobjectrecognitionfromphotosarea\nspecializedkindoffeedforwardnetwork.Feedforwardnetworksareaconceptual\nsteppingstoneonthepathtorecurrentnetworks,whichpowermanynatural\nlanguageapplications.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 404, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 728}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0405_cf3bb4a8", "text": "Feedforwardneuralnetworksarecallednetworksbecausetheyaretypically\nrepresentedbycomposingtogethermanydiﬀerentfunctions.Themodelisasso-\nciatedwithadirectedacyclicgraphdescribinghowthefunctionsarecomposed\ntogether.Forexample,wemighthavethreefunctions f( 1 ), f( 2 ),and f( 3 )connected\ninachain,toform f(x) = f( 3 )( f( 2 )( f( 1 )(x))).Thesechainstructuresarethemost\ncommonlyusedstructuresofneuralnetworks.Inthiscase, f( 1 )iscalledtheﬁrst\nlayerofthenetwork, f( 2 )iscalledthesecondlayer,andsoon.Theoverall\n168\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nlengthofthechaingivesthedepthofthemodel.Itisfromthisterminologythat\nthename“deeplearning”arises.Theﬁnallayerofafeedforwardnetworkiscalled\ntheoutputlayer.Duringneuralnetworktraining,wedrive f(x)tomatch f∗(x). Thetrainingdataprovidesuswithnoisy,approximateexamplesof f∗(x) evaluated\natdiﬀerenttrainingpoints.Eachexamplexisaccompanied byalabel y f≈∗(x).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 405, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 892}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0406_976e0f32", "text": "Thetrainingdataprovidesuswithnoisy,approximateexamplesof f∗(x) evaluated\natdiﬀerenttrainingpoints.Eachexamplexisaccompanied byalabel y f≈∗(x). Thetrainingexamplesspecifydirectlywhattheoutputlayermustdoateachpoint\nx;itmustproduceavaluethatiscloseto y.Thebehavioroftheotherlayersis\nnotdirectlyspeciﬁedbythetrainingdata. Thelearningalgorithmmustdecide\nhowtousethoselayerstoproducethedesiredoutput,butthetrainingdatadoes\nnotsaywhateachindividuallayershoulddo.Instead,thelearningalgorithmmust\ndecidehowtousetheselayerstobestimplementanapproximation of f∗.Because\nthetrainingdatadoesnotshowthedesiredoutputforeachoftheselayers,these\nlayersarecalledhiddenlayers. Finally,thesenetworksarecalled ne u r a lbecausetheyarelooselyinspiredby\nneuroscience.Eachhiddenlayerofthenetworkistypicallyvector-valued.The\ndimensionalityofthesehiddenlayersdeterminesthewidthofthemodel.Each\nelementofthevectormaybeinterpretedasplayingaroleanalogoustoaneuron.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 406, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 932}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0407_04a2032d", "text": "Ratherthanthinkingofthelayerasrepresentingasinglevector-to-vectorfunction,\nwecanalsothinkofthelayerasconsistingofmanyunitsthatactinparallel,\neachrepresentingavector-to-scalarfunction.Eachunitresemblesaneuronin\nthesensethatitreceivesinputfrommanyotherunitsandcomputesitsown\nactivationvalue. Theideaofusingmanylayersofvector-valuedrepresentation\nisdrawnfromneuroscience.Thechoiceofthefunctions f( ) i(x)usedtocompute\ntheserepresentationsisalsolooselyguidedbyneuroscientiﬁcobservationsabout\nthefunctionsthatbiologicalneuronscompute.However,modernneuralnetwork\nresearchisguidedbymanymathematical andengineeringdisciplines,andthe\ngoalofneuralnetworksisnottoperfectlymodelthebrain.Itisbesttothinkof\nfeedforwardnetworksasfunctionapproximation machinesthataredesignedto\nachievestatisticalgeneralization, occasionallydrawingsomeinsightsfromwhatwe\nknowaboutthebrain,ratherthanasmodelsofbrainfunction. Onewaytounderstandfeedforwardnetworksistobeginwithlinearmodels\nandconsiderhowtoovercometheirlimitations.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 407, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 995}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0408_09a74522", "text": "Onewaytounderstandfeedforwardnetworksistobeginwithlinearmodels\nandconsiderhowtoovercometheirlimitations. Linearmodels,suchaslogistic\nregressionandlinearregression,areappealingbecausetheymaybeﬁteﬃciently\nandreliably,eitherinclosedformorwithconvexoptimization. Linearmodelsalso\nhavetheobviousdefectthatthemodelcapacityislimitedtolinearfunctions,so\nthemodelcannotunderstandtheinteractionbetweenanytwoinputvariables. Toextendlinearmodelstorepresentnonlinearfunctionsofx,wecanapply\nthelinearmodelnottoxitselfbuttoatransformedinput φ(x),where φisa\n1 6 9\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nnonlineartransformation.Equivalently,wecanapplythekerneltrickdescribedin\nsection,toobtainanonlinearlearningalgorithmbasedonimplicitlyapplying 5.7.2\nthe φmapping.Wecanthinkof φasprovidingasetoffeaturesdescribingx,or\nasprovidinganewrepresentationfor.x\nThequestionisthenhowtochoosethemapping. φ\n1.Oneoptionistouseaverygeneric φ,suchastheinﬁnite-dimens ional φthat\nisimplicitlyusedbykernelmachinesbasedontheRBFkernel.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 408, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 994}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0409_6713b083", "text": "φ\n1.Oneoptionistouseaverygeneric φ,suchastheinﬁnite-dimens ional φthat\nisimplicitlyusedbykernelmachinesbasedontheRBFkernel. If φ(x)is\nofhighenoughdimension,wecanalwayshaveenoughcapacitytoﬁtthe\ntrainingset,butgeneralization tothetestsetoftenremainspoor.Very\ngenericfeaturemappingsareusuallybasedonlyontheprincipleoflocal\nsmoothnessanddonotencodeenoughpriorinformationtosolveadvanced\nproblems. 2.Anotheroptionistomanuallyengineer φ.Untiltheadventofdeeplearning,\nthiswasthedominantapproach.Thisapproachrequiresdecadesofhuman\neﬀortfor eachseparate task, withpractitioners specializing in diﬀerent\ndomainssuchasspeech recognition or computer vision, and with little\ntransferbetweendomains. 3.Thestrategyofdeeplearningistolearn φ.Inthisapproach,wehaveamodel\ny= f(x;θw ,) = φ(x;θ)w.Wenowhaveparametersθthatweusetolearn\nφfromabroadclassoffunctions,andparameterswthatmapfrom φ(x)to\nthedesiredoutput.Thisisanexampleofadeepfeedforwardnetwork,with\nφdeﬁningahiddenlayer.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 409, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 958}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0410_4d2e44ec", "text": "Thisapproachistheonlyoneofthethreethat\ngivesupontheconvexityofthetrainingproblem,butthebeneﬁtsoutweigh\ntheharms.Inthisapproach,weparametrizetherepresentationas φ(x;θ)\nandusetheoptimization algorithmtoﬁndtheθthatcorrespondstoagood\nrepresentation.Ifwewish,thisapproachcancapturethebeneﬁtoftheﬁrst\napproachbybeinghighlygeneric—wedosobyusingaverybroadfamily\nφ(x;θ).Thisapproachcanalsocapturethebeneﬁtofthesecondapproach. Humanpractitioners canencodetheirknowledgetohelpgeneralization by\ndesigningfamilies φ(x;θ)thattheyexpectwillperformwell.Theadvantage\nisthatthehumandesigneronlyneedstoﬁndtherightgeneralfunction\nfamilyratherthanﬁndingpreciselytherightfunction. Thisgeneralprincipleofimprovingmodelsbylearningfeaturesextendsbeyond\nthefeedforwardnetworksdescribedinthischapter.Itisarecurringthemeofdeep\nlearningthatappliestoallofthekindsofmodelsdescribedthroughoutthisbook.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 410, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 869}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0411_d738dc49", "text": "Feedforwardnetworksaretheapplicationofthisprincipletolearningdeterministic\n1 7 0\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nmappingsfromxtoythatlackfeedbackconnections. Othermodelspresented\nlaterwillapplytheseprinciplestolearningstochasticmappings,learningfunctions\nwithfeedback,andlearningprobabilitydistributionsoverasinglevector. Webeginthischapterwithasimpleexampleofafeedforwardnetwork.Next,\nweaddresseachofthedesigndecisionsneededtodeployafeedforwardnetwork.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 411, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 454}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0412_8c978f70", "text": "Webeginthischapterwithasimpleexampleofafeedforwardnetwork.Next,\nweaddresseachofthedesigndecisionsneededtodeployafeedforwardnetwork. First,trainingafeedforwardnetworkrequiresmakingmanyofthesamedesign\ndecisionsasarenecessaryforalinearmodel:choosingtheoptimizer,thecost\nfunction,andtheformoftheoutputunits.Wereviewthesebasicsofgradient-based\nlearning,thenproceedtoconfrontsomeofthedesigndecisionsthatareunique\ntofeedforwardnetworks.Feedforwardnetworkshaveintroducedtheconceptofa\nhiddenlayer,andthisrequiresustochoosetheactivationfunctionsthatwill\nbeusedtocomputethehiddenlayervalues.Wemustalsodesignthearchitecture\nofthenetwork,includinghowmanylayersthenetworkshouldcontain,howthese\nlayersshould beconnectedto each other, and howmanyunitsshould bein\neachlayer.Learningindeepneuralnetworksrequirescomputingthegradients\nofcomplicatedfunctions.Wepresenttheback-propagationalgorithmandits\nmoderngeneralizations ,whichcanbeusedtoeﬃcientlycomputethesegradients. Finally,weclosewithsomehistoricalperspective. 6.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 412, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1001}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0413_35791413", "text": "Finally,weclosewithsomehistoricalperspective. 6. 1 E x am p l e: L earni n g X O R\nTomaketheideaofafeedforwardnetworkmoreconcrete,webeginwithan\nexampleofafullyfunctioningfeedforwardnetworkonaverysimpletask:learning\ntheXORfunction. TheXORfunction(“exclusiveor”)isanoperationontwobinaryvalues, x 1\nand x 2.Whenexactlyoneofthesebinaryvaluesisequalto,theXORfunction 1\nreturns.Otherwise,itreturns0.TheXORfunctionprovidesthetargetfunction 1\ny= f∗(x)thatwewanttolearn.Ourmodelprovidesafunction y= f(x;θ)and\nourlearningalgorithmwilladapttheparametersθtomake fassimilaraspossible\nto f∗. Inthissimpleexample,wewillnotbeconcernedwithstatisticalgeneralization. Wewantournetworktoperformcorrectlyonthefourpoints X={[0 ,0],[0 ,1],\n[1 ,0],and[1 ,1]}. Wewilltrainthenetworkonallfourofthesepoints. The\nonlychallengeistoﬁtthetrainingset.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 413, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 823}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0414_e8788b70", "text": "Wewantournetworktoperformcorrectlyonthefourpoints X={[0 ,0],[0 ,1],\n[1 ,0],and[1 ,1]}. Wewilltrainthenetworkonallfourofthesepoints. The\nonlychallengeistoﬁtthetrainingset. Wecantreatthisproblemasaregressionproblemanduseameansquared\nerrorlossfunction.Wechoosethislossfunctiontosimplifythemathforthis\nexampleasmuchaspossible.Inpracticalapplications,MSEisusuallynotan\n1 7 1\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nappropriatecostfunctionformodelingbinarydata.Moreappropriateapproaches\naredescribedinsection.6.2.2.2\nEvaluatedonourwholetrainingset,theMSElossfunctionis\nJ() =θ1\n4\nx∈ X( f∗() (;))x− fxθ2. (6.1)\nNowwemustchoosetheformofourmodel, f(x;θ).Supposethatwechoose\nalinearmodel,withconsistingofand.Ourmodelisdeﬁnedtobe θw b\nf , b (;xw) = xw+ b . (6.2)\nWecanminimize J(θ)inclosedformwithrespecttowand busingthenormal\nequations. Aftersolvingthenormalequations,weobtainw= 0and b=1\n2.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 414, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 878}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0415_2583870b", "text": "(6.2)\nWecanminimize J(θ)inclosedformwithrespecttowand busingthenormal\nequations. Aftersolvingthenormalequations,weobtainw= 0and b=1\n2. Thelinear\nmodelsimplyoutputs 0 .5everywhere.Whydoesthishappen?Figureshows6.1\nhowalinearmodelisnotabletorepresenttheXORfunction.Onewaytosolve\nthisproblemistouseamodelthatlearnsadiﬀerentfeaturespaceinwhicha\nlinearmodelisabletorepresentthesolution. Speciﬁcally,wewillintroduceaverysimplefeedforwardnetworkwithone\nhiddenlayercontainingtwohiddenunits.Seeﬁgureforanillustrationof 6.2\nthismodel.Thisfeedforwardnetworkhasavectorofhiddenunitshthatare\ncomputedbyafunction f( 1 )(x;Wc ,).Thevaluesofthesehiddenunitsarethen\nusedastheinputforasecondlayer.Thesecondlayeristheoutputlayerofthe\nnetwork.Theoutputlayerisstilljustalinearregressionmodel,butnowitis\nappliedtohratherthantox.Thenetworknowcontainstwofunctionschained\ntogether:h= f( 1 )(x;Wc ,)and y= f( 2 )(h;w , b),withthecompletemodelbeing\nf , , , b f (;xWcw) = ( 2 )( f( 1 )())x .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 415, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 961}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0416_f1b46db3", "text": "Whatfunctionshould f( 1 )compute?Linearmodelshaveserveduswellsofar,\nanditmaybetemptingtomake f( 1 )belinearaswell.Unfortunately,if f( 1 )were\nlinear,thenthefeedforwardnetworkasawholewouldremainalinearfunctionof\nitsinput.Ignoringtheintercepttermsforthemoment,suppose f( 1 )(x) =Wx\nand f( 2 )(h) =hw.Then f(x) =wWx.Wecouldrepresentthisfunctionas\nf() = xxwwherew= Ww. Clearly,wemustuseanonlinearfunctiontodescribethefeatures.Mostneural\nnetworksdosousinganaﬃnetransformationcontrolledbylearnedparameters,\nfollowedbyaﬁxed,nonlinearfunctioncalledanactivationfunction.Weusethat\nstrategyhere,bydeﬁningh= g(Wx+c) ,whereWprovidestheweightsofa\nlineartransformationandcthebiases.Previously,todescribealinearregression\n1 7 2\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\n0 1\nx 101x 2O r i g i n a l s p a c e x\n0 1 2\nh 101h 2L e a r n e d s p a c e h\nFigure6.1:SolvingtheXORproblembylearningarepresentation.Theboldnumbers\nprintedontheplotindicatethevaluethatthelearnedfunctionmustoutputateachpoint.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 416, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 980}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0417_b37f0a4d", "text": "( L e f t )AlinearmodelapplieddirectlytotheoriginalinputcannotimplementtheXOR\nfunction.When x1= 0,themodel’soutputmustincreaseas x2increases.When x1= 1,\nthemodel’soutputmustdecreaseas x 2increases.Alinearmodelmustapplyaﬁxed\ncoeﬃcient w 2to x 2.Thelinearmodelthereforecannotusethevalueof x 1tochange\nthecoeﬃcienton x 2andcannotsolvethisproblem. ( R i g h t )Inthetransformedspace\nrepresentedbythefeaturesextractedbyaneuralnetwork,alinearmodelcannowsolve\ntheproblem.Inourexamplesolution,thetwopointsthatmusthaveoutputhavebeen 1\ncollapsedintoasinglepointinfeaturespace.Inotherwords,thenonlinearfeatureshave\nmappedbothx= [1 ,0]andx= [0 ,1]toasinglepointinfeaturespace,h= [1 ,0]. Thelinearmodelcannowdescribethefunctionasincreasingin h1anddecreasingin h2. Inthisexample,themotivationforlearningthefeaturespaceisonlytomakethemodel\ncapacitygreatersothatitcanﬁtthetrainingset.Inmorerealisticapplications,learned\nrepresentationscanalsohelpthemodeltogeneralize.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 417, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 954}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0418_d229f981", "text": "Inthisexample,themotivationforlearningthefeaturespaceisonlytomakethemodel\ncapacitygreatersothatitcanﬁtthetrainingset.Inmorerealisticapplications,learned\nrepresentationscanalsohelpthemodeltogeneralize. 1 7 3\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nyy\nhh\nx xWwyy\nh 1 h 1\nx 1 x 1h 2 h 2\nx 2 x 2\nFigure6.2:Anexampleofafeedforwardnetwork,drawnintwodiﬀerentstyles.Speciﬁcally,\nthisisthefeedforwardnetworkweusetosolvetheXORexample.Ithasasinglehidden\nlayercontainingtwounits. ( L e f t )Inthisstyle,wedraweveryunitasanodeinthegraph. Thisstyleisveryexplicitandunambiguousbutfornetworkslargerthanthisexample\nitcanconsumetoomuchspace. Inthisstyle,wedrawanodeinthegraphfor ( R i g h t )\neachentirevectorrepresentingalayer’sactivations. Thisstyleismuchmorecompact.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 418, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 744}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0419_3f158ec6", "text": "Inthisstyle,wedrawanodeinthegraphfor ( R i g h t )\neachentirevectorrepresentingalayer’sactivations. Thisstyleismuchmorecompact. Sometimesweannotatetheedgesinthisgraphwiththenameoftheparametersthat\ndescribetherelationshipbetweentwolayers.Here,weindicatethatamatrixWdescribes\nthemappingfromxtoh,andavectorwdescribesthemappingfromhto y.We\ntypicallyomittheinterceptparametersassociatedwitheachlayerwhenlabelingthiskind\nofdrawing. model,weusedavectorofweightsandascalarbiasparametertodescribean\naﬃnetransformationfromaninputvectortoanoutputscalar.Now,wedescribe\nanaﬃnetransformationfromavectorxtoavectorh,soanentirevectorofbias\nparametersisneeded.Theactivationfunction gistypicallychosentobeafunction\nthatisappliedelement-wise,with h i= g(xW : , i+ c i).Inmodernneuralnetworks,\nthedefaultrecommendation istousetherectiﬁedlinearunitorReLU(Jarrett\ne t a l . e t a l . ,; ,; 2009NairandHinton2010Glorot,)deﬁnedbytheactivation 2011a\nfunction depictedinﬁgure.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 419, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 950}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0420_e741e6b0", "text": "e t a l . ,; ,; 2009NairandHinton2010Glorot,)deﬁnedbytheactivation 2011a\nfunction depictedinﬁgure. g z , z () = max0{} 6.3\nWecannowspecifyourcompletenetworkas\nf , , , b (;xWcw) = wmax0{ ,Wxc+}+ b . (6.3)\nWecannowspecifyasolutiontotheXORproblem.Let\nW=11\n11\n, (6.4)\nc=\n0\n−1\n, (6.5)\n1 7 4\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\n0\nz0g z ( ) = m a x 0{ , z}\nFigure6.3:Therectiﬁedlinearactivationfunction.Thisactivationfunctionisthedefault\nactivationfunctionrecommendedforusewithmostfeedforwardneuralnetworks.Applying\nthisfunctiontotheoutputofalineartransformationyieldsanonlineartransformation. However,thefunctionremainsveryclosetolinear,inthesensethatisapiecewiselinear\nfunctionwithtwolinearpieces.Becauserectiﬁedlinearunitsarenearlylinear,they\npreservemanyofthepropertiesthatmakelinearmodelseasytooptimizewithgradient-\nbasedmethods.Theyalsopreservemanyofthepropertiesthatmakelinearmodels\ngeneralizewell.Acommonprinciplethroughoutcomputerscienceisthatwecanbuild\ncomplicatedsystemsfromminimalcomponents.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 420, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 999}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0421_37e444c9", "text": "MuchasaTuringmachine’smemory\nneedsonlytobeabletostore0or1states,wecanbuildauniversalfunctionapproximator\nfromrectiﬁedlinearfunctions. 1 7 5\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nw=1\n−2\n, (6.6)\nand. b= 0\nWecannowwalkthroughthewaythatthemodelprocessesabatchofinputs. LetXbethedesignmatrixcontainingallfourpointsinthebinaryinputspace,\nwithoneexampleperrow:\nX=\n00\n01\n10\n11\n. (6.7)\nTheﬁrststepintheneuralnetworkistomultiplytheinputmatrixbytheﬁrst\nlayer’sweightmatrix:\nXW=\n00\n11\n11\n22\n. (6.8)\nNext,weaddthebiasvector,toobtainc\n\n0 1−\n10\n10\n21\n. (6.9)\nInthisspace,alloftheexamplesliealongalinewithslope.Aswemovealong 1\nthisline,theoutputneedstobeginat,thenriseto,thendropbackdownto. 0 1 0\nAlinearmodelcannotimplementsuchafunction.Toﬁnishcomputingthevalue\nofforeachexample,weapplytherectiﬁedlineartransformation: h\n\n00\n10\n10\n21\n.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 421, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 849}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0422_8037ee5a", "text": "0 1 0\nAlinearmodelcannotimplementsuchafunction.Toﬁnishcomputingthevalue\nofforeachexample,weapplytherectiﬁedlineartransformation: h\n\n00\n10\n10\n21\n. (6.10)\nThistransformationhaschangedtherelationshipbetweentheexamples.Theyno\nlongerlieonasingleline.Asshowninﬁgure,theynowlieinaspacewherea 6.1\nlinearmodelcansolvetheproblem. Weﬁnishbymultiplyingbytheweightvector:w\n\n0\n1\n1\n0\n. (6.11)\n1 7 6\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nTheneuralnetworkhasobtainedthecorrectanswerforeveryexampleinthebatch. Inthisexample,wesimplyspeciﬁedthesolution,thenshowedthatitobtained\nzeroerror.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 422, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 580}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0423_4267e468", "text": "(6.11)\n1 7 6\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nTheneuralnetworkhasobtainedthecorrectanswerforeveryexampleinthebatch. Inthisexample,wesimplyspeciﬁedthesolution,thenshowedthatitobtained\nzeroerror. Inarealsituation,theremightbebillionsofmodelparametersand\nbillionsoftrainingexamples,soonecannotsimplyguessthesolutionaswedid\nhere.Instead,agradient-basedoptimization algorithmcanﬁndparametersthat\nproduceverylittleerror.ThesolutionwedescribedtotheXORproblemisata\nglobalminimumofthelossfunction,sogradientdescentcouldconvergetothis\npoint.ThereareotherequivalentsolutionstotheXORproblemthatgradient\ndescentcouldalsoﬁnd.Theconvergencepointofgradientdescentdependsonthe\ninitialvaluesoftheparameters.Inpractice,gradientdescentwouldusuallynot\nﬁndclean,easilyunderstood,integer-valuedsolutionsliketheonewepresented\nhere. 6.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 423, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 810}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0424_2479e767", "text": "6. 2 Gradi en t - Bas e d L earni n g\nDesigningandtraininganeuralnetworkisnotmuchdiﬀerentfromtrainingany\nothermachinelearningmodelwithgradientdescent.Insection,wedescribed 5.10\nhowtobuildamachinelearningalgorithmbyspecifyinganoptimizationprocedure,\nacostfunction,andamodelfamily.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 424, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 279}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0425_4f0ff57e", "text": "Thelargestdiﬀerencebetweenthelinearmodelswehaveseensofarandneural\nnetworksisthatthenonlinearityofaneuralnetworkcausesmostinterestingloss\nfunctionstobecomenon-convex.Thismeansthatneuralnetworksareusually\ntrainedbyusingiterative,gradient-basedoptimizersthatmerelydrivethecost\nfunctiontoaverylowvalue,ratherthanthelinearequationsolversusedtotrain\nlinearregressionmodelsortheconvexoptimization algorithmswithglobalconver-\ngenceguaranteesusedtotrainlogisticregressionorSVMs.Convexoptimization\nconvergesstartingfromanyinitialparameters(intheory—inpracticeitisvery\nrobustbutcanencounternumericalproblems).Stochasticgradientdescentapplied\ntonon-convexlossfunctionshasnosuchconvergenceguarantee,andissensitive\ntothevaluesoftheinitialparameters.Forfeedforwardneuralnetworks,itis\nimportanttoinitializeallweightstosmallrandomvalues.Thebiasesmaybe\ninitializedtozeroortosmallpositivevalues.Theiterativegradient-basedopti-\nmizationalgorithmsusedtotrainfeedforwardnetworksandalmostallotherdeep\nmodelswillbedescribedindetailinchapter,withparameterinitialization in 8\nparticulardiscussedinsection.Forthemoment,itsuﬃcestounderstandthat 8.4\nthetrainingalgorithmisalmostalwaysbasedonusingthegradienttodescendthe\ncostfunctioninonewayoranother.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 425, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1221}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0426_c09f068f", "text": "The speciﬁcalgorithmsareimprovements\nandreﬁnementsontheideasofgradientdescent,introducedinsection,and,4.3\n1 7 7\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nmorespeciﬁcally,aremostoftenimprovementsofthestochasticgradientdescent\nalgorithm,introducedinsection.5.9\nWecanofcourse,trainmodelssuchaslinearregressionandsupportvector\nmachineswithgradientdescenttoo,andinfactthisiscommonwhenthetraining\nsetisextremelylarge.Fromthispointofview,traininganeuralnetworkisnot\nmuchdiﬀerentfromtraininganyothermodel.Computingthegradientisslightly\nmorecomplicatedforaneuralnetwork,butcanstillbedoneeﬃcientlyandexactly. Sectionwilldescribehowtoobtainthegradientusingtheback-propagation 6.5\nalgorithmandmoderngeneralizations oftheback-propagationalgorithm. Aswithothermachinelearningmodels,toapplygradient-basedlearningwe\nmustchooseacostfunction,andwemustchoosehowtorepresenttheoutputof\nthemodel.Wenowrevisitthesedesignconsiderationswithspecialemphasison\ntheneuralnetworksscenario.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 426, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 950}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0427_7b11d572", "text": "6.2.1CostFunctions\nAnimportantaspectofthedesignofadeepneuralnetworkisthechoiceofthe\ncostfunction.Fortunately,thecostfunctionsforneuralnetworksaremoreorless\nthesameasthoseforotherparametricmodels,suchaslinearmodels. Inmostcases,ourparametricmodeldeﬁnesadistribution p(yx|;θ)and\nwesimplyuse theprinciple ofmaximumlikelihood.Thismeansweusethe\ncross-entropybetweenthetrainingdataandthemodel’spredictionsasthecost\nfunction. Sometimes,wetakeasimplerapproach,whereratherthanpredictingacomplete\nprobabilitydistributionovery,wemerelypredictsomestatisticofyconditioned\non.Specializedlossfunctionsallowustotrainapredictoroftheseestimates.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 427, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 627}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0428_16987085", "text": "x\nThetotalcostfunctionusedtotrainaneuralnetworkwilloftencombineone\noftheprimarycostfunctionsdescribedherewitharegularizationterm.Wehave\nalreadyseensomesimpleexamplesofregularizationappliedtolinearmodelsin\nsection.Theweightdecayapproachusedforlinearmodelsisalsodirectly 5.2.2\napplicabletodeepneuralnetworksandisamongthemostpopularregularization\nstrategies.Moreadvancedregularizationstrategiesforneuralnetworkswillbe\ndescribedinchapter.7\n6.2.1.1LearningConditionalDistributionswithMaximumLikelihood\nMostmodernneuralnetworksaretrainedusingmaximumlikelihood.Thismeans\nthatthecostfunctionissimplythenegativelog-likelihood,equivalentlydescribed\n1 7 8\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nasthecross-entropybetweenthetrainingdataandthemodeldistribution.This\ncostfunctionisgivenby\nJ() = θ − E x y ,∼ ˆ pdatalog p m o de l( )yx| .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 428, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 817}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0429_3c363f5b", "text": "(6.12)\nThespeciﬁcformofthecostfunctionchangesfrommodeltomodel,depending\nonthespeciﬁcformoflog p m o de l.Theexpansionoftheaboveequationtypically\nyieldssometermsthatdonotdependonthemodelparametersandmaybedis-\ncarded.Forexample,aswesawinsection,if5.5.1 p m o de l(yx|) =N(y; f(x;θ) ,I),\nthenwerecoverthemeansquarederrorcost,\nJ θ() =1\n2E x y ,∼ ˆ pdata||− ||y f(;)xθ2+const , (6.13)\nuptoascalingfactorof1\n2andatermthatdoesnotdependon.Thediscardedθ\nconstantisbasedonthevarianceoftheGaussiandistribution,whichinthiscase\nwechosenottoparametrize. Previously,wesawthattheequivalencebetween\nmaximumlikelihoodestimationwithanoutputdistributionandminimization of\nmeansquarederrorholdsforalinearmodel,butinfact,theequivalenceholds\nregardlessoftheusedtopredictthemeanoftheGaussian. f(;)xθ\nAnadvantageofthisapproachofderivingthecostfunctionfrommaximum\nlikelihoodisthatitremovestheburdenofdesigningcostfunctionsforeachmodel. Specifyingamodel p(yx|)automatically determinesacostfunction log p(yx|).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 429, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 982}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0430_f6ffc501", "text": "Specifyingamodel p(yx|)automatically determinesacostfunction log p(yx|). Onerecurringthemethroughoutneuralnetworkdesignisthatthegradientof\nthecostfunctionmustbelargeandpredictableenoughtoserveasagoodguide\nforthelearningalgorithm.Functionsthatsaturate(becomeveryﬂat)undermine\nthisobjectivebecausetheymakethegradientbecomeverysmall.Inmanycases\nthishappensbecausetheactivationfunctionsusedtoproducetheoutputofthe\nhiddenunitsortheoutputunitssaturate.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 430, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 446}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0431_60c664bb", "text": "Thenegativelog-likelihoodhelpsto\navoidthisproblemformanymodels.Manyoutputunitsinvolveanexpfunction\nthatcansaturatewhenitsargumentisverynegative.The logfunctioninthe\nnegativelog-likelihoodcostfunctionundoestheexpofsomeoutputunits.Wewill\ndiscusstheinteractionbetweenthecostfunctionandthechoiceofoutputunitin\nsection.6.2.2\nOneunusualpropertyofthecross-entropycostusedtoperformmaximum\nlikelihoodestimationisthatitusuallydoesnothaveaminimumvaluewhenapplied\ntothemodelscommonlyusedinpractice.Fordiscreteoutputvariables,most\nmodelsareparametrized insuchawaythattheycannotrepresentaprobability\nofzeroorone,butcancomearbitrarilyclosetodoingso.Logisticregression\nisanexampleofsuchamodel.Forreal-valuedoutputvariables,ifthemodel\n1 7 9\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\ncancontrolthedensityoftheoutputdistribution(forexample,bylearningthe\nvarianceparameterofaGaussianoutputdistribution)thenitbecomespossible\ntoassignextremelyhighdensitytothecorrecttrainingsetoutputs,resultingin\ncross-entropyapproachingnegativeinﬁnity.Regularizationtechniquesdescribed\ninchapterprovideseveraldiﬀerentwaysofmodifyingthelearningproblemso 7\nthatthemodelcannotreapunlimitedrewardinthisway.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 431, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1156}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0432_95efef03", "text": "6.2.1.2LearningConditionalStatistics\nInsteadoflearningafullprobabilitydistribution p(yx|;θ)weoftenwanttolearn\njustoneconditionalstatisticofgiven.yx\nForexample,wemayhaveapredictor f(x;θ) thatwewishtopredictthemean\nof.y\nIfweuseasuﬃcientlypowerfulneuralnetwork,wecanthinkoftheneural\nnetworkasbeingabletorepresentanyfunction ffromawideclassoffunctions,\nwiththisclassbeinglimitedonlybyfeaturessuchascontinuityandboundedness\nratherthanbyhavingaspeciﬁcparametricform.Fromthispointofview,we\ncanviewthecostfunctionasbeingafunctionalratherthanjustafunction.A\nfunctionalisamappingfromfunctionstorealnumbers.Wecanthusthinkof\nlearningaschoosingafunctionratherthanmerelychoosingasetofparameters. Wecandesignourcostfunctionaltohaveitsminimumoccuratsomespeciﬁc\nfunctionwedesire.Forexample,wecandesignthecostfunctionaltohaveits\nminimumlieonthefunctionthatmapsxtotheexpectedvalueofygivenx.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 432, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 871}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0433_53454890", "text": "Wecandesignourcostfunctionaltohaveitsminimumoccuratsomespeciﬁc\nfunctionwedesire.Forexample,wecandesignthecostfunctionaltohaveits\nminimumlieonthefunctionthatmapsxtotheexpectedvalueofygivenx. Solvinganoptimizationproblemwithrespecttoafunctionrequiresamathematical\ntoolcalledcalculusofvariations,describedinsection.Itisnotnecessary 19.4.2\ntounderstandcalculusofvariationstounderstandthecontentofthischapter.At\nthemoment,itisonlynecessarytounderstandthatcalculusofvariationsmaybe\nusedtoderivethefollowingtworesults. Ourﬁrstresultderivedusingcalculusofvariationsisthatsolvingtheoptimiza-\ntionproblem\nf∗= argmin\nfE x y ,∼ pdata||− ||y f()x2(6.14)\nyields\nf∗() = x E y∼ pdata ( ) y x|[]y , (6.15)\nsolongasthisfunctionlieswithintheclassweoptimizeover.Inotherwords,ifwe\ncouldtrainoninﬁnitelymanysamplesfromthetruedatageneratingdistribution,\nminimizingthemeansquarederrorcostfunctiongivesafunctionthatpredictsthe\nmeanofforeachvalueof.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 433, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 923}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0434_2d515743", "text": "y x\n1 8 0\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nDiﬀerentcostfunctionsgivediﬀerentstatistics.Asecondresultderivedusing\ncalculusofvariationsisthat\nf∗= argmin\nfE x y ,∼ pdata||− ||y f()x 1 (6.16)\nyieldsafunctionthatpredictsthe m e d i a nvalueofyforeachx,solongassucha\nfunctionmaybedescribedbythefamilyoffunctionsweoptimizeover.Thiscost\nfunctioniscommonlycalled . meanabsoluteerror\nUnfortunately,meansquarederrorandmeanabsoluteerroroftenleadtopoor\nresultswhenusedwithgradient-basedoptimization. Someoutputunitsthat\nsaturateproduceverysmallgradientswhencombinedwiththesecostfunctions. Thisisonereasonthatthecross-entropycostfunctionismorepopularthanmean\nsquarederrorormeanabsoluteerror,evenwhenitisnotnecessarytoestimatean\nentiredistribution. p( )yx|\n6.2.2OutputUnits\nThechoiceofcostfunctionistightlycoupledwiththechoiceofoutputunit.Most\nofthetime,wesimplyusethecross-entropybetweenthedatadistributionandthe\nmodeldistribution. Thechoiceofhowtorepresenttheoutputthendetermines\ntheformofthecross-entropyfunction.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 434, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1001}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0435_a48b7ec5", "text": "Thechoiceofhowtorepresenttheoutputthendetermines\ntheformofthecross-entropyfunction. Anykindofneuralnetworkunitthatmaybeusedasanoutputcanalsobe\nusedasahiddenunit.Here,wefocusontheuseoftheseunitsasoutputsofthe\nmodel,butinprincipletheycanbeusedinternallyaswell.Werevisittheseunits\nwithadditionaldetailabouttheiruseashiddenunitsinsection.6.3\nThroughoutthissection,wesupposethatthefeedforwardnetworkprovidesa\nsetofhiddenfeaturesdeﬁnedbyh= f(x;θ).Theroleoftheoutputlayeristhen\ntoprovidesomeadditionaltransformationfromthefeaturestocompletethetask\nthatthenetworkmustperform. 6.2.2.1LinearUnitsforGaussianOutputDistributions\nOnesimplekindofoutputunitisanoutputunitbasedonanaﬃnetransformation\nwithnononlinearity.Theseareoftenjustcalledlinearunits. Givenfeaturesh,alayeroflinearoutputunitsproducesavectorˆy=Wh+b. Linearoutputlayersareoftenusedtoproducethemeanofaconditional\nGaussiandistribution:\np( ) = (;yx| NyˆyI ,) .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 435, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 910}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0436_7b4b67fb", "text": "Givenfeaturesh,alayeroflinearoutputunitsproducesavectorˆy=Wh+b. Linearoutputlayersareoftenusedtoproducethemeanofaconditional\nGaussiandistribution:\np( ) = (;yx| NyˆyI ,) . (6.17)\n1 8 1\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nMaximizingthelog-likelihoodisthenequivalenttominimizingthemeansquared\nerror. Themaximumlikelihoodframeworkmakesitstraightforwardtolearnthe\ncovarianceoftheGaussiantoo,ortomakethecovarianceoftheGaussianbea\nfunctionoftheinput.However,thecovariancemustbeconstrainedtobeapositive\ndeﬁnitematrixforallinputs.Itisdiﬃculttosatisfysuchconstraintswithalinear\noutputlayer,sotypicallyotheroutputunitsareusedtoparametrizethecovariance. Approachestomodelingthecovariancearedescribedshortly,insection.6.2.2.4\nBecauselinearunitsdonotsaturate,theyposelittlediﬃcultyforgradient-\nbasedoptimizationalgorithmsandmaybeusedwithawidevarietyofoptimization\nalgorithms.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 436, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 859}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0437_cf40a49c", "text": "6.2.2.2SigmoidUnitsforBernoulliOutputDistributions\nManytasksrequirepredictingthevalueofabinaryvariable y.Classiﬁcation\nproblemswithtwoclassescanbecastinthisform. Themaximum-likelihoodapproachistodeﬁneaBernoullidistributionover y\nconditionedon.x\nABernoullidistributionisdeﬁnedbyjustasinglenumber.Theneuralnet\nneedstopredictonly P( y= 1|x).Forthisnumbertobeavalidprobability,it\nmustlieintheinterval[0,1]. Satisfyingthisconstraintrequiressomecarefuldesigneﬀort.Supposewewere\ntousealinearunit,andthresholditsvaluetoobtainavalidprobability:\nP y(= 1 ) = max |x\n0min ,\n1 ,wh+ b\n.(6.18)\nThiswouldindeeddeﬁneavalidconditionaldistribution,butwewouldnotbeable\ntotrainitveryeﬀectivelywithgradientdescent.Anytimethatwh+ bstrayed\noutsidetheunitinterval,thegradientoftheoutputofthemodelwithrespectto\nitsparameterswouldbe 0.Agradientof 0istypicallyproblematicbecausethe\nlearningalgorithmnolongerhasaguideforhowtoimprovethecorresponding\nparameters.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 437, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 937}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0438_aa507d36", "text": "Instead,itisbettertouseadiﬀerentapproachthatensuresthereisalwaysa\nstronggradientwheneverthemodelhasthewronganswer.Thisapproachisbased\nonusingsigmoidoutputunitscombinedwithmaximumlikelihood. Asigmoidoutputunitisdeﬁnedby\nˆ y σ= \nwh+ b\n(6.19)\n1 8 2\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nwhereisthelogisticsigmoidfunctiondescribedinsection. σ 3.10\nWecanthinkofthesigmoidoutputunitashavingtwocomponents.First,it\nusesalinearlayertocompute z=wh+ b.Next,itusesthesigmoidactivation\nfunctiontoconvertintoaprobability. z\nWeomitthedependenceonxforthemomenttodiscusshowtodeﬁnea\nprobabilitydistributionover yusingthevalue z.Thesigmoidcanbemotivated\nbyconstructinganunnormalized probabilitydistribution˜ P( y),whichdoesnot\nsumto1.Wecanthendividebyanappropriateconstanttoobtainavalid\nprobabilitydistribution.Ifwebeginwiththeassumptionthattheunnormalized log\nprobabilitiesarelinearin yand z,wecanexponentiatetoobtaintheunnormalized\nprobabilities.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 438, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 928}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0439_222e32e2", "text": "WethennormalizetoseethatthisyieldsaBernoullidistribution\ncontrolledbyasigmoidaltransformationof: z\nlog˜ P y y z () = (6.20)\n˜ P y y z () = exp() (6.21)\nP y() =exp() y z1\ny= 0exp( yz)(6.22)\nP y σ y z . () = ((2−1)) (6.23)\nProbabilitydistributionsbasedonexponentiationandnormalization arecommon\nthroughoutthestatisticalmodelingliterature.The zvariabledeﬁningsucha\ndistributionoverbinaryvariablesiscalleda.logit\nThisapproachtopredictingtheprobabilities inlog-spaceisnaturaltouse\nwithmaximumlikelihoodlearning.Becausethecostfunctionusedwithmaximum\nlikelihoodis−log P( y|x),theloginthecostfunctionundoestheexpofthe\nsigmoid.Withoutthiseﬀect,thesaturationofthesigmoidcouldpreventgradient-\nbased learningfrom makinggoodprogress.Theloss functionfor maximum\nlikelihoodlearningofaBernoulliparametrized byasigmoidis\nJ P y () = logθ − (|x) (6.24)\n= log((2 1)) − σ y− z (6.25)\n= ((12)) ζ − y z .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 439, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 884}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0440_c3941b36", "text": "(6.26)\nThisderivationmakesuseofsomepropertiesfromsection.Byrewriting3.10\nthelossintermsofthesoftplusfunction,wecanseethatitsaturatesonlywhen\n(1−2 y) zisverynegative.Saturationthusoccursonlywhenthemodelalready\nhastherightanswer—when y= 1and zisverypositive,or y= 0and zisvery\nnegative.When zhasthewrongsign,theargumenttothesoftplusfunction,\n1 8 3\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\n(1−2 y) z,maybesimpliﬁedto|| z.As|| zbecomeslargewhile zhasthewrongsign,\nthesoftplusfunctionasymptotestowardsimplyreturningitsargument || z.The\nderivativewithrespectto zasymptotestosign( z),so,inthelimitofextremely\nincorrect z,thesoftplusfunctiondoesnotshrinkthegradientatall.Thisproperty\nisveryusefulbecauseitmeansthatgradient-basedlearningcanacttoquickly\ncorrectamistaken. z\nWhenweuseotherlossfunctions,suchasmeansquarederror,thelosscan\nsaturateanytime σ( z)saturates.Thesigmoidactivationfunctionsaturatesto0\nwhen zbecomesverynegativeandsaturatestowhen1 zbecomesverypositive.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 440, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 956}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0441_3845768f", "text": "Thegradientcanshrinktoosmalltobeusefulforlearningwheneverthishappens,\nwhetherthemodelhasthecorrectanswerortheincorrectanswer.Forthisreason,\nmaximumlikelihoodisalmostalwaysthepreferredapproachtotrainingsigmoid\noutputunits. Analytically,thelogarithmofthesigmoidisalwaysdeﬁnedandﬁnite,because\nthesigmoidreturnsvaluesrestrictedtotheopeninterval(0 ,1),ratherthanusing\ntheentireclosedintervalofvalidprobabilities [0 ,1].Insoftwareimplementations,\ntoavoidnumericalproblems,itisbesttowritethenegativelog-likelihoodasa\nfunctionof z,ratherthanasafunctionofˆ y= σ( z).Ifthesigmoidfunction\nunderﬂowstozero,thentakingthelogarithmofˆ yyieldsnegativeinﬁnity. 6.2.2.3SoftmaxUnitsforMultinoulliOutputDistributions\nAnytimewewishtorepresentaprobabilitydistributionoveradiscretevariable\nwith npossiblevalues,wemayusethesoftmaxfunction.Thiscanbeseenasa\ngeneralization ofthesigmoidfunctionwhichwasusedtorepresentaprobability\ndistributionoverabinaryvariable.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 441, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 935}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0442_11d83db9", "text": "Softmaxfunctionsaremostoftenusedastheoutputofaclassiﬁer,torepresent\ntheprobabilitydistributionover ndiﬀerentclasses.Morerarely,softmaxfunctions\ncanbeusedinsidethemodelitself,ifwewishthemodeltochoosebetweenoneof\nndiﬀerentoptionsforsomeinternalvariable. Inthecaseofbinaryvariables,wewishedtoproduceasinglenumber\nˆ y P y . = (= 1 )|x (6.27)\nBecausethisnumberneededtoliebetweenand,andbecausewewantedthe 0 1\nlogarithmofthenumbertobewell-behavedforgradient-basedoptimization of\nthelog-likelihood,wechosetoinsteadpredictanumber z=log˜ P( y=1|x). ExponentiatingandnormalizinggaveusaBernoullidistributioncontrolledbythe\nsigmoidfunction.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 442, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 627}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0443_67695ea7", "text": "ExponentiatingandnormalizinggaveusaBernoullidistributioncontrolledbythe\nsigmoidfunction. 1 8 4\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nTogeneralizetothecaseofadiscretevariablewith nvalues,wenowneed\ntoproduceavectorˆy,with ˆ y i= P( y= i|x).Werequirenotonlythateach\nelementofˆ y ibebetweenand,butalsothattheentirevectorsumstosothat 0 1 1\nitrepresentsavalidprobabilitydistribution.Thesameapproachthatworkedfor\ntheBernoullidistributiongeneralizestothemultinoullidistribution.First,alinear\nlayerpredictsunnormalized logprobabilities:\nzW= hb+ , (6.28)\nwhere z i=log˜ P( y= i|x) .Thesoftmaxfunctioncanthenexponentiateand\nnormalizetoobtainthedesired z ˆy.Formally,thesoftmaxfunctionisgivenby\nsoftmax()z i=exp( z i)\njexp( z j).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 443, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 714}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0444_89f64cf9", "text": "(6.29)\nAswiththelogisticsigmoid,theuseoftheexpfunctionworksverywellwhen\ntrainingthesoftmaxtooutputatargetvalueyusingmaximumlog-likelihood.In\nthiscase,wewishtomaximize log P(y= i;z)=logsoftmax(z) i.Deﬁningthe\nsoftmaxintermsofexpisnaturalbecausetheloginthelog-likelihoodcanundo\ntheofthesoftmax: exp\nlogsoftmax()z i= z i−log\njexp( z j) .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 444, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 335}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0445_ed315768", "text": "(6.30)\nTheﬁrsttermofequationshowsthattheinput 6.30 z ialwayshasadirect\ncontributiontothecostfunction.Becausethistermcannotsaturate,weknow\nthatlearningcanproceed,evenifthecontributionof z itothesecondtermof\nequationbecomesverysmall.Whenmaximizingthelog-likelihood,theﬁrst 6.30\ntermencourages z itobepushedup,whilethesecondtermencouragesallofztobe\npusheddown.Togainsomeintuitionforthesecondterm,log\njexp( z j),observe\nthatthistermcanberoughlyapproximatedbymax j z j.Thisapproximation is\nbasedontheideathatexp( z k) isinsigniﬁcantforany z kthatisnoticeablylessthan\nmax j z j.Theintuitionwecangainfromthisapproximation isthatthenegative\nlog-likelihoodcostfunctionalwaysstronglypenalizesthemostactiveincorrect\nprediction.Ifthecorrectansweralreadyhasthelargestinputtothesoftmax,then\nthe− z itermandthelog\njexp( z j)≈max j z j= z itermswillroughlycancel. Thisexamplewillthencontributelittletotheoveralltrainingcost,whichwillbe\ndominatedbyotherexamplesthatarenotyetcorrectlyclassiﬁed.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 445, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 978}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0446_18d2d9a2", "text": "Thisexamplewillthencontributelittletotheoveralltrainingcost,whichwillbe\ndominatedbyotherexamplesthatarenotyetcorrectlyclassiﬁed. Sofarwehavediscussedonlyasingleexample.Overall,unregularized maximum\nlikelihoodwilldrivethemodeltolearnparametersthatdrivethesoftmaxtopredict\n1 8 5\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nthefractionofcountsofeachoutcomeobservedinthetrainingset:\nsoftmax((;))zxθ i≈m\nj = 1 1y() j= i , x() j= xm\nj = 1 1x() j = x. (6.31)\nBecausemaximumlikelihoodisaconsistentestimator,thisisguaranteedtohappen\nsolongasthemodelfamilyiscapableofrepresentingthetrainingdistribution.In\npractice,limitedmodelcapacityandimperfectoptimization willmeanthatthe\nmodelisonlyabletoapproximatethesefractions.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 446, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 700}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0447_29e8e4fc", "text": "Manyobjectivefunctionsotherthanthelog-likelihooddonotworkaswell\nwiththesoftmaxfunction.Speciﬁcally,objectivefunctionsthatdonotusealogto\nundotheexpofthesoftmaxfailtolearnwhentheargumenttotheexpbecomes\nverynegative,causingthegradienttovanish.Inparticular,squarederrorisa\npoorlossfunctionforsoftmaxunits,andcanfailtotrainthemodeltochangeits\noutput,evenwhenthemodelmakeshighlyconﬁdentincorrectpredictions(,Bridle\n1990).Tounderstandwhytheseotherlossfunctionscanfail,weneedtoexamine\nthesoftmaxfunctionitself. Likethesigmoid,thesoftmaxactivationcansaturate.Thesigmoidfunctionhas\nasingleoutputthatsaturateswhenitsinputisextremelynegativeorextremely\npositive.Inthecaseofthesoftmax,therearemultipleoutputvalues.These\noutputvaluescansaturatewhenthediﬀerencesbetweeninputvaluesbecome\nextreme.Whenthesoftmaxsaturates,manycostfunctionsbasedonthesoftmax\nalsosaturate,unlesstheyareabletoinvertthesaturatingactivatingfunction.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 447, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 909}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0448_f26eda3d", "text": "Toseethatthesoftmaxfunctionrespondstothediﬀerencebetweenitsinputs,\nobservethatthesoftmaxoutputisinvarianttoaddingthesamescalartoallofits\ninputs:\nsoftmax() = softmax(+) zz c . (6.32)\nUsingthisproperty,wecanderiveanumericallystablevariantofthesoftmax:\nsoftmax() = softmax( max zz−\niz i) . (6.33)\nThereformulatedversionallowsustoevaluatesoftmaxwithonlysmallnumerical\nerrorsevenwhen zcontainsextremelylargeorextremelynegativenumbers.Ex-\naminingthenumericallystablevariant,weseethatthesoftmaxfunctionisdriven\nbytheamountthatitsargumentsdeviatefrommax i z i. Anoutput softmax(z) isaturatestowhenthecorrespondinginputismaximal 1\n( z i=max i z i)and z iismuchgreaterthanalloftheotherinputs.Theoutput\nsoftmax(z) icanalsosaturatetowhen0 z iisnotmaximalandthemaximumis\nmuchgreater.Thisisageneralization ofthewaythatsigmoidunitssaturate,and\n1 8 6\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\ncancausesimilardiﬃcultiesforlearningifthelossfunctionisnotdesignedto\ncompensateforit.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 448, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 953}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0449_025c37d6", "text": "Theargumentztothesoftmaxfunctioncanbeproducedintwodiﬀerentways. Themostcommonissimplytohaveanearlierlayeroftheneuralnetworkoutput\neveryelementofz,asdescribedaboveusingthelinearlayerz=Wh+b.While\nstraightforward,thisapproachactuallyoverparametrizes thedistribution.The\nconstraintthatthe noutputsmustsumtomeansthatonly 1 n−1parametersare\nnecessary;theprobabilityofthe n-thvaluemaybeobtainedbysubtractingthe\nﬁrst n−1 1 probabilitiesfrom.Wecanthusimposearequirementthatoneelement\nofzbeﬁxed.Forexample,wecanrequirethat z n=0.Indeed,thisisexactly\nwhatthesigmoidunitdoes.Deﬁning P( y= 1|x) = σ( z)isequivalenttodeﬁning\nP( y= 1|x) =softmax(z) 1withatwo-dimensionalzand z 1= 0.Boththe n−1\nargumentandthe nargumentapproachestothesoftmaxcandescribethesame\nsetofprobabilitydistributions,buthavediﬀerentlearningdynamics.Inpractice,\nthereisrarelymuchdiﬀerencebetweenusingtheoverparametrized versionorthe\nrestrictedversion,anditissimplertoimplementtheoverparametrized version.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 449, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 961}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0450_00e07bb0", "text": "Fromaneuroscientiﬁcpointofview,itisinterestingtothinkofthesoftmaxas\nawaytocreateaformofcompetitionbetweentheunitsthatparticipateinit:the\nsoftmaxoutputsalwayssumto1soanincreaseinthevalueofoneunitnecessarily\ncorrespondstoadecreaseinthevalueofothers.Thisisanalogoustothelateral\ninhibitionthatisbelievedtoexistbetweennearbyneuronsinthecortex.Atthe\nextreme(whenthediﬀerencebetweenthemaximal a iandtheothersislargein\nmagnitude)itbecomesaformofwinner-take-all(oneoftheoutputsisnearly1\nandtheothersarenearly0). Thename“softmax”canbesomewhatconfusing.Thefunctionismoreclosely\nrelatedtotheargmaxfunctionthanthemaxfunction. Theterm“soft”derives\nfromthefactthatthesoftmaxfunctioniscontinuousanddiﬀerentiable. The\nargmaxfunction,withitsresultrepresentedasaone-hotvector,isnotcontinuous\nordiﬀerentiable. Thesoftmaxfunctionthusprovidesa“softened”versionofthe\nargmax.Thecorrespondingsoftversionofthemaximumfunctionissoftmax(z)z.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 450, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 913}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0451_93b5e70c", "text": "Thesoftmaxfunctionthusprovidesa“softened”versionofthe\nargmax.Thecorrespondingsoftversionofthemaximumfunctionissoftmax(z)z. Itwouldperhapsbebettertocallthesoftmaxfunction“softargmax,” butthe\ncurrentnameisanentrenchedconvention. 6.2.2.4OtherOutputTypes\nThelinear, sigmoid, andsoftmaxoutputunitsdescribedabovearethemost\ncommon.Neuralnetworkscangeneralizetoalmostanykindofoutputlayerthat\nwewish.Theprincipleofmaximumlikelihoodprovidesaguideforhowtodesign\n1 8 7\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nagoodcostfunctionfornearlyanykindofoutputlayer. Ingeneral,ifwedeﬁneaconditionaldistribution p(yx|;θ),theprincipleof\nmaximumlikelihoodsuggestsweuse asourcostfunction. − | log( pyxθ;)\nIngeneral,wecanthinkoftheneuralnetworkasrepresentingafunction f(x;θ). Theoutputsofthisfunctionarenotdirectpredictionsofthevaluey.Instead,\nf(x;θ) =ωprovidestheparametersforadistributionover y.Ourlossfunction\ncanthenbeinterpretedas .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 451, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 904}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0452_91dba44b", "text": "Theoutputsofthisfunctionarenotdirectpredictionsofthevaluey.Instead,\nf(x;θ) =ωprovidestheparametersforadistributionover y.Ourlossfunction\ncanthenbeinterpretedas . −log(;()) p yωx\nForexample,wemaywishtolearnthevarianceofaconditionalGaussianfor y,\ngiven x.Inthesimplecase,wherethevariance σ2isaconstant,thereisaclosed\nformexpressionbecausethemaximumlikelihoodestimatorofvarianceissimplythe\nempiricalmeanofthesquareddiﬀerencebetweenobservations yandtheirexpected\nvalue.Acomputationally moreexpensiveapproachthatdoesnotrequirewriting\nspecial-casecodeistosimplyincludethevarianceasoneofthepropertiesofthe\ndistribution p( y|x)thatiscontrolledbyω= f(x;θ).Thenegativelog-likelihood\n−log p(y;ω(x))willthenprovideacostfunctionwiththeappropriateterms\nnecessarytomakeouroptimization procedureincrementally learnthevariance.In\nthesimplecasewherethestandarddeviationdoesnotdependontheinput,we\ncanmakeanewparameterinthenetworkthatiscopieddirectlyintoω.Thisnew\nparametermightbe σitselforcouldbeaparameter vrepresenting σ2oritcould\nbeaparameter βrepresenting1\nσ2,dependingonhowwechoosetoparametrize\nthedistribution.Wemaywishourmodeltopredictadiﬀerentamountofvariance\nin yfordiﬀerentvaluesof x.Thisiscalledaheteroscedasticmodel.Inthe\nheteroscedasticcase,wesimplymakethespeciﬁcationofthevariancebeoneof\nthevaluesoutputby f( x;θ).AtypicalwaytodothisistoformulatetheGaussian\ndistributionusingprecision,ratherthanvariance,asdescribedinequation.3.22\nInthemultivariatecaseitismostcommontouseadiagonalprecisionmatrix\ndiag (6.34) ()β .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 452, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1508}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0453_e3f2cdfb", "text": "Thisformulationworkswellwithgradientdescentbecausetheformulaforthe\nlog-likelihoodoftheGaussiandistributionparametrized byβinvolvesonlymul-\ntiplicationby β iandadditionoflogβ i.Thegradientofmultiplication, addition,\nandlogarithmoperationsiswell-behaved.Bycomparison,ifweparametrized the\noutputintermsofvariance,wewouldneedtousedivision.Thedivisionfunction\nbecomesarbitrarilysteepnearzero.Whilelargegradientscanhelplearning,\narbitrarilylargegradientsusuallyresultininstability.Ifweparametrized the\noutputintermsofstandarddeviation,thelog-likelihoodwouldstillinvolvedivision,\nandwouldalsoinvolvesquaring.Thegradientthroughthesquaringoperation\ncanvanishnearzero,makingitdiﬃculttolearnparametersthataresquared. 1 8 8\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nRegardlessofwhetherweusestandarddeviation,variance,orprecision,wemust\nensurethatthecovariancematrixoftheGaussianispositivedeﬁnite.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 453, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 875}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0454_3221d130", "text": "1 8 8\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nRegardlessofwhetherweusestandarddeviation,variance,orprecision,wemust\nensurethatthecovariancematrixoftheGaussianispositivedeﬁnite. Because\ntheeigenvaluesoftheprecisionmatrixarethereciprocalsoftheeigenvaluesof\nthecovariancematrix,thisisequivalenttoensuringthattheprecisionmatrixis\npositivedeﬁnite.Ifweuseadiagonalmatrix,orascalartimesthediagonalmatrix,\nthentheonlyconditionweneedtoenforceontheoutputofthemodelispositivity. Ifwesupposethataistherawactivationofthemodelusedtodeterminethe\ndiagonalprecision,wecanusethesoftplusfunctiontoobtainapositiveprecision\nvector:β= ζ(a) .Thissamestrategyappliesequallyifusingvarianceorstandard\ndeviationratherthanprecisionorifusingascalartimesidentityratherthan\ndiagonalmatrix. Itisraretolearnacovarianceorprecisionmatrixwithricherstructurethan\ndiagonal. Ifthecovarianceisfullandconditional,thenaparametrization must\nbechosenthatguaranteespositive-deﬁnitenessofthepredictedcovariancematrix.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 454, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 964}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0455_8233b09d", "text": "Ifthecovarianceisfullandconditional,thenaparametrization must\nbechosenthatguaranteespositive-deﬁnitenessofthepredictedcovariancematrix. Thiscanbeachievedbywriting Σ() = ()xBxB()x,whereBisanunconstrained\nsquarematrix.Onepracticalissueifthematrixisfullrankisthatcomputingthe\nlikelihoodisexpensive,witha d d×matrixrequiring O( d3)computationforthe\ndeterminantandinverseof Σ(x)(orequivalently,andmorecommonlydone,its\neigendecompositionorthatof).Bx()\nWeoftenwanttoperformmultimodalregression,thatis,topredictrealvalues\nthatcomefromaconditionaldistribution p(yx|)thatcanhaveseveraldiﬀerent\npeaksinyspaceforthesamevalueofx.Inthiscase,aGaussianmixtureis\nanaturalrepresentationfortheoutput( ,;,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 455, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 688}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0456_56325f8c", "text": "Jacobs e t a l .1991Bishop1994\nNeuralnetworkswithGaussianmixturesastheiroutputareoftencalledmixture\ndensitynetworks.AGaussianmixtureoutputwith ncomponentsisdeﬁnedby\ntheconditionalprobabilitydistribution\np( ) =yx|n\ni = 1p i (= c |Nx)(;yµ( ) i()x , Σ( ) i())x .(6.35)\nTheneuralnetworkmusthavethreeoutputs:avectordeﬁning p(c= i|x),a\nmatrixprovidingµ( ) i(x)forall i,andatensorproviding Σ( ) i(x)forall i.These\noutputsmustsatisfydiﬀerentconstraints:\n1.Mixturecomponents p(c= i|x):theseformamultinoullidistribution\noverthe ndiﬀerentcomponentsassociatedwithlatentvariable1c,andcan\n1W e c o n s i d e r c t o b e l a t e n t b e c a u s e we d o n o t o b s e rv e i t i n t h e d a t a : g i v e n i n p u t x a n d t a rg e t\ny , i t i s n o t p o s s i b l e t o k n o w with c e rta i n t y wh i c h Ga u s s i a n c o m p o n e n t wa s re s p o n s i b l e f o r y , b u t\nw e c a n i m a g i n e t h a t y w a s g e n e ra t e d b y p i c k i n g o n e o f t h e m , a n d m a k e t h a t u n o b s e rv e d c h o i c e a\nra n d o m v a ria b l e .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 456, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1049}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0457_43883acb", "text": "1 8 9\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\ntypicallybeobtainedbyasoftmaxoveran n-dimensionalvector,toguarantee\nthattheseoutputsarepositiveandsumto1. 2.Meansµ( ) i(x):theseindicatethecenterormeanassociatedwiththe i-th\nGaussiancomponent,andareunconstrained(typicallywithnononlinearity\natallfortheseoutputunits).If yisa d-vector,thenthenetworkmustoutput\nan n d×matrixcontainingall nofthese d-dimensionalvectors. Learning\nthesemeanswithmaximumlikelihoodisslightlymorecomplicatedthan\nlearningthemeansofadistributionwithonlyoneoutputmode.Weonly\nwanttoupdatethemeanforthecomponentthatactuallyproducedthe\nobservation.Inpractice,wedonotknowwhichcomponentproducedeach\nobservation.Theexpressionforthenegativelog-likelihoodnaturallyweights\neachexample’scontributiontothelossforeachcomponentbytheprobability\nthatthecomponentproducedtheexample.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 457, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 826}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0458_63ac555d", "text": "3.Covariances Σ( ) i(x):thesespecifythecovariancematrixforeachcomponent\ni.AswhenlearningasingleGaussiancomponent,wetypicallyuseadiagonal\nmatrixtoavoidneedingtocomputedeterminants. Aswithlearningthemeans\nofthemixture,maximumlikelihoodiscomplicatedbyneedingtoassign\npartialresponsibilityforeachpointtoeachmixturecomponent.Gradient\ndescentwillautomatically followthecorrectprocessifgiventhecorrect\nspeciﬁcationofthenegativelog-likelihoodunderthemixturemodel. Ithasbeenreportedthatgradient-basedoptimization ofconditionalGaussian\nmixtures(ontheoutputofneuralnetworks)canbeunreliable,inpartbecauseone\ngetsdivisions(bythevariance)whichcanbenumericallyunstable(whensome\nvariancegetstobesmallforaparticularexample,yieldingverylargegradients). Onesolutionistoclipgradients(seesection)whileanotheristoscale 10.11.1\nthegradientsheuristically( ,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 458, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 835}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0459_95901dd2", "text": "Onesolutionistoclipgradients(seesection)whileanotheristoscale 10.11.1\nthegradientsheuristically( ,). MurrayandLarochelle2014\nGaussianmixtureoutputsareparticularlyeﬀectiveingenerativemodelsof\nspeech(Schuster1999,)ormovementsofphysicalobjects(Graves2013,).The\nmixturedensitystrategygivesawayforthenetworktorepresentmultipleoutput\nmodesandtocontrolthevarianceofitsoutput,whichiscrucialforobtaining\nahighdegreeofqualityinthesereal-valueddomains.Anexampleofamixture\ndensitynetworkisshowninﬁgure.6.4\nIngeneral,wemaywishtocontinuetomodellargervectorsycontainingmore\nvariables,andtoimposericherandricherstructuresontheseoutputvariables.For\nexample,wemaywishforourneuralnetworktooutputasequenceofcharacters\nthatformsasentence.Inthese cases,wemaycontinuetousetheprinciple\nofmaximumlikelihoodappliedtoourmodel p(y;ω(x)),butthemodelweuse\n1 9 0\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nxy\nFigure6.4:Samplesdrawnfromaneuralnetworkwithamixturedensityoutputlayer.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 459, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 939}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0460_2c290b8f", "text": "Theinput xissampledfromauniformdistributionandtheoutput yissampledfrom\np m o d e l( y x|).Theneuralnetworkisabletolearnnonlinearmappingsfromtheinputto\ntheparametersoftheoutputdistribution.Theseparametersincludetheprobabilities\ngoverningwhichofthreemixturecomponentswillgeneratetheoutputaswellasthe\nparametersforeachmixturecomponent.EachmixturecomponentisGaussianwith\npredictedmeanandvariance.Alloftheseaspectsoftheoutputdistributionareableto\nvarywithrespecttotheinput,andtodosoinnonlinearways. x\ntodescribeybecomescomplexenoughtobebeyondthescopeofthischapter. Chapterdescribeshowtouserecurrentneuralnetworkstodeﬁnesuchmodels 10\noversequences,andpartdescribesadvancedtechniquesformodelingarbitrary III\nprobabilitydistributions. 6. 3 Hi d d en Un i t s\nSofarwehavefocusedourdiscussionondesignchoicesforneuralnetworksthat\narecommontomostparametricmachinelearningmodelstrainedwithgradient-\nbasedoptimization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 460, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 904}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0461_72db79f5", "text": "6. 3 Hi d d en Un i t s\nSofarwehavefocusedourdiscussionondesignchoicesforneuralnetworksthat\narecommontomostparametricmachinelearningmodelstrainedwithgradient-\nbasedoptimization. Nowweturntoanissuethatisuniquetofeedforwardneural\nnetworks:howtochoosethetypeofhiddenunittouseinthehiddenlayersofthe\nmodel. Thedesignofhiddenunitsisanextremelyactiveareaofresearchanddoesnot\nyethavemanydeﬁnitiveguidingtheoreticalprinciples. Rectiﬁedlinearunitsareanexcellentdefaultchoiceofhiddenunit.Manyother\ntypesofhiddenunitsareavailable.Itcanbediﬃculttodeterminewhentouse\nwhichkind(thoughrectiﬁedlinearunitsareusuallyanacceptablechoice). We\n1 9 1\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\ndescribeheresomeofthebasicintuitionsmotivatingeachtypeofhiddenunits.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 461, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 729}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0462_1165ccae", "text": "We\n1 9 1\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\ndescribeheresomeofthebasicintuitionsmotivatingeachtypeofhiddenunits. Theseintuitionscanhelpdecidewhentotryouteachoftheseunits.Itisusually\nimpossibletopredictinadvancewhichwillworkbest.Thedesignprocessconsists\noftrialanderror,intuitingthatakindofhiddenunitmayworkwell,andthen\ntraininganetworkwiththatkindofhiddenunitandevaluatingitsperformance\nonavalidationset. Someofthehiddenunitsincludedinthislistarenotactuallydiﬀerentiableat\nallinputpoints.Forexample,therectiﬁedlinearfunction g( z) =max{0 , z}isnot\ndiﬀerentiableat z= 0.Thismayseemlikeitinvalidates gforusewithagradient-\nbasedlearningalgorithm.Inpractice,gradientdescentstillperformswellenough\nforthesemodelstobeusedformachinelearningtasks.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 462, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 737}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0463_00bdc729", "text": "Thisisinpartbecause\nneuralnetworktrainingalgorithmsdonotusuallyarriveatalocalminimumof\nthecostfunction,butinsteadmerelyreduceitsvaluesigniﬁcantly,asshownin\nﬁgure.Theseideaswillbedescribedfurtherinchapter.Becausewedonot 4.3 8\nexpecttrainingtoactuallyreachapointwherethegradientis 0,itisacceptable\nfortheminimaofthecostfunctiontocorrespondtopointswithundeﬁnedgradient.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 463, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 366}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0464_286fffcb", "text": "Hiddenunitsthatarenotdiﬀerentiableareusuallynon-diﬀerentiable atonlya\nsmallnumberofpoints.Ingeneral,afunction g( z)hasaleftderivativedeﬁned\nbytheslopeofthefunctionimmediately totheleftof zandarightderivative\ndeﬁnedbytheslopeofthefunctionimmediately totherightof z.Afunction\nisdiﬀerentiableat zonlyifboththeleftderivativeandtherightderivativeare\ndeﬁnedandequaltoeachother.Thefunctionsusedinthecontextofneural\nnetworksusuallyhavedeﬁnedleftderivativesanddeﬁnedrightderivatives.Inthe\ncaseof g( z) =max{0 , z},theleftderivativeat z= 00isandtherightderivative\nis.Softwareimplementations ofneuralnetworktrainingusuallyreturnoneof 1\ntheone-sidedderivativesratherthanreportingthatthederivativeisundeﬁnedor\nraisinganerror. Thismaybeheuristicallyjustiﬁedbyobservingthatgradient-\nbasedoptimization onadigitalcomputerissubjecttonumericalerroranyway.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 464, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 836}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0465_a7257aad", "text": "Thismaybeheuristicallyjustiﬁedbyobservingthatgradient-\nbasedoptimization onadigitalcomputerissubjecttonumericalerroranyway. Whenafunctionisaskedtoevaluate g(0),itisveryunlikelythattheunderlying\nvaluetrulywas.Instead,itwaslikelytobesomesmallvalue 0 thatwasrounded\nto.Insomecontexts,moretheoreticallypleasingjustiﬁcationsareavailable,but 0\ntheseusuallydonotapplytoneuralnetworktraining.Theimportantpointisthat\ninpracticeonecansafelydisregardthenon-diﬀerentiabilityofthehiddenunit\nactivationfunctionsdescribedbelow. Unlessindicatedotherwise,mosthiddenunitscanbedescribedasaccepting\navectorofinputsx,computinganaﬃnetransformationz=Wx+b,and\nthenapplyinganelement-wisenonlinearfunction g(z).Mosthiddenunitsare\ndistinguishedfromeachotheronlybythechoiceoftheformoftheactivation\nfunction. g()z\n1 9 2\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.3.1RectiﬁedLinearUnitsandTheirGeneralizations\nRectiﬁedlinearunitsusetheactivationfunction .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 465, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 919}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0466_52dbf8a9", "text": "g()z\n1 9 2\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.3.1RectiﬁedLinearUnitsandTheirGeneralizations\nRectiﬁedlinearunitsusetheactivationfunction . g z , z () = max0{}\nRectiﬁedlinearunitsareeasytooptimizebecausetheyaresosimilartolinear\nunits.Theonlydiﬀerencebetweenalinearunitandarectiﬁedlinearunitis\nthatarectiﬁedlinearunitoutputszeroacrosshalfitsdomain. This makesthe\nderivativesthrougharectiﬁedlinearunitremainlargewhenevertheunitisactive. Thegradientsarenotonlylargebutalsoconsistent.Thesecondderivativeofthe\nrectifyingoperationisalmosteverywhere,andthederivativeoftherectifying 0\noperationiseverywherethattheunitisactive.Thismeansthatthegradient 1\ndirectionisfarmoreusefulforlearningthanitwouldbewithactivationfunctions\nthatintroducesecond-ordereﬀects. Rectiﬁedlinearunitsaretypicallyusedontopofanaﬃnetransformation:\nhW= ( gxb+) .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 466, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 826}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0467_0aff6f38", "text": "Rectiﬁedlinearunitsaretypicallyusedontopofanaﬃnetransformation:\nhW= ( gxb+) . (6.36)\nWheninitializingtheparametersoftheaﬃnetransformation,itcanbeagood\npracticetosetallelementsofbtoasmall,positivevalue,suchas0 .1.Thismakes\nitverylikelythattherectiﬁedlinearunitswillbeinitiallyactiveformostinputs\ninthetrainingsetandallowthederivativestopassthrough. Severalgeneralizations ofrectiﬁedlinearunitsexist.Mostofthesegeneral-\nizationsperformcomparablytorectiﬁedlinearunitsandoccasionallyperform\nbetter. Onedrawbacktorectiﬁedlinearunitsisthattheycannotlearnviagradient-\nbased methods onexamples for which their activ ation iszero.Avariety of\ngeneralizations ofrectiﬁedlinearunitsguaranteethattheyreceivegradientevery-\nwhere.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 467, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 716}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0468_712af0fa", "text": "Threegeneralizations ofrectiﬁedlinearunitsarebasedonusinganon-zero\nslope α iwhen z i <0: h i= g(zα ,) i=max(0 , z i)+ α imin(0 , z i).Absolutevalue\nrectiﬁcationﬁxes α i=−1toobtain g( z) =|| z.Itisusedforobjectrecognition\nfromimages( ,),whereitmakessensetoseekfeaturesthatare Jarrett e t a l .2009\ninvariantunderapolarityreversaloftheinputillumination. Othergeneralizations\nofrectiﬁedlinearunitsaremorebroadlyapplicable.AleakyReLU(,Maas e t a l . 2013)ﬁxes α itoasmallvaluelike0.01whileaparametricReLUorPReLU\ntreats α iasalearnableparameter(,). He e t a l .2015\nMaxoutunits( ,)generalizerectiﬁedlinearunits Goodfellow e t a l .2013a\nfurther.Insteadofapplyinganelement-wisefunction g( z),maxoutunitsdividez\nintogroupsof kvalues.Eachmaxoutunitthenoutputsthemaximumelementof\n1 9 3\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\noneofthesegroups:\ng()z i=max\nj∈ G() iz j (6.37)\nwhere G( ) iisthesetofindicesintotheinputsforgroup i,{( i−1) k+1 , . . . , i k}.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 468, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 938}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0469_4f61bd10", "text": ". . , i k}. Thisprovidesawayoflearningapiecewiselinearfunctionthatrespondstomultiple\ndirectionsintheinputspace.x\nAmaxoutunitcanlearnapiecewiselinear,convexfunctionwithupto kpieces. Maxoutunitscanthusbeseenas l e a r ning t h e a c t i v a t i o n f u nc t i o nitselfrather\nthanjusttherelationshipbetweenunits.Withlargeenough k,amaxoutunitcan\nlearntoapproximateanyconvexfunctionwitharbitraryﬁdelity.Inparticular,\namaxoutlayerwithtwopiecescanlearntoimplementthesamefunctionofthe\ninputxasatraditionallayerusingtherectiﬁedlinearactivationfunction,absolute\nvaluerectiﬁcationfunction,ortheleakyorparametricReLU,orcanlearnto\nimplementatotallydiﬀerentfunctionaltogether.Themaxoutlayerwillofcourse\nbeparametrized diﬀerentlyfromanyoftheseotherlayertypes,sothelearning\ndynamicswillbediﬀerenteveninthecaseswheremaxoutlearnstoimplementthe\nsamefunctionofasoneoftheotherlayertypes.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 469, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 867}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0470_28db6f13", "text": "x\nEachmaxoutunitisnowparametrized by kweightvectorsinsteadofjustone,\nsomaxoutunitstypicallyneedmoreregularizationthanrectiﬁedlinearunits.They\ncanworkwellwithoutregularizationifthetrainingsetislargeandthenumberof\npiecesperunitiskeptlow(,). Cai e t a l .2013\nMaxoutunitshaveafewotherbeneﬁts.Insomecases,onecangainsomesta-\ntisticalandcomputational advantagesbyrequiringfewerparameters.Speciﬁcally,\nifthefeaturescapturedby ndiﬀerentlinearﬁlterscanbesummarizedwithout\nlosinginformationbytakingthemaxovereachgroupof kfeatures,thenthenext\nlayercangetbywithtimesfewerweights. k\nBecauseeachunitisdrivenbymultipleﬁlters,maxoutunitshavesomeredun-\ndancythathelpsthemtoresistaphenomenon calledcatastrophicforgetting\ninwhichneuralnetworksforgethowtoperformtasksthattheyweretrainedonin\nthepast( ,). Goodfellow e t a l .2014a\nRectiﬁedlinearunitsandallofthesegeneralizations ofthemarebasedonthe\nprinciplethatmodelsareeasiertooptimizeiftheirbehaviorisclosertolinear.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 470, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 948}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0471_c77ae9cb", "text": "Goodfellow e t a l .2014a\nRectiﬁedlinearunitsandallofthesegeneralizations ofthemarebasedonthe\nprinciplethatmodelsareeasiertooptimizeiftheirbehaviorisclosertolinear. Thissamegeneralprincipleofusinglinearbehaviortoobtaineasieroptimization\nalsoappliesinothercontextsbesidesdeeplinearnetworks.Recurrentnetworkscan\nlearnfromsequencesandproduceasequenceofstatesandoutputs.Whentraining\nthem,oneneedstopropagateinformationthroughseveraltimesteps,whichismuch\neasierwhensomelinearcomputations (withsomedirectionalderivativesbeingof\nmagnitudenear1)areinvolved.Oneofthebest-performingrecurrentnetwork\n1 9 4\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\narchitectures,theLSTM,propagatesinformationthroughtimeviasummation—a\nparticularstraightforwardkindofsuchlinearactivation.Thisisdiscussedfurther\ninsection.10.10\n6.3.2LogisticSigmoidandHyperbolicTangent\nPriortotheintroduction ofrectiﬁedlinearunits,mostneuralnetworksusedthe\nlogisticsigmoidactivationfunction\ng z σ z () = () (6.38)\northehyperbolictangentactivationfunction\ng z z .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 471, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1005}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0472_949f9642", "text": "() = tanh( ) (6.39)\nTheseactivationfunctionsarecloselyrelatedbecause . tanh( ) = 2(2)1 z σ z−\nWe havealready seen sigmoid unitsasoutput units, usedto predictthe\nprobabilitythatabinaryvariableis.Unlikepiecewiselinearunits,sigmoidal 1\nunitssaturateacrossmostoftheirdomain—they saturatetoahighvaluewhen\nzisverypositive,saturatetoalowvaluewhen zisverynegative,andareonly\nstronglysensitivetotheirinputwhen zisnear0.Thewidespreadsaturationof\nsigmoidalunitscanmakegradient-basedlearningverydiﬃcult.Forthisreason,\ntheiruseashiddenunitsinfeedforwardnetworksisnowdiscouraged.Theiruse\nasoutputunitsiscompatiblewiththeuseofgradient-basedlearningwhenan\nappropriatecostfunctioncanundothesaturationofthesigmoidintheoutput\nlayer. Whenasigmoidalactivationfunctionmustbeused,thehyperbolictangent\nactivationfunctiontypicallyperformsbetterthanthelogisticsigmoid.Itresembles\ntheidentityfunctionmoreclosely,inthesensethattanh(0) = 0while σ(0) =1\n2.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 472, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 926}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0473_94102d3d", "text": "Becausetanhissimilartotheidentityfunctionnear,trainingadeepneural 0\nnetworkˆ y=wtanh(Utanh(Vx))resemblestrainingalinearmodelˆ y=\nwUVxsolongastheactivationsofthenetworkcanbekeptsmall.This\nmakestrainingthenetworkeasier. tanh\nSigmoidalactivationfunctionsaremorecommoninsettingsotherthanfeed-\nforwardnetworks.Recurrentnetworks,manyprobabilisticmodels,andsome\nautoencodershaveadditionalrequirementsthatruleouttheuseofpiecewise\nlinearactivationfunctionsandmakesigmoidalunitsmoreappealingdespitethe\ndrawbacksofsaturation. 1 9 5\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.3.3OtherHiddenUnits\nManyothertypesofhiddenunitsarepossible,butareusedlessfrequently. Ingeneral,awidevarietyofdiﬀerentiable functionsperformperfectlywell. Manyunpublishedactivationfunctionsperformjustaswellasthepopularones.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 473, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 784}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0474_a655eb8d", "text": "Ingeneral,awidevarietyofdiﬀerentiable functionsperformperfectlywell. Manyunpublishedactivationfunctionsperformjustaswellasthepopularones. Toprovideaconcreteexample,theauthorstestedafeedforwardnetworkusing\nh=cos(Wx+b)ontheMNISTdatasetandobtainedanerrorrateoflessthan\n1%,whichiscompetitivewithresultsobtainedusingmoreconventionalactivation\nfunctions.Duringresearchanddevelopmentofnewtechniques,itiscommon\ntotestmanydiﬀerentactivationfunctionsandﬁndthatseveralvariationson\nstandardpracticeperformcomparably.Thismeansthatusuallynewhiddenunit\ntypesarepublishedonlyiftheyareclearlydemonstratedtoprovideasigniﬁcant\nimprovement.Newhiddenunittypesthatperformroughlycomparablytoknown\ntypesaresocommonastobeuninteresting. Itwouldbeimpracticaltolistallofthehiddenunittypesthathaveappeared\nintheliterature.Wehighlightafewespeciallyusefulanddistinctiveones.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 474, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 843}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0475_cbe4bb44", "text": "Itwouldbeimpracticaltolistallofthehiddenunittypesthathaveappeared\nintheliterature.Wehighlightafewespeciallyusefulanddistinctiveones. Onepossibilityistonothaveanactivation g( z)atall.Onecanalsothinkof\nthisasusingtheidentityfunctionastheactivationfunction.Wehavealready\nseenthatalinearunitcanbeusefulastheoutputofaneuralnetwork.Itmay\nalsobeusedasahiddenunit.Ifeverylayeroftheneuralnetworkconsistsofonly\nlineartransformations,thenthenetworkasawholewillbelinear.However,it\nisacceptableforsomelayersoftheneuralnetworktobepurelylinear.Consider\naneuralnetworklayerwith ninputsand poutputs,h= g(Wx+b).Wemay\nreplacethiswithtwolayers,withonelayerusingweightmatrixUandtheother\nusingweightmatrixV.Iftheﬁrstlayerhasnoactivationfunction,thenwehave\nessentiallyfactoredtheweightmatrixoftheoriginallayerbasedonW.The\nfactoredapproachistocomputeh= g(VUx+b).IfUproduces qoutputs,\nthenUandVtogethercontainonly ( n+ p) qparameters,whileWcontains n p\nparameters.Forsmall q,thiscanbeaconsiderablesavinginparameters.It\ncomesatthecostofconstrainingthelineartransformationtobelow-rank,but\ntheselow-rankrelationshipsareoftensuﬃcient.Linearhiddenunitsthusoﬀeran\neﬀectivewayofreducingthenumberofparametersinanetwork.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 475, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1189}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0476_fb8485af", "text": "Softmaxunitsareanotherkindofunitthatisusuallyusedasanoutput(as\ndescribedinsection)butmaysometimesbeusedasahiddenunit.Softmax 6.2.2.3\nunitsnaturallyrepresentaprobabilitydistributionoveradiscretevariablewith k\npossiblevalues,sotheymaybeusedasakindofswitch.Thesekindsofhidden\nunitsareusuallyonlyusedinmoreadvancedarchitectures thatexplicitlylearnto\nmanipulatememory,describedinsection.10.12\n1 9 6\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nAfewotherreasonablycommonhiddenunittypesinclude:\n•RadialbasisfunctionorRBFunit: h i=exp\n−1\nσ2\ni||W : , i−||x2\n.This\nfunctionbecomesmoreactiveasxapproachesatemplateW : , i.Becauseit\nsaturatestoformost,itcanbediﬃculttooptimize. 0x\n•Softplus: g( a) = ζ( a) =log(1+ ea).Thisisasmoothversionoftherectiﬁer,\nintroducedby ()forfunctionapproximationandby Dugas e t a l .2001 Nair\nandHinton2010()fortheconditionaldistributionsofundirectedprobabilistic\nmodels.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 476, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 878}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0477_fbdfeb28", "text": "()comparedthesoftplusandrectiﬁerandfound Glorot e t a l .2011a\nbetterresultswiththelatter.Theuseofthesoftplusisgenerallydiscouraged. Thesoftplusdemonstratesthattheperformanceofhiddenunittypescan\nbeverycounterintuitive—onemightexpectittohaveanadvantageover\ntherectiﬁerduetobeingdiﬀerentiableeverywhereorduetosaturatingless\ncompletely,butempiricallyitdoesnot. •Hardtanh:thisisshapedsimilarlytothetanhandtherectiﬁerbutunlike\nthelatter,itisbounded, g( a)=max(−1 ,min(1 , a)).Itwasintroduced\nby(). Collobert2004\nHiddenunitdesignremainsanactiveareaofresearchandmanyusefulhidden\nunittypesremaintobediscovered. 6. 4 A rc h i t ec t u re D es i gn\nAnotherkeydesignconsiderationforneuralnetworksisdeterminingthearchitecture. Thewordarchitecturereferstotheoverallstructureofthenetwork:howmany\nunitsitshouldhaveandhowtheseunitsshouldbeconnectedtoeachother. Mostneuralnetworksareorganizedintogroupsofunitscalledlayers.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 477, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 905}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0478_ce10e34d", "text": "Thewordarchitecturereferstotheoverallstructureofthenetwork:howmany\nunitsitshouldhaveandhowtheseunitsshouldbeconnectedtoeachother. Mostneuralnetworksareorganizedintogroupsofunitscalledlayers. Most\nneuralnetworkarchitectures arrangetheselayersinachainstructure,witheach\nlayerbeingafunctionofthelayerthatprecededit.Inthisstructure,theﬁrstlayer\nisgivenby\nh( 1 )= g( 1 )\nW( 1 )xb+( 1 )\n, (6.40)\nthesecondlayerisgivenby\nh( 2 )= g( 2 )\nW( 2 )h( 1 )+b( 2 )\n, (6.41)\nandsoon. 1 9 7\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nInthesechain-basedarchitectures,themainarchitecturalconsiderationsare\ntochoosethedepthofthenetworkandthewidthofeachlayer.Aswewillsee,\nanetworkwithevenonehiddenlayerissuﬃcienttoﬁtthetrainingset.Deeper\nnetworksoftenareabletousefarfewerunitsperlayerandfarfewerparameters\nandoftengeneralizetothetestset,butarealsooftenhardertooptimize. The\nidealnetworkarchitectureforataskmustbefoundviaexperimentationguidedby\nmonitoringthevalidationseterror.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 478, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 950}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0479_4c009a26", "text": "The\nidealnetworkarchitectureforataskmustbefoundviaexperimentationguidedby\nmonitoringthevalidationseterror. 6.4.1UniversalApproximationPropertiesandDepth\nAlinearmodel,mappingfromfeaturestooutputsviamatrixmultiplication, can\nbydeﬁnitionrepresentonlylinearfunctions.Ithastheadvantageofbeingeasyto\ntrainbecausemanylossfunctionsresultinconvexoptimization problemswhen\nappliedtolinearmodels.Unfortunately,weoftenwanttolearnnonlinearfunctions. Atﬁrstglance,wemightpresumethatlearninganonlinearfunctionrequires\ndesigningaspecializedmodelfamilyforthekindofnonlinearitywewanttolearn.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 479, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 573}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0480_bbdc9c15", "text": "Atﬁrstglance,wemightpresumethatlearninganonlinearfunctionrequires\ndesigningaspecializedmodelfamilyforthekindofnonlinearitywewanttolearn. Fortunately,feedforwardnetworkswithhiddenlayersprovideauniversalapproxi-\nmationframework.Speciﬁcally,theuniversalapproximationtheorem(Hornik\ne t a l .,;,)statesthatafeedforwardnetworkwithalinearoutput 1989Cybenko1989\nlayerandatleastonehiddenlayerwithany“squashing”activationfunction(such\nasthelogisticsigmoidactivationfunction)canapproximateanyBorelmeasurable\nfunctionfromoneﬁnite-dimensional spacetoanotherwithanydesirednon-zero\namountoferror,providedthatthenetworkisgivenenoughhiddenunits.The\nderivativesofthefeedforwardnetworkcanalsoapproximate thederivativesofthe\nfunctionarbitrarilywell( ,).TheconceptofBorelmeasurability Hornik e t a l .1990\nisbeyondthescopeofthisbook; forourpurposesitsuﬃcestosaythatany\ncontinuousfunctiononaclosedandboundedsubsetof RnisBorelmeasurable\nandthereforemaybeapproximatedbyaneuralnetwork.Aneuralnetworkmay\nalsoapproximateanyfunctionmappingfromanyﬁnitedimensionaldiscretespace\ntoanother.Whiletheoriginaltheoremswereﬁrststatedintermsofunitswith\nactivationfunctionsthatsaturatebothforverynegativeandforverypositive\narguments,universalapproximation theoremshavealsobeenprovedforawider\nclassofactivationfunctions,whichincludesthenowcommonlyusedrectiﬁedlinear\nunit( ,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 480, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1335}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0481_cc7f37f9", "text": "Leshno e t a l .1993\nTheuniversalapproximationtheoremmeansthatregardlessofwhatfunction\nwearetryingtolearn,weknowthatalargeMLPwillbeableto r e p r e s e ntthis\nfunction.However,wearenotguaranteedthatthetrainingalgorithmwillbeable\nto l e a r nthatfunction.EveniftheMLPisabletorepresentthefunction,learning\ncanfailfortwodiﬀerentreasons.First,theoptimizationalgorithmusedfortraining\n1 9 8\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nmaynotbeabletoﬁndthevalueoftheparametersthatcorrespondstothedesired\nfunction.Second,thetrainingalgorithmmightchoosethewrongfunctiondueto\noverﬁtting.Recallfromsectionthatthe“nofreelunch”theoremshowsthat 5.2.1\nthereisnouniversallysuperiormachinelearningalgorithm.Feedforwardnetworks\nprovideauniversalsystemforrepresentingfunctions,inthesensethat,givena\nfunction,thereexistsafeedforwardnetworkthatapproximatesthefunction.There\nisnouniversalprocedureforexaminingatrainingsetofspeciﬁcexamplesand\nchoosingafunctionthatwillgeneralizetopointsnotinthetrainingset.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 481, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 972}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0482_a9a510bd", "text": "Theuniversalapproximationtheoremsaysthatthereexistsanetworklarge\nenoughtoachieveanydegreeofaccuracywedesire,butthetheoremdoesnot\nsayhowlargethisnetworkwillbe.()providessomeboundsonthe Barron1993\nsizeofasingle-layernetworkneededtoapproximate abroadclassoffunctions. Unfortunately,intheworsecase,anexponentialnumberofhiddenunits(possibly\nwithonehiddenunitcorrespondingtoeachinputconﬁgurationthatneedstobe\ndistinguished)mayberequired.Thisiseasiesttoseeinthebinarycase:the\nnumberofpossiblebinaryfunctionsonvectorsv∈{0 ,1}nis22nandselecting\nonesuchfunctionrequires 2nbits,whichwillingeneralrequire O(2n)degreesof\nfreedom. Insummary,afeedforwardnetworkwithasinglelayerissuﬃcienttorepresent\nanyfunction,butthelayermaybeinfeasiblylargeandmayfailtolearnand\ngeneralizecorrectly.Inmanycircumstances,usingdeepermodelscanreducethe\nnumberofunitsrequiredtorepresentthedesiredfunctionandcanreducethe\namountofgeneralization error.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 482, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 913}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0483_963ab2f8", "text": "Thereexistfamiliesoffunctionswhichcanbeapproximated eﬃcientlybyan\narchitecturewithdepthgreaterthansomevalue d,butwhichrequireamuchlarger\nmodelifdepthisrestrictedtobelessthanorequalto d.Inmanycases,thenumber\nofhiddenunitsrequiredbytheshallowmodelisexponentialin n. Suchresults\nwereﬁrstprovedformodelsthatdonotresemblethecontinuous,diﬀerentiable\nneuralnetworksusedformachinelearning,buthavesincebeenextendedtothese\nmodels.Theﬁrstresultswereforcircuitsoflogicgates(,).Later Håstad1986\nworkextendedtheseresultstolinearthresholdunitswithnon-negativeweights\n( ,; ,),andthentonetworkswith HåstadandGoldmann1991Hajnal e t a l .1993\ncontinuous-valuedactivations(,; ,). Manymodern Maass1992Maass e t a l .1994\nneuralnetworksuserectiﬁedlinearunits.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 483, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 737}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0484_8ee29d38", "text": "Manymodern Maass1992Maass e t a l .1994\nneuralnetworksuserectiﬁedlinearunits. ()demonstrated Leshno e t a l .1993\nthatshallownetworkswithabroadfamilyofnon-polynomialactivationfunctions,\nincludingrectiﬁedlinearunits,haveuniversalapproximation properties,butthese\nresultsdonotaddressthequestionsofdepthoreﬃciency—theyspecifyonlythat\nasuﬃcientlywiderectiﬁernetworkcouldrepresentanyfunction.Montufar e t a l . 1 9 9\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\n()showedthatfunctionsrepresentablewithadeeprectiﬁernetcanrequire 2014\nanexponentialnumberofhiddenunitswithashallow(onehiddenlayer)network.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 484, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 583}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0485_e529537a", "text": "1 9 9\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\n()showedthatfunctionsrepresentablewithadeeprectiﬁernetcanrequire 2014\nanexponentialnumberofhiddenunitswithashallow(onehiddenlayer)network. Moreprecisely,theyshowedthatpiecewiselinearnetworks(whichcanbeobtained\nfromrectiﬁernonlinearities ormaxoutunits)canrepresentfunctionswithanumber\nofregionsthatisexponentialinthedepthofthenetwork.Figureillustrateshow 6.5\nanetworkwithabsolutevaluerectiﬁcationcreatesmirrorimagesofthefunction\ncomputedontopofsomehiddenunit,withrespecttotheinputofthathidden\nunit.Eachhiddenunitspeciﬁeswheretofoldtheinputspaceinordertocreate\nmirrorresponses(onbothsidesoftheabsolutevaluenonlinearity). Bycomposing\nthesefoldingoperations,weobtainanexponentiallylargenumberofpiecewise\nlinearregionswhichcancaptureallkindsofregular(e.g.,repeating)patterns. Figure6.5:Anintuitive,geometricexplanationoftheexponentialadvantageofdeeper\nrectiﬁernetworksformallyby ().", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 485, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 916}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0486_90af1282", "text": "Figure6.5:Anintuitive,geometricexplanationoftheexponentialadvantageofdeeper\nrectiﬁernetworksformallyby (). Montufar e t a l .2014 ( L e f t )Anabsolutevaluerectiﬁcation\nunithasthesameoutputforeverypairofmirrorpointsinitsinput.Themirroraxis\nofsymmetryisgivenbythehyperplanedeﬁnedbytheweightsandbiasoftheunit.A\nfunctioncomputedontopofthatunit(thegreendecisionsurface)willbeamirrorimage\nofasimplerpatternacrossthataxisofsymmetry.Thefunctioncanbeobtained ( C e n t e r )\nbyfoldingthespacearoundtheaxisofsymmetry.Anotherrepeatingpatterncan ( R i g h t )\nbefoldedontopoftheﬁrst(byanotherdownstreamunit)toobtainanothersymmetry\n(whichisnowrepeatedfourtimes,withtwohiddenlayers).Figurereproducedwith\npermissionfrom ().", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 486, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 709}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0487_17effca3", "text": "Montufar e t a l .2014\nMoreprecisely,themaintheoremin ()statesthatthe Montufar e t a l .2014\nnumberoflinearregionscarvedoutbyadeeprectiﬁernetworkwith dinputs,\ndepth,andunitsperhiddenlayer,is l n\nOn\ndd l (− 1 )\nnd\n, (6.42)\ni.e.,exponentialinthedepth.Inthecaseofmaxoutnetworkswithﬁltersper l k\nunit,thenumberoflinearregionsis\nO\nk( 1 ) + l− d\n. (6.43)\n2 0 0\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nOfcourse,thereisnoguaranteethatthekindsoffunctionswewanttolearnin\napplicationsofmachinelearning(andinparticularforAI)sharesuchaproperty. Wemayalsowanttochooseadeepmodelforstatisticalreasons.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 487, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 584}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0488_efd671b6", "text": "Wemayalsowanttochooseadeepmodelforstatisticalreasons. Anytime\nwechooseaspeciﬁcmachinelearningalgorithm,weareimplicitlystatingsome\nsetofpriorbeliefswehaveaboutwhatkindoffunctionthealgorithmshould\nlearn.Choosingadeepmodelencodesaverygeneralbeliefthatthefunctionwe\nwanttolearnshouldinvolvecompositionofseveralsimplerfunctions.Thiscanbe\ninterpretedfromarepresentationlearningpointofviewassayingthatwebelieve\nthelearningproblemconsistsofdiscoveringasetofunderlyingfactorsofvariation\nthatcaninturnbedescribedintermsofother,simplerunderlyingfactorsof\nvariation.Alternately,wecaninterprettheuseofadeeparchitectureasexpressing\nabeliefthatthefunctionwewanttolearnisacomputerprogramconsistingof\nmultiplesteps,whereeachstepmakesuseofthepreviousstep’soutput.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 488, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 745}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0489_4cd92eb9", "text": "These\nintermediateoutputsarenotnecessarilyfactorsofvariation,butcaninsteadbe\nanalogoustocountersorpointersthatthenetworkusestoorganizeitsinternal\nprocessing.Empirically,greaterdepthdoesseemtoresultinbettergeneralization\nforawidevarietyoftasks( ,; ,;,; Bengio e t a l .2007Erhan e t a l .2009Bengio2009\nMesnil2011Ciresan2012Krizhevsky2012Sermanet e t a l .,; e t a l .,; e t a l .,; e t a l .,\n2013Farabet2013Couprie 2013Kahou 2013Goodfellow ; e t a l .,; e t a l .,; e t a l .,;\ne t a l . e t a l . ,;2014dSzegedy ,).Seeﬁgureandﬁgureforexamplesof 2014a 6.6 6.7\nsomeoftheseempiricalresults.Thissuggeststhatusingdeeparchitecturesdoes\nindeedexpressausefulprioroverthespaceoffunctionsthemodellearns. 6.4.2OtherArchitecturalConsiderations\nSofarwehavedescribedneuralnetworksasbeingsimplechainsoflayers,withthe\nmainconsiderationsbeingthedepthofthenetworkandthewidthofeachlayer. Inpractice,neuralnetworksshowconsiderablymorediversity. Manyneuralnetworkarchitectures havebeendevelopedforspeciﬁctasks.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 489, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 991}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0490_ae6b7439", "text": "Inpractice,neuralnetworksshowconsiderablymorediversity. Manyneuralnetworkarchitectures havebeendevelopedforspeciﬁctasks. Specializedarchitecturesforcomputervisioncalledconvolutionalnetworksare\ndescribedinchapter.Feedforwardnetworksmayalsobegeneralizedtothe 9\nrecurrentneuralnetworksforsequenceprocessing,describedinchapter,which10\nhavetheirownarchitecturalconsiderations. Ingeneral,thelayersneednotbeconnectedinachain,eventhoughthisisthe\nmostcommonpractice.Manyarchitecturesbuildamainchainbutthenaddextra\narchitecturalfeaturestoit,suchasskipconnectionsgoingfromlayer itolayer\ni+2orhigher.Theseskipconnectionsmakeiteasierforthegradienttoﬂowfrom\noutputlayerstolayersnearertheinput.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 490, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 679}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0491_46bab833", "text": "2 0 1\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\n3 4 5 6 7 8 9 1 0 1 1\nN u m b e r o f h i d d e n l a y e r s9 2 0 .9 2 5 .9 3 0 .9 3 5 .9 4 0 .9 4 5 .9 5 0 .9 5 5 .9 6 0 .9 6 5 .T e s t a c c u r a c y ( p e r c e n t )\nFigure6.6:Empiricalresultsshowingthatdeepernetworksgeneralizebetterwhenused\ntotranscribemulti-digitnumbersfromphotographsofaddresses.DatafromGoodfellow\ne t a l .(). Thetestsetaccuracyconsistentlyincreaseswithincreasingdepth. See 2014d\nﬁgureforacontrolexperimentdemonstratingthatotherincreasestothemodelsize 6.7\ndonotyieldthesameeﬀect.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 491, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 546}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0492_7b136639", "text": "Thetestsetaccuracyconsistentlyincreaseswithincreasingdepth. See 2014d\nﬁgureforacontrolexperimentdemonstratingthatotherincreasestothemodelsize 6.7\ndonotyieldthesameeﬀect. Anotherkeyconsiderationofarchitecturedesignisexactlyhowtoconnecta\npairoflayerstoeachother.Inthedefaultneuralnetworklayerdescribedbyalinear\ntransformationviaamatrixW,everyinputunitisconnectedtoeveryoutput\nunit.Manyspecializednetworksinthechaptersaheadhavefewerconnections,so\nthateachunitintheinputlayerisconnectedtoonlyasmallsubsetofunitsin\ntheoutputlayer.Thesestrategiesforreducingthenumberofconnectionsreduce\nthenumberofparametersandtheamountofcomputationrequiredtoevaluate\nthenetwork,butareoftenhighlyproblem-dependent.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 492, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 691}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0493_1a7e2b53", "text": "Forexample,convolutional\nnetworks,describedinchapter,usespecializedpatternsofsparseconnections 9\nthatareveryeﬀectiveforcomputervisionproblems.Inthischapter,itisdiﬃcult\ntogivemuchmorespeciﬁcadviceconcerningthearchitectureofagenericneural\nnetwork.Subsequentchaptersdeveloptheparticulararchitecturalstrategiesthat\nhavebeenfoundtoworkwellfordiﬀerentapplicationdomains. 2 0 2\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\n0 0 0 2 0 4 0 6 0 8 1 0 . . . . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 493, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 439}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0494_e185ebd9", "text": "2 0 2\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\n0 0 0 2 0 4 0 6 0 8 1 0 . . . . . . N u m b e r o f p a r a m e t e r s × 1 089 19 29 39 49 59 69 7T e s t a c c u r a c y ( p e r c e n t ) 3,convolutional\n3,fullyconnected\n11,convolutional\nFigure6.7:Deepermodelstendtoperformbetter.Thisisnotmerelybecausethemodelis\nlarger.ThisexperimentfromGoodfellow2014d e t a l .()showsthatincreasingthenumber\nofparametersinlayersofconvolutionalnetworkswithoutincreasingtheirdepthisnot\nnearlyaseﬀectiveatincreasingtestsetperformance.Thelegendindicatesthedepthof\nnetworkusedtomakeeachcurveandwhetherthecurverepresentsvariationinthesizeof\ntheconvolutionalorthefullyconnectedlayers.Weobservethatshallowmodelsinthis\ncontextoverﬁtataround20millionparameterswhiledeeponescanbeneﬁtfromhaving\nover60million.Thissuggeststhatusingadeepmodelexpressesausefulpreferenceover\nthespaceoffunctionsthemodelcanlearn.Speciﬁcally,itexpressesabeliefthatthe\nfunctionshouldconsistofmanysimplerfunctionscomposedtogether.Thiscouldresult\neitherinlearningarepresentationthatiscomposedinturnofsimplerrepresentations(e.g.,\ncornersdeﬁnedintermsofedges)orinlearningaprogramwithsequentiallydependent\nsteps(e.g.,ﬁrstlocateasetofobjects,thensegmentthemfromeachother,thenrecognize\nthem).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 494, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1227}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0495_a9e229dd", "text": "2 0 3\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\n6. 5 Bac k - Prop a g a t i o n an d O t h er D i ﬀ eren t i at i on A l go-\nri t h m s\nWhenweuseafeedforwardneuralnetworktoacceptaninputxandproducean\noutput ˆy,informationﬂowsforwardthroughthenetwork.Theinputsxprovide\ntheinitialinformationthatthenpropagatesuptothehiddenunitsateachlayer\nandﬁnallyproduces ˆy.Thisiscalledforwardpropagation.Duringtraining,\nforwardpropagationcancontinueonwarduntilitproducesascalarcost J(θ). Theback-propagationalgorithm( ,),oftensimplycalled Rumelhart e t a l .1986a\nbackprop,allowstheinformationfromthecosttothenﬂowbackwardsthrough\nthenetwork,inordertocomputethegradient. Computingananalyticalexpressionforthegradientisstraightforward,but\nnumericallyevaluatingsuchanexpressioncanbecomputationally expensive.The\nback-propagationalgorithmdoessousingasimpleandinexpensiveprocedure.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 495, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 851}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0496_263cdba2", "text": "Thetermback-propagation isoften misunders toodasmeaningthewhole\nlearningalgorithmformulti-layerneuralnetworks.Actually,back-propagation\nrefersonlytothemethodforcomputingthegradient,whileanotheralgorithm,\nsuchasstochasticgradientdescent,isusedtoperformlearningusingthisgradient. Furthermore,back-propagation isoftenmisunderstoodasbeingspeciﬁctomulti-\nlayerneuralnetworks,butinprincipleitcancomputederivativesofanyfunction\n(forsomefunctions,thecorrectresponseistoreportthatthederivativeofthe\nfunctionisundeﬁned).Speciﬁcally,wewilldescribehowtocomputethegradient\n∇ x f(xy ,)foranarbitraryfunction f,wherexisasetofvariableswhosederivatives\naredesired,andyisanadditionalsetofvariablesthatareinputstothefunction\nbutwhosederivativesarenotrequired.Inlearningalgorithms,thegradientwemost\noftenrequireisthegradientofthecostfunctionwithrespecttotheparameters,\n∇ θ J(θ).Manymachinelearningtasksinvolvecomputingotherderivatives,either\naspartof thelearning process, or to analyzethelearned model.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 496, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 982}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0497_3bd17320", "text": "The back-\npropagationalgorithmcanbeappliedtothesetasksaswell,andisnotrestricted\ntocomputingthegradientofthecostfunctionwithrespecttotheparameters.The\nideaofcomputingderivativesbypropagatinginformationthroughanetworkis\nverygeneral,andcanbeusedtocomputevaluessuchastheJacobianofafunction\nfwithmultipleoutputs.Werestrictourdescriptionheretothemostcommonly\nusedcasewherehasasingleoutput. f\n2 0 4\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.5.1ComputationalGraphs\nSofarwehavediscussedneuralnetworkswitharelativelyinformalgraphlanguage. Todescribetheback-propagationalgorithmmoreprecisely,itishelpfultohavea\nmoreprecise language. computationalgraph\nManywaysofformalizingcomputationasgraphsarepossible. Here,weuseeachnodeinthegraphtoindicateavariable.Thevariablemay\nbeascalar,vector,matrix,tensor,orevenavariableofanothertype. Toformalizeourgraphs,wealsoneedtointroducetheideaofanoperation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 497, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 874}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0498_b03a499a", "text": "Here,weuseeachnodeinthegraphtoindicateavariable.Thevariablemay\nbeascalar,vector,matrix,tensor,orevenavariableofanothertype. Toformalizeourgraphs,wealsoneedtointroducetheideaofanoperation. Anoperationisasimplefunctionofoneormorevariables.Ourgraphlanguage\nisaccompanied byasetofallowableoperations.Functionsmorecomplicated\nthantheoperationsinthissetmaybedescribedbycomposingmanyoperations\ntogether. Withoutlossofgenerality, wedeﬁneanoperationtoreturnonlyasingle\noutputvariable.Thisdoesnotlosegeneralitybecausetheoutputvariablecanhave\nmultipleentries,suchasavector.Softwareimplementationsofback-propagation\nusuallysupportoperationswithmultipleoutputs,butweavoidthiscaseinour\ndescriptionbecauseitintroducesmanyextradetailsthatarenotimportantto\nconceptualunderstanding. Ifavariable yiscomputedbyapplyinganoperationtoavariable x,then\nwedrawadirectededgefrom xto y. Wesometimesannotatetheoutputnode\nwiththenameoftheoperationapplied,andothertimesomitthislabelwhenthe\noperationisclearfromcontext.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 498, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 987}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0499_3993503c", "text": "Wesometimesannotatetheoutputnode\nwiththenameoftheoperationapplied,andothertimesomitthislabelwhenthe\noperationisclearfromcontext. Examplesofcomputational graphsareshowninﬁgure.6.8\n6.5.2ChainRuleofCalculus\nThechainruleofcalculus(nottobeconfusedwiththechainruleofprobability)is\nusedtocomputethederivativesoffunctionsformedbycomposingotherfunctions\nwhosederivativesareknown.Back-propagati onisanalgorithmthatcomputesthe\nchainrule,withaspeciﬁcorderofoperationsthatishighlyeﬃcient. Let xbearealnumber,andlet fand gbothbefunctionsmappingfromareal\nnumbertoarealnumber.Supposethat y= g( x)and z= f( g( x)) = f( y).Then\nthechainrulestatesthatd z\nd x=d z\nd yd y\nd x.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 499, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 655}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0500_e9ecca12", "text": "Let xbearealnumber,andlet fand gbothbefunctionsmappingfromareal\nnumbertoarealnumber.Supposethat y= g( x)and z= f( g( x)) = f( y).Then\nthechainrulestatesthatd z\nd x=d z\nd yd y\nd x. (6.44)\nWecangeneralizethisbeyondthescalarcase.Supposethatx∈ Rm,y∈ Rn,\n2 0 5\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nz z\nxx yy\n( a)×\nx x ww\n( b)u( 1 )u( 1 )\nd o t\nbbu( 2 )u( 2 )\n+ˆ y ˆ y\nσ\n( c )XXWWU( 1 )U( 1 )\nm a t m u l\nbbU( 2 )U( 2 )\n+HH\nr e l u\nxx ww\n( d)ˆ yˆ y\nd o t\nλ λu( 1 )u( 1 )\ns q ru( 2 )u( 2 )\ns u mu( 3 )u( 3 )\n×\nFigure6.8:Examplesofcomputationalgraphs.Thegraphusingthe ( a ) ×operationto\ncompute z= x y.Thegraphforthelogisticregressionprediction ( b ) ˆ y= σ\nxw+ b\n.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 500, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 656}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0501_96e90cea", "text": "Someoftheintermediateexpressionsdonothavenamesinthealgebraicexpression\nbutneednamesinthegraph.Wesimplynamethe i-thsuchvariableu( ) i.The ( c )\ncomputationalgraphfortheexpressionH=max{0 ,XW+b},whichcomputesadesign\nmatrixofrectiﬁedlinearunitactivationsHgivenadesignmatrixcontainingaminibatch\nofinputsX.Examplesa–cappliedatmostoneoperationtoeachvariable,butit ( d )\nispossibletoapplymorethanoneoperation.Hereweshowacomputationgraphthat\nappliesmorethanoneoperationtotheweightswofalinearregressionmodel.The\nweightsareusedtomakeboththepredictionˆ yandtheweightdecaypenalty λ\niw2\ni. 2 0 6\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\ngmapsfrom Rmto Rn,and fmapsfrom Rnto R.Ify= g(x) and z= f(y),then\n∂ z\n∂ x i=\nj∂ z\n∂ y j∂ y j\n∂ x i. (6.45)\nInvectornotation,thismaybeequivalentlywrittenas\n∇ x z=∂y\n∂x\n∇ y z , (6.46)\nwhere∂ y\n∂ xistheJacobianmatrixof.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 501, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 836}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0502_a3992419", "text": "(6.45)\nInvectornotation,thismaybeequivalentlywrittenas\n∇ x z=∂y\n∂x\n∇ y z , (6.46)\nwhere∂ y\n∂ xistheJacobianmatrixof. n m× g\nFromthisweseethatthegradientofavariablexcanbeobtainedbymultiplying\naJacobianmatrix∂ y\n∂ xbyagradient∇ y z.Theback-propagation algorithmconsists\nofperformingsuchaJacobian-gradient productforeachoperationinthegraph. Usuallywedonotapplytheback-propagationalgorithmmerelytovectors,\nbutrathertotensorsofarbitrarydimensionality.Conceptually,thisisexactlythe\nsameasback-propagation withvectors.Theonlydiﬀerenceishowthenumbers\narearrangedinagridtoformatensor.Wecouldimagineﬂatteningeachtensor\nintoavectorbeforewerunback-propagation,computingavector-valuedgradient,\nandthenreshapingthegradientbackintoatensor.Inthisrearrangedview,\nback-propagationisstilljustmultiplyingJacobiansbygradients.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 502, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 808}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0503_3b18d055", "text": "Todenotethegradientofavalue zwithrespecttoatensor X,wewrite ∇ X z,\njustasif Xwereavector.Theindicesinto Xnowhavemultiplecoordinates—for\nexample,a3-Dtensorisindexedbythreecoordinates.Wecanabstractthisaway\nbyusingasinglevariable itorepresentthecompletetupleofindices.Forall\npossibleindextuples i,(∇ X z) igives∂ z\n∂ X i.Thisisexactlythesameashowforall\npossibleintegerindices iintoavector,(∇ x z) igives∂ z\n∂ x i.Usingthisnotation,we\ncanwritethechainruleasitappliestotensors.Ifand ,then Y X= ( g) z f= () Y\n∇ X z=\nj(∇ X Y j)∂ z\n∂ Y j. (6.47)\n6.5.3RecursivelyApplyingtheChainRuletoObtainBackprop\nUsingthechainrule,itisstraightforwardtowritedownanalgebraicexpressionfor\nthegradientofascalarwithrespecttoanynodeinthecomputational graphthat\nproducedthatscalar.However,actuallyevaluatingthatexpressioninacomputer\nintroducessomeextraconsiderations.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 503, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 840}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0504_1cb26e17", "text": "Speciﬁcally,manysubexpressionsmayberepeatedseveraltimeswithinthe\noverallexpressionforthegradient.Anyprocedurethatcomputesthegradient\n2 0 7\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nwillneedtochoosewhethertostorethesesubexpressionsortorecomputethem\nseveraltimes.Anexampleofhowtheserepeatedsubexpressionsariseisgivenin\nﬁgure.Insomecases,computingthesamesubexpressiontwicewouldsimply 6.9\nbewasteful. Forcomplicatedgraphs,therecanbeexponentiallymanyofthese\nwastedcomputations, makinganaiveimplementation ofthechainruleinfeasible. Inothercases,computingthesamesubexpressiontwicecouldbeavalidwayto\nreducememoryconsumptionatthecostofhigherruntime.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 504, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 631}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0505_68b19265", "text": "Inothercases,computingthesamesubexpressiontwicecouldbeavalidwayto\nreducememoryconsumptionatthecostofhigherruntime. Weﬁrstbeginbyaversionoftheback-propagationalgorithmthatspeciﬁesthe\nactualgradientcomputationdirectly(algorithm alongwithalgorithm forthe 6.2 6.1\nassociatedforwardcomputation), intheorderitwillactuallybedoneandaccording\ntotherecursiveapplicationofchainrule.Onecouldeitherdirectlyperformthese\ncomputations orviewthedescriptionofthealgorithmasasymbolicspeciﬁcation\nofthecomputational graphforcomputingtheback-propagation. However,this\nformulationdoesnotmakeexplicitthemanipulation andtheconstructionofthe\nsymbolicgraphthatperformsthegradientcomputation. Such aformulationis\npresentedbelowinsection,withalgorithm ,wherewealsogeneralizeto 6.5.6 6.5\nnodesthatcontainarbitrarytensors. Firstconsideracomputational graphdescribinghowtocomputeasinglescalar\nu( ) n(saythelossonatrainingexample).Thisscalaristhequantitywhose\ngradientwewanttoobtain,withrespecttothe n iinputnodes u( 1 )to u( n i ).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 505, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1000}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0506_aff3d88f", "text": "In\notherwordswewishtocompute∂ u() n\n∂ u() iforall i∈{1 ,2 , . . . , n i}.Intheapplication\nofback-propagationtocomputinggradientsforgradientdescentoverparameters,\nu( ) nwillbethecostassociatedwithanexampleoraminibatch,while u( 1 )to u( n i )\ncorrespondtotheparametersofthemodel. Wewillassumethatthenodesofthegraphhavebeenorderedinsuchaway\nthatwecancomputetheiroutputoneaftertheother,startingat u( n i + 1 )and\ngoingupto u( ) n.Asdeﬁnedinalgorithm ,eachnode6.1 u( ) iisassociatedwithan\noperation f( ) iandiscomputedbyevaluatingthefunction\nu( ) i= ( f A( ) i) (6.48)\nwhere A( ) iisthesetofallnodesthatareparentsof u( ) i.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 506, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 618}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0507_57e53dc4", "text": "Thatalgorithmspeciﬁestheforwardpropagationcomputation,whichwecould\nputinagraph G.Inordertoperformback-propagation, wecanconstructa\ncomputational graphthatdependsonGandaddstoitanextrasetofnodes.These\nformasubgraph BwithonenodepernodeofG.Computation inBproceedsin\nexactlythereverseoftheorderofcomputationinG,andeachnodeofBcomputes\nthederivative∂ u() n\n∂ u() iassociatedwiththeforwardgraphnode u( ) i.Thisisdone\n2 0 8\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.1Aprocedurethatperformsthecomputations mapping n iinputs\nu( 1 )to u( n i )toanoutput u( ) n.Thisdeﬁnesacomputational graphwhereeachnode\ncomputesnumericalvalue u( ) ibyapplyingafunction f( ) itothesetofarguments\nA( ) ithatcomprisesthevaluesofpreviousnodes u( ) j, j < i,with j P a∈ ( u( ) i).The\ninputtothecomputational graphisthevectorx,andissetintotheﬁrst n inodes\nu( 1 )to u( n i ).Theoutputofthecomputational graphisreadoﬀthelast(output)\nnode u( ) n. for i , . . . , n = 1 ido\nu( ) i← x i\nendfor\nfor i n= i+1 , . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 507, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 978}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0508_1f739f83", "text": "for i , . . . , n = 1 ido\nu( ) i← x i\nendfor\nfor i n= i+1 , . . . , ndo\nA( ) i←{ u( ) j|∈ j P a u(( ) i)}\nu( ) i← f( ) i( A( ) i)\nendfor\nreturn u( ) n\nusingthechainrulewithrespecttoscalaroutput u( ) n:\n∂ u( ) n\n∂ u( ) j=\ni j P a u :∈ (() i )∂ u( ) n\n∂ u( ) i∂ u( ) i\n∂ u( ) j(6.49)\nasspeciﬁedbyalgorithm .Thesubgraph6.2 Bcontainsexactlyoneedgeforeach\nedgefromnode u( ) jtonode u( ) iofG.Theedgefrom u( ) jto u( ) iisassociatedwith\nthecomputationof∂ u() i\n∂ u() j.Inaddition,adotproductisperformedforeachnode,\nbetweenthegradientalreadycomputedwithrespecttonodes u( ) ithatarechildren\nof u( ) jandthevectorcontainingthepartialderivatives∂ u() i\n∂ u() jforthesamechildren\nnodes u( ) i.Tosummarize,theamountofcomputationrequiredforperforming\ntheback-propagationscaleslinearlywiththenumberofedgesinG,wherethe\ncomputationforeachedgecorrespondstocomputingapartialderivative(ofone\nnodewithrespecttooneofitsparents)aswellasperformingonemultiplication\nandoneaddition.Below,wegeneralizethisanalysistotensor-valuednodes,which\nisjustawaytogroupmultiplescalarvaluesinthesamenodeandenablemore\neﬃcientimplementations.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 508, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1102}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0509_84cb8984", "text": "Theback-propagationalgorithmisdesignedtoreducethenumberofcommon\nsubexpressionswithoutregardtomemory.Speciﬁcally,itperformsontheorder\nofoneJacobianproductpernodeinthegraph. Thiscanbeseenfromthefact\nthatbackprop(algorithm )visitseachedgefromnode 6.2 u( ) jtonode u( ) iof\nthegraphexactlyonceinordertoobtaintheassociatedpartialderivative∂ u() i\n∂ u() j. 2 0 9\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.2Simpliﬁedversionoftheback-propagation algorithmforcomputing\nthederivativesof u( ) nwithrespecttothevariablesinthegraph.Thisexampleis\nintendedtofurtherunderstandingbyshowingasimpliﬁedcasewhereallvariables\narescalars,andwewishtocomputethederivativeswithrespectto u( 1 ), . . . , u( n i ). Thissimpliﬁedversioncomputesthederivativesofallnodesinthegraph. The\ncomputational costofthisalgorithmisproportional tothenumberofedgesin\nthegraph,assumingthatthepartialderivativeassociatedwitheachedgerequires\naconstanttime.Thisisofthesameorderasthenumberofcomputations for\ntheforwardpropagation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 509, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 984}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0510_d348229a", "text": "Each∂ u() i\n∂ u() jisafunctionoftheparents u( ) jof u( ) i,thus\nlinkingthenodesoftheforwardgraphtothoseaddedfortheback-propagation\ngraph. Runforwardpropagation(algorithm forthisexample)toobtaintheactiva- 6.1\ntionsofthenetwork\nInitialize grad_table,adatastructurethatwillstorethederivativesthathave\nbeencomputed.Theentry g r a d t a b l e_ [ u( ) i]willstorethecomputedvalueof\n∂ u() n\n∂ u() i. g r a d t a b l e_ [ u( ) n] 1←\nfor do j n= −1downto1\nThenextlinecomputes∂ u() n\n∂ u() j=\ni j P a u :∈ (() i )∂ u() n\n∂ u() i∂ u() i\n∂ u() jusingstoredvalues:\ng r a d t a b l e_ [ u( ) j] ←\ni j P a u :∈ (() i ) g r a d t a b l e_ [ u( ) i]∂ u() i\n∂ u() j\nendfor\nreturn{ g r a d t a b l e_ [ u( ) i] = 1 | i , . . . , n i}\nBack-propagationthusavoidstheexponentialexplosioninrepeatedsubexpressions.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 510, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 791}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0511_d8e1b360", "text": ". . , n i}\nBack-propagationthusavoidstheexponentialexplosioninrepeatedsubexpressions. However,otheralgorithmsmaybeabletoavoidmoresubexpressionsbyperforming\nsimpliﬁcationsonthecomputational graph,ormaybeabletoconservememoryby\nrecomputingratherthanstoringsomesubexpressions.Wewillrevisittheseideas\nafterdescribingtheback-propagation algorithmitself. 6.5.4Back-PropagationComputationinFully-ConnectedMLP\nToclarifytheabovedeﬁnitionoftheback-propagation computation,letusconsider\nthespeciﬁcgraphassociatedwithafully-connected multi-layerMLP. Algorithmﬁrstshowstheforwardpropagation, whichmapsparametersto 6.3\nthesupervisedloss L(ˆyy ,)associatedwithasingle(input,target) trainingexample\n( )xy ,,with ˆytheoutputoftheneuralnetworkwhenisprovidedininput.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 511, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 746}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0512_1dccd2e4", "text": "x\nAlgorithm  then shows thecorresponding computation to be donefor 6.4\n2 1 0\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nz z\nxxyy\nw wfff\nFigure6.9:Acomputationalgraphthatresultsinrepeatedsubexpressionswhencomputing\nthegradient.Let w∈ Rbetheinputtothegraph.Weusethesamefunction f: R R→\nastheoperationthatweapplyateverystepofachain: x= f( w), y= f( x), z= f( y).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 512, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 349}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0513_bb85841c", "text": "Tocompute∂ z\n∂ w,weapplyequationandobtain: 6.44\n∂ z\n∂ w(6.50)\n=∂ z\n∂ y∂ y\n∂ x∂ x\n∂ w(6.51)\n= f() y f() x f() w (6.52)\n= f((())) f f w f(()) f w f() w (6.53)\nEquationsuggestsanimplementationinwhichwecomputethevalueof 6.52 f( w)only\nonceandstoreitinthevariable x.Thisistheapproachtakenbytheback-propagation\nalgorithm.Analternativeapproachissuggestedbyequation,wherethesubexpression 6.53\nf( w)appearsmorethanonce.Inthealternativeapproach, f( w)isrecomputedeachtime\nitisneeded.Whenthememoryrequiredtostorethevalueoftheseexpressionsislow,the\nback-propagationapproachofequationisclearlypreferablebecauseofitsreduced 6.52\nruntime.However,equationisalsoavalidimplementationofthechainrule,andis 6.53\nusefulwhenmemoryislimited. 2 1 1\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\napplyingtheback-propagation algorithmtothisgraph. Algorithms andaredemonstrationsthatarechosentobesimpleand 6.36.4\nstraightforwardtounderstand.However, theyarespecializedtoonespeciﬁc\nproblem.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 513, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 954}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0514_4b152c5e", "text": "Algorithms andaredemonstrationsthatarechosentobesimpleand 6.36.4\nstraightforwardtounderstand.However, theyarespecializedtoonespeciﬁc\nproblem. Modernsoftwareimplementations arebasedonthegeneralizedformofback-\npropagationdescribedinsectionbelow,whichcanaccommodateanycompu- 6.5.6\ntationalgraphbyexplicitlymanipulating adatastructureforrepresentingsymbolic\ncomputation. Algorithm6.3Forwardpropagationthroughatypicaldeepneuralnetworkand\nthecomputationofthecostfunction.Theloss L(ˆyy ,)dependsontheoutput\nˆyandonthetargety(seesectionforexamplesoflossfunctions).To 6.2.1.1\nobtainthetotalcost J,thelossmaybeaddedtoaregularizer Ω( θ),where θ\ncontainsalltheparameters(weightsandbiases).Algorithm showshowto 6.4\ncomputegradientsof JwithrespecttoparametersWandb.Forsimplicity,this\ndemonstrationusesonlyasingleinputexamplex.Practicalapplicationsshould\nuseaminibatch.Seesectionforamorerealisticdemonstration. 6.5.7\nRequire:Networkdepth, l\nRequire:W( ) i, i , . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 514, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 951}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0515_6c0072a8", "text": "6.5.7\nRequire:Networkdepth, l\nRequire:W( ) i, i , . . . , l , ∈{1 }theweightmatricesofthemodel\nRequire:b( ) i, i , . . . , l , ∈{1 }thebiasparametersofthemodel\nRequire:x,theinputtoprocess\nRequire:y,thetargetoutput\nh( 0 )= x\nfordo k , . . . , l = 1\na( ) k= b( ) k+W( ) kh( 1 ) k−\nh( ) k= ( fa( ) k)\nendfor\nˆyh= ( ) l\nJ L= (ˆyy ,)+Ω() λ θ\n6.5.5Symbol-to-SymbolDerivatives\nAlgebraicexpressionsandcomputational graphsbothoperateonsymbols,or\nvariables thatdo not havespeciﬁc values.Thesealgebraic and graph-based\nrepresentationsarecalledsymbolicrepresentations.Whenweactuallyuseor\ntrainaneuralnetwork,wemustassignspeciﬁcvaluestothesesymbols.We\nreplaceasymbolicinputtothenetworkxwithaspeciﬁcnumericvalue,suchas\n[123765 18] . , . ,− ..", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 515, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 729}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0516_741d1fd9", "text": ", . ,− .. 2 1 2\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.4Backwardcomputationforthedeepneuralnetworkofalgo-\nrithm,whichusesinadditiontotheinput 6.3 xatargety.Thiscomputation\nyieldsthegradientsontheactivationsa( ) kforeachlayer k,startingfromthe\noutputlayerandgoingbackwardstotheﬁrsthiddenlayer.Fromthesegradients,\nwhichcanbeinterpretedasanindicationofhoweachlayer’soutputshouldchange\ntoreduceerror,onecanobtainthegradientontheparametersofeachlayer.The\ngradientsonweightsandbiasescanbeimmediately usedaspartofastochas-\nticgradientupdate(performingtheupdaterightafterthegradientshavebeen\ncomputed)orusedwithothergradient-basedoptimization methods. Aftertheforwardcomputation,computethegradientontheoutputlayer:\ng←∇ ˆ y J= ∇ ˆ y L(ˆyy ,)\nfor do k l , l , . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 516, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 761}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0517_81e609f0", "text": "Aftertheforwardcomputation,computethegradientontheoutputlayer:\ng←∇ ˆ y J= ∇ ˆ y L(ˆyy ,)\nfor do k l , l , . . . , = −1 1\nConvert thegradienton thelayer’s output into a gradient into thepre-\nnonlinearityactivation(element-wisemultiplicationifiselement-wise): f\ng←∇a() k J f = g(a( ) k)\nComputegradientsonweightsandbiases(includingtheregularizationterm,\nwhereneeded):\n∇b() k J λ = +g ∇b() kΩ() θ\n∇W() k J= gh( 1 ) k−+ λ∇W() kΩ() θ\nPropagatethegradientsw.r.t.thenextlower-levelhiddenlayer’sactivations:\ng←∇h(1) k − J= W( ) kg\nendfor\n2 1 3\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nz z\nxxyy\nw wfffz z\nxxyy\nw wfff\nd z\nd yd z\nd yf\nd y\nd xd y\nd xf\nd z\nd xd z\nd x×\nd x\nd wd x\nd wf\nd z\nd wd z\nd w×\nFigure6.10:Anexampleofthesymbol-to-symbolapproachtocomputingderivatives.In\nthisapproach,theback-propagationalgorithmdoesnotneedtoeveraccessanyactual\nspeciﬁcnumericvalues.Instead,itaddsnodestoacomputationalgraphdescribinghow\ntocomputethesederivatives.Agenericgraphevaluationenginecanlatercomputethe\nderivativesforanyspeciﬁcnumericvalues.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 517, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1023}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0518_c4b24710", "text": "( L e f t )Inthisexample,webeginwithagraph\nrepresenting z= f( f( f( w))).Weruntheback-propagationalgorithm,instructing ( R i g h t )\nittoconstructthegraphfortheexpressioncorrespondingtod z\nd w.Inthisexample,wedo\nnotexplainhowtheback-propagationalgorithmworks.Thepurposeisonlytoillustrate\nwhatthedesiredresultis:acomputationalgraphwithasymbolicdescriptionofthe\nderivative. Someapproachestoback-propagationtakeacomputational graphandaset\nofnumericalvaluesfortheinputstothegraph,thenreturnasetofnumerical\nvaluesdescribingthegradientatthoseinputvalues.Wecallthisapproach“symbol-\nto-number”diﬀerentiation. ThisistheapproachusedbylibrariessuchasTorch\n( ,)andCaﬀe(,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 518, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 660}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0519_8ea9add8", "text": "ThisistheapproachusedbylibrariessuchasTorch\n( ,)andCaﬀe(,). Collobert e t a l .2011b Jia2013\nAnotherapproachistotakeacomputational graphandaddadditionalnodes\ntothegraphthatprovideasymbolicdescriptionofthedesiredderivatives.This\nistheapproachtakenbyTheano( ,; ,) Bergstra e t a l .2010Bastien e t a l .2012\nandTensorFlow( ,).Anexampleofhowthisapproachworks Abadi e t a l .2015\nisillustratedinﬁgure.Theprimaryadvantageofthisapproachisthat 6.10\nthederivativesaredescribedinthesamelanguageastheoriginalexpression.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 519, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 509}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0520_64a12490", "text": "Becausethederivativesarejustanothercomputational graph,itispossibletorun\nback-propagationagain,diﬀerentiating thederivativesinordertoobtainhigher\nderivatives.Computation ofhigher-orderderivativesisdescribedinsection.6.5.10\nWewillusethelatterapproachanddescribetheback-propagationalgorithmin\n2 1 4\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\ntermsofconstructingacomputational graphforthederivatives.Anysubsetofthe\ngraphmaythenbeevaluatedusingspeciﬁcnumericalvaluesatalatertime.This\nallowsustoavoidspecifyingexactlywheneachoperationshouldbecomputed. Instead,agenericgraphevaluationenginecanevaluateeverynodeassoonasits\nparents’valuesareavailable. Thedescriptionofthesymbol-to-symbolbasedapproachsubsumesthesymbol-\nto-numberapproach.Thesymbol-to-numberapproachcanbeunderstoodas\nperformingexactlythesamecomputations asaredoneinthegraphbuiltbythe\nsymbol-to-symbolapproach.Thekeydiﬀerenceisthatthesymbol-to-number\napproachdoesnotexposethegraph.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 520, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 927}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0521_29ddb514", "text": "6.5.6GeneralBack-Propagation\nTheback-propagationalgorithmisverysimple.Tocomputethegradientofsome\nscalar zwithrespecttooneofitsancestorsxinthegraph,webeginbyobserving\nthatthegradientwithrespectto zisgivenbyd z\nd z=1.Wecanthencompute\nthegradientwithrespecttoeachparentof zinthegraphbymultiplyingthe\ncurrentgradientbytheJacobianoftheoperationthatproduced z.Wecontinue\nmultiplyingbyJacobianstravelingbackwardsthroughthegraphinthiswayuntil\nwereachx.Foranynodethatmaybereachedbygoingbackwardsfrom zthrough\ntwoormorepaths,wesimplysumthegradientsarrivingfromdiﬀerentpathsat\nthatnode. Moreformally,eachnodeinthegraph Gcorrespondstoavariable.Toachieve\nmaximumgenerality,wedescribethisvariableasbeingatensor V. Tensorcan\ningeneralhaveanynumberofdimensions. Theysubsumescalars,vectors,and\nmatrices.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 521, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 786}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0522_cd6c24bc", "text": "Tensorcan\ningeneralhaveanynumberofdimensions. Theysubsumescalars,vectors,and\nmatrices. Weassumethateachvariableisassociatedwiththefollowingsubroutines: V\n• g e t o p e r a t i o n_ ( V):Thisreturnstheoperationthatcomputes V,repre-\nsentedbytheedgescominginto Vinthecomputational graph.Forexample,\ntheremaybeaPythonorC++classrepresentingthematrixmultiplication\noperation,andtheget_operationfunction.Supposewehaveavariablethat\niscreatedbymatrixmultiplication,C=AB.Then g e t o p e r a t i o n_ ( V)\nreturnsapointertoaninstanceofthecorrespondingC++class. • g e t c o n s u m e r s_ ( V ,G):Thisreturnsthelistofvariablesthatarechildrenof\nVinthecomputational graph.G\n• G g e t i n p u t s_ ( V ,):Thisreturnsthelistofvariablesthatareparentsof V\ninthecomputational graph.G\n2 1 5\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nEachoperationopisalsoassociatedwithabpropoperation.Thisbprop\noperationcancomputeaJacobian-vectorproductasdescribedbyequation.6.47\nThisishowtheback-propagationalgorithmisabletoachievegreatgenerality.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 522, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1003}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0523_64ece098", "text": "Eachoperationisresponsibleforknowinghowtoback-propagate throughthe\nedgesinthegraphthatitparticipatesin.Forexample,wemightuseamatrix\nmultiplicationoperationtocreateavariableC=AB.Supposethatthegradient\nofascalar zwithrespecttoCisgivenbyG.Thematrixmultiplication operation\nisresponsiblefordeﬁningtwoback-propagation rules,oneforeachofitsinput\narguments.Ifwecallthebpropmethodtorequestthegradientwithrespectto\nAgiventhatthegradientontheoutputisG,thenthe b p r o pmethodofthe\nmatrixmultiplicationoperationmuststatethatthegradientwithrespecttoA\nisgivenbyGB.Likewise,ifwecallthe b p r o pmethodtorequestthegradient\nwithrespecttoB,thenthematrixoperationisresponsibleforimplementing the\nb p r o pmethodandspecifyingthatthedesiredgradientisgivenbyAG.The\nback-propagationalgorithmitselfdoesnotneedtoknowanydiﬀerentiation rules.It\nonlyneedstocalleachoperation’sbpropruleswiththerightarguments.Formally,\no p b p r o p i n p u t s .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 523, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 920}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0524_152ec5e9", "text": "( , , X G)mustreturn\n\ni(∇ X o p f i n p u t s .( ) i) G i , (6.54)\nwhichisjustanimplementation ofthechainruleasexpressedinequation.6.47\nHere, i n p u t sisalistofinputsthataresuppliedtotheoperation, op.fisthe\nmathematical functionthattheoperationimplements, Xistheinputwhosegradient\nwewishtocompute,andisthegradientontheoutputoftheoperation. G\nTheop.bpropmethodshouldalwayspretendthatallofitsinputsaredistinct\nfromeachother,eveniftheyarenot.Forexample,ifthemuloperatorispassed\ntwocopiesof xtocompute x2,theop.bpropmethodshouldstillreturn xasthe\nderivativewithrespecttobothinputs.Theback-propagation algorithmwilllater\naddbothoftheseargumentstogethertoobtain 2 x,whichisthecorrecttotal\nderivativeon.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 524, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 699}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0525_06ae18fd", "text": "x\nSoftwareimplementationsofback-propagation usuallyprovideboththeopera-\ntionsandtheirbpropmethods,sothatusersofdeeplearningsoftwarelibrariesare\nabletoback-propagatethroughgraphsbuiltusingcommonoperationslikematrix\nmultiplication, exponents,logarithms,andsoon.Softwareengineerswhobuilda\nnewimplementationofback-propagationoradvanceduserswhoneedtoaddtheir\nownoperationtoanexistinglibrarymustusuallyderivetheop.bpropmethodfor\nanynewoperationsmanually. Theback-propagationalgorithmisformallydescribedinalgorithm .6.5\n2 1 6\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.5Theoutermostskeletonoftheback-propagation algorithm.This\nportiondoessimplesetupandcleanupwork.Mostoftheimportantworkhappens\ninthe subroutineofalgorithm build_grad 6.6. Require: T,thetargetsetofvariableswhosegradientsmustbecomputed. Require:G,thecomputational graph\nRequire: z,thevariabletobediﬀerentiated\nLetGbeGprunedtocontainonlynodesthatareancestorsof zanddescendents\nofnodesin.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 525, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 946}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0526_a6c78af3", "text": "Require:G,thecomputational graph\nRequire: z,thevariabletobediﬀerentiated\nLetGbeGprunedtocontainonlynodesthatareancestorsof zanddescendents\nofnodesin. T\nInitialize ,adatastructureassociatingtensorstotheirgradients grad_table\ng r a d t a b l e_ [] 1 z←\nfordo Vin T\nb u i l d g r a d_ ( V , ,GG, g r a d t a b l e_ )\nendfor\nReturn restrictedto grad_table T\nInsection,weexplainedthatback-propagation wasdevelopedinorderto 6.5.2\navoidcomputingthesamesubexpressioninthechainrulemultipletimes.Thenaive\nalgorithmcouldhaveexponentialruntimeduetotheserepeatedsubexpressions.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 526, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 566}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0527_425c7d51", "text": "Nowthatwehavespeciﬁedtheback-propagationalgorithm,wecanunderstandits\ncomputational cost.Ifweassumethateachoperationevaluationhasroughlythe\nsamecost,thenwemayanalyzethecomputational costintermsofthenumber\nofoperationsexecuted.Keepinmindherethatwerefertoanoperationasthe\nfundamentalunitofourcomputational graph,whichmightactuallyconsistofvery\nmanyarithmeticoperations(forexample,wemighthaveagraphthattreatsmatrix\nmultiplicationasasingleoperation).Computingagradientinagraphwith nnodes\nwillneverexecutemorethan O( n2)operationsorstoretheoutputofmorethan\nO( n2) operations.Herewearecountingoperationsinthecomputational graph,not\nindividualoperationsexecutedbytheunderlyinghardware,soitisimportantto\nrememberthattheruntimeofeachoperationmaybehighlyvariable.Forexample,\nmultiplyingtwomatricesthateachcontainmillionsofentriesmightcorrespondto\nasingleoperationinthegraph.Wecanseethatcomputingthegradientrequiresas\nmost O( n2) operationsbecausetheforwardpropagationstagewillatworstexecute\nall nnodesintheoriginalgraph(dependingonwhichvalueswewanttocompute,\nwemaynotneedtoexecutetheentiregraph).Theback-propagationalgorithm\naddsoneJacobian-vectorproduct,whichshouldbeexpressedwith O(1)nodes,per\nedgeintheoriginalgraph.Becausethecomputational graphisadirectedacyclic\ngraphithasatmost O( n2)edges.Forthekindsofgraphsthatarecommonlyused\ninpractice,thesituationisevenbetter.Mostneuralnetworkcostfunctionsare\n2 1 7\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.6Theinnerloopsubroutine b u i l d g r a d_ ( V , ,GG, g r a d t a b l e_ )of\ntheback-propagationalgorithm,calledbytheback-propagationalgorithmdeﬁned\ninalgorithm .6.5\nRequire: V,thevariablewhosegradientshouldbeaddedtoand .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 527, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1666}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0528_34bf5146", "text": "Ggrad_table\nRequire:G,thegraphtomodify. Require:G,therestrictionoftonodesthatparticipateinthegradient. G\nRequire:grad_table,adatastructuremappingnodestotheirgradients\nif then Visingrad_table\nReturn_ g r a d t a b l e[] V\nendif\ni←1\nfor C V in_ g e t c o n s u m e r s( ,G)do\no p g e t o p e r a t i o n ←_ () C\nD C ← b u i l d g r a d_ ( , ,GG, g r a d t a b l e_ )\nG( ) i← G o p b p r o p g e t i n p u t s . (_ ( C ,) ) , , V D\ni i←+1\nendfor\nG←\ni G( ) i\ng r a d t a b l e_ [] = V G\nInsertandtheoperationscreatingitinto G G\nReturn G\nroughlychain-structured,causingback-propagationtohave O( n)cost.Thisisfar\nbetterthanthenaiveapproach,whichmightneedtoexecuteexponentiallymany\nnodes.Thispotentiallyexponentialcostcanbeseenbyexpandingandrewriting\ntherecursivechainrule(equation)non-recursively: 6.49\n∂ u( ) n\n∂ u( ) j=\npa t h ( u( π1), u( π2), . . . , u( π t)) ,\nf r o m π1 = t o j π t = nt\nk = 2∂ u( π k )\n∂ u( π k −1 ).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 528, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 925}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0529_d2ebc51c", "text": ". . , u( π t)) ,\nf r o m π1 = t o j π t = nt\nk = 2∂ u( π k )\n∂ u( π k −1 ). (6.55)\nSincethenumberofpathsfromnode jtonode ncangrowexponentiallyinthe\nlengthofthesepaths,thenumberoftermsintheabovesum,whichisthenumber\nofsuchpaths,cangrowexponentiallywiththedepthoftheforwardpropagation\ngraph.Thislargecostwouldbeincurredbecausethesamecomputationfor\n∂ u() i\n∂ u() jwouldberedonemanytimes. Toavoidsuchrecomputation, wecanthink\nofback-propagation asatable-ﬁllingalgorithmthattakesadvantageofstoring\nintermediateresults∂ u() n\n∂ u() i.Eachnodeinthegraphhasacorrespondingslotina\ntabletostorethegradientforthatnode.Byﬁllinginthesetableentriesinorder,\n2 1 8\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nback-propagationavoidsrepeatingmanycommonsubexpressions.Thistable-ﬁlling\nstrategyissometimescalled . dynamicprogramming\n6.5.7Example:Back-PropagationforMLPTraining\nAsanexample,wewalkthroughtheback-propagation algorithmasitisusedto\ntrainamultilayerperceptron.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 529, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 939}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0530_eeb21af7", "text": "dynamicprogramming\n6.5.7Example:Back-PropagationforMLPTraining\nAsanexample,wewalkthroughtheback-propagation algorithmasitisusedto\ntrainamultilayerperceptron. Herewedevelopaverysimplemultilayerperceptionwithasinglehidden\nlayer.Totrainthismodel,wewilluseminibatchstochasticgradientdescent. Theback-propagationalgorithmisusedtocomputethegradientofthecostona\nsingleminibatch.Speciﬁcally,weuseaminibatchofexamplesfromthetraining\nsetformattedasadesignmatrixXandavectorofassociatedclasslabelsy. ThenetworkcomputesalayerofhiddenfeaturesH=max{0 ,XW( 1 )}.To\nsimplifythepresentationwedonotusebiasesinthismodel.Weassumethatour\ngraphlanguageincludesareluoperationthatcancompute max{0 ,Z}element-\nwise.Thepredictionsoftheunnormalized logprobabilities overclassesarethen\ngivenbyHW( 2 ).Weassumethatourgraphlanguageincludesacross_entropy\noperationthatcomputesthecross-entropybetweenthetargetsyandtheprobability\ndistributiondeﬁnedbytheseunnormalized logprobabilities.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 530, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 951}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0531_c3d53fa4", "text": "Theresultingcross-\nentropydeﬁnesthecost J M LE.Minimizingthiscross-entropyperformsmaximum\nlikelihoodestimationoftheclassiﬁer.However,tomakethisexamplemorerealistic,\nwealsoincludearegularizationterm.Thetotalcost\nJ J= M LE+ λ\n\ni , j\nW( 1 )\ni , j2\n+\ni , j\nW( 2 )\ni , j2\n (6.56)\nconsistsofthecross-entropyandaweightdecaytermwithcoeﬃcient λ.The\ncomputational graphisillustratedinﬁgure.6.11\nThecomputational graphforthegradientofthisexampleislargeenoughthat\nitwouldbetedioustodrawortoread.Thisdemonstratesoneofthebeneﬁts\noftheback-propagation algorithm,whichisthatitcanautomatically generate\ngradientsthatwouldbestraightforwardbuttediousforasoftwareengineerto\nderivemanually.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 531, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 679}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0532_0f0bfce6", "text": "Wecanroughlytraceoutthebehavioroftheback-propagation algorithm\nbylookingattheforwardpropagationgraphinﬁgure.Totrain,wewish 6.11\ntocomputeboth∇W(1) Jand ∇W(2) J.Therearetwodiﬀerentpathsleading\nbackwardfrom Jtotheweights:onethroughthecross-entropycost,andone\nthroughtheweightdecaycost.Theweightdecaycostisrelativelysimple;itwill\nalwayscontribute 2 λW( ) itothegradientonW( ) i. 2 1 9\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nXXW( 1 )W( 1 )U( 1 )U( 1 )\nm a t m u lHH\nr e l u\nU( 3 )U( 3 )\ns q ru( 4 )u( 4 )\ns u mλ λ u( 7 )u( 7 )W( 2 )W( 2 )U( 2 )U( 2 )\nm a t m u ly yJ M L E J M L E\nc r o s s _ e n t r o p y\nU( 5 )U( 5 )\ns q ru( 6 )u( 6 )\ns u mu( 8 )u( 8 )J J\n+\n×\n+\nFigure6.11:Thecomputationalgraphusedtocomputethecostusedtotrainourexample\nofasingle-layerMLPusingthecross-entropylossandweightdecay. Theotherpaththroughthecross-entropycostisslightlymorecomplicated.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 532, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 853}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0533_e11e5b24", "text": "Theotherpaththroughthecross-entropycostisslightlymorecomplicated. LetGbethegradientontheunnormalized logprobabilitiesU( 2 )providedby\nthecross_entropyoperation.Theback-propagation algorithmnowneedsto\nexploretwodiﬀerentbranches.Ontheshorterbranch,itaddsHGtothe\ngradientonW( 2 ),usingtheback-propagation ruleforthesecondargumentto\nthematrixmultiplication operation.Theotherbranchcorrespondstothelonger\nchaindescendingfurtheralongthenetwork.First,theback-propagationalgorithm\ncomputes ∇ H J=GW( 2 )usingtheback-propagationrulefortheﬁrstargument\ntothematrixmultiplication operation.Next,thereluoperationusesitsback-\npropagationruletozerooutcomponentsofthegradientcorrespondingtoentries\nofU( 1 )thatwerelessthan.Lettheresultbecalled 0 G.Thelaststepofthe\nback-propagationalgorithmistousetheback-propagation ruleforthesecond\nargumentoftheoperationtoadd matmul XGtothegradientonW( 1 ).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 533, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 882}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0534_fdea605e", "text": "Afterthesegradientshavebeencomputed,itistheresponsibilityofthegradient\ndescentalgorithm,oranotheroptimization algorithm,tousethesegradientsto\nupdatetheparameters. FortheMLP,thecomputational costisdominatedbythecostofmatrix\nmultiplication. Duringtheforwardpropagationstage,wemultiplybyeachweight\n2 2 0\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nmatrix,resultingin O( w) multiply-adds,where wisthenumberofweights.During\nthebackwardpropagationstage,wemultiplybythetransposeofeachweight\nmatrix,whichhasthesamecomputational cost.Themainmemorycostofthe\nalgorithmisthatweneedtostoretheinputtothenonlinearityofthehiddenlayer. Thisvalueisstoredfromthetimeitiscomputeduntilthebackwardpasshas\nreturnedtothesamepoint.Thememorycostisthus O( m n h),where misthe\nnumberofexamplesintheminibatchand n histhenumberofhiddenunits. 6.5.8Complications\nOurdescriptionoftheback-propagation algorithmhereissimplerthantheimple-\nmentationsactuallyusedinpractice.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 534, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 925}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0535_22fcbc2f", "text": "6.5.8Complications\nOurdescriptionoftheback-propagation algorithmhereissimplerthantheimple-\nmentationsactuallyusedinpractice. Asnotedabove,wehaverestrictedthedeﬁnitionofanoperationtobea\nfunctionthatreturnsasingletensor.Mostsoftwareimplementations needto\nsupportoperationsthatcanreturnmorethanonetensor.Forexample,ifwewish\ntocomputeboththemaximumvalueinatensorandtheindexofthatvalue,itis\nbesttocomputebothinasinglepassthroughmemory,soitismosteﬃcientto\nimplementthisprocedureasasingleoperationwithtwooutputs. We havenot described how tocontrolthememoryconsumption ofback-\npropagation. Back-propagati onofteninvolvessummationofmanytensorstogether. Inthenaiveapproach,eachofthesetensorswouldbecomputedseparately,then\nallofthemwouldbeaddedinasecondstep.Thenaiveapproachhasanoverly\nhighmemorybottleneckthatcanbeavoidedbymaintainingasinglebuﬀerand\naddingeachvaluetothatbuﬀerasitiscomputed.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 535, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 881}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0536_4556437a", "text": "Real-worldimplementationsofback-propagation alsoneedtohandlevarious\ndatatypes,suchas32-bitﬂoatingpoint,64-bitﬂoatingpoint,andintegervalues. Thepolicyforhandlingeachofthesetypestakesspecialcaretodesign. Someoperationshaveundeﬁnedgradients,anditisimportanttotrackthese\ncasesanddeterminewhetherthegradientrequestedbytheuserisundeﬁned. Variousothertechnicalitiesmakereal-worlddiﬀerentiation morecomplicated. Thesetechnicalitiesarenotinsurmountable,andthischapterhasdescribedthekey\nintellectualtoolsneededtocomputederivatives,butitisimportanttobeaware\nthatmanymoresubtletiesexist. 6.5.9DiﬀerentiationoutsidetheDeepLearningCommunity\nThe deeplearning comm unityhas beensomewhat isolatedfrom the broader\ncomputersciencecommunityandhaslargelydevelopeditsownculturalattitudes\n2 2 1\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nconcerninghowtoperformdiﬀerentiation. Moregenerally,theﬁeldofautomatic\ndiﬀerentiationisconcernedwithhowtocomputederivativesalgorithmically .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 536, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 945}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0537_a09aafcd", "text": "Moregenerally,theﬁeldofautomatic\ndiﬀerentiationisconcernedwithhowtocomputederivativesalgorithmically . Theback-propagationalgorithmdescribedhereisonlyoneapproachtoautomatic\ndiﬀerentiation.Itisaspecialcaseofabroaderclassoftechniquescalledreverse\nmodeaccumulation.Otherapproachesevaluatethesubexpressionsofthechain\nruleindiﬀerentorders.Ingeneral, determining theorderofevaluationthat\nresultsinthelowestcomputational costisadiﬃcultproblem.Findingtheoptimal\nsequenceofoperationstocomputethegradientisNP-complete(,), Naumann2008\ninthesensethatitmayrequiresimplifyingalgebraicexpressionsintotheirleast\nexpensiveform. Forexample,supposewehavevariables p 1 , p 2 , . . . , p nrepresentingprobabilities\nandvariables z 1 , z 2 , . . . , z nrepresentingunnormalized logprobabilities.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 537, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 772}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0538_03ada4c8", "text": "Forexample,supposewehavevariables p 1 , p 2 , . . . , p nrepresentingprobabilities\nandvariables z 1 , z 2 , . . . , z nrepresentingunnormalized logprobabilities. Suppose\nwedeﬁne\nq i=exp( z i)\niexp( z i), (6.57)\nwherewebuildthesoftmaxfunctionoutofexponentiation,summationanddivision\noperations, and construct a cross-entropyloss J=−\ni p ilog q i.Ahuman\nmathematician canobservethatthederivativeof Jwithrespectto z itakesavery\nsimpleform: q i− p i.Theback-propagation algorithmisnotcapableofsimplifying\nthegradientthisway,andwillinsteadexplicitlypropagategradientsthroughallof\nthelogarithmandexponentiationoperationsintheoriginalgraph.Somesoftware\nlibrariessuchasTheano( ,; ,)areableto Bergstra e t a l .2010Bastien e t a l .2012\nperformsomekindsofalgebraicsubstitutiontoimproveoverthegraphproposed\nbythepureback-propagation algorithm.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 538, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 835}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0539_63b37a67", "text": "Whentheforwardgraph Ghasasingleoutputnodeandeachpartialderivative\n∂ u() i\n∂ u() jcanbecomputedwithaconstantamountofcomputation,back-propagation\nguaranteesthatthenumberofcomputations forthegradientcomputationisof\nthesameorderasthenumberofcomputations fortheforwardcomputation: this\ncanbeseeninalgorithm becauseeachlocalpartialderivative 6.2∂ u() i\n∂ u() jneedsto\nbecomputedonlyoncealongwithanassociatedmultiplication andadditionfor\ntherecursivechain-ruleformulation(equation).Theoverallcomputationis 6.49\ntherefore O(#edges).However,itcanpotentiallybereducedbysimplifyingthe\ncomputational graphconstructedbyback-propagation,andthisisanNP-complete\ntask.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 539, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 651}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0540_569a64ed", "text": "ImplementationssuchasTheanoandTensorFlowuseheuristicsbasedon\nmatchingknownsimpliﬁcationpatternsinordertoiterativelyattempttosimplify\nthegraph.Wedeﬁnedback-propagation onlyforthecomputationofagradientofa\nscalaroutputbutback-propagationcanbeextendedtocomputeaJacobian(either\nof kdiﬀerentscalarnodesinthegraph,orofatensor-valuednodecontaining k\nvalues).Anaiveimplementation maythenneed ktimesmorecomputation: for\n2 2 2\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\neachscalarinternalnodeintheoriginalforwardgraph,thenaiveimplementation\ncomputes kgradientsinsteadofasinglegradient.Whenthenumberofoutputsof\nthegraphislargerthanthenumberofinputs,itissometimespreferabletouse\nanotherformofautomaticdiﬀerentiationcalledforwardmodeaccumulation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 540, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 722}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0541_ad5fb046", "text": "Forwardmodecomputationhasbeenproposedforobtainingreal-timecomputation\nofgradientsinrecurrentnetworks,forexample( ,).This WilliamsandZipser1989\nalsoavoidstheneedtostorethevaluesandgradientsforthewholegraph,trading\noﬀcomputational eﬃciencyformemory.Therelationshipbetweenforwardmode\nandbackwardmodeisanalogoustotherelationshipbetweenleft-multiplyingversus\nright-multiplyingasequenceofmatrices,suchas\nABCD , (6.58)\nwherethematricescanbethoughtofasJacobianmatrices.Forexample,ifD\nisacolumnvectorwhileAhasmanyrows,thiscorrespondstoagraphwitha\nsingleoutputandmanyinputs,andstartingthemultiplications fromtheend\nandgoingbackwardsonlyrequiresmatrix-vector products.Thiscorrespondsto\nthebackwardmode.Instead,startingtomultiplyfromtheleftwouldinvolvea\nseriesofmatrix-matrix products,whichmakesthewholecomputationmuchmore\nexpensive.However,ifAhasfewerrowsthanDhascolumns,itischeapertorun\nthemultiplications left-to-right,correspondingtotheforwardmode.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 541, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 940}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0542_d2f9d293", "text": "Inmanycommunitiesoutsideofmachinelearning,itismorecommontoim-\nplementdiﬀerentiationsoftwarethatactsdirectlyontraditionalprogramming\nlanguagecode,suchasPythonorCcode,andautomatically generatesprograms\nthatdiﬀerentiatefunctionswrittenintheselanguages.Inthedeeplearningcom-\nmunity,computational graphsareusuallyrepresentedbyexplicitdatastructures\ncreatedbyspecializedlibraries.Thespecializedapproachhasthedrawbackof\nrequiringthelibrarydevelopertodeﬁnethebpropmethodsforeveryoperation\nandlimitingtheuserofthelibrarytoonlythoseoperationsthathavebeendeﬁned. However,thespecializedapproachalsohasthebeneﬁtofallowingcustomized\nback-propagationrulestobedevelopedforeachoperation,allowingthedeveloper\ntoimprovespeedorstabilityinnon-obviouswaysthatanautomaticprocedure\nwouldpresumablybeunabletoreplicate.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 542, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 793}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0543_b78072f4", "text": "Back-propagationisthereforenottheonlywayortheoptimalwayofcomputing\nthegradient,butitisaverypracticalmethodthatcontinuestoservethedeep\nlearningcommunityverywell.Inthefuture,diﬀerentiation technologyfordeep\nnetworksmayimproveasdeeplearningpractitionersbecomemoreawareofadvances\ninthebroaderﬁeldofautomaticdiﬀerentiation. 2 2 3\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.5.10Higher-OrderDerivatives\nSomesoftwareframeworkssupporttheuseofhigher-orderderivatives.Amongthe\ndeeplearningsoftwareframeworks,thisincludesatleastTheanoandTensorFlow. Theselibrariesusethesamekindofdatastructuretodescribetheexpressionsfor\nderivativesastheyusetodescribetheoriginalfunctionbeingdiﬀerentiated.This\nmeansthatthesymbolicdiﬀerentiation machinerycanbeappliedtoderivatives. Inthecontextofdeeplearning,itisraretocomputeasinglesecondderivative\nofascalarfunction.Instead,weareusuallyinterestedinpropertiesoftheHessian\nmatrix.Ifwehaveafunction f: Rn→ R,thentheHessianmatrixisofsize n n×.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 543, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 953}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0544_ca124f70", "text": "Intypicaldeeplearningapplications, nwillbethenumberofparametersinthe\nmodel,whichcouldeasilynumberinthebillions.TheentireHessianmatrixis\nthusinfeasibletoevenrepresent. InsteadofexplicitlycomputingtheHessian,thetypicaldeeplearningapproach\nistouseKrylovmethods.Krylovmethodsareasetofiterativetechniquesfor\nperformingvariousoperationslikeapproximately invertingamatrixorﬁnding\napproximationstoitseigenvectorsoreigenvalues,withoutusinganyoperation\notherthanmatrix-vector products. InordertouseKrylovmethodsontheHessian,weonlyneedtobeableto\ncomputetheproductbetweentheHessianmatrixHandanarbitraryvectorv.A\nstraightforwardtechnique( ,)fordoingsoistocompute Christianson1992\nHv= ∇ x\n(∇ x f x())v\n. (6.59)\nBothofthegradientcomputations inthisexpressionmaybecomputedautomati-\ncallybytheappropriatesoftwarelibrary.Notethattheoutergradientexpression\ntakesthegradientofafunctionoftheinnergradientexpression.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 544, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 897}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0545_c9e97067", "text": "Ifvisitselfavectorproducedbyacomputational graph,itisimportantto\nspecifythattheautomaticdiﬀerentiationsoftwareshouldnotdiﬀerentiatethrough\nthegraphthatproduced.v\nWhilecomputingtheHessianisusuallynotadvisable,itispossibletodowith\nHessianvectorproducts.OnesimplycomputesHe( ) iforall i= 1 , . . . , n ,where\ne( ) iistheone-hotvectorwith e( ) i\ni= 1andallotherentriesequalto0. 6. 6 Hi s t or i c a l Not es\nFeedforwardnetworkscanbeseenaseﬃcientnonlinearfunctionapproximators\nbasedonusinggradientdescenttominimizetheerrorinafunctionapproximation. 2 2 4\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nFromthispointofview,themodernfeedforwardnetworkistheculminationof\ncenturiesofprogressonthegeneralfunctionapproximationtask.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 545, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 705}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0546_dffce744", "text": "2 2 4\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nFromthispointofview,themodernfeedforwardnetworkistheculminationof\ncenturiesofprogressonthegeneralfunctionapproximationtask. Thechainrulethatunderliestheback-propagation algorithmwasinvented\ninthe17thcentury(,;,).Calculusandalgebrahave Leibniz1676L’Hôpital1696\nlongbeenusedtosolveoptimization problemsinclosedform,butgradientdescent\nwasnotintroducedasatechniqueforiterativelyapproximating thesolutionto\noptimization problemsuntilthe19thcentury(Cauchy1847,). Beginninginthe1940s,thesefunctionapproximation techniqueswereusedto\nmotivatemachinelearningmodelssuchastheperceptron.However,theearliest\nmodelswerebasedonlinearmodels.CriticsincludingMarvinMinskypointedout\nseveraloftheﬂawsofthelinearmodelfamily,suchasitsinabilitytolearnthe\nXORfunction,whichledtoabacklashagainsttheentireneuralnetworkapproach.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 546, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 839}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0547_bfe19aae", "text": "Learningnonlinearfunctionsrequiredthedevelopmentofamultilayerper-\nceptronandameansofcomputingthegradientthroughsuchamodel.Eﬃcient\napplicationsofthechainrulebasedondynamicprogramming begantoappear\ninthe1960sand1970s,mostlyforcontrolapplications(,;Kelley1960Brysonand\nDenham1961Dreyfus1962BrysonandHo1969Dreyfus1973 ,;,; ,;,)butalsofor\nsensitivityanalysis(,). Linnainmaa1976Werbos1981()proposedapplyingthese\ntechniquestotrainingartiﬁcialneuralnetworks.Theideawasﬁnallydeveloped\ninpracticeafterbeingindependentlyrediscoveredindiﬀerentways(,;LeCun1985\nParker1985Rumelhart 1986a ,; e t a l .,).ThebookParallelDistributedPro-\ncessingpresentedtheresultsofsomeoftheﬁrstsuccessfulexperimentswith\nback-propagationinachapter( ,)thatcontributedgreatly Rumelhart e t a l .1986b\ntothepopularization ofback-propagation andinitiatedaveryactiveperiodof\nresearchinmulti-layerneuralnetworks. However,theideasputforwardbythe\nauthorsofthatbookandinparticularbyRumelhartandHintongomuchbeyond\nback-propagation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 547, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 987}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0548_7d79daee", "text": "However,theideasputforwardbythe\nauthorsofthatbookandinparticularbyRumelhartandHintongomuchbeyond\nback-propagation. Theyincludecrucialideasaboutthepossiblecomputational\nimplementationofseveralcentralaspectsofcognitionandlearning,whichcame\nunderthenameof“connectionism” becauseoftheimportancethisschoolofthought\nplacesontheconnectionsbetweenneuronsasthelocusoflearningandmemory. Inparticular,theseideasincludethenotionofdistributedrepresentation(Hinton\ne t a l .,).1986\nFollowingthesuccessofback-propagatio n,neuralnetworkresearchgainedpop-\nularityandreachedapeakintheearly1990s.Afterwards,othermachinelearning\ntechniquesbecamemorepopularuntilthemoderndeeplearningrenaissancethat\nbeganin2006. Thecoreideasbehindmodernfeedforwardnetworkshavenotchangedsub-\nstantiallysincethe1980s.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 548, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 777}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0549_53845887", "text": "Thecoreideasbehindmodernfeedforwardnetworkshavenotchangedsub-\nstantiallysincethe1980s. Thesameback-propagationalgorithmandthesame\n2 2 5\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\napproachestogradientdescentarestillinuse.Mostoftheimprovementinneural\nnetworkperformancefrom1986to2015canbeattributedtotwofactors.First,\nlargerdatasetshavereducedthedegreetowhichstatisticalgeneralization isa\nchallengeforneuralnetworks.Second,neuralnetworkshavebecomemuchlarger,\nduetomorepowerfulcomputers,andbettersoftwareinfrastructure.However,a\nsmallnumberofalgorithmicchangeshaveimprovedtheperformance ofneural\nnetworksnoticeably.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 549, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 602}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0550_1a01caa4", "text": "Oneofthesealgorithmicchangeswasthereplacementofmeansquarederror\nwiththecross-entropyfamilyoflossfunctions.Meansquarederrorwaspopularin\nthe1980sand1990s,butwasgraduallyreplacedbycross-entropylossesandthe\nprincipleofmaximumlikelihoodasideasspreadbetweenthestatisticscommunity\nandthemachinelearningcommunity.Theuseofcross-entropylossesgreatly\nimprovedtheperformanceofmodelswithsigmoidandsoftmaxoutputs,which\nhadpreviouslysuﬀeredfromsaturationandslowlearningwhenusingthemean\nsquarederrorloss.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 550, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 488}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0551_83e2bfb0", "text": "Theothermajoralgorithmicchangethathasgreatlyimprovedtheperformance\noffeedforwardnetworkswasthereplacementofsigmoidhiddenunitswithpiecewise\nlinearhiddenunits,suchasrectiﬁedlinearunits.Rectiﬁcationusingthemax{0 , z}\nfunctionwasintroducedinearlyneuralnetworkmodelsanddatesbackatleast\nasfarastheCognitronandNeocognitron(Fukushima19751980,,).Theseearly\nmodelsdid notuserectiﬁed linearunits, but insteadappliedrectiﬁcation to\nnonlinearfunctions.Despitetheearlypopularityofrectiﬁcation,rectiﬁcationwas\nlargelyreplacedbysigmoidsinthe1980s,perhapsbecausesigmoidsperformbetter\nwhenneuralnetworksareverysmall.Asoftheearly2000s,rectiﬁedlinearunits\nwereavoidedduetoasomewhatsuperstitiousbeliefthatactivationfunctionswith\nnon-diﬀerentiablepointsmustbeavoided.Thisbegantochangeinabout2009. Jarrett2009 e t a l .()observedthat“usingarectifyingnonlinearityisthesinglemost\nimportantfactorinimprovingtheperformanceofarecognitionsystem”among\nseveraldiﬀerentfactorsofneuralnetworkarchitecturedesign.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 551, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 978}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0552_f1343735", "text": "Forsmalldatasets, ()observedthatusingrectifyingnon- Jarrett e t a l .2009\nlinearitiesisevenmoreimportantthanlearningtheweightsofthehiddenlayers. Randomweightsaresuﬃcienttopropagateusefulinformationthrougharectiﬁed\nlinearnetwork,allowingtheclassiﬁerlayeratthetoptolearnhowtomapdiﬀerent\nfeaturevectorstoclassidentities. Whenmoredataisavailable,learningbeginstoextractenoughusefulknowledge\ntoexceedtheperformanceofrandomlychosenparameters. () Glorot e t a l .2011a\nshowedthatlearningisfareasierindeeprectiﬁedlinearnetworksthanindeep\nnetworksthathavecurvatureortwo-sidedsaturationintheiractivationfunctions. 2 2 6\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nRectiﬁedlinearunitsarealsoofhistoricalinterestbecausetheyshowthat\nneurosciencehascontinuedtohave aninﬂuenceonthe developmentofdeep\nlearningalgorithms.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 552, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 793}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0553_2f2bd90f", "text": "2 2 6\nCHAPTER6.DEEPFEEDFORWARDNETWORKS\nRectiﬁedlinearunitsarealsoofhistoricalinterestbecausetheyshowthat\nneurosciencehascontinuedtohave aninﬂuenceonthe developmentofdeep\nlearningalgorithms. ()motivaterectiﬁedlinearunitsfrom Glorot e t a l .2011a\nbiologicalconsiderations.Thehalf-rectifying nonlinearitywasintendedtocapture\nthesepropertiesofbiologicalneurons:1)Forsomeinputs,biologicalneuronsare\ncompletelyinactive.2)Forsomeinputs,abiologicalneuron’soutputisproportional\ntoitsinput.3)Mostofthetime,biologicalneuronsoperateintheregimewhere\ntheyareinactive(i.e.,theyshouldhavesparseactivations). Whenthemodernresurgenceofdeeplearningbeganin2006,feedforward\nnetworkscontinuedtohaveabadreputation.Fromabout2006-2012,itwaswidely\nbelievedthatfeedforwardnetworkswouldnotperformwellunlesstheywereassisted\nbyothermodels,suchasprobabilisticmodels.Today,itisnowknownthatwiththe\nrightresourcesandengineeringpractices,feedforwardnetworksperformverywell.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 553, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 939}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0554_0bd948f5", "text": "Today,gradient-basedlearninginfeedforwardnetworksisusedasatooltodevelop\nprobabilisticmodels,suchasthevariationalautoencoderandgenerativeadversarial\nnetworks,describedinchapter.Ratherthanbeingviewedasanunreliable 20\ntechnologythatmustbesupportedbyothertechniques,gradient-basedlearningin\nfeedforwardnetworkshasbeenviewedsince2012asapowerfultechnologythat\nmaybeappliedtomanyothermachinelearningtasks.In2006,thecommunity\nusedunsupervisedlearningtosupportsupervisedlearning,andnow,ironically,it\nismorecommontousesupervisedlearningtosupportunsupervisedlearning. Feedforwardnetworkscontinuetohaveunfulﬁlledpotential.Inthefuture,we\nexpecttheywillbeappliedtomanymoretasks,andthatadvancesinoptimization\nalgorithmsandmodeldesignwillimprovetheirperformanceevenfurther.This\nchapterhasprimarilydescribedtheneuralnetworkfamilyofmodels.Inthe\nsubsequentchapters,weturntohowtousethesemodels—howtoregularizeand\ntrainthem.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 554, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 903}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0555_e57c6773", "text": "2 2 7\nC h a p t e r 7\nRegularization f or D e e p L e ar n i n g\nAcentralprobleminmachinelearningishowtomakeanalgorithmthatwill\nperformwellnotjustonthetrainingdata,butalsoonnewinputs.Manystrategies\nusedinmachinelearningareexplicitlydesignedtoreducethetesterror,possibly\nattheexpenseofincreasedtrainingerror.Thesestrategiesareknowncollectively\nasregularization. As wewillseethereareagreatmanyformsofregularization\navailabletothedeeplearningpractitioner. Infact, developingmoreeﬀective\nregularizationstrategieshasbeenoneofthemajorresearcheﬀortsintheﬁeld. Chapterintroducedthebasicconceptsofgeneralization, underﬁtting,overﬁt- 5\nting,bias,varianceandregularization. Ifyouarenotalreadyfamiliarwiththese\nnotions,pleaserefertothatchapterbeforecontinuingwiththisone. Inthischapter,wedescriberegularizationinmoredetail,focusingonregular-\nizationstrategiesfordeepmodelsormodelsthatmaybeusedasbuildingblocks\ntoformdeepmodels. Somesectionsofthischapterdealwithstandardconceptsinmachinelearning.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 555, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 983}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0556_fe478290", "text": "Somesectionsofthischapterdealwithstandardconceptsinmachinelearning. Ifyouarealreadyfamiliarwiththeseconcepts, feelfreetoskiptherelevant\nsections.However,mostofthischapterisconcernedwiththeextensionofthese\nbasicconceptstotheparticularcaseofneuralnetworks.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 556, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 254}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0557_7340b49c", "text": "Ifyouarealreadyfamiliarwiththeseconcepts, feelfreetoskiptherelevant\nsections.However,mostofthischapterisconcernedwiththeextensionofthese\nbasicconceptstotheparticularcaseofneuralnetworks. Insection,wedeﬁnedregularizationas“anymodiﬁcationwemaketo 5.2.2\nalearningalgorithmthatisintendedtoreduceitsgeneralization errorbutnot\nitstrainingerror.”Therearemanyregularizationstrategies.Someputextra\nconstraints ona machine learning model, such asadding restrictionson the\nparametervalues.Someaddextratermsintheobjectivefunctionthatcanbe\nthoughtofascorrespondingtoasoftconstraintontheparametervalues.Ifchosen\ncarefully,theseextraconstraintsandpenaltiescanleadtoimprovedperformance\n228\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nonthetestset.Sometimestheseconstraintsandpenaltiesaredesignedtoencode\nspeciﬁckindsofpriorknowledge.Othertimes,theseconstraintsandpenalties\naredesignedtoexpressagenericpreferenceforasimplermodelclassinorderto\npromotegeneralization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 557, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 943}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0558_affa95bb", "text": "Sometimespenaltiesandconstraintsarenecessarytomake\nanunderdetermined problemdetermined.Otherformsofregularization,knownas\nensemblemethods,combinemultiplehypothesesthatexplainthetrainingdata. Inthecontextofdeeplearning,mostregularizationstrategiesarebasedon\nregularizingestimators.Regularizationofanestimatorworksbytradingincreased\nbiasforreducedvariance.Aneﬀectiveregularizerisonethatmakesaproﬁtable\ntrade,reducingvariancesigniﬁcantlywhilenotoverlyincreasingthebias.Whenwe\ndiscussedgeneralization andoverﬁttinginchapter,wefocusedonthreesituations, 5\nwherethemodelfamilybeingtrainedeither(1)excludedthetruedatagenerating\nprocess—correspondingtounderﬁttingandinducingbias,or(2)matchedthetrue\ndatageneratingprocess,or(3)includedthegeneratingprocessbutalsomany\notherpossiblegeneratingprocesses—theoverﬁttingregimewherevariancerather\nthanbiasdominatestheestimationerror.Thegoalofregularizationistotakea\nmodelfromthethirdregimeintothesecondregime.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 558, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 941}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0559_9ed90d8b", "text": "Inpractice,anoverlycomplexmodelfamilydoesnotnecessarilyincludethe\ntargetfunctionorthetruedatageneratingprocess,orevenacloseapproximation\nofeither.Wealmostneverhaveaccesstothetruedatageneratingprocessso\nwecanneverknowforsureifthemodelfamilybeingestimatedincludesthe\ngeneratingprocessornot.However,mostapplicationsofdeeplearningalgorithms\naretodomainswherethetruedatageneratingprocessisalmostcertainlyoutside\nthemodelfamily.Deeplearningalgorithmsaretypicallyappliedtoextremely\ncomplicateddomainssuchasimages,audiosequencesandtext,forwhichthetrue\ngenerationprocessessentiallyinvolvessimulatingtheentireuniverse.Tosome\nextent,wearealwaystryingtoﬁtasquarepeg(thedatageneratingprocess)into\naroundhole(ourmodelfamily).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 559, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 711}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0560_19abd834", "text": "Whatthismeansisthatcontrollingthecomplexityofthemodelisnota\nsimplematterofﬁndingthemodeloftherightsize,withtherightnumberof\nparameters.Instead,wemightﬁnd—andindeedinpracticaldeeplearningscenarios,\nwealmostalwaysdoﬁnd—thatthebestﬁttingmodel(inthesenseofminimizing\ngeneralization error)isalargemodelthathasbeenregularizedappropriately . Wenowreviewseveralstrategiesforhowtocreatesuchalarge,deep,regularized\nmodel. 2 2 9\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7.1ParameterNormPenalties\nRegularizationhasbeenusedfordecadespriortotheadventofdeeplearning.Linear\nmodelssuchaslinearregressionandlogisticregressionallowsimple,straightforward,\nandeﬀectiveregularizationstrategies.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 560, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 670}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0561_c152182e", "text": "Manyregularizationapproachesarebasedonlimitingthecapacityofmodels,\nsuchasneuralnetworks,linearregression,orlogisticregression,byaddingapa-\nrameternormpenalty Ω(θ)totheobjectivefunction J.Wedenotetheregularized\nobjectivefunctionby˜ J:\n˜ J , J , α (;θXy) = (;θXy)+Ω()θ (7.1)\nwhere α∈[0 ,∞)isahyperparameter thatweightstherelativecontributionofthe\nnormpenaltyterm,,relativetothestandardobjectivefunction Ω J.Setting αto0\nresultsinnoregularization. Largervaluesof αcorrespondtomoreregularization. Whenourtrainingalgorithmminimizestheregularizedobjectivefunction ˜ Jit\nwilldecreaseboththeoriginalobjective Jonthetrainingdataandsomemeasure\nofthesizeoftheparametersθ(orsomesubsetoftheparameters).Diﬀerent\nchoicesfortheparameternormcanresultindiﬀerentsolutionsbeingpreferred. Ω\nInthissection,wediscusstheeﬀectsofthevariousnormswhenusedaspenalties\nonthemodelparameters.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 561, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 860}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0562_2153b657", "text": "Ω\nInthissection,wediscusstheeﬀectsofthevariousnormswhenusedaspenalties\nonthemodelparameters. Beforedelvingintotheregularizationbehaviorofdiﬀerentnorms,wenotethat\nforneuralnetworks,wetypicallychoosetouseaparameternormpenaltythatΩ\npenalizes oftheaﬃnetransformationateachlayerandleaves onlytheweights\nthebiasesunregularized. Thebiasestypicallyrequirelessdatatoﬁtaccurately\nthantheweights. Eachweightspeciﬁeshowtwovariablesinteract. Fittingthe\nweightwellrequiresobservingbothvariablesinavarietyofconditions.Each\nbiascontrolsonlyasinglevariable.Thismeansthatwedonotinducetoomuch\nvariancebyleavingthebiasesunregularized. Also,regularizingthebiasparameters\ncanintroduceasigniﬁcantamountofunderﬁtting. Wethereforeusethevectorw\ntoindicatealloftheweightsthatshouldbeaﬀectedbyanormpenalty,whilethe\nvectorθdenotesalloftheparameters,includingbothwandtheunregularized\nparameters.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 562, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 865}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0563_26f30f85", "text": "Wethereforeusethevectorw\ntoindicatealloftheweightsthatshouldbeaﬀectedbyanormpenalty,whilethe\nvectorθdenotesalloftheparameters,includingbothwandtheunregularized\nparameters. Inthecontextofneuralnetworks,itissometimesdesirabletouseaseparate\npenaltywithadiﬀerent αcoeﬃcientforeachlayerofthenetwork.Becauseitcan\nbeexpensivetosearchforthecorrectvalueofmultiplehyperparameters,itisstill\nreasonabletousethesameweightdecayatalllayersjusttoreducethesizeof\nsearchspace. 2 3 0\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7 . 1 . 1 L2P a ra m et e r Regu l a ri z a t i o n\nWehavealreadyseen,insection,oneofthesimplestandmostcommonkinds 5.2.2\nofparameternormpenalty:the L2parameternormpenaltycommonlyknownas\nweightdecay.Thisregularizationstrategydrivestheweightsclosertotheorigin1\nbyaddingaregularizationtermΩ(θ) =1\n2w2\n2totheobjectivefunction.Inother\nacademiccommunities, L2regularizationisalsoknownasridgeregressionor\nTikhonovregularization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 563, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 927}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0564_e24b280d", "text": "Wecangainsomeinsightintothebehaviorofweightdecayregularization\nbystudyingthegradientoftheregularizedobjectivefunction.Tosimplifythe\npresentation,weassumenobiasparameter,soθisjustw.Suchamodelhasthe\nfollowingtotalobjectivefunction:\n˜ J , (;wXy) =α\n2wwwXy +( J; ,) , (7.2)\nwiththecorrespondingparametergradient\n∇ w˜ J , α (;wXy) = w+∇ w J , . (;wXy) (7.3)\nTotakeasinglegradientsteptoupdatetheweights,weperformthisupdate:\nwww ← −  α( +∇ w J , . (;wXy)) (7.4)\nWrittenanotherway,theupdateis:\nww ← −(1  α)−∇  w J , . (;wXy) (7.5)\nWecanseethattheadditionoftheweightdecaytermhasmodiﬁedthelearning\nruletomultiplicativelyshrinktheweightvectorbyaconstantfactoroneachstep,\njustbeforeperformingtheusualgradientupdate.Thisdescribeswhathappensin\nasinglestep.Butwhathappensovertheentirecourseoftraining?", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 564, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 790}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0565_dcc370a5", "text": "Wewillfurthersimplifytheanalysisbymakingaquadraticapproximation\ntotheobjectivefunctionintheneighborhoodofthevalueoftheweightsthat\nobtainsminimalunregularized trainingcost,w∗=argminw J(w).Iftheobjective\nfunctionistrulyquadratic,asinthecaseofﬁttingalinearregressionmodelwith\n1M o re g e n e ra l l y , we c o u l d re g u l a riz e t h e p a ra m e t e rs t o b e n e a r a n y s p e c i ﬁ c p o i n t i n s p a c e\na n d , s u rp ris i n g l y , s t i l l g e t a re g u l a riz a t i o n e ﬀ e c t , b u t b e t t e r re s u l t s will b e o b t a i n e d f o r a v a l u e\nc l o s e r t o t h e t ru e o n e , with z e ro b e i n g a d e f a u l t v a l u e t h a t m a k e s s e n s e wh e n we d o n o t k n o w i f\nt h e c o rre c t v a l u e s h o u l d b e p o s i t i v e o r n e g a t i v e .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 565, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 800}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0566_77060c22", "text": "S i n c e i t i s f a r m o re c o m m o n t o re g u l a riz e t h e\nm o d e l p a ra m e t e rs t o w a rd s z e ro , w e will f o c u s o n t h i s s p e c i a l c a s e i n o u r e x p o s i t i o n . 2 3 1\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nmeansquarederror,thentheapproximationisperfect.Theapproximation ˆ Jis\ngivenby\nˆ J J () = θ (w∗)+1\n2(ww−∗)Hww (−∗) , (7.6)\nwhereHistheHessianmatrixof Jwithrespecttowevaluatedatw∗.Thereis\nnoﬁrst-orderterminthisquadraticapproximation, becausew∗isdeﬁnedtobea\nminimum,wherethegradientvanishes.Likewise,becausew∗isthelocationofa\nminimumof,wecanconcludethatispositivesemideﬁnite. J H\nTheminimumofˆ Joccurswhereitsgradient\n∇ wˆ J() = (wHww−∗) (7.7)\nisequalto. 0\nTostudytheeﬀectofweightdecay,wemodifyequationbyaddingthe 7.7\nweightdecaygradient.Wecannowsolvefortheminimumoftheregularized\nversionofˆ J.Weusethevariable ˜wtorepresentthelocationoftheminimum. α˜wH+ (˜ww−∗) = 0 (7.8)\n(+ )H αI˜wHw = ∗(7.9)\n˜wHI = (+ α)− 1Hw∗.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 566, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 962}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0567_667e455f", "text": "α˜wH+ (˜ww−∗) = 0 (7.8)\n(+ )H αI˜wHw = ∗(7.9)\n˜wHI = (+ α)− 1Hw∗. (7.10)\nAs αapproaches0,theregularizedsolution ˜wapproachesw∗.Butwhat\nhappensas αgrows?BecauseHisrealandsymmetric,wecandecomposeit\nintoadiagonalmatrix Λandanorthonormal basisofeigenvectors,Q,suchthat\nHQQ = Λ.Applyingthedecompositiontoequation,weobtain:7.10\n˜wQQ = ( Λ+ ) αI− 1QQ Λw∗(7.11)\n=\nQIQ (+ Λ α)− 1\nQQ Λw∗(7.12)\n= (+ )Q Λ αI− 1ΛQw∗.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 567, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 412}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0568_01e735f1", "text": "(7.13)\nWeseethattheeﬀectofweightdecayistorescalew∗alongtheaxesdeﬁnedby\ntheeigenvectorsofH.Speciﬁcally,thecomponentofw∗thatisalignedwiththe\ni-theigenvectorofHisrescaledbyafactorofλ i\nλ i + α.(Youmaywishtoreview\nhowthiskindofscalingworks,ﬁrstexplainedinﬁgure).2.3\nAlongthedirectionswheretheeigenvaluesofHarerelativelylarge,forexample,\nwhere λ i α,theeﬀectofregularizationisrelativelysmall.However,components\nwith λ i αwillbeshrunktohavenearlyzeromagnitude.Thiseﬀectisillustrated\ninﬁgure.7.1\n2 3 2\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nw 1w 2w∗\n˜ w\nFigure7.1:Anillustrationoftheeﬀectof L2(orweightdecay)regularizationonthevalue\noftheoptimalw.Thesolidellipsesrepresentcontoursofequalvalueoftheunregularized\nobjective.Thedottedcirclesrepresentcontoursofequalvalueofthe L2regularizer.At\nthepoint˜w,thesecompetingobjectivesreachanequilibrium.Intheﬁrstdimension,the\neigenvalueoftheHessianof Jissmall.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 568, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 895}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0569_9a462d80", "text": "Theobjectivefunctiondoesnotincreasemuch\nwhenmovinghorizontallyawayfromw∗.Becausetheobjectivefunctiondoesnotexpress\nastrongpreferencealongthisdirection,theregularizerhasastrongeﬀectonthisaxis. Theregularizerpulls w1closetozero.Intheseconddimension,theobjectivefunction\nisverysensitivetomovementsawayfromw∗.Thecorrespondingeigenvalueislarge,\nindicatinghighcurvature.Asaresult,weightdecayaﬀectsthepositionof w2relatively\nlittle. Onlydirectionsalongwhichtheparameterscontributesigniﬁcantlytoreducing\ntheobjectivefunctionarepreservedrelativelyintact.Indirectionsthatdonot\ncontributetoreducingtheobjectivefunction,asmalleigenvalueoftheHessian\ntellsusthatmovementinthisdirectionwillnotsigniﬁcantlyincreasethegradient. Componentsoftheweightvectorcorrespondingtosuchunimportant directions\naredecayedawaythroughtheuseoftheregularizationthroughouttraining.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 569, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 845}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0570_7a69efed", "text": "Componentsoftheweightvectorcorrespondingtosuchunimportant directions\naredecayedawaythroughtheuseoftheregularizationthroughouttraining. Sofarwehavediscussedweightdecayintermsofitseﬀectontheoptimization\nofanabstract,general,quadraticcostfunction.Howdotheseeﬀectsrelateto\nmachinelearninginparticular?Wecanﬁndoutbystudyinglinearregression,a\nmodelforwhichthetruecostfunctionisquadraticandthereforeamenabletothe\nsamekindofanalysiswehaveusedsofar.Applyingtheanalysisagain,wewill\nbeabletoobtainaspecialcaseofthesameresults,butwiththesolutionnow\nphrasedintermsofthetrainingdata.Forlinearregression,thecostfunctionis\n2 3 3\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nthesumofsquarederrors:\n( )Xwy−( )Xwy− . (7.14)\nWhenweadd L2regularization, theobjectivefunctionchangesto\n( )Xwy−( )+Xwy−1\n2αww . (7.15)\nThischangesthenormalequationsforthesolutionfrom\nwX= (X)− 1Xy (7.16)\nto\nwX= (XI+ α)− 1Xy . (7.17)\nThematrixXXinequationisproportionaltothecovariancematrix 7.161\nmXX.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 570, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 961}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0571_0abdf399", "text": "(7.15)\nThischangesthenormalequationsforthesolutionfrom\nwX= (X)− 1Xy (7.16)\nto\nwX= (XI+ α)− 1Xy . (7.17)\nThematrixXXinequationisproportionaltothecovariancematrix 7.161\nmXX. Using L2regularizationreplacesthismatrixwith\nXXI+ α− 1inequation.7.17\nThenewmatrixisthesameastheoriginalone,butwiththeadditionof αtothe\ndiagonal.Thediagonalentriesofthismatrixcorrespondtothevarianceofeach\ninputfeature.Wecanseethat L2regularizationcausesthelearningalgorithm\nto“perceive”theinputXashavinghighervariance,whichmakesitshrinkthe\nweightsonfeatureswhosecovariancewiththeoutputtargetislowcomparedto\nthisaddedvariance. 7 . 1 . 2 L1Regu l a ri z a t i o n\nWhile L2weightdecayisthemostcommonformofweightdecay,thereareother\nwaystopenalizethesizeofthemodelparameters. Anotheroptionistouse L1\nregularization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 571, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 791}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0572_8b9bd110", "text": "7 . 1 . 2 L1Regu l a ri z a t i o n\nWhile L2weightdecayisthemostcommonformofweightdecay,thereareother\nwaystopenalizethesizeofthemodelparameters. Anotheroptionistouse L1\nregularization. Formally, L1regularizationonthemodelparameter isdeﬁnedas:w\nΩ() = θ ||||w 1=\ni| w i| , (7.18)\nthatis,asthesumofabsolutevaluesoftheindividualparameters.2Wewill\nnowdiscusstheeﬀectof L1regularizationonthesimplelinearregressionmodel,\nwithnobiasparameter,thatwestudiedinouranalysisof L2regularization. In\nparticular,weareinterestedindelineatingthediﬀerencesbetween L1and L2forms\n2As with L2re g u l a riz a t i o n , w e c o u l d re g u l a riz e t h e p a ra m e t e rs t o w a rd s a v a l u e t h a t i s n o t\nz e ro , b u t i n s t e a d t o wa rd s s o m e p a ra m e t e r v a l u e w( ) o. In t h a t c a s e t h e L1re g u l a riz a t i o n wo u l d\ni n t ro d u c e t h e t e rmΩ() = θ ||− w w( ) o|| 1=\ni| w i− w( ) o\ni| . 2 3 4\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nofregularization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 572, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 978}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0573_2f0518e8", "text": "In t h a t c a s e t h e L1re g u l a riz a t i o n wo u l d\ni n t ro d u c e t h e t e rmΩ() = θ ||− w w( ) o|| 1=\ni| w i− w( ) o\ni| . 2 3 4\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nofregularization. Aswith L2weightdecay, L1weightdecaycontrolsthestrength\noftheregularizationbyscalingthepenaltyusingapositivehyperparameter Ω α. Thus,theregularizedobjectivefunction ˜ J , (;wXy)isgivenby\n˜ J , α (;wXy) = ||||w 1+(; ) JwXy , , (7.19)\nwiththecorrespondinggradient(actually,sub-gradient):\n∇ w˜ J , α (;wXy) = sign( )+w ∇ w J ,(Xyw;) (7.20)\nwhere issimplythesignofappliedelement-wise. sign( )w w\nByinspectingequation,wecanseeimmediately thattheeﬀectof 7.20 L1\nregularizationisquitediﬀerentfromthatof L2regularization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 573, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 712}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0574_53b9d356", "text": "sign( )w w\nByinspectingequation,wecanseeimmediately thattheeﬀectof 7.20 L1\nregularizationisquitediﬀerentfromthatof L2regularization. Speciﬁcally,wecan\nseethattheregularizationcontributiontothegradientnolongerscaleslinearly\nwitheach w i;insteaditisaconstantfactorwithasignequaltosign( w i).One\nconsequenceofthisformofthegradientisthatwewillnotnecessarilyseeclean\nalgebraicsolutionstoquadraticapproximationsof J(Xy ,;w)aswedidfor L2\nregularization. Oursimplelinearmodelhasaquadraticcostfunctionthatwecanrepresent\nviaitsTaylorseries.Alternately,wecouldimaginethatthisisatruncatedTaylor\nseriesapproximatingthecostfunctionofamoresophisticatedmodel.Thegradient\ninthissettingisgivenby\n∇ wˆ J() = (wHww−∗) , (7.21)\nwhere,again,istheHessianmatrixofwithrespecttoevaluatedat H J ww∗. Becausethe L1penaltydoesnotadmitcleanalgebraicexpressionsinthecase\nofafullygeneralHessian,wewillalsomakethefurthersimplifyingassumption\nthattheHessianisdiagonal,H=diag([ H 1 1 , , . . . , H n , n]),whereeach H i , i >0.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 574, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0575_883caca3", "text": ". . , H n , n]),whereeach H i , i >0. Thisassumptionholdsifthedataforthelinearregressionproblemhasbeen\npreprocessedtoremoveallcorrelationbetweentheinputfeatures,whichmaybe\naccomplishedusingPCA. Ourquadraticapproximationofthe L1regularizedobjectivefunctiondecom-\nposesintoasumovertheparameters:\nˆ J , J (;wXy) = (w∗; )+Xy ,\ni1\n2H i , i(w i−w∗\ni)2+ α w| i|\n.(7.22)\nTheproblemofminimizingthisapproximatecostfunctionhasananalyticalsolution\n(foreachdimension),withthefollowingform: i\nw i= sign( w∗\ni)max\n| w∗\ni|−α\nH i , i,0\n. (7.23)\n2 3 5\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nConsiderthesituationwhere w∗\ni > i 0forall.Therearetwopossibleoutcomes:\n1.Thecasewhere w∗\ni≤α\nH i , i.Heretheoptimalvalueof w iundertheregularized\nobjectiveissimply w i= 0.Thisoccursbecausethecontributionof J(w;Xy ,)\ntotheregularizedobjective˜ J(w;Xy ,)isoverwhelmed—indirection i—by\nthe L1regularizationwhichpushesthevalueof w itozero.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 575, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 914}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0576_9c56cad7", "text": "2.Thecasewhere w∗\ni >α\nH i , i.Inthiscase,theregularizationdoesnotmovethe\noptimalvalueof w itozerobutinsteaditjustshiftsitinthatdirectionbya\ndistanceequaltoα\nH i , i. Asimilarprocesshappenswhen w∗\ni <0,butwiththe L1penaltymaking w iless\nnegativebyα\nH i , i,or0. Incomparisonto L2regularization, L1regularizationresultsinasolutionthat\nismoresparse.Sparsityinthiscontextreferstothefactthatsomeparameters\nhaveanoptimalvalueofzero.Thesparsityof L1regularizationisaqualitatively\ndiﬀerentbehaviorthanariseswith L2regularization. Equationgavethe7.13\nsolution ˜ wfor L2regularization. Ifwerevisitthatequationusingtheassumption\nofadiagonalandpositivedeﬁniteHessianHthatweintroducedforouranalysisof\nL1regularization,weﬁndthat˜ w i=H i , i\nH i , i + αw∗\ni.If w∗\niwasnonzero,then ˜ w iremains\nnonzero.Thisdemonstratesthat L2regularizationdoesnotcausetheparameters\ntobecomesparse,while L1regularizationmaydosoforlargeenough.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 576, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 911}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0577_941b3c7c", "text": "α\nThesparsitypropertyinducedby L1regularizationhasbeenusedextensively\nasafeatureselectionmechanism.Featureselectionsimpliﬁesamachinelearning\nproblembychoosingwhichsubsetoftheavailablefeaturesshouldbeused.In\nparticular,thewellknownLASSO(,)(leastabsoluteshrinkageand Tibshirani1995\nselectionoperator)modelintegratesan L1penaltywithalinearmodelandaleast\nsquarescostfunction.The L1penaltycausesasubsetoftheweightstobecome\nzero,suggestingthatthecorrespondingfeaturesmaysafelybediscarded. Insection,wesawthatmanyregularizationstrategiescanbeinterpreted 5.6.1\nasMAPBayesianinference,andthatinparticular, L2regularizationisequivalent\ntoMAPBayesianinferencewithaGaussianpriorontheweights.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 577, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 679}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0578_b8f1a873", "text": "Insection,wesawthatmanyregularizationstrategiescanbeinterpreted 5.6.1\nasMAPBayesianinference,andthatinparticular, L2regularizationisequivalent\ntoMAPBayesianinferencewithaGaussianpriorontheweights. For L1regu-\nlarization,thepenalty αΩ(w)= α\ni| w i|usedtoregularizeacostfunctionis\nequivalenttothelog-priortermthatismaximizedbyMAPBayesianinference\nwhenthepriorisanisotropicLaplacedistribution(equation)over3.26w∈ Rn:\nlog() = pw\nilogLaplace( w i;0 ,1\nα) = −|||| αw 1+log log2 n α n− .(7.24)\n2 3 6\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nFromthepointofviewoflearningviamaximization withrespecttow,wecan\nignorethe termsbecausetheydonotdependon. log log2 α− w\n7.2NormPenaltiesasConstrainedOptimization\nConsiderthecostfunctionregularizedbyaparameternormpenalty:\n˜ J , J , α .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 578, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 768}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0579_34457a61", "text": "log log2 α− w\n7.2NormPenaltiesasConstrainedOptimization\nConsiderthecostfunctionregularizedbyaparameternormpenalty:\n˜ J , J , α . (;θXy) = (;θXy)+Ω()θ (7.25)\nRecallfromsectionthatwecanminimizeafunctionsubjecttoconstraints 4.4\nbyconstructingageneralizedLagrangefunction,consistingoftheoriginalobjective\nfunctionplusasetofpenalties.Eachpenaltyisaproductbetweenacoeﬃcient,\ncalledaKarush–Kuhn–Tucker(KKT)multiplier,andafunctionrepresenting\nwhethertheconstraintissatisﬁed.IfwewantedtoconstrainΩ(θ)tobelessthan\nsomeconstant,wecouldconstructageneralizedLagrangefunction k\nL − (; ) = (; )+(Ω() θ , αXy , JθXy , αθ k .) (7.26)\nThesolutiontotheconstrainedproblemisgivenby\nθ∗= argmin\nθmax\nα , α≥ 0L()θ , α .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 579, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 695}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0580_21bca94e", "text": "(7.27)\nAsdescribedinsection,solvingthisproblemrequiresmodifyingboth 4.4 θ\nand α.Sectionprovidesaworkedexampleoflinearregressionwithan 4.5 L2\nconstraint.Manydiﬀerentproceduresarepossible—somemayusegradientdescent,\nwhileothersmayuseanalyticalsolutionsforwherethegradientiszero—butinall\nprocedures αmustincreasewheneverΩ(θ) > kanddecreasewheneverΩ(θ) < k. Allpositive αencourage Ω(θ)toshrink.Theoptimalvalue α∗willencourage Ω(θ)\ntoshrink,butnotsostronglytomakebecomelessthan. Ω()θ k\nTogainsomeinsightintotheeﬀectoftheconstraint,wecanﬁx α∗andview\ntheproblemasjustafunctionof:θ\nθ∗= argmin\nθL(θ , α∗) = argmin\nθJ , α (;θXy)+∗Ω()θ .(7.28)\nThisisexactlythesameastheregularizedtrainingproblemofminimizing ˜ J. Wecanthusthinkofaparameternormpenaltyasimposingaconstraintonthe\nweights.Ifisthe Ω L2norm,thentheweightsareconstrainedtolieinan L2\nball.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 580, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 836}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0581_5dd6a3a2", "text": "Wecanthusthinkofaparameternormpenaltyasimposingaconstraintonthe\nweights.Ifisthe Ω L2norm,thentheweightsareconstrainedtolieinan L2\nball. Ifisthe Ω L1norm,thentheweightsareconstrainedtolieinaregionof\n2 3 7\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nlimited L1norm.Usuallywedonotknowthesizeoftheconstraintregionthatwe\nimposebyusingweightdecaywithcoeﬃcient α∗becausethevalueof α∗doesnot\ndirectlytellusthevalueof k.Inprinciple,onecansolvefor k,buttherelationship\nbetween kand α∗dependsontheformof J.Whilewedonotknowtheexactsize\noftheconstraintregion,wecancontrolitroughlybyincreasingordecreasing α\ninordertogroworshrinktheconstraintregion.Larger αwillresultinasmaller\nconstraintregion.Smallerwillresultinalargerconstraintregion.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 581, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 719}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0582_dcd0bb90", "text": "α\nSometimeswemaywishtouseexplicitconstraintsratherthanpenalties.As\ndescribedinsection,wecanmodifyalgorithmssuchasstochasticgradient 4.4\ndescenttotakeastepdownhillon J(θ)andthenprojectθbacktothenearest\npointthatsatisﬁesΩ(θ) < k.Thiscanbeusefulifwehaveanideaofwhatvalue\nof kisappropriateanddonotwanttospendtimesearchingforthevalueof αthat\ncorrespondstothis. k\nAnotherreasontouseexplicitconstraintsandreprojectionratherthanenforcing\nconstraintswithpenaltiesisthatpenaltiescancausenon-convexoptimization\nprocedurestogetstuckinlocalminimacorrespondingtosmallθ.Whentraining\nneuralnetworks,thisusuallymanifestsasneuralnetworksthattrainwithseveral\n“deadunits.”Theseareunitsthatdonotcontributemuchtothebehaviorofthe\nfunctionlearnedbythenetworkbecausetheweightsgoingintooroutofthemare\nallverysmall.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 582, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 788}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0583_a149b2e0", "text": "Whentrainingwithapenaltyonthenormoftheweights,these\nconﬁgurations canbelocallyoptimal,evenifitispossibletosigniﬁcantlyreduce\nJbymakingtheweightslarger.Explicitconstraintsimplementedbyre-projection\ncanworkmuchbetterinthesecasesbecausetheydonotencouragetheweights\ntoapproachtheorigin.Explicitconstraintsimplemented byre-projectiononly\nhaveaneﬀectwhentheweightsbecomelargeandattempttoleavetheconstraint\nregion. Finally,explicitconstraintswithreprojectioncanbeusefulbecausetheyimpose\nsomestabilityontheoptimization procedure.Whenusinghighlearningrates,it\nispossibletoencounterapositivefeedbackloopinwhichlargeweightsinduce\nlargegradientswhichtheninducealargeupdatetotheweights.Iftheseupdates\nconsistentlyincreasethesizeoftheweights,thenθrapidlymovesawayfrom\ntheoriginuntilnumericaloverﬂowoccurs.Explicitconstraintswithreprojection\npreventthisfeedbackloopfromcontinuingtoincreasethemagnitudeoftheweights\nwithoutbound.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 583, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 912}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0584_275d6bd1", "text": "()recommendusingconstraintscombinedwith Hintonetal.2012c\nahighlearningratetoallowrapidexplorationofparameterspacewhilemaintaining\nsomestability. Inparticular,Hinton2012cetal.()recommendastrategyintroducedbySrebro\nandShraibman2005():constrainingthenormofeachcolumnoftheweightmatrix\n2 3 8\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nofaneuralnetlayer,ratherthanconstrainingtheFrobeniusnormoftheentire\nweightmatrix.Constrainingthenormofeachcolumnseparatelypreventsanyone\nhiddenunitfromhavingverylargeweights.Ifweconvertedthisconstraintintoa\npenaltyinaLagrangefunction,itwouldbesimilarto L2weightdecaybutwitha\nseparateKKTmultiplierfortheweightsofeachhiddenunit.EachoftheseKKT\nmultiplierswouldbedynamicallyupdatedseparatelytomakeeachhiddenunit\nobeytheconstraint.Inpractice,columnnormlimitationisalwaysimplementedas\nanexplicitconstraintwithreprojection.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 584, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 842}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0585_4e0a3550", "text": "7.3RegularizationandUnder-ConstrainedProblems\nInsomecases,regularizationisnecessaryformachinelearningproblemstobeprop-\nerlydeﬁned.Manylinearmodelsinmachinelearning,includinglinearregression\nandPCA,dependoninvertingthematrixXX.Thisisnotpossiblewhenever\nXXissingular.Thismatrixcanbesingularwheneverthedatageneratingdistri-\nbutiontrulyhasnovarianceinsomedirection,orwhennovarianceisobservedin\nsomedirectionbecausetherearefewerexamples(rowsofX)thaninputfeatures\n(columnsofX).Inthiscase,manyformsofregularizationcorrespondtoinverting\nXXI+ αinstead.Thisregularizedmatrixisguaranteedtobeinvertible. Theselinearproblemshaveclosedformsolutionswhentherelevantmatrix\nisinvertible.Itisalsopossibleforaproblemwithnoclosedformsolutiontobe\nunderdetermined. Anexampleislogisticregressionappliedtoaproblemwhere\ntheclassesarelinearlyseparable.Ifaweightvectorwisabletoachieveperfect\nclassiﬁcation,then2wwillalsoachieveperfectclassiﬁcationandhigherlikelihood.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 585, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 942}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0586_ce96d672", "text": "Anexampleislogisticregressionappliedtoaproblemwhere\ntheclassesarelinearlyseparable.Ifaweightvectorwisabletoachieveperfect\nclassiﬁcation,then2wwillalsoachieveperfectclassiﬁcationandhigherlikelihood. Aniterativeoptimization procedurelikestochasticgradientdescentwillcontinually\nincreasethemagnitudeofwand,intheory,willneverhalt.Inpractice,anumerical\nimplementationofgradientdescentwilleventuallyreachsuﬃcientlylargeweights\ntocausenumericaloverﬂow,atwhichpointitsbehaviorwilldependonhowthe\nprogrammerhasdecidedtohandlevaluesthatarenotrealnumbers. Mostformsofregularizationareabletoguaranteetheconvergenceofiterative\nmethodsappliedtounderdetermined problems. Forexample,weightdecaywill\ncausegradientdescenttoquitincreasingthemagnitudeoftheweightswhenthe\nslopeofthelikelihoodisequaltotheweightdecaycoeﬃcient. Theideaofusingregularizationtosolveunderdetermined problemsextends\nbeyondmachinelearning.Thesameideaisusefulforseveralbasiclinearalgebra\nproblems.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 586, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 950}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0587_3430efdb", "text": "Theideaofusingregularizationtosolveunderdetermined problemsextends\nbeyondmachinelearning.Thesameideaisusefulforseveralbasiclinearalgebra\nproblems. Aswesawinsection,wecansolveunderdetermined linearequationsusing 2.9\n2 3 9\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\ntheMoore-Penrosepseudoinverse.Recallthatonedeﬁnitionofthepseudoinverse\nX+ofamatrixisX\nX+=lim\nα 0(XXI+ α)− 1X. (7.29)\nWecannowrecognizeequationasperforminglinearregressionwithweight 7.29\ndecay.Speciﬁcally,equationisthelimitofequationastheregularization 7.29 7.17\ncoeﬃcientshrinkstozero.Wecanthusinterpretthepseudoinverseasstabilizing\nunderdetermined problemsusingregularization. 7.4DatasetAugmentation\nThebestwaytomakeamachinelearningmodelgeneralizebetteristotrainiton\nmoredata.Ofcourse,inpractice,theamountofdatawehaveislimited.Oneway\ntogetaroundthisproblemistocreatefakedataandaddittothetrainingset. Forsomemachinelearningtasks,itisreasonablystraightforwardtocreatenew\nfakedata.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 587, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 942}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0588_09276eb7", "text": "Forsomemachinelearningtasks,itisreasonablystraightforwardtocreatenew\nfakedata. Thisapproachiseasiestforclassiﬁcation.Aclassiﬁerneedstotakeacompli-\ncated,highdimensionalinputxandsummarizeitwithasinglecategoryidentity y. Thismeansthatthemaintaskfacingaclassiﬁeristobeinvarianttoawidevariety\noftransformations.Wecangeneratenew(x , y)pairseasilyjustbytransforming\ntheinputsinourtrainingset. x\nThisapproachisnotasreadilyapplicabletomanyothertasks.Forexample,it\nisdiﬃculttogeneratenewfakedataforadensityestimationtaskunlesswehave\nalreadysolvedthedensityestimationproblem. Datasetaugmentationhasbeenaparticularlyeﬀectivetechniqueforaspeciﬁc\nclassiﬁcationproblem:objectrecognition.Imagesarehighdimensionalandinclude\nanenormousvarietyoffactorsofvariation,manyofwhichcanbeeasilysimulated.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 588, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 778}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0589_2907f8bc", "text": "Operationsliketranslatingthetrainingimagesafewpixelsineachdirectioncan\noftengreatlyimprovegeneralization, evenifthemodelhasalreadybeendesignedto\nbepartiallytranslationinvariantbyusingtheconvolutionandpoolingtechniques\ndescribedinchapter.Manyotheroperationssuchasrotatingtheimageorscaling 9\ntheimagehavealsoprovenquiteeﬀective. Onemustbecarefulnottoapplytransformationsthatwouldchangethecorrect\nclass.Forexample,opticalcharacterrecognitiontasksrequirerecognizingthe\ndiﬀerencebetween‘b’and‘d’andthediﬀerencebetween‘6’and‘9’,sohorizontal\nﬂipsand180◦rotationsarenotappropriatewaysofaugmentingdatasetsforthese\ntasks. 2 4 0\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nTherearealsotransformationsthatwewouldlikeourclassiﬁerstobeinvariant\nto,butwhicharenoteasytoperform.Forexample,out-of-planerotationcannot\nbeimplementedasasimplegeometricoperationontheinputpixels. Datasetaugmentationiseﬀectiveforspeechrecognitiontasksaswell(Jaitly\nandHinton2013,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 589, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 937}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0590_9a15a889", "text": "Datasetaugmentationiseﬀectiveforspeechrecognitiontasksaswell(Jaitly\nandHinton2013,). Injectingnoiseintheinputtoaneuralnetwork(SietsmaandDow1991,)\ncanalsobeseenasaformofdataaugmentation.Formanyclassiﬁcationand\nevensomeregressiontasks,thetaskshouldstillbepossibletosolveevenifsmall\nrandomnoiseisaddedtotheinput.Neuralnetworksprovenottobeveryrobust\ntonoise,however(TangandEliasmith2010,).Onewaytoimprovetherobustness\nofneuralnetworksissimplytotrainthemwithrandomnoiseappliedtotheir\ninputs.Inputnoiseinjectionispartofsomeunsupervisedlearningalgorithmssuch\nasthedenoisingautoencoder(Vincent2008etal.,).Noiseinjectionalsoworks\nwhenthenoiseisappliedtothehiddenunits,whichcanbeseenasdoingdataset\naugmentationatmultiplelevelsofabstraction.Poole2014etal.()recentlyshowed\nthatthisapproachcanbehighlyeﬀectiveprovidedthatthemagnitudeofthe\nnoiseiscarefullytuned.Dropout,apowerfulregularizationstrategythatwillbe\ndescribedinsection,canbeseenasaprocessofconstructingnewinputsby 7.12\nmultiplyingbynoise.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 590, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0591_e6a0f054", "text": "Whencomparingmachinelearningbenchmarkresults,itisimportanttotake\ntheeﬀectofdatasetaugmentationintoaccount.Often,hand-designeddataset\naugmentationschemescandramaticallyreducethegeneralization errorofamachine\nlearningtechnique.Tocomparetheperformanceofonemachinelearningalgorithm\ntoanother,itisnecessarytoperformcontrolledexperiments.Whencomparing\nmachinelearningalgorithmAandmachinelearningalgorithmB,itisnecessary\ntomakesurethatbothalgorithmswereevaluatedusingthesamehand-designed\ndatasetaugmentationschemes.SupposethatalgorithmAperformspoorlywith\nnodatasetaugmentationandalgorithmBperformswellwhencombinedwith\nnumeroussynthetictransformationsoftheinput.Insuchacaseitislikelythe\nsynthetictransformationscausedtheimprovedperformance,ratherthantheuse\nofmachinelearningalgorithmB.Sometimesdecidingwhetheranexperiment\nhasbeenproperlycontrolledrequiressubjectivejudgment.Forexample,machine\nlearningalgorithmsthatinjectnoiseintotheinputareperformingaformofdataset\naugmentation.Usually,operationsthataregenerallyapplicable(suchasadding\nGaussiannoisetotheinput)areconsideredpartofthemachinelearningalgorithm,\nwhileoperationsthatarespeciﬁctooneapplicationdomain(suchasrandomly\ncroppinganimage)areconsideredtobeseparatepre-processingsteps.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 591, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1229}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0592_9d9a9e0a", "text": "2 4 1\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7.5NoiseRobustness\nSectionhasmotivatedtheuseofnoiseappliedtotheinputsasadataset 7.4\naugmentationstrategy.Forsomemodels,theadditionofnoisewithinﬁnitesimal\nvarianceattheinputofthemodelisequivalenttoimposingapenaltyonthe\nnormoftheweights(,,).Inthegeneralcase,itisimportantto Bishop1995ab\nrememberthatnoiseinjectioncanbemuchmorepowerfulthansimplyshrinking\ntheparameters,especiallywhenthenoiseisaddedtothehiddenunits.Noise\nappliedtothehiddenunitsissuchanimportanttopicthatitmerititsownseparate\ndiscussion;thedropoutalgorithmdescribedinsectionisthemaindevelopment 7.12\nofthatapproach. Anotherwaythatnoisehasbeenusedintheserviceofregularizingmodels\nisbyaddingittotheweights.Thistechniquehasbeenusedprimarilyinthe\ncontextofrecurrentneuralnetworks(,; Jimetal.1996Graves2011,). Thiscan\nbeinterpretedasa stochasticimplementation of Bayesianinference overthe\nweights.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 592, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 900}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0593_9392cd21", "text": "Thiscan\nbeinterpretedasa stochasticimplementation of Bayesianinference overthe\nweights. TheBayesiantreatmentoflearningwouldconsiderthemodelweights\ntobeuncertainandrepresentableviaaprobabilitydistributionthatreﬂectsthis\nuncertainty.Addingnoisetotheweightsisapractical,stochasticwaytoreﬂect\nthisuncertainty. Noiseappliedtotheweightscanalsobeinterpretedasequivalent(undersome\nassumptions)toamoretraditionalformofregularization, encouragingstabilityof\nthefunctiontobelearned.Considertheregressionsetting,wherewewishtotrain\nafunction ˆ y(x)thatmapsasetoffeaturesxtoascalarusingtheleast-squares\ncostfunctionbetweenthemodelpredictions ˆ y()xandthetruevalues: y\nJ= E p x , y ( )(ˆ y y ()x−)2\n. (7.30)\nThetrainingsetconsistsoflabeledexamples m {(x( 1 ), y( 1 )) ( , . . . ,x( ) m, y( ) m)}.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 593, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 783}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0594_34a35748", "text": "(7.30)\nThetrainingsetconsistsoflabeledexamples m {(x( 1 ), y( 1 )) ( , . . . ,x( ) m, y( ) m)}. Wenowassumethatwitheachinputpresentationwealsoincludearandom\nperturbation  W∼N(; 0 , ηI)ofthenetworkweights.Letusimaginethatwe\nhaveastandard l-layerMLP.Wedenotetheperturbedmodelasˆ y  W(x).Despite\ntheinjectionofnoise,wearestillinterestedinminimizingthesquarederrorofthe\noutputofthenetwork.Theobjectivefunctionthusbecomes:\n˜ J W= E p , y , ( x  W )\n(ˆ y  W() )x− y2\n(7.31)\n= E p , y , ( x  W )\nˆ y2\n W()2ˆx− y y  W()+x y2\n.(7.32)\nForsmall η,theminimization of Jwithaddedweightnoise(withcovariance\nηI)isequivalenttominimization of Jwithanadditionalregularizationterm:\n2 4 2\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nη E p , y ( x )∇ Wˆ y()x2\n.Thisformofregularizationencouragestheparametersto\ngotoregionsofparameterspacewheresmallperturbationsoftheweightshave\narelativelysmallinﬂuenceontheoutput.Inotherwords,itpushesthemodel\nintoregionswherethemodelisrelativelyinsensitivetosmallvariationsinthe\nweights,ﬁndingpointsthatarenotmerelyminima,butminimasurroundedby\nﬂatregions(HochreiterandSchmidhuber1995,).Inthesimpliﬁedcaseoflinear\nregression(where,forinstance, ˆ y(x) =wx+ b),thisregularizationtermcollapses\ninto η E p ( ) x\nx2\n,whichisnotafunctionofparametersandthereforedoesnot\ncontributetothegradientof˜ J Wwithrespecttothemodelparameters.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 594, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1353}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0595_e76cc01d", "text": "7 . 5 .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 595, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 7}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0596_2e85dc2b", "text": "1 In j ect i n g No i s e a t t h e O u t p u t T a rg et s\nMostdatasetshavesomeamountofmistakesinthe ylabels.Itcanbeharmfulto\nmaximize log p( y|x)when yisamistake.Onewaytopreventthisistoexplicitly\nmodelthenoiseonthelabels.Forexample,wecanassumethatforsomesmall\nconstant ,thetrainingsetlabel yiscorrectwithprobability 1− ,andotherwise\nanyoftheotherpossiblelabelsmightbecorrect.Thisassumptioniseasyto\nincorporateintothecostfunctionanalytically,ratherthanbyexplicitlydrawing\nnoisesamples.Forexample,labelsmoothingregularizesamodelbasedona\nsoftmaxwith koutputvaluesbyreplacingthehardandclassiﬁcationtargets 0 1\nwithtargetsof\nk− 1and1− ,respectively.Thestandardcross-entropylossmay\nthenbeusedwiththesesofttargets.Maximumlikelihoodlearningwithasoftmax\nclassiﬁerandhardtargetsmayactuallyneverconverge—thesoftmaxcannever\npredictaprobabilityofexactlyorexactly,soitwillcontinuetolearnlarger 0 1\nandlargerweights,makingmoreextremepredictionsforever.Itispossibleto\npreventthisscenariousingotherregularizationstrategieslikeweightdecay.Label\nsmoothinghastheadvantageofpreventingthepursuitofhardprobabilitieswithout\ndiscouragingcorrectclassiﬁcation.Thisstrategyhasbeenusedsincethe1980s\nandcontinuestobefeaturedprominentlyinmodernneuralnetworks(Szegedy\netal.,).2015\n7.6Semi-SupervisedLearning\nIntheparadigmofsemi-supervisedlearning,bothunlabeledexamplesfrom P( x)\nandlabeledexamplesfrom P( x y ,)areusedtoestimate P( y x|)orpredict yfrom\nx.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 596, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1429}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0597_cb9e7891", "text": "Inthecontextofdeeplearning,semi-supervisedlearningusuallyrefersto\nlearningarepresentationh= f(x) .Thegoalistolearnarepresentationso\n2 4 3\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nthatexamplesfromthesameclasshavesimilarrepresentations.Unsupervised\nlearningcanprovideusefulcuesforhowtogroupexamplesinrepresentation\nspace.Examplesthatclustertightlyintheinputspaceshouldbemappedto\nsimilarrepresentations.Alinearclassiﬁerinthenewspacemayachievebetter\ngeneralization inmanycases(BelkinandNiyogi2002Chapelle2003 ,; etal.,).A\nlong-standingvariantofthisapproachistheapplicationofprincipalcomponents\nanalysisasapre-processingstepbeforeapplyingaclassiﬁer(ontheprojected\ndata).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 597, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 663}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0598_08bbf5e8", "text": "Insteadofhavingseparateunsupervisedandsupervisedcomponentsinthe\nmodel,onecanconstructmodelsinwhichagenerativemodelofeither P( x)or\nP( x y ,)sharesparameterswithadiscriminativemodelof P( y x|).Onecan\nthentrade-oﬀthesupervisedcriterion −log P( y x|)withtheunsupervisedor\ngenerativeone(suchas−log P( x)or−log P( x y ,)).Thegenerativecriterionthen\nexpressesaparticularformofpriorbeliefaboutthesolutiontothesupervised\nlearningproblem( ,),namelythatthestructureof Lasserreetal.2006 P( x)is\nconnectedtothestructureof P( y x|)inawaythatiscapturedbytheshared\nparametrization. Bycontrollinghowmuchofthegenerativecriterionisincluded\ninthetotalcriterion,onecanﬁndabettertrade-oﬀthanwithapurelygenerative\norapurelydiscriminativetrainingcriterion( ,; Lasserreetal.2006Larochelleand\nBengio2008,). SalakhutdinovandHinton2008()describeamethodforlearningthekernel\nfunctionofakernelmachineusedforregression,inwhichtheusageofunlabeled\nexamplesformodeling improvesquitesigniﬁcantly.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 598, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 961}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0599_ff59740a", "text": "SalakhutdinovandHinton2008()describeamethodforlearningthekernel\nfunctionofakernelmachineusedforregression,inwhichtheusageofunlabeled\nexamplesformodeling improvesquitesigniﬁcantly. P() x P( ) y x|\nSee ()formoreinformationaboutsemi-supervisedlearning. Chapelle etal.2006\n7.7Multi-TaskLearning\nMulti-tasklearning(,)isawaytoimprovegeneralization bypooling Caruana1993\ntheexamples(whichcanbeseenassoftconstraintsimposedontheparameters)\narisingoutofseveraltasks. Inthesamewaythatadditionaltrainingexamples\nputmorepressureontheparametersofthemodeltowardsvaluesthatgeneralize\nwell,whenpartofamodelissharedacrosstasks,thatpartofthemodelismore\nconstrainedtowardsgoodvalues(assumingthesharingisjustiﬁed),oftenyielding\nbettergeneralization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 599, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 728}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0600_863c8ef1", "text": "Figureillustratesaverycommonformofmulti-tasklearning,inwhich 7.2\ndiﬀerentsupervisedtasks(predicting y( ) igiven x)sharethesameinput x,aswell\nassomeintermediate-lev elrepresentationh( s ha r e d)capturingacommonpoolof\n2 4 4\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nfactors.Themodelcangenerallybedividedintotwokindsofpartsandassociated\nparameters:\n1.Task-speciﬁcparameters(whichonlybeneﬁtfromtheexamplesoftheirtask\ntoachievegoodgeneralization).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 600, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 440}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0601_6f956a38", "text": "Thesearetheupperlayersoftheneural\nnetworkinﬁgure.7.2\n2.Genericparameters,sharedacrossallthetasks(whichbeneﬁtfromthe\npooleddataofallthetasks).Thesearethelowerlayersoftheneuralnetwork\ninﬁgure.7.2\nh( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )y( 1 )y( 1 )y( 2 )y( 2 )\nh( s h a r e d )h( s h a r e d )\nxx\nFigure7.2:Multi-tasklearningcanbecastinseveralwaysindeeplearningframeworks\nandthisﬁgureillustratesthecommonsituationwherethetasksshareacommoninputbut\ninvolvediﬀerenttargetrandomvariables.Thelowerlayersofadeepnetwork(whetherit\nissupervisedandfeedforwardorincludesagenerativecomponentwithdownwardarrows)\ncanbesharedacrosssuchtasks,whiletask-speciﬁcparameters(associatedrespectively\nwiththeweightsintoandfromh(1)andh(2))canbelearnedontopofthoseyieldinga\nsharedrepresentationh(shared).Theunderlyingassumptionisthatthereexistsacommon\npooloffactorsthatexplainthevariationsintheinput x,whileeachtaskisassociated\nwithasubsetofthesefactors.Inthisexample,itisadditionallyassumedthattop-level\nhiddenunitsh(1)andh(2)arespecializedtoeachtask(respectivelypredicting y(1)and\ny(2))whilesomeintermediate-levelrepresentationh(shared)issharedacrossalltasks.In\ntheunsupervisedlearningcontext,itmakessenseforsomeofthetop-levelfactorstobe\nassociatedwithnoneoftheoutputtasks(h(3)):thesearethefactorsthatexplainsomeof\ntheinputvariationsbutarenotrelevantforpredicting y(1)or y(2).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 601, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1346}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0602_90871531", "text": "Improvedgeneralization andgeneralization errorbounds(,)canbe Baxter1995\nachievedbecauseofthesharedparameters,forwhichstatisticalstrengthcanbe\n2 4 5\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\n0 50 100 150 200 250\nTime(epochs)000 .005 .010 .015 .020 .Loss(negative log-likelihood)T r a i n i n g s e t l o s s\nV a l i d a t i o n s e t l o s s\nFigure7.3:Learningcurvesshowinghowthenegativelog-likelihoodlosschangesover\ntime(indicatedasnumberoftrainingiterationsoverthedataset,or e p o c h s).Inthis\nexample,wetrainamaxoutnetworkonMNIST.Observethatthetrainingobjective\ndecreasesconsistentlyovertime,butthevalidationsetaveragelosseventuallybeginsto\nincreaseagain,forminganasymmetricU-shapedcurve. greatlyimproved(inproportionwiththeincreasednumberofexamplesforthe\nsharedparameters,comparedtothescenarioofsingle-taskmodels).Ofcoursethis\nwillhappenonlyifsomeassumptionsaboutthestatisticalrelationshipbetween\nthediﬀerenttasksarevalid,meaningthatthereissomethingsharedacrosssome\nofthetasks.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 602, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 978}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0603_c96dad0d", "text": "Fromthepointofviewofdeeplearning,theunderlyingpriorbeliefisthe\nfollowing:amongthefactorsthat explainthevariations observed inthedata\nassociatedwiththediﬀerenttasks,somearesharedacrosstwoormoretasks. 7.8EarlyStopping\nWhentraininglargemodelswithsuﬃcientrepresentationalcapacitytooverﬁt\nthetask,weoftenobservethattrainingerrordecreasessteadilyovertime,but\nvalidationseterrorbeginstoriseagain.Seeﬁgureforanexampleofthis 7.3\nbehavior.Thisbehavioroccursveryreliably.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 603, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 460}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0604_56971ecf", "text": "Thismeanswecanobtainamodelwithbettervalidationseterror(andthus,\nhopefullybettertestseterror)byreturningtotheparametersettingatthepointin\ntimewiththelowestvalidationseterror.Everytimetheerroronthevalidationset\nimproves,westoreacopyofthemodelparameters.Whenthetrainingalgorithm\nterminates,wereturntheseparameters,ratherthanthelatestparameters.The\n2 4 6\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nalgorithmterminateswhennoparametershaveimprovedoverthebestrecorded\nvalidationerrorforsomepre-speciﬁednumberofiterations.Thisprocedureis\nspeciﬁedmoreformallyinalgorithm .7.1\nAlgorithm 7.1Theearlystopping meta-algorithmfor determiningthe best\namountoftimetotrain.Thismeta-algorithm isageneralstrategythatworks\nwellwithavarietyoftrainingalgorithmsandwaysofquantifyingerroronthe\nvalidationset. Letbethenumberofstepsbetweenevaluations. n\nLet pbethe“patience,”thenumberoftimestoobserveworseningvalidationset\nerrorbeforegivingup. Letθ obetheinitialparameters.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 604, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 942}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0605_c5801704", "text": "Letbethenumberofstepsbetweenevaluations. n\nLet pbethe“patience,”thenumberoftimestoobserveworseningvalidationset\nerrorbeforegivingup. Letθ obetheinitialparameters. θθ← o\ni←0\nj←0\nv←∞\nθ∗←θ\ni∗← i\nwhiledo j < p\nUpdatebyrunningthetrainingalgorithmforsteps. θ n\ni i n ←+\nv←ValidationSetError ()θ\nif v< vthen\nj←0\nθ∗←θ\ni∗← i\nv v←\nelse\nj j←+1\nendif\nendwhile\nBestparametersareθ∗,bestnumberoftrainingstepsis i∗\nThisstrategyisknownasearlystopping.Itisprobablythemostcommonly\nusedformofregularizationindeeplearning.Itspopularityisduebothtoits\neﬀectivenessanditssimplicity. Onewaytothinkofearlystoppingisasaveryeﬃcienthyperparameter selection\nalgorithm.Inthisview,thenumberoftrainingstepsisjustanotherhyperparameter.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 605, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 704}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0606_29b8529e", "text": "Onewaytothinkofearlystoppingisasaveryeﬃcienthyperparameter selection\nalgorithm.Inthisview,thenumberoftrainingstepsisjustanotherhyperparameter. Wecanseeinﬁgurethatthishyperparameter hasaU-shapedvalidationset 7.3\n2 4 7\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nperformancecurve.Mosthyperparameters thatcontrolmodelcapacityhavesucha\nU-shapedvalidationsetperformancecurve,asillustratedinﬁgure.Inthecaseof 5.3\nearlystopping,wearecontrollingtheeﬀectivecapacityofthemodelbydetermining\nhowmanystepsitcantaketoﬁtthetrainingset.Mosthyperparametersmustbe\nchosenusinganexpensiveguessandcheckprocess,wherewesetahyperparameter\natthestartoftraining,thenruntrainingforseveralstepstoseeitseﬀect.The\n“trainingtime” hyperparam eterisuniqueinthatbydeﬁnitionasinglerunof\ntrainingtriesoutmanyvaluesofthehyperparameter.Theonlysigniﬁcantcost\ntochoosingthishyperparameter automatically viaearlystoppingisrunningthe\nvalidationsetevaluationperiodicallyduringtraining.Ideally,thisisdonein\nparalleltothetrainingprocessonaseparatemachine,separateCPU,orseparate\nGPUfromthemaintrainingprocess.Ifsuchresourcesarenotavailable,thenthe\ncostoftheseperiodicevaluationsmaybereducedbyusingavalidationsetthatis\nsmallcomparedtothetrainingsetorbyevaluatingthevalidationseterrorless\nfrequentlyandobtainingalowerresolutionestimateoftheoptimaltrainingtime.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 606, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1307}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0607_25a157a6", "text": "Anadditionalcosttoearlystoppingistheneedtomaintainacopyofthe\nbestparameters.Thiscostisgenerallynegligible,becauseitisacceptabletostore\ntheseparametersinaslowerandlargerformofmemory(forexample,trainingin\nGPUmemory,butstoringtheoptimalparametersinhostmemoryoronadisk\ndrive).Sincethebestparametersarewrittentoinfrequentlyandneverreadduring\ntraining,theseoccasionalslowwriteshavelittleeﬀectonthetotaltrainingtime. Earlystoppingisaveryunobtrusiveformofregularization, inthatitrequires\nalmostnochangeintheunderlyingtrainingprocedure,theobjectivefunction,\northesetofallowableparametervalues.Thismeansthatitiseasytouseearly\nstoppingwithoutdamagingthelearningdynamics.Thisisincontrasttoweight\ndecay,whereonemustbecarefulnottousetoomuchweightdecayandtrapthe\nnetworkinabadlocalminimumcorrespondingtoasolutionwithpathologically\nsmallweights.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 607, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 829}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0608_72da1ecf", "text": "Earlystoppingmaybeusedeitheraloneorinconjunctionwithotherregulariza-\ntionstrategies.Evenwhenusingregularizationstrategiesthatmodifytheobjective\nfunctiontoencouragebettergeneralization, itisrareforthebestgeneralization to\noccuratalocalminimumofthetrainingobjective. Earlystoppingrequiresavalidationset,whichmeanssometrainingdataisnot\nfedtothemodel.Tobestexploitthisextradata,onecanperformextratraining\naftertheinitialtrainingwithearlystoppinghascompleted.Inthesecond,extra\ntrainingstep,allofthetrainingdataisincluded.Therearetwobasicstrategies\nonecanuseforthissecondtrainingprocedure.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 608, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 583}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0609_05939caa", "text": "Onestrategy(algorithm )istoinitializethemodelagainandretrainonall 7.2\n2 4 8\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nofthedata.Inthissecondtrainingpass,wetrainforthesamenumberofstepsas\ntheearlystoppingproceduredeterminedwasoptimalintheﬁrstpass.Thereare\nsomesubtletiesassociatedwiththisprocedure.Forexample,thereisnotagood\nwayofknowingwhethertoretrainforthesamenumberofparameterupdatesor\nthesamenumberofpassesthroughthedataset.Onthesecondroundoftraining,\neachpassthroughthedatasetwillrequiremoreparameterupdatesbecausethe\ntrainingsetisbigger. Algorithm7.2Ameta-algorithm forusingearlystoppingtodeterminehowlong\ntotrain,thenretrainingonallthedata. LetX( ) t r a i nandy( ) t r a i nbethetrainingset. SplitX( ) t r a i nandy( ) t r a i ninto(X( ) s ubtr a i n,X( v a l i d )) (andy( ) s ubtr a i n,y( v a l i d ))\nrespectively.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 609, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 822}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0610_bc42e62f", "text": "LetX( ) t r a i nandy( ) t r a i nbethetrainingset. SplitX( ) t r a i nandy( ) t r a i ninto(X( ) s ubtr a i n,X( v a l i d )) (andy( ) s ubtr a i n,y( v a l i d ))\nrespectively. Runearlystopping(algorithm )startingfromrandom 7.1 θusingX( ) s ubtr a i nand\ny( ) s ubtr a i nfortrainingdataandX( v a l i d )andy( v a l i d )forvalidationdata.This\nreturns i∗,theoptimalnumberofsteps. Settorandomvaluesagain. θ\nTrainonX( ) t r a i nandy( ) t r a i nfor i∗steps. Anotherstrategyforusingallofthedataistokeeptheparametersobtained\nfromtheﬁrstroundoftrainingandthencontinuetrainingbutnowusingallof\nthedata.Atthisstage,wenownolongerhaveaguideforwhentostopinterms\nofanumberofsteps.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 610, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 671}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0611_96ab997e", "text": "Instead,wecanmonitortheaveragelossfunctiononthe\nvalidationset,andcontinuetraininguntilitfallsbelowthevalueofthetraining\nsetobjectiveatwhichtheearlystoppingprocedurehalted.Thisstrategyavoids\nthehighcostofretrainingthemodelfromscratch,butisnotaswell-behaved.For\nexample,thereisnotanyguaranteethattheobjectiveonthevalidationsetwill\neverreachthetargetvalue,sothisstrategyisnotevenguaranteedtoterminate. Thisprocedureispresentedmoreformallyinalgorithm .7.3\nEarlystoppingisalsousefulbecauseitreducesthecomputational costofthe\ntrainingprocedure.Besidestheobviousreductionincostduetolimitingthenumber\noftrainingiterations,italsohasthebeneﬁtofprovidingregularizationwithout\nrequiringtheadditionofpenaltytermstothecostfunctionorthecomputationof\nthegradientsofsuchadditionalterms.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 611, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 769}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0612_e9a01306", "text": "Howearlystoppingactsasaregularizer:Sofarwehavestatedthatearly\nstoppingaregularizationstrategy,butwehavesupportedthisclaimonlyby is\nshowinglearningcurveswherethevalidationseterrorhasaU-shapedcurve.What\n2 4 9\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nAlgorithm7.3Meta-algorithm usingearlystoppingtodetermineatwhatobjec-\ntivevaluewestarttooverﬁt,thencontinuetraininguntilthatvalueisreached. LetX( ) t r a i nandy( ) t r a i nbethetrainingset. SplitX( ) t r a i nandy( ) t r a i ninto(X( ) s ubtr a i n,X( v a l i d )) (andy( ) s ubtr a i n,y( v a l i d ))\nrespectively. Runearlystopping(algorithm )startingfromrandom 7.1 θusingX( ) s ubtr a i nand\ny( ) s ubtr a i nfortrainingdataandX( v a l i d )andy( v a l i d )forvalidationdata.This\nupdates.θ\n J , ←(θX( ) s ubtr a i n,y( ) s ubtr a i n)\nwhile J ,(θX( v a l i d ),y( v a l i d )) > do\nTrainonX( ) t r a i nandy( ) t r a i nforsteps.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 612, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 882}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0613_4048be67", "text": "n\nendwhile\nistheactualmechanismbywhichearlystoppingregularizesthemodel?Bishop\n()and ()arguedthatearlystoppinghastheeﬀectof 1995aSjöbergandLjung1995\nrestrictingtheoptimization proceduretoarelativelysmallvolumeofparameter\nspaceintheneighborhoodoftheinitialparametervalueθ o,asillustratedin\nﬁgure.Morespeciﬁcally,imaginetaking 7.4 τoptimization steps(corresponding\nto τtrainingiterations)andwithlearningrate .Wecanviewtheproduct  τ\nasameasureofeﬀectivecapacity.Assumingthegradientisbounded,restricting\nboththenumberofiterationsandthelearningratelimitsthevolumeofparameter\nspacereachablefromθ o.Inthissense,  τbehavesasifitwerethereciprocalof\nthecoeﬃcientusedforweightdecay. Indeed,wecanshowhow—inthecaseofasimplelinearmodelwithaquadratic\nerrorfunctionandsimplegradientdescent—earlystoppingisequivalentto L2\nregularization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 613, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 822}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0614_0f95d8f9", "text": "Indeed,wecanshowhow—inthecaseofasimplelinearmodelwithaquadratic\nerrorfunctionandsimplegradientdescent—earlystoppingisequivalentto L2\nregularization. Inordertocomparewithclassical L2regularization, weexamineasimple\nsettingwheretheonlyparametersarelinearweights(θ=w).Wecanmodel\nthecostfunction Jwithaquadraticapproximationintheneighborhoodofthe\nempiricallyoptimalvalueoftheweightsw∗:\nˆ J J () = θ (w∗)+1\n2(ww−∗)Hww (−∗) , (7.33)\nwhereHistheHessianmatrixof Jwithrespecttowevaluatedatw∗.Giventhe\nassumptionthatw∗isaminimumof J(w),weknowthatHispositivesemideﬁnite. UnderalocalTaylorseriesapproximation,thegradientisgivenby:\n∇ wˆ J() = (wHww−∗) . (7.34)\n2 5 0\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nw 1w 2w∗\n˜ w\nw 1w 2w∗\n˜ w\nFigure7.4:Anillustrationoftheeﬀectofearlystopping.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 614, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 770}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0615_1bb6dd08", "text": "(7.34)\n2 5 0\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nw 1w 2w∗\n˜ w\nw 1w 2w∗\n˜ w\nFigure7.4:Anillustrationoftheeﬀectofearlystopping. ( L e f t )Thesolidcontourlines\nindicatethecontoursofthenegativelog-likelihood.Thedashedlineindicatesthetrajectory\ntakenbySGDbeginningfromtheorigin.Ratherthanstoppingatthepointw∗that\nminimizesthecost,earlystoppingresultsinthetrajectorystoppingatanearlierpoint˜w. ( R i g h t )Anillustrationoftheeﬀectof L2regularizationforcomparison.Thedashedcircles\nindicatethecontoursofthe L2penalty,whichcausestheminimumofthetotalcosttolie\nnearertheoriginthantheminimumoftheunregularizedcost. Wearegoingtostudythetrajectoryfollowedbytheparametervectorduring\ntraining.Forsimplicity,letussettheinitialparametervectortotheorigin,3that\nisw( 0 )= 0.Letusstudytheapproximatebehaviorofgradientdescenton Jby\nanalyzinggradientdescentonˆ J:\nw( ) τ= w( 1 ) τ−−∇  wˆ J(w( 1 ) τ−) (7.35)\n= w( 1 ) τ−− Hw(( 1 ) τ−−w∗) (7.36)\nw( ) τ−w∗= ( )(IH− w( 1 ) τ−−w∗) .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 615, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 963}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0616_df1d7a4f", "text": "(7.37)\nLetusnowrewritethisexpressioninthespaceoftheeigenvectorsofH,exploiting\ntheeigendecompositionofH:H=QQ Λ,where ΛisadiagonalmatrixandQ\nisanorthonormalbasisofeigenvectors. w( ) τ−w∗= (IQQ −  Λ)(w( 1 ) τ−−w∗)(7.38)\nQ(w( ) τ−w∗) = ( )I−  ΛQ(w( 1 ) τ−−w∗) (7.39)\n3F o r n e u ra l n e t w o rk s , t o o b t a i n s y m m e t ry b re a k i n g b e t w e e n h i d d e n u n i t s , w e c a n n o t i n i t i a l i z e\na l l t h e p a ra m e t e rs t o 0 , a s d i s c u s s e d i n s e c t i o n . Ho w e v e r, t h e a rg u m e n t h o l d s f o r a n y o t h e r 6 . 2\ni n i t i a l v a l u e w( 0 ). 2 5 1\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nAssumingthatw( 0 )=0andthat ischosentobesmallenoughtoguarantee\n|1−  λ i| <1,theparametertrajectoryduringtrainingafter τparameterupdates\nisasfollows:\nQw( ) τ= [ ( )I−I−  Λτ]Qw∗.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 616, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 835}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0617_76aea1e4", "text": "(7.40)\nNow,theexpressionforQ˜winequationfor7.13 L2regularizationcanberear-\nrangedas:\nQ˜wI = (+ Λ α)− 1ΛQw∗(7.41)\nQ˜wII = [−(+ Λ α)− 1α]Qw∗(7.42)\nComparingequationandequation,weseethatifthehyperparameters 7.40 7.42 ,\nα τ,andarechosensuchthat\n( )I−  Λτ= (+ ) Λ αI− 1α , (7.43)\nthen L2regularizationandearlystoppingcanbeseentobeequivalent(atleast\nunderthequadraticapproximation oftheobjectivefunction).Goingevenfurther,\nbytakinglogarithmsandusingtheseriesexpansionforlog(1+ x),wecanconclude\nthatifall λ iaresmall(thatis,  λ i1and λ i /α1)then\nτ≈1\n α, (7.44)\nα≈1\nτ . (7.45)\nThatis,undertheseassumptions,thenumberoftrainingiterations τplaysarole\ninverselyproportionaltothe L2regularizationparameter,andtheinverseof τ \nplaystheroleoftheweightdecaycoeﬃcient.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 617, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 764}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0618_0da6909d", "text": "(7.45)\nThatis,undertheseassumptions,thenumberoftrainingiterations τplaysarole\ninverselyproportionaltothe L2regularizationparameter,andtheinverseof τ \nplaystheroleoftheweightdecaycoeﬃcient. Parametervaluescorrespondingtodirectionsofsigniﬁcantcurvature(ofthe\nobjectivefunction)areregularizedlessthandirectionsoflesscurvature.Ofcourse,\ninthecontextofearlystopping,thisreallymeansthatparametersthatcorrespond\ntodirectionsofsigniﬁcantcurvaturetendtolearnearlyrelativetoparameters\ncorrespondingtodirectionsoflesscurvature.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 618, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 517}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0619_ca52bbf9", "text": "Thederivationsinthissectionhaveshownthatatrajectoryoflength τends\natapointthatcorrespondstoaminimumofthe L2-regularizedobjective.Early\nstoppingisofcoursemorethanthemererestrictionofthetrajectorylength;\ninstead,earlystoppingtypicallyinvolvesmonitoringthevalidationseterrorin\nordertostopthetrajectoryataparticularlygoodpointinspace.Earlystopping\nthereforehastheadvantageoverweightdecaythatearlystoppingautomatically\ndeterminesthecorrectamountofregularizationwhileweightdecayrequiresmany\ntrainingexperimentswithdiﬀerentvaluesofitshyperparameter. 2 5 2\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7.9ParameterTyingandParameterSharing\nThusfar,inthischapter,whenwehavediscussedaddingconstraintsorpenalties\ntotheparameters,wehavealwaysdonesowithrespecttoaﬁxedregionorpoint. Forexample, L2regularization(orweightdecay)penalizesmodelparametersfor\ndeviatingfromtheﬁxedvalueofzero.However,sometimeswemayneedother\nwaystoexpressourpriorknowledgeaboutsuitablevaluesofthemodelparameters.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 619, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 967}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0620_0484c9e1", "text": "Sometimeswemightnotknowpreciselywhatvaluestheparametersshouldtake\nbutweknow,fromknowledgeofthedomainandmodelarchitecture, thatthere\nshouldbesomedependencies betweenthemodelparameters. Acommontypeofdependencythatweoftenwanttoexpressisthatcertain\nparametersshouldbeclosetooneanother.Considerthefollowingscenario:we\nhavetwomodelsperformingthesameclassiﬁcationtask(withthesamesetof\nclasses)butwithsomewhatdiﬀerentinputdistributions.Formally,wehavemodel\nAwithparametersw( ) Aandmodel Bwithparametersw( ) B.Thetwomodels\nmaptheinput totwo diﬀerent, but related outputs:ˆ y( ) A= f(w( ) A,x)and\nˆ y( ) B= ( gw( ) B,x). Letusimaginethatthetasksaresimilarenough(perhapswithsimilarinput\nandoutputdistributions)thatwebelievethemodelparametersshouldbeclose\ntoeachother: ∀ i, w( ) A\nishouldbecloseto w( ) B\ni.Wecanleveragethisinformation\nthroughregularization. Speciﬁcally,wecanuseaparameternormpenaltyofthe\nform: Ω(w( ) A,w( ) B)=w( ) A−w( ) B2\n2. Hereweusedan L2penalty,butother\nchoicesarealsopossible.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 620, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0621_de429315", "text": "Speciﬁcally,wecanuseaparameternormpenaltyofthe\nform: Ω(w( ) A,w( ) B)=w( ) A−w( ) B2\n2. Hereweusedan L2penalty,butother\nchoicesarealsopossible. Thiskindofapproachwasproposedby (),whoregularized Lasserreetal.2006\ntheparametersofonemodel,trainedasaclassiﬁerinasupervisedparadigm,to\nbeclosetotheparametersofanothermodel,trainedinanunsupervisedparadigm\n(tocapturethedistributionoftheobservedinputdata).Thearchitectures were\nconstructedsuchthatmanyoftheparametersintheclassiﬁermodelcouldbe\npairedtocorrespondingparametersintheunsupervisedmodel.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 621, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 541}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0622_3ed43ec7", "text": "Whileaparameternormpenaltyisonewaytoregularizeparameterstobe\nclosetooneanother,themorepopularwayistouseconstraints:toforcesets\nofparameterstobeequal.Thismethodofregularizationisoftenreferredtoas\nparametersharing,becauseweinterpretthevariousmodelsormodelcomponents\nassharingauniquesetofparameters.Asigniﬁcantadvantageofparametersharing\noverregularizingtheparameterstobeclose(viaanormpenalty)isthatonlya\nsubsetoftheparameters(theuniqueset)needtobestoredinmemory.Incertain\nmodels—suchastheconvolutionalneuralnetwork—thiscanleadtosigniﬁcant\nreductioninthememoryfootprintofthemodel. 2 5 3\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nConvolutionalNeuralNetworksByfarthemostpopularandextensiveuse\nofparametersharingoccursinconvolutionalneuralnetworks(CNNs)applied\ntocomputervision. Naturalimageshavemanystatisticalpropertiesthatareinvarianttotranslation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 622, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 842}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0623_05723e53", "text": "Naturalimageshavemanystatisticalpropertiesthatareinvarianttotranslation. Forexample,aphotoofacatremainsaphotoofacatifitistranslatedonepixel\ntotheright.CNNstakethispropertyintoaccountbysharingparametersacross\nmultipleimagelocations.Thesamefeature(ahiddenunitwiththesameweights)\niscomputedoverdiﬀerentlocationsintheinput.Thismeansthatwecanﬁnda\ncatwiththesamecatdetectorwhetherthecatappearsatcolumn iorcolumn\ni+1intheimage. ParametersharinghasallowedCNNstodramaticallylowerthenumberofunique\nmodelparametersandtosigniﬁcantlyincreasenetworksizeswithoutrequiringa\ncorrespondingincreaseintrainingdata. Itremainsoneofthebestexamplesof\nhowtoeﬀectivelyincorporatedomainknowledgeintothenetworkarchitecture. CNNswillbediscussedinmoredetailinchapter.9\n7.10SparseRepresentations\nWeightdecayactsbyplacingapenaltydirectlyonthemodelparameters.Another\nstrategyistoplaceapenaltyontheactivationsoftheunitsinaneuralnetwork,\nencouragingtheiractivationstobesparse.Thisindirectlyimposesacomplicated\npenaltyonthemodelparameters.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 623, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1003}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0624_70021537", "text": "Wehave alreadydiscussed (insection)how7.1.2 L1penalizationinduces\nasparseparametrization—meaning thatmanyoftheparametersbecomezero\n(orcloseto zero).Representationalsparsity, on theother hand, des cribesa\nrepresentationwheremanyoftheelementsoftherepresentationarezero(orclose\ntozero).Asimpliﬁedviewofthisdistinctioncanbeillustratedinthecontextof\nlinearregression:\n\n18\n5\n15\n−9\n−3\n=\n400 20 0 −\n00 10 3 0 −\n050 0 0 0\n100 10 4 − −\n100 0 50 −\n\n2\n3\n−2\n−5\n1\n4\n\ny∈ RmA∈ Rm n×x∈ Rn(7.46)\n2 5 4\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\n\n−14\n1\n19\n2\n23\n=\n3 12 54 1 − −\n4 2 3 11 3 − −\n− − − 15 4 2 3 2\n3 1 2 30 3 − −\n− − − − 54 22 5 1\n\n0\n2\n0\n0\n−3\n0\n\ny∈ RmB∈ Rm n×h∈ Rn(7.47)\nIntheﬁrstexpression,wehaveanexampleofasparselyparametrized linear\nregressionmodel.Inthesecond,wehavelinearregressionwithasparserepresenta-\ntionhofthedatax.Thatis,hisafunctionofxthat,insomesense,represents\ntheinformationpresentin,butdoessowithasparsevector.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 624, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 995}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0625_15c1ace9", "text": "x\nRepresentationalregularizationisaccomplishedbythesamesortsofmechanisms\nthatwehaveusedinparameterregularization. Normpenaltyregularizationofrepresentationsisperformedbyaddingtothe\nlossfunction Janormpenaltyontherepresentation.Thispenaltyisdenoted\nΩ()h.Asbefore,wedenotetheregularizedlossfunctionby˜ J:\n˜ J , J , α (;θXy) = (;θXy)+Ω()h (7.48)\nwhere α∈[0 ,∞)weightstherelativecontributionofthenormpenaltyterm,with\nlargervaluesofcorrespondingtomoreregularization. α\nJustasan L1penaltyontheparametersinducesparametersparsity,an L1\npenaltyontheelementsoftherepresentationinducesrepresentationalsparsity:\nΩ(h) =||||h 1=\ni| h i|. Ofcourse,the L1penaltyisonlyonechoiceofpenalty\nthatcanresultinasparserepresentation.Othersincludethepenaltyderivedfrom\naStudent- tpriorontherepresentation( ,;,) OlshausenandField1996Bergstra2011\nandKLdivergencepenalties( ,)thatareespecially LarochelleandBengio2008\nusefulforrepresentationswithelementsconstrainedtolieontheunitinterval.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 625, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 960}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0626_f5248a3a", "text": "Lee2008Goodfellow 2009 etal.()and etal.()bothprovideexamplesofstrategies\nbasedonregularizingtheaverageactivationacrossseveralexamples,1\nm\nih( ) i,to\nbenearsometargetvalue,suchasavectorwith.01foreachentry. Otherapproachesobtainrepresentationalsparsitywithahardconstrainton\ntheactivationvalues.Forexample,orthogonalmatchingpursuit(Patietal.,\n1993)encodesaninputxwiththerepresentationhthatsolvestheconstrained\noptimization problem\nargmin\nh h , 0 < k− xWh2, (7.49)\nwhere h 0isthenumberofnon-zeroentriesofh. Thisproblemcanbesolved\neﬃcientlywhenWisconstrainedtobeorthogonal.Thismethodisoftencalled\n2 5 5\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nOMP- kwiththevalueof kspeciﬁedtoindicatethenumberofnon-zerofeatures\nallowed. ()demonstratedthatOMP-canbeaveryeﬀective CoatesandNg2011 1\nfeatureextractorfordeeparchitectures. Essentiallyanymodelthathashiddenunitscanbemadesparse.Throughout\nthisbook,wewillseemanyexamplesofsparsityregularizationusedinavarietyof\ncontexts.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 626, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 962}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0627_25bec2c7", "text": "Essentiallyanymodelthathashiddenunitscanbemadesparse.Throughout\nthisbook,wewillseemanyexamplesofsparsityregularizationusedinavarietyof\ncontexts. 7.11BaggingandOtherEnsembleMethods\nBagging(shortforbootstrapaggregating)isatechniqueforreducinggen-\neralizationerrorbycombiningseveralmodels(,).Theideaisto Breiman1994\ntrainseveraldiﬀerentmodelsseparately,thenhaveallofthemodelsvoteonthe\noutputfortestexamples.Thisisanexampleofageneralstrategyinmachine\nlearningcalledmodelaveraging.Techniquesemployingthisstrategyareknown\nasensemblemethods. Thereasonthatmodelaveragingworksisthatdiﬀerentmodelswillusually\nnotmakeallthesameerrorsonthetestset. Considerforexampleasetof kregressionmodels.Supposethateachmodel\nmakesanerror  ioneachexample, withtheerrorsdrawnfromazero-mean\nmultivariatenormaldistributionwithvariances E[ 2\ni] = vandcovariances E[  i  j] =\nc.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 627, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 851}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0628_2c3206c3", "text": "Thentheerrormadebytheaveragepredictionofalltheensemblemodelsis\n1\nk\ni  i.Theexpectedsquarederroroftheensemblepredictoris\nE\n\n1\nk\ni i2\n=1\nk2E\n\ni\n 2\ni+\nj i= i  j\n\n(7.50)\n=1\nkv+k−1\nkc . (7.51)\nInthecasewheretheerrorsareperfectlycorrelatedand c= v,themeansquared\nerrorreducesto v,sothemodelaveragingdoesnothelpatall.Inthecasewhere\ntheerrorsareperfectlyuncorrelated and c= 0,theexpectedsquarederrorofthe\nensembleisonly1\nkv.Thismeansthattheexpectedsquarederroroftheensemble\ndecreaseslinearlywiththeensemblesize.Inotherwords,onaverage,theensemble\nwillperformatleastaswellasanyofitsmembers,andifthemembersmake\nindependenterrors,theensemblewillperformsigniﬁcantlybetterthanitsmembers. Diﬀerentensemblemethodsconstructtheensembleofmodelsindiﬀerentways.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 628, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 764}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0629_ebd7f42d", "text": "Diﬀerentensemblemethodsconstructtheensembleofmodelsindiﬀerentways. Forexample,eachmemberoftheensemblecouldbeformedbytrainingacompletely\n2 5 6\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\n8\n8F i r s t   e nse m b l e   m e m b e r\nSe c ond e nse m b l e   m e m b e rO r i gi nal   data s e t\nF i r s t   r e s am pl e d   d a t a s e t\nSe c ond re s am p l e d   d a t a s e t\nFigure7.5:Acartoondepictionofhowbaggingworks.Supposewetrainan8detectoron\nthedatasetdepictedabove,containingan8,a6anda9.Supposewemaketwodiﬀerent\nresampleddatasets.Thebaggingtrainingprocedureistoconstructeachofthesedatasets\nbysamplingwithreplacement.Theﬁrstdatasetomitsthe9andrepeatsthe8.Onthis\ndataset,thedetectorlearnsthataloopontopofthedigitcorrespondstoan8.On\ntheseconddataset,werepeatthe9andomitthe6.Inthiscase,thedetectorlearns\nthatalooponthebottomofthedigitcorrespondstoan8.Eachoftheseindividual\nclassiﬁcationrulesisbrittle,butifweaveragetheiroutputthenthedetectorisrobust,\nachievingmaximalconﬁdenceonlywhenbothloopsofthe8arepresent.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 629, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1009}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0630_1c60e035", "text": "diﬀerentkindofmodelusingadiﬀerentalgorithmorobjectivefunction.Bagging\nisamethodthatallowsthesamekindofmodel,trainingalgorithmandobjective\nfunctiontobereusedseveraltimes.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 630, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 169}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0631_6ad595c9", "text": "Speciﬁcally,bagginginvolvesconstructing kdiﬀerentdatasets.Eachdataset\nhasthesamenumberofexamplesastheoriginaldataset,buteachdatasetis\nconstructedbysamplingwithreplacementfromtheoriginaldataset.Thismeans\nthat,withhighprobability,eachdatasetismissingsomeoftheexamplesfromthe\noriginaldatasetandalsocontainsseveralduplicateexamples(onaveragearound\n2/3oftheexamplesfromtheoriginaldatasetarefoundintheresultingtraining\nset,ifithasthesamesizeastheoriginal).Model iisthentrainedondataset\ni.Thediﬀerencesbetweenwhichexamplesareincludedineachdatasetresultin\ndiﬀerencesbetweenthetrainedmodels.Seeﬁgureforanexample.7.5\nNeuralnetworksreachawideenoughvarietyofsolutionpointsthattheycan\noftenbeneﬁtfrommodelaveragingevenifallofthemodelsaretrainedonthesame\ndataset.Diﬀerencesinrandominitialization, randomselectionofminibatches,\ndiﬀerencesinhyperparameters,ordiﬀerentoutcomesofnon-determinis ticimple-\nmentationsofneuralnetworksareoftenenoughtocausediﬀerentmembersofthe\n2 5 7\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nensembletomakepartiallyindependenterrors.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 631, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1040}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0632_1ad56ef0", "text": "Modelaveragingisanextremelypowerfulandreliablemethodforreducing\ngeneralization error.Itsuseisusuallydiscouragedwhenbenchmarkingalgorithms\nforscientiﬁcpapers,becauseanymachinelearningalgorithmcanbeneﬁtsubstan-\ntiallyfrommodelaveragingatthepriceofincreasedcomputationandmemory. Forthisreason,benchmarkcomparisonsareusuallymadeusingasinglemodel. Machinelearningcontestsareusuallywonbymethodsusingmodelaverag-\ningoverdozensofmodels.ArecentprominentexampleistheNetﬂixGrand\nPrize(Koren2009,). Notalltechniquesforconstructingensemblesaredesignedtomaketheensemble\nmoreregularizedthantheindividualmodels.Forexample,atechniquecalled\nboosting(FreundandSchapire1996ba,,)constructsanensemblewithhigher\ncapacitythantheindividualmodels.Boostinghasbeenappliedtobuildensembles\nofneuralnetworks(SchwenkandBengio1998,)byincrementallyaddingneural\nnetworkstotheensemble.Boostinghasalsobeenappliedinterpretinganindividual\nneuralnetworkasanensemble( ,),incrementallyaddinghidden Bengioetal.2006a\nunitstotheneuralnetwork.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 632, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 997}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0633_3f82762a", "text": "7.12Dropout\nDropout(Srivastava2014etal.,)providesacomputationally inexpensivebut\npowerfulmethodofregularizingabroadfamilyofmodels.Toaﬁrstapproximation,\ndropoutcanbethoughtofasamethodofmakingbaggingpracticalforensembles\nofverymanylargeneuralnetworks.Bagginginvolvestrainingmultiplemodels,\nandevaluatingmultiplemodelsoneachtestexample.Thisseemsimpractical\nwheneachmodelisalargeneuralnetwork,sincetrainingandevaluatingsuch\nnetworksiscostlyintermsofruntimeandmemory.Itiscommontouseensembles\nofﬁvetotenneuralnetworks— ()usedsixtowintheILSVRC— Szegedy etal.2014a\nbutmorethanthisrapidlybecomesunwieldy.Dropoutprovidesaninexpensive\napproximationtotrainingandevaluatingabaggedensembleofexponentiallymany\nneuralnetworks.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 633, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 710}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0634_0630f5f4", "text": "Speciﬁcally,dropouttrainstheensembleconsistingofallsub-networksthat\ncanbeformedbyremovingnon-outputunitsfromanunderlyingbasenetwork,\nasillustratedinﬁgure.Inmostmodernneuralnetworks,basedonaseriesof 7.6\naﬃnetransformationsandnonlinearities, wecaneﬀectivelyremoveaunitfroma\nnetworkbymultiplyingitsoutputvaluebyzero. Thisprocedurerequiressome\nslightmodiﬁcationformodelssuchasradialbasisfunctionnetworks,whichtake\n2 5 8\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nthediﬀerencebetweentheunit’sstateandsomereferencevalue.Here,wepresent\nthedropoutalgorithmintermsofmultiplication byzeroforsimplicity,butitcan\nbetriviallymodiﬁedtoworkwithotheroperationsthatremoveaunitfromthe\nnetwork.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 634, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 671}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0635_39098606", "text": "Recallthattolearnwithbagging,wedeﬁne kdiﬀerentmodels,construct k\ndiﬀerentdatasetsbysamplingfromthetrainingsetwithreplacement,andthen\ntrainmodel iondataset i.Dropoutaimstoapproximatethisprocess,butwithan\nexponentiallylargenumberofneuralnetworks.Speciﬁcally,totrainwithdropout,\nweuseaminibatch-bas edlearningalgorithmthatmakessmallsteps,suchas\nstochasticgradientdescent.Eachtimeweloadanexampleintoaminibatch,we\nrandomlysampleadiﬀerentbinarymasktoapplytoalloftheinputandhidden\nunitsinthenetwork.Themaskforeachunitissampledindependentlyfromallof\ntheothers.Theprobabilityofsamplingamaskvalueofone(causingaunittobe\nincluded)isahyperparameter ﬁxedbeforetrainingbegins. Itisnotafunction\nofthecurrentvalueofthemodelparametersortheinputexample.Typically,\naninputunitisincludedwithprobability0.8andahiddenunitisincludedwith\nprobability0.5.Wethenrunforwardpropagation, back-propagation,andthe\nlearningupdateasusual.Figureillustrateshowtorunforwardpropagation 7.7\nwithdropout.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 635, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 963}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0636_f4d65788", "text": "Moreformally,supposethatamaskvectorµspeciﬁeswhichunitstoinclude,\nand J(θµ ,)deﬁnesthecostofthemodeldeﬁnedbyparametersθandmaskµ.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 636, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 127}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0637_5c83325f", "text": "Thendropouttrainingconsistsinminimizing E µ J(θµ ,).Theexpectationcontains\nexponentiallymanytermsbutwecanobtainanunbiasedestimateofitsgradient\nbysamplingvaluesof.µ\nDropouttrainingisnotquitethesameasbaggingtraining.Inthecaseof\nbagging,themodelsareallindependent.Inthecaseofdropout,themodelsshare\nparameters,witheachmodelinheritingadiﬀerentsubsetofparametersfromthe\nparentneuralnetwork.Thisparametersharingmakesitpossibletorepresentan\nexponentialnumberofmodelswithatractableamountofmemory.Inthecaseof\nbagging,eachmodelistrainedtoconvergenceonitsrespectivetrainingset.Inthe\ncaseofdropout,typicallymostmodelsarenotexplicitlytrainedatall—usually,\nthemodelislargeenoughthatitwouldbeinfeasibletosampleallpossiblesub-\nnetworkswithinthelifetimeoftheuniverse.Instead,atinyfractionofthepossible\nsub-networksareeachtrainedforasinglestep,andtheparametersharingcauses\ntheremainingsub-networkstoarriveatgoodsettingsoftheparameters.These\naretheonlydiﬀerences.Beyondthese,dropoutfollowsthebaggingalgorithm.For\nexample,thetrainingsetencounteredbyeachsub-networkisindeedasubsetof\ntheoriginaltrainingsetsampledwithreplacement.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 637, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1106}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0638_a3e26790", "text": "2 5 9\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nyy\nh 1 h 1 h 2 h 2\nx 1 x 1 x 2 x 2yy\nh 1 h 1 h 2 h 2\nx 1 x 1 x 2 x 2yy\nh 1 h 1 h 2 h 2\nx 2 x 2yy\nh 1 h 1 h 2 h 2\nx 1 x 1yy\nh 2 h 2\nx 1 x 1 x 2 x 2\nyy\nh 1 h 1\nx 1 x 1 x 2 x 2yy\nh 1 h 1 h 2 h 2yy\nx 1 x 1 x 2 x 2yy\nh 2 h 2\nx 2 x 2\nyy\nh 1 h 1\nx 1 x 1yy\nh 1 h 1\nx 2 x 2yy\nh 2 h 2\nx 1 x 1yy\nx 1 x 1\nyy\nx 2 x 2yy\nh 2 h 2yy\nh 1 h 1yyB ase   ne t w or k\nE nse m bl e   of   s u b n e t w or k s\nFigure 7.6:Dropout trainsan ensemble consistingof allsub-networks that canbe\nconstructedbyremovingnon-outputunitsfromanunderlyingbasenetwork.Here,we\nbeginwithabasenetworkwithtwovisibleunitsandtwohiddenunits.Therearesixteen\npossiblesubsetsofthesefourunits.Weshowallsixteensubnetworksthatmaybeformed\nbydroppingoutdiﬀerentsubsetsofunitsfromtheoriginalnetwork.Inthissmallexample,\nalargeproportionoftheresultingnetworkshavenoinputunitsornopathconnecting\ntheinputtotheoutput.Thisproblembecomesinsigniﬁcantfornetworkswithwider\nlayers,wheretheprobabilityofdroppingallpossiblepathsfrominputstooutputsbecomes\nsmaller.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 638, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1038}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0639_713705b2", "text": "2 6 0\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nˆ x 1ˆ x 1\nµ x 1 µ x 1 x 1 x 1ˆ x 2ˆ x 2\nx 2 x 2 µ x 2 µ x 2h 1 h 1 h 2 h 2µ h 1 µ h 1 µ h 2 µ h 2ˆ h 1ˆ h 1ˆ h 2ˆ h 2yyyy\nh 1 h 1 h 2 h 2\nx 1 x 1 x 2 x 2\nFigure7.7:Anexampleofforwardpropagationthroughafeedforwardnetworkusing\ndropout. ( T o p )Inthisexample,weuseafeedforwardnetworkwithtwoinputunits,one\nhiddenlayerwithtwohiddenunits,andoneoutputunit.Toperformforward ( Bottom )\npropagationwithdropout,werandomlysampleavectorµwithoneentryforeachinput\norhiddenunitinthenetwork.Theentriesofµarebinaryandaresampledindependently\nfromeachother.Theprobabilityofeachentrybeingisahyperparameter,usually 1 0 .5\nforthehiddenlayersand0 .8fortheinput.Eachunitinthenetworkismultipliedby\nthecorrespondingmask,andthenforwardpropagationcontinuesthroughtherestofthe\nnetworkasusual.Thisisequivalenttorandomlyselectingoneofthesub-networksfrom\nﬁgureandrunningforwardpropagationthroughit.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 639, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 912}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0640_997b7e65", "text": "7.6\n2 6 1\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nTomakeaprediction,abaggedensemblemustaccumulatevotesfromallof\nitsmembers.Werefertothisprocessasinferenceinthiscontext. Sofar,our\ndescriptionofbagginganddropouthasnotrequiredthatthemodelbeexplicitly\nprobabilistic.Now,weassumethatthemodel’sroleistooutputaprobability\ndistribution.Inthecaseofbagging,eachmodel iproducesaprobabilitydistribution\np( ) i( y|x).Thepredictionoftheensembleisgivenbythearithmeticmeanofall\nofthesedistributions,\n1\nkk\ni = 1p( ) i( ) y|x . (7.52)\nInthecaseofdropout,eachsub-modeldeﬁnedbymaskvectorµdeﬁnesaprob-\nabilitydistribution p( y ,|xµ).Thearithmeticmeanoverallmasksisgiven\nby\nµp p y , ()µ(|xµ) (7.53)\nwhere p(µ)istheprobabilitydistributionthatwasusedtosampleµattraining\ntime.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 640, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 752}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0641_b5a064f2", "text": "Becausethissumincludesanexponentialnumberofterms,itisintractable\ntoevaluateexceptincaseswherethestructureofthemodelpermitssomeform\nofsimpliﬁcation.Sofar,deepneuralnetsarenotknowntopermitanytractable\nsimpliﬁcation.Instead, wecan approximatetheinferencewithsampling, by\naveragingtogethertheoutputfrommanymasks.Even10-20masksareoften\nsuﬃcienttoobtaingoodperformance. However,thereisanevenbetterapproach,thatallowsustoobtainagood\napproximationtothepredictionsoftheentireensemble,atthecostofonlyone\nforwardpropagation. Todoso,wechangetousingthegeometricmeanratherthan\nthearithmeticmeanoftheensemblemembers’predicteddistributions.Warde-\nFarley2014etal.()presentargumentsandempiricalevidencethatthegeometric\nmeanperformscomparablytothearithmeticmeaninthiscontext.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 641, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 756}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0642_4a267451", "text": "Thegeometricmeanofmultipleprobabilitydistributionsisnotguaranteedtobe\naprobabilitydistribution.Toguaranteethattheresultisaprobabilitydistribution,\nweimposetherequirementthatnoneofthesub-modelsassignsprobability0toany\nevent,andwerenormalizetheresultingdistribution.Theunnormalized probability\ndistributiondeﬁneddirectlybythegeometricmeanisgivenby\n˜ p e nse m bl e( ) = y|x 2d\nµp y , (|xµ) (7.54)\nwhere disthenumberofunitsthatmaybedropped.Hereweuseauniform\ndistributionoverµtosimplifythepresentation,butnon-uniformdistributionsare\n2 6 2\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nalsopossible.Tomakepredictionswemustre-normalizetheensemble:\np e nse m bl e( ) = y|x˜ p e nse m bl e( ) y|x\ny˜ p e nse m bl e( y|x).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 642, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 711}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0643_309d2ee5", "text": "(7.55)\nAkeyinsight( ,)involvedindropoutisthatwecanapproxi- Hintonetal.2012c\nmate p e nse m bl ebyevaluating p( y|x)inonemodel:themodelwithallunits,but\nwiththeweightsgoingoutofunit imultipliedbytheprobabilityofincludingunit\ni.Themotivationforthismodiﬁcationistocapturetherightexpectedvalueofthe\noutputfromthatunit.Wecallthisapproachtheweightscalinginferencerule. Thereisnotyetanytheoreticalargumentfortheaccuracyofthisapproximate\ninferenceruleindeepnonlinearnetworks,butempiricallyitperformsverywell. Becauseweusuallyuseaninclusionprobabilityof1\n2,theweightscalingrule\nusuallyamountstodividingtheweightsbyattheendoftraining,andthenusing 2 \nthemodelasusual.Anotherwaytoachievethesameresultistomultiplythe\nstatesoftheunitsbyduringtraining.Eitherway,thegoalistomakesurethat 2\ntheexpectedtotalinputtoaunitattesttimeisroughlythesameastheexpected\ntotalinputtothatunitattraintime,eventhoughhalftheunitsattraintimeare\nmissingonaverage.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 643, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 926}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0644_719543d2", "text": "Formanyclassesofmodelsthatdonothavenonlinearhiddenunits,theweight\nscalinginferenceruleisexact.Forasimpleexample,considerasoftmaxregression\nclassiﬁerwithinputvariablesrepresentedbythevector: n v\nP y (= y | v) = softmax\nWv+b\ny. (7.56)\nWecanindexintothefamilyofsub-modelsbyelement-wisemultiplicationofthe\ninputwithabinaryvector: d\nP y (= y | v;) = dsoftmax\nW( )+d vb\ny.(7.57)\nTheensemblepredictorisdeﬁnedbyre-normalizingthegeometricmeanoverall\nensemblemembers’predictions:\nP e nse m bl e(= ) =y y| v˜ P e nse m bl e(= )y y| v\ny˜ P e nse m bl e(= y y| v)(7.58)\nwhere\n˜ P e nse m bl e(= ) =y y| v2n\nd∈{} 0 1 ,nP y .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 644, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 622}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0645_079e4df2", "text": "(= y | v;)d (7.59)\n2 6 3\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nToseethattheweightscalingruleisexact,wecansimplify ˜ P e nse m bl e:\n˜ P e nse m bl e(= ) =y y| v2n\nd∈{} 0 1 ,nP y (= y | v;)d(7.60)\n= 2n\nd∈{} 0 1 ,nsoftmax (W( )+)d vby (7.61)\n= 2n\nd∈{} 0 1 ,nexp\nWy , :( )+d v b y\n\nyexp\nW\ny , :( )+d v b y (7.62)\n=2n\nd∈{} 0 1 ,nexp\nWy , :( )+d v b y\n2n\nd∈{} 0 1 ,n\nyexp\nW\ny , :( )+d v b y(7.63)\nBecause˜ Pwillbenormalized,wecansafelyignoremultiplication byfactorsthat\nareconstantwithrespectto: y\n˜ P e nse m bl e(= ) y y| v∝2n\nd∈{} 0 1 ,nexp\nWy , :( )+d v b y\n(7.64)\n= exp\n1\n2n\nd∈{} 0 1 ,nW\ny , :( )+d v b y\n (7.65)\n= exp1\n2W\ny , : v+ b y\n. (7.66)\nSubstitutingthisbackintoequationweobtainasoftmaxclassiﬁerwithweights 7.58\n1\n2W. Theweightscalingruleisalsoexactinothersettings,includingregression\nnetworkswithconditionallynormaloutputs,anddeepnetworksthathavehidden\nlayerswithoutnonlinearities.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 645, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 945}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0646_732935df", "text": "Theweightscalingruleisalsoexactinothersettings,includingregression\nnetworkswithconditionallynormaloutputs,anddeepnetworksthathavehidden\nlayerswithoutnonlinearities. However,theweightscalingruleisonlyanapproxi-\nmationfordeepmodelsthathavenonlinearities. Thoughtheapproximationhas\nnotbeentheoreticallycharacterized, itoftenworkswell,empirically.Goodfellow\netal.()foundexperimentallythattheweightscalingapproximationcanwork 2013a\nbetter(intermsofclassiﬁcationaccuracy)thanMonteCarloapproximations tothe\nensemblepredictor.ThisheldtrueevenwhentheMonteCarloapproximationwas\nallowedtosampleupto1,000sub-networks. ()found GalandGhahramani2015\nthatsomemodelsobtainbetterclassiﬁcationaccuracyusingtwentysamplesand\n2 6 4\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\ntheMonteCarloapproximation.Itappearsthattheoptimalchoiceofinference\napproximationisproblem-dependent.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 646, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 850}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0647_621101dc", "text": "Srivastava2014etal.()showedthatdropoutismoreeﬀectivethanother\nstandardcomputationally inexpensiveregularizers,suchasweightdecay,ﬁlter\nnormconstraintsandsparseactivityregularization. Dropoutmayalsobecombined\nwithotherformsofregularizationtoyieldafurtherimprovement. Oneadvantageofdropoutisthatitisverycomputationally cheap.Using\ndropoutduringtrainingrequiresonly O( n)computationperexampleperupdate,\ntogenerate nrandombinarynumbersandmultiplythembythestate.Depending\nontheimplementation,itmayalsorequire O( n)memorytostorethesebinary\nnumbersuntiltheback-propagationstage.Runninginferenceinthetrainedmodel\nhasthesamecostper-exampleasifdropoutwerenotused,thoughwemustpay\nthecostofdividingtheweightsby2oncebeforebeginningtoruninferenceon\nexamples.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 647, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 743}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0648_e7c8bfc8", "text": "Anothersigniﬁcantadvantageofdropoutisthatitdoesnotsigniﬁcantlylimit\nthetypeofmodelortrainingprocedurethatcanbeused.Itworkswellwithnearly\nanymodelthatusesadistributedrepresentationandcanbetrainedwithstochastic\ngradientdescent.Thisincludesfeedforwardneuralnetworks,probabilisticmodels\nsuchasrestrictedBoltzmannmachines(Srivastava2014etal.,),andrecurrent\nneuralnetworks(BayerandOsendorfer2014Pascanu2014a ,; etal.,).Manyother\nregularizationstrategiesofcomparablepowerimposemoresevererestrictionson\nthearchitectureofthemodel.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 648, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 521}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0649_fbd14d47", "text": "Thoughthecostper-stepofapplyingdropouttoaspeciﬁcmodelisnegligible,\nthecostofusingdropoutinacompletesystemcanbesigniﬁcant.Becausedropout\nisaregularizationtechnique,itreducestheeﬀectivecapacityofamodel.Tooﬀset\nthiseﬀect,wemustincreasethesizeofthemodel.Typicallytheoptimalvalidation\nseterrorismuchlowerwhenusingdropout,butthiscomesatthecostofamuch\nlargermodelandmanymoreiterationsofthetrainingalgorithm.Forverylarge\ndatasets,regularizationconferslittlereductioningeneralization error. Inthese\ncases,thecomputational costofusingdropoutandlargermodelsmayoutweigh\nthebeneﬁtofregularization. Whenextremelyfewlabeledtrainingexamplesareavailable,dropoutisless\neﬀective.Bayesian neuralnetworks(, )outperform dropout onthe Neal1996\nAlternativeSplicingDataset(,)wherefewerthan5,000examples Xiongetal.2011\nareavailable(Srivastava2014etal.,).Whenadditionalunlabeleddataisavailable,\nunsupervisedfeaturelearningcangainanadvantageoverdropout.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 649, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 925}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0650_7e6011cc", "text": "Wager2013etal.()showedthat,whenappliedtolinearregression,dropout\nisequivalentto L2weightdecay,withadiﬀerentweightdecaycoeﬃcientfor\n2 6 5\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\neachinputfeature.Themagnitudeofeachfeature’sweightdecaycoeﬃcientis\ndeterminedbyitsvariance.Similarresultsholdforotherlinearmodels.Fordeep\nmodels,dropoutisnotequivalenttoweightdecay. Thestochasticityusedwhiletrainingwithdropoutisnotnecessaryforthe\napproach’ssuccess.Itisjustameansofapproximating thesumoverallsub-\nmodels.WangandManning2013()derivedanalyticalapproximationstothis\nmarginalization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 650, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 570}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0651_5132e5c6", "text": "Theirapproximation,knownasfastdropoutresultedinfaster\nconvergencetimeduetothereducedstochasticityinthecomputationofthe\ngradient.Thismethodcanalsobeappliedattesttime,asamoreprincipled\n(butalsomorecomputationally expensive)approximation totheaverageoverall\nsub-networksthantheweightscalingapproximation.Fastdropouthasbeenused\ntonearlymatchtheperformanceofstandarddropoutonsmallneuralnetwork\nproblems,buthasnotyetyieldedasigniﬁcantimprovementorbeenappliedtoa\nlargeproblem.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 651, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 469}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0652_0ada108b", "text": "Justasstochasticityisnotnecessarytoachievetheregularizing eﬀect of\ndropout,itisalsonotsuﬃcient.Todemonstratethis,Warde-Farley2014etal.()\ndesignedcontrolexperimentsusingamethodcalleddropoutboostingthatthey\ndesignedtouseexactlythesamemasknoiseastraditionaldropoutbutlack\nitsregularizingeﬀect.Dropoutboostingtrainstheentireensembletojointly\nmaximizethelog-likelihoodonthetrainingset.Inthesamesensethattraditional\ndropoutisanalogoustobagging, this approachisanalogoustoboosting.As\nintended,experimentswithdropoutboostingshowalmostnoregularizationeﬀect\ncomparedtotrainingtheentirenetworkasasinglemodel.Thisdemonstratesthat\ntheinterpretationofdropoutasbagginghasvaluebeyondtheinterpretationof\ndropoutasrobustnesstonoise.Theregularizationeﬀectofthebaggedensembleis\nonlyachievedwhenthestochasticallysampledensemblemembersaretrainedto\nperformwellindependently ofeachother. Dropouthasinspiredotherstochasticapproachestotrainingexponentially\nlargeensemblesofmodelsthatshareweights.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 652, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 970}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0653_e388c3d1", "text": "Dropouthasinspiredotherstochasticapproachestotrainingexponentially\nlargeensemblesofmodelsthatshareweights. DropConnectisaspecialcaseof\ndropoutwhereeachproductbetweenasinglescalarweightandasinglehidden\nunitstateisconsideredaunitthatcanbedropped(Wan2013etal.,).Stochastic\npoolingisaformofrandomizedpooling(seesection)forbuildingensembles 9.3\nofconvolutionalnetworkswitheachconvolutionalnetworkattendingtodiﬀerent\nspatiallocationsofeachfeaturemap. Sofar,dropoutremainsthemostwidely\nusedimplicitensemblemethod. Oneofthekeyinsightsofdropoutisthattraininganetworkwithstochastic\nbehaviorandmakingpredictionsbyaveragingovermultiplestochasticdecisions\nimplementsaformofbaggingwithparametersharing.Earlier, wedescribed\n2 6 6\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\ndropoutas bagginganensembleofmodelsformedbyincludingor excluding\nunits.However,thereisnoneedforthismodelaveragingstrategytobebasedon\ninclusionandexclusion.Inprinciple,anykindofrandommodiﬁcationisadmissible.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 653, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 960}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0654_378faaa3", "text": "Inpractice,wemustchoosemodiﬁcationfamiliesthatneuralnetworksareable\ntolearntoresist.Ideally,weshouldalsousemodelfamiliesthatallowafast\napproximateinferencerule.Wecanthinkofanyformofmodiﬁcationparametrized\nbyavectorµastraininganensembleconsistingof p( y ,|xµ)forallpossible\nvaluesofµ.Thereisnorequirementthatµhaveaﬁnitenumberofvalues.For\nexample,µcanbereal-valued.Srivastava2014etal.()showedthatmultiplyingthe\nweightsbyµ∼N( 1 , I)canoutperformdropoutbasedonbinarymasks.Because\nE[µ] = 1thestandardnetworkautomatically implementsapproximate inference\nintheensemble,withoutneedinganyweightscaling. Sofarwehavedescribeddropoutpurelyasameansofperformingeﬃcient,\napproximatebagging.However,thereisanotherviewofdropoutthatgoesfurther\nthanthis.Dropouttrainsnotjustabaggedensembleofmodels,butanensemble\nofmodelsthatsharehiddenunits.Thismeanseachhiddenunitmustbeableto\nperformwellregardlessofwhichotherhiddenunitsareinthemodel.Hiddenunits\nmustbepreparedtobeswappedandinterchangedbetweenmodels.Hintonetal.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 654, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 993}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0655_ad4a81b9", "text": "()wereinspiredbyanideafrombiology:sexualreproduction,whichinvolves 2012c\nswappinggenesbetweentwodiﬀerentorganisms,createsevolutionarypressurefor\ngenestobecomenotjustgood,buttobecomereadilyswappedbetweendiﬀerent\norganisms.Suchgenesandsuchfeaturesareveryrobusttochangesintheir\nenvironmentbecausetheyarenotabletoincorrectlyadapttounusualfeatures\nofanyoneorganismormodel.Dropoutthusregularizeseachhiddenunittobe\nnotmerelyagoodfeaturebutafeaturethatisgoodinmanycontexts. Warde-\nFarley2014etal.()compareddropouttrainingtotrainingoflargeensemblesand\nconcludedthatdropoutoﬀersadditionalimprovementstogeneralization error\nbeyondthoseobtainedbyensemblesofindependentmodels.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 655, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 663}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0656_63114677", "text": "Warde-\nFarley2014etal.()compareddropouttrainingtotrainingoflargeensemblesand\nconcludedthatdropoutoﬀersadditionalimprovementstogeneralization error\nbeyondthoseobtainedbyensemblesofindependentmodels. Itisimportanttounderstandthatalargeportionofthepowerofdropout\narisesfromthefactthatthemaskingnoiseisappliedtothehiddenunits.This\ncanbeseenasaformofhighlyintelligent,adaptivedestructionoftheinformation\ncontentoftheinputratherthandestructionoftherawvaluesoftheinput.For\nexample,ifthemodellearnsahiddenunit h ithatdetectsafacebyﬁndingthenose,\nthendropping h icorrespondstoerasingtheinformationthatthereisanosein\ntheimage.Themodelmustlearnanother h i,eitherthatredundantlyencodesthe\npresenceofanose,orthatdetectsthefacebyanotherfeature,suchasthemouth.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 656, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 745}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0657_ebe351f8", "text": "Traditionalnoiseinjectiontechniquesthataddunstructurednoiseattheinputare\nnotabletorandomlyerasetheinformationaboutanosefromanimageofaface\nunlessthemagnitudeofthenoiseissogreatthatnearlyalloftheinformationin\n2 6 7\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\ntheimageisremoved.Destroyingextractedfeaturesratherthanoriginalvalues\nallowsthedestructionprocesstomakeuseofalloftheknowledgeabouttheinput\ndistributionthatthemodelhasacquiredsofar. Anotherimportantaspectofdropoutisthatthenoiseismultiplicative. Ifthe\nnoisewereadditivewithﬁxedscale,thenarectiﬁedlinearhiddenunit h iwith\naddednoise couldsimplylearntohave h ibecomeverylargeinordertomake\ntheaddednoise insigniﬁcantbycomparison.Multiplicativenoisedoesnotallow\nsuchapathologicalsolutiontothenoiserobustnessproblem.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 657, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 763}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0658_ed57f102", "text": "Anotherdeeplearningalgorithm,batchnormalization, reparametrizes themodel\ninawaythatintroducesbothadditiveandmultiplicativenoiseonthehidden\nunitsattrainingtime.Theprimarypurposeofbatchnormalization istoimprove\noptimization, butthenoisecanhavearegularizingeﬀect,andsometimesmakes\ndropoutunnecessary.Batchnormalization isdescribedfurtherinsection.8.7.1\n7.13AdversarialTraining\nInmanycases,neuralnetworkshavebeguntoreachhumanperformancewhen\nevaluatedonani.i.d.testset.Itisnaturalthereforetowonderwhetherthese\nmodelshaveobtainedatruehuman-levelunderstandingofthesetasks.Inorder\ntoprobethelevelofunderstandinganetworkhasoftheunderlyingtask,wecan\nsearchforexamplesthatthemodelmisclassiﬁes.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 658, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 682}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0659_5da0f5a8", "text": "()foundthat Szegedy etal.2014b\nevenneuralnetworksthatperformathumanlevelaccuracyhaveanearly100%\nerrorrateonexamplesthatareintentionallyconstructedbyusinganoptimization\nproceduretosearchforaninputxnearadatapointxsuchthatthemodel\noutputisverydiﬀerentatx.Inmanycases,xcanbesosimilartoxthata\nhumanobservercannottellthediﬀerencebetweentheoriginalexampleandthe\nadversarialexample,butthenetworkcanmakehighlydiﬀerentpredictions.See\nﬁgureforanexample.7.8\nAdversarialexampleshavemanyimplications,forexample,incomputersecurity,\nthatarebeyondthescopeofthischapter. However,theyareinterestinginthe\ncontextofregularizationbecauseonecanreducetheerrorrateontheoriginali.i.d. testsetviaadversarialtraining—trainingonadversariallyperturbedexamples\nfromthetrainingset( ,; Szegedy etal.2014bGoodfellow2014betal.,). Goodfellow2014betal.()showedthatoneoftheprimarycausesofthese\nadversarial examplesis excessive linearity.Neural networks arebuilt out of\nprimarilylinearbuildingblocks.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 659, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 964}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0660_fdabdc7b", "text": "Goodfellow2014betal.()showedthatoneoftheprimarycausesofthese\nadversarial examplesis excessive linearity.Neural networks arebuilt out of\nprimarilylinearbuildingblocks. Insomeexperimentstheoverallfunctionthey\nimplementprovestobehighlylinearasaresult.Theselinearfunctionsareeasy\n2 6 8\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\n+ .007× =\nx sign(∇ x J(θx , , y))x+\nsign(∇ x J(θx , , y))\ny=“panda” “nematode”“gibbon”\nw/57.7%\nconﬁdencew/8.2%\nconﬁdencew/99.3%\nconﬁdence\nFigure7.8: AdemonstrationofadversarialexamplegenerationappliedtoGoogLeNet\n( ,)onImageNet.Byaddinganimperceptiblysmallvectorwhose Szegedy e t a l .2014a\nelementsareequaltothesignoftheelementsofthegradientofthecostfunctionwith\nrespecttotheinput,wecanchangeGoogLeNet’sclassiﬁcationoftheimage.Reproduced\nwithpermissionfrom ().", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 660, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 782}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0661_4759c034", "text": "Goodfellow e t a l .2014b\ntooptimize.Unfortunately,thevalueofalinearfunctioncanchangeveryrapidly\nifithasnumerousinputs.Ifwechangeeachinputby ,thenalinearfunction\nwithweightswcanchangebyasmuchas ||||w 1,whichcanbeaverylarge\namountifwishigh-dimensional.Adversarialtrainingdiscouragesthishighly\nsensitivelocallylinearbehaviorbyencouragingthenetworktobelocallyconstant\nintheneighborhoodofthetrainingdata.Thiscanbeseenasawayofexplicitly\nintroducingalocalconstancypriorintosupervisedneuralnets. Adversarialtraininghelpstoillustratethepowerofusingalargefunction\nfamilyincombinationwithaggressiveregularization. Purelylinearmodels,like\nlogisticregression,arenotabletoresistadversarialexamplesbecausetheyare\nforcedtobelinear.Neuralnetworksareabletorepresentfunctionsthatcanrange\nfromnearlylineartonearlylocallyconstantandthushavetheﬂexibilitytocapture\nlineartrendsinthetrainingdatawhilestilllearningtoresistlocalperturbation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 661, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 918}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0662_9c97dafb", "text": "Adversarialexamplesalsoprovideameansofaccomplishingsemi-supervised\nlearning.Atapointxthatisnotassociatedwithalabelinthedataset,the\nmodelitselfassignssomelabel ˆ y.Themodel’slabel ˆ ymaynotbethetruelabel,\nbutifthemodelishighquality,thenˆ yhasahighprobabilityofprovidingthe\ntruelabel.Wecanseekanadversarialexamplexthatcausestheclassiﬁerto\noutputalabel ywith y=ˆ y.Adversarialexamplesgeneratedusingnotthetrue\nlabelbutalabelprovidedbyatrainedmodelarecalledvirtualadversarial\nexamples(Miyato2015etal.,).Theclassiﬁermaythenbetrainedtoassignthe\nsamelabeltoxandx.Thisencouragestheclassiﬁertolearnafunctionthatis\n2 6 9\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nrobusttosmallchangesanywherealongthemanifoldwheretheunlabeleddata\nlies.Theassumptionmotivatingthisapproachisthatdiﬀerentclassesusuallylie\nondisconnectedmanifolds,andasmallperturbationshouldnotbeabletojump\nfromoneclassmanifoldtoanotherclassmanifold.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 662, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 902}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0663_6f980fa0", "text": "7.14Tangent Distance, TangentProp,and Manifold\nTangentClassiﬁer\nManymachinelearningalgorithmsaimtoovercomethecurseofdimensionality\nbyassumingthatthedataliesnearalow-dimensional manifold,asdescribedin\nsection.5.11.3\nOneoftheearlyattemptstotakeadvantageofthemanifoldhypothesisisthe\ntangentdistancealgorithm( ,,).Itisanon-parametric Simard etal.19931998\nnearest-neighboralgorithminwhichthemetricusedisnotthegenericEuclidean\ndistancebutonethatisderivedfromknowledgeofthemanifoldsnearwhich\nprobabilityconcentrates.Itisassumedthatwearetryingtoclassifyexamplesand\nthatexamplesonthesamemanifoldsharethesamecategory.Sincetheclassiﬁer\nshouldbeinvarianttothelocalfactorsofvariationthatcorrespondtomovement\nonthemanifold,itwouldmakesensetouseasnearest-neighbordistancebetween\npointsx 1andx 2thedistancebetweenthemanifolds M 1and M 2towhichthey\nrespectivelybelong.Althoughthatmaybecomputationally diﬃcult(itwould\nrequiresolvinganoptimization problem,toﬁndthenearestpairofpointson M 1\nand M 2),acheapalternativethatmakessenselocallyistoapproximate M ibyits\ntangentplaneatx iandmeasurethedistancebetweenthetwotangents,orbetween\natangentplaneandapoint.Thatcanbeachievedbysolvingalow-dimensional\nlinearsystem(inthedimensionofthemanifolds).Ofcourse,thisalgorithmrequires\nonetospecifythetangentvectors.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 663, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1283}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0664_8f5c317d", "text": "Inarelatedspirit,thetangentpropalgorithm( ,)(ﬁgure) Simardetal.19927.9\ntrainsaneuralnetclassiﬁerwithanextrapenaltytomakeeachoutput f(x)of\ntheneuralnetlocallyinvarianttoknownfactorsofvariation.Thesefactorsof\nvariationcorrespondtomovementalongthemanifoldnearwhichexamplesofthe\nsameclassconcentrate.Localinvarianceisachievedbyrequiring ∇ x f(x)tobe\northogonaltotheknownmanifoldtangentvectorsv( ) iatx,orequivalentlythat\nthedirectionalderivativeof fatxinthedirectionsv( ) ibesmallbyaddinga\nregularizationpenalty:Ω\nΩ() = f\ni\n(∇ x f())xv( ) i2\n. (7.67)\n2 7 0\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nThisregularizercanofcoursebescaledbyanappropriatehyperparameter, and,for\nmostneuralnetworks,wewouldneedtosumovermanyoutputsratherthanthelone\noutput f(x)describedhereforsimplicity.Aswiththetangentdistancealgorithm,\nthetangentvectorsarederivedapriori,usuallyfromtheformalknowledgeof\ntheeﬀectoftransformationssuchastranslation,rotation,andscalinginimages.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 664, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 948}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0665_c255e4b6", "text": "Tangentprophasbeenusednotjustforsupervisedlearning( ,) Simardetal.1992\nbutalsointhecontextofreinforcementlearning(,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 665, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 117}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0666_58ee3dd8", "text": "Thrun1995\nTangentpropagation is closelyrelated todataset augmentation.In both\ncases,theuserofthealgorithmencodeshisorherpriorknowledgeofthetask\nbyspecifyingasetoftransformationsthatshouldnotaltertheoutputofthe\nnetwork.Thediﬀerenceisthatinthecaseofdatasetaugmentation, thenetworkis\nexplicitlytrainedtocorrectlyclassifydistinctinputsthatwerecreatedbyapplying\nmorethananinﬁnitesimalamountofthesetransformations.Tangentpropagation\ndoesnotrequireexplicitlyvisitinganewinputpoint.Instead,itanalytically\nregularizesthemodeltoresistperturbationinthedirectionscorrespondingto\nthe speciﬁed transformation.While thisanalytical approac h isintellectually\nelegant,ithastwomajordrawbacks.First,itonlyregularizesthemodeltoresist\ninﬁnitesimalperturbation.Explicitdatasetaugmentationconfersresistanceto\nlargerperturbations.Second,theinﬁnitesimalapproachposesdiﬃcultiesformodels\nbasedonrectiﬁedlinearunits.Thesemodelscanonlyshrinktheirderivatives\nbyturningunitsoﬀorshrinkingtheirweights.Theyarenotabletoshrinktheir\nderivativesbysaturatingatahighvaluewithlargeweights,assigmoidortanh\nunitscan.Datasetaugmentation workswellwithrectiﬁedlinearunitsbecause\ndiﬀerentsubsetsofrectiﬁedunitscanactivatefordiﬀerenttransformedversionsof\neachoriginalinput.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 666, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1226}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0667_2b51e694", "text": "Tangentpropagationisalsorelatedtodoublebackprop(DruckerandLeCun,\n1992)andadversarialtraining( ,; ,). Szegedy etal.2014bGoodfellowetal.2014b\nDoublebackpropregularizestheJacobiantobesmall,whileadversarialtraining\nﬁndsinputsneartheoriginalinputsandtrainsthemodeltoproducethesame\noutputontheseasontheoriginalinputs.Tangentpropagation anddataset\naugmentationusingmanuallyspeciﬁedtransformationsbothrequirethatthe\nmodelshouldbeinvarianttocertainspeciﬁeddirectionsofchangeintheinput. Doublebackpropandadversarialtrainingbothrequirethatthemodelshouldbe\ninvarianttodirectionsofchangeintheinputsolongasthechangeissmall.Just all\nasdatasetaugmentationisthenon-inﬁnitesimalversionoftangentpropagation,\nadversarialtrainingisthenon-inﬁnitesimalversionofdoublebackprop.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 667, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 753}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0668_86f1047f", "text": "Themanifoldtangentclassiﬁer(,),eliminatestheneedto Rifaietal.2011c\nknowthetangentvectorsapriori.Aswewillseeinchapter,autoencoderscan 14\n2 7 1\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nx 1x 2N o r m a lT a ng e nt\nFigure7.9: Illustrationofthemainideaofthetangentpropalgorithm( , Simard e t a l . 1992 Rifai2011c )andmanifoldtangentclassiﬁer( e t a l .,),whichbothregularizethe\nclassiﬁeroutputfunction f(x).Eachcurverepresentsthemanifoldforadiﬀerentclass,\nillustratedhereasaone-dimensionalmanifoldembeddedinatwo-dimensionalspace.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 668, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 524}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0669_7d7ab392", "text": "Ononecurve,wehavechosenasinglepointanddrawnavectorthatistangenttothe\nclassmanifold(paralleltoandtouchingthemanifold)andavectorthatisnormaltothe\nclassmanifold(orthogonaltothemanifold).Inmultipledimensionstheremaybemany\ntangentdirectionsandmanynormaldirections.Weexpecttheclassiﬁcationfunctionto\nchangerapidlyasitmovesinthedirectionnormaltothemanifold,andnottochangeas\nitmovesalongtheclassmanifold.Bothtangentpropagationandthemanifoldtangent\nclassiﬁerregularize f(x) tonotchangeverymuchasxmovesalongthemanifold.Tangent\npropagationrequirestheusertomanuallyspecifyfunctionsthatcomputethetangent\ndirections(suchasspecifyingthatsmalltranslationsofimagesremaininthesameclass\nmanifold)whilethemanifoldtangentclassiﬁerestimatesthemanifoldtangentdirections\nbytraininganautoencodertoﬁtthetrainingdata.Theuseofautoencoderstoestimate\nmanifoldswillbedescribedinchapter.14\nestimatethemanifoldtangentvectors.Themanifoldtangentclassiﬁermakesuse\nofthistechniquetoavoidneedinguser-speciﬁedtangentvectors.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 669, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 985}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0670_ec298554", "text": "Asillustrated\ninﬁgure,theseestimatedtangentvectorsgobeyondtheclassicalinvariants 14.10\nthatariseoutofthegeometryofimages(suchastranslation,rotationandscaling)\nandincludefactorsthatmustbelearnedbecausetheyareobject-speciﬁc(suchas\nmovingbodyparts).Thealgorithmproposedwiththemanifoldtangentclassiﬁer\nisthereforesimple:(1)useanautoencodertolearnthemanifoldstructureby\nunsupervisedlearning,and(2)usethesetangentstoregularizeaneuralnetclassiﬁer\nasintangentprop(equation).7.67\nThischapterhasdescribedmostofthegeneralstrategiesusedtoregularize\nneuralnetworks.Regularizationisacentralthemeofmachinelearningandassuch\n2 7 2\nCHAPTER7.REGULARIZATIONFORDEEPLEARNING\nwillberevisitedperiodicallybymostoftheremainingchapters.Anothercentral\nthemeofmachinelearningisoptimization, describednext.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 670, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 776}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0671_c9fdbc1b", "text": "2 7 3\nC h a p t e r 8\nOptimizationforTrainingDeep\nModels\nDeeplearningalgorithmsinvolveoptimization inmanycontexts.Forexample,\nperforminginferenceinmodelssuchasPCAinvolvessolvinganoptimization\nproblem.Weoftenuseanalyticaloptimization towriteproofsordesignalgorithms. Ofallofthemanyoptimization problemsinvolvedindeeplearning,themost\ndiﬃcultisneuralnetworktraining.Itisquitecommontoinvestdaystomonthsof\ntimeonhundredsofmachinesinordertosolveevenasingleinstanceoftheneural\nnetworktrainingproblem.Becausethisproblemissoimportantandsoexpensive,\naspecializedsetofoptimization techniqueshavebeendevelopedforsolvingit. Thischapterpresentstheseoptimization techniquesforneuralnetworktraining. Ifyouareunfamiliarwiththebasicprinciplesofgradient-basedoptimization,\nwesuggestreviewingchapter.Thatchapterincludesabriefoverviewofnumerical 4\noptimization ingeneral.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 671, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 850}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0672_b00ea993", "text": "Ifyouareunfamiliarwiththebasicprinciplesofgradient-basedoptimization,\nwesuggestreviewingchapter.Thatchapterincludesabriefoverviewofnumerical 4\noptimization ingeneral. Thischapterfocusesononeparticularcaseofoptimization: ﬁndingtheparam-\netersθofaneuralnetworkthatsigniﬁcantlyreduceacostfunction J(θ),which\ntypicallyincludesaperformancemeasureevaluatedontheentiretrainingsetas\nwellasadditionalregularizationterms. Webeginwithadescriptionofhowoptimization usedasatrainingalgorithm\nforamachinelearningtaskdiﬀersfrompureoptimization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 672, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 528}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0673_3b80d92f", "text": "Webeginwithadescriptionofhowoptimization usedasatrainingalgorithm\nforamachinelearningtaskdiﬀersfrompureoptimization. Next,wepresentseveral\noftheconcretechallengesthatmakeoptimization ofneuralnetworksdiﬃcult.We\nthendeﬁneseveralpracticalalgorithms,includingbothoptimization algorithms\nthemselvesandstrategiesforinitializingtheparameters.Moreadvancedalgorithms\nadapttheirlearningratesduringtrainingorleverageinformationcontainedin\n274\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthesecondderivativesofthecostfunction.Finally,weconcludewithareviewof\nseveraloptimization strategiesthatareformedbycombiningsimpleoptimization\nalgorithmsintohigher-levelprocedures. 8.1HowLearningDiﬀersfromPureOptimization\nOptimization algorithmsusedfortrainingofdeepmodelsdiﬀerfromtraditional\noptimization algorithmsinseveralways.Machinelearningusuallyactsindirectly.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 673, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 843}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0674_ee257bef", "text": "8.1HowLearningDiﬀersfromPureOptimization\nOptimization algorithmsusedfortrainingofdeepmodelsdiﬀerfromtraditional\noptimization algorithmsinseveralways.Machinelearningusuallyactsindirectly. Inmostmachinelearningscenarios,wecareaboutsomeperformancemeasure\nP,thatisdeﬁnedwithrespecttothetestsetandmayalsobeintractable.We\nthereforeoptimize Ponlyindirectly.Wereduceadiﬀerentcostfunction J(θ)in\nthehopethatdoingsowillimprove P.Thisisincontrasttopureoptimization,\nwhereminimizing Jisagoalinandofitself.Optimization algorithmsfortraining\ndeepmodelsalsotypicallyincludesomespecializationonthespeciﬁcstructureof\nmachinelearningobjectivefunctions.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 674, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 634}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0675_a58eda5c", "text": "Typically,thecostfunctioncanbewrittenasanaverageoverthetrainingset,\nsuchas\nJ() = θ E ( ) ˆ x ,y ∼ pdataL f , y , ((;)xθ) (8.1)\nwhere Listheper-examplelossfunction, f(x;θ)isthepredictedoutputwhen\ntheinputisx,ˆ p da t aistheempiricaldistribution.Inthesupervisedlearningcase,\nyisthetargetoutput.Throughoutthischapter,wedeveloptheunregularized\nsupervisedcase,wheretheargumentsto Lare f(x;θ)and y.However,itistrivial\ntoextendthisdevelopment,forexample,toincludeθorxasarguments,orto\nexclude yasarguments,inordertodevelopvariousformsofregularizationor\nunsupervisedlearning. Equationdeﬁnesanobjectivefunctionwithrespecttothetrainingset.We 8.1\nwouldusuallyprefertominimizethecorrespondingobjectivefunctionwherethe\nexpectationistakenacrossthedatageneratingdistribution p da t aratherthanjust\novertheﬁnitetrainingset:\nJ∗() = θ E ( ) x ,y ∼ pdataL f , y .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 675, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 843}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0676_7aaca551", "text": "((;)xθ) (8.2)\n8.1.1EmpiricalRiskMinimization\nThegoalofamachinelearningalgorithmistoreducetheexpectedgeneralization\nerrorgivenbyequation.Thisquantityisknownasthe 8.2 risk.Weemphasizehere\nthattheexpectationistakenoverthetrueunderlyingdistribution p da t a.Ifweknew\nthetruedistribution p da t a(x , y),riskminimization wouldbeanoptimization task\n2 7 5\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nsolvablebyanoptimization algorithm.However,whenwedonotknow p da t a(x , y)\nbutonlyhaveatrainingsetofsamples,wehaveamachinelearningproblem. Thesimplestwaytoconvertamachinelearningproblembackintoanop-\ntimizationproblemistominimizetheexpectedlossonthetrainingset.This\nmeansreplacingthetruedistribution p(x , y) withtheempiricaldistributionˆ p(x , y)\ndeﬁnedbythetrainingset.Wenowminimizetheempiricalrisk\nE x ,y ∼ ˆ pdata ( ) x , y[((;))] = L fxθ , y1\nmm \ni = 1L f((x( ) i;)θ , y( ) i)(8.3)\nwhereisthenumberoftrainingexamples.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 676, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 915}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0677_bdadd673", "text": "m\nThetrainingprocessbasedonminimizingthisaveragetrainingerrorisknown\nasempiricalriskminimization.Inthissetting,machinelearningisstillvery\nsimilartostraightforwardoptimization. Ratherthanoptimizingtheriskdirectly,\nweoptimizetheempiricalrisk,andhopethattheriskdecreasessigniﬁcantlyas\nwell.Avarietyoftheoreticalresultsestablishconditionsunderwhichthetruerisk\ncanbeexpectedtodecreasebyvariousamounts. However,empiricalriskminimization ispronetooverﬁtting.Modelswith\nhighcapacitycansimplymemorizethetrainingset.Inmanycases,empirical\nriskminimization isnotreallyfeasible.Themosteﬀectivemodernoptimization\nalgorithmsarebasedongradientdescent,butmanyusefullossfunctions,such\nas0-1loss,havenousefulderivatives(thederivativeiseitherzeroorundeﬁned\neverywhere).Thesetwoproblemsmeanthat,inthecontextofdeeplearning,we\nrarelyuseempiricalriskminimization. Instead,wemustuseaslightlydiﬀerent\napproach,inwhichthequantitythatweactuallyoptimizeisevenmorediﬀerent\nfromthequantitythatwetrulywanttooptimize.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 677, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 984}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0678_5ea8bbad", "text": "Instead,wemustuseaslightlydiﬀerent\napproach,inwhichthequantitythatweactuallyoptimizeisevenmorediﬀerent\nfromthequantitythatwetrulywanttooptimize. 8.1.2SurrogateLossFunctionsandEarlyStopping\nSometimes,thelossfunctionweactuallycareabout(sayclassiﬁcationerror)isnot\nonethatcanbeoptimizedeﬃciently.Forexample,exactlyminimizingexpected0-1\nlossistypicallyintractable(exponentialintheinputdimension),evenforalinear\nclassiﬁer(MarcotteandSavard1992,).Insuchsituations,onetypicallyoptimizes\nasurrogatelossfunctioninstead,whichactsasaproxybuthasadvantages. Forexample,thenegativelog-likelihoodofthecorrectclassistypicallyusedasa\nsurrogateforthe0-1loss.Thenegativelog-likelihoodallowsthemodeltoestimate\ntheconditionalprobabilityoftheclasses,giventheinput,andifthemodelcan\ndothatwell,thenitcanpicktheclassesthatyieldtheleastclassiﬁcationerrorin\nexpectation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 678, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 843}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0679_8b433fba", "text": "2 7 6\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nInsomecases,asurrogatelossfunctionactuallyresultsinbeingabletolearn\nmore.Forexample,thetestset0-1lossoftencontinuestodecreaseforalong\ntimeafterthetrainingset0-1losshasreachedzero,whentrainingusingthe\nlog-likelihoodsurrogate.Thisisbecauseevenwhentheexpected0-1lossiszero,\nonecanimprovetherobustnessoftheclassiﬁerbyfurtherpushingtheclassesapart\nfromeachother,obtainingamoreconﬁdentandreliableclassiﬁer,thusextracting\nmoreinformationfromthetrainingdatathanwouldhavebeenpossiblebysimply\nminimizingtheaverage0-1lossonthetrainingset.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 679, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 577}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0680_6d4e2533", "text": "Averyimportantdiﬀerencebetweenoptimization ingeneralandoptimization\nasweuseitfortrainingalgorithmsisthattrainingalgorithmsdonotusuallyhalt\natalocalminimum.Instead,amachinelearningalgorithmusuallyminimizes\nasurrogatelossfunctionbuthaltswhenaconvergencecriterionbasedonearly\nstopping(section)issatisﬁed.Typicallytheearlystoppingcriterionisbased 7.8\nonthetrueunderlyinglossfunction,suchas0-1lossmeasuredonavalidationset,\nandisdesignedtocausethealgorithmtohaltwheneveroverﬁttingbeginstooccur. Trainingoftenhaltswhilethesurrogatelossfunctionstillhaslargederivatives,\nwhichisverydiﬀerentfromthepureoptimization setting,whereanoptimization\nalgorithmisconsideredtohaveconvergedwhenthegradientbecomesverysmall.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 680, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 701}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0681_2631538e", "text": "8.1.3BatchandMinibatchAlgorithms\nOneaspectofmachinelearningalgorithmsthatseparatesthemfromgeneral\noptimization algorithmsisthattheobjectivefunctionusuallydecomposesasasum\noverthetrainingexamples.Optimization algorithmsformachinelearningtypically\ncomputeeachupdatetotheparametersbasedonanexpectedvalueofthecost\nfunctionestimatedusingonlyasubsetofthetermsofthefullcostfunction. Forexample,maximumlikelihoodestimationproblems,whenviewedinlog\nspace,decomposeintoasumovereachexample:\nθ M L= argmax\nθm \ni = 1log p m o de l(x( ) i, y( ) i;)θ . (8.4)\nMaximizingthissumisequivalenttomaximizingtheexpectationoverthe\nempiricaldistributiondeﬁnedbythetrainingset:\nJ() = θ E x ,y ∼ ˆ pdatalog p m o de l(;)x , yθ . (8.5)\nMostofthepropertiesoftheobjectivefunction Jusedbymostofouropti-\nmizationalgorithmsarealsoexpectationsoverthetrainingset.Forexample,the\n2 7 7\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nmostcommonlyusedpropertyisthegradient:\n∇ θ J() = θ E x ,y ∼ ˆ pdata∇ θlog p m o de l(;)x , yθ .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 681, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 988}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0682_e91f489a", "text": "(8.6)\nComputing this expectation exactly isvery expensive because it requires\nevaluatingthemodeloneveryexampleintheentiredataset.Inpractice,wecan\ncomputetheseexpectationsbyrandomlysamplingasmallnumberofexamples\nfromthedataset,thentakingtheaverageoveronlythoseexamples. Recallthatthestandarderrorofthemean(equation)estimatedfrom 5.46 n\nsamplesisgivenby σ /√n ,where σisthetruestandarddeviationofthevalueof\nthesamples.Thedenominator of√nshowsthattherearelessthanlinearreturns\ntousingmoreexamplestoestimatethegradient.Comparetwohypothetical\nestimatesofthegradient,onebasedon100examplesandanotherbasedon10,000\nexamples.Thelatterrequires100timesmorecomputationthantheformer,but\nreducesthestandarderrorofthemeanonlybyafactorof10.Mostoptimization\nalgorithmsconvergemuchfaster(intermsoftotalcomputation,notintermsof\nnumberofupdates)iftheyareallowedtorapidlycomputeapproximate estimates\nofthegradientratherthanslowlycomputingtheexactgradient.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 682, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 933}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0683_040673d6", "text": "Anotherconsiderationmotivatingstatisticalestimationofthegradientfroma\nsmallnumberofsamplesisredundancyinthetrainingset.Intheworstcase,all\nmsamplesinthetrainingsetcouldbeidenticalcopiesofeachother.Asampling-\nbasedestimateofthegradientcouldcomputethecorrectgradientwithasingle\nsample,using mtimeslesscomputationthanthenaiveapproach.Inpractice,we\nareunlikelytotrulyencounterthisworst-casesituation,butwemayﬁndlarge\nnumbersofexamplesthatallmakeverysimilarcontributionstothegradient. Optimization algorithmsthatusetheentiretrainingsetarecalledbatchor\ndeterministicgradientmethods,becausetheyprocessallofthetrainingexamples\nsimultaneouslyinalargebatch.Thisterminologycanbesomewhatconfusing\nbecausetheword“batch”isalsooftenusedtodescribetheminibatchusedby\nminibatchstochasticgradientdescent.Typicallytheterm“batchgradientdescent”\nimpliestheuseofthefulltrainingset,whiletheuseoftheterm“batch”todescribe\nagroupofexamplesdoesnot. Forexample,itisverycommontousetheterm\n“batchsize”todescribethesizeofaminibatch.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 683, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 999}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0684_79df03ca", "text": "Forexample,itisverycommontousetheterm\n“batchsize”todescribethesizeofaminibatch. Optimization algorithmsthatuseonlyasingleexampleatatimearesometimes\ncalledstochasticorsometimesonlinemethods.Thetermonlineisusually\nreservedforthecasewheretheexamplesaredrawnfromastreamofcontinually\ncreatedexamplesratherthanfromaﬁxed-sizetrainingsetoverwhichseveral\npassesaremade. Mostalgorithmsusedfordeeplearningfallsomewhereinbetween,usingmore\n2 7 8\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthanonebutlessthanallofthetrainingexamples.Theseweretraditionallycalled\nminibatchorminibatchstochasticmethodsanditisnowcommontosimply\ncallthemstochasticmethods. Thecanonicalexampleofastochasticmethodisstochasticgradientdescent,\npresentedindetailinsection.8.3.1\nMinibatchsizesaregenerallydrivenbythefollowingfactors:\n•Largerbatchesprovideamoreaccurateestimateofthegradient,butwith\nlessthanlinearreturns. •Multicorearchitectures areusuallyunderutilized byextremelysmallbatches.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 684, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 952}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0685_66b8ee3c", "text": "•Multicorearchitectures areusuallyunderutilized byextremelysmallbatches. Thismotivatesusingsomeabsoluteminimumbatchsize,belowwhichthere\nisnoreductioninthetimetoprocessaminibatch. •Ifallexamplesinthebatcharetobeprocessedinparallel(asistypically\nthecase),thentheamountofmemoryscaleswiththebatchsize.Formany\nhardwaresetupsthisisthelimitingfactorinbatchsize. •Somekindsofhardwareachievebetterruntimewithspeciﬁcsizesofarrays. EspeciallywhenusingGPUs,itiscommonforpowerof2batchsizestooﬀer\nbetterruntime.Typicalpowerof2batchsizesrangefrom32to256,with16\nsometimesbeingattemptedforlargemodels. •Smallbatchescanoﬀeraregularizingeﬀect( ,), WilsonandMartinez2003\nperhapsduetothenoisetheyaddtothelearningprocess.Generalization\nerrorisoftenbestforabatchsizeof1.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 685, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 747}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0686_dc998aca", "text": "•Smallbatchescanoﬀeraregularizingeﬀect( ,), WilsonandMartinez2003\nperhapsduetothenoisetheyaddtothelearningprocess.Generalization\nerrorisoftenbestforabatchsizeof1. Trainingwithsuchasmallbatch\nsizemightrequireasmalllearningratetomaintainstabilityduetothehigh\nvarianceintheestimateofthegradient.Thetotalruntimecanbeveryhigh\nduetotheneedtomakemoresteps,bothbecauseofthereducedlearning\nrateandbecauseittakesmorestepstoobservetheentiretrainingset.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 686, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 441}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0687_27e4c72e", "text": "Diﬀerentkindsofalgorithmsusediﬀerentkindsofinformationfromthemini-\nbatchindiﬀerentways.Somealgorithmsaremoresensitivetosamplingerrorthan\nothers,eitherbecausetheyuseinformationthatisdiﬃculttoestimateaccurately\nwithfewsamples,orbecausetheyuseinformationinwaysthatamplifysampling\nerrorsmore.Methodsthatcomputeupdatesbasedonlyonthegradientgare\nusuallyrelativelyrobustandcanhandlesmallerbatchsizeslike100.Second-order\nmethods,whichusealsotheHessianmatrixHandcomputeupdatessuchas\nH− 1g,typicallyrequiremuchlargerbatchsizeslike10,000.Theselargebatch\nsizesarerequiredtominimizeﬂuctuationsintheestimatesofH− 1g.Suppose\nthatHisestimatedperfectlybuthasapoorconditionnumber.Multiplication by\n2 7 9\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nHoritsinverseampliﬁespre-existingerrors,inthiscase,estimationerrorsing.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 687, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 801}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0688_f4ec5d45", "text": "Verysmallchangesintheestimateofgcanthuscauselargechangesintheupdate\nH− 1g,evenifHwereestimatedperfectly.Ofcourse,Hwillbeestimatedonly\napproximately,sotheupdateH− 1gwillcontainevenmoreerrorthanwewould\npredictfromapplyingapoorlyconditionedoperationtotheestimateof.g\nItisalsocrucialthattheminibatchesbeselectedrandomly.Computingan\nunbiasedestimateoftheexpectedgradientfromasetofsamplesrequiresthatthose\nsamplesbeindependent.Wealsowishfortwosubsequentgradientestimatestobe\nindependentfromeachother,sotwosubsequentminibatchesofexamplesshould\nalsobeindependentfromeachother.Manydatasetsaremostnaturallyarranged\ninawaywheresuccessiveexamplesarehighlycorrelated.Forexample,wemight\nhaveadatasetofmedicaldatawithalonglistofbloodsampletestresults.This\nlistmightbearrangedsothatﬁrstwehaveﬁvebloodsamplestakenatdiﬀerent\ntimesfromtheﬁrstpatient,thenwehavethreebloodsamplestakenfromthe\nsecondpatient,thenthebloodsamplesfromthethirdpatient,andsoon.Ifwe\nweretodrawexamplesinorderfromthislist,theneachofourminibatcheswould\nbeextremelybiased,becauseitwouldrepresentprimarilyonepatientoutofthe\nmanypatientsinthedataset.Incasessuchasthesewheretheorderofthedataset\nholdssomesigniﬁcance,itisnecessarytoshuﬄetheexamplesbeforeselecting\nminibatches.Forverylargedatasets,forexampledatasetscontainingbillionsof\nexamplesinadatacenter,itcanbeimpracticaltosampleexamplestrulyuniformly\natrandomeverytimewewanttoconstructaminibatch.Fortunately,inpractice\nitisusuallysuﬃcienttoshuﬄetheorderofthedatasetonceandthenstoreitin\nshuﬄedfashion.Thiswillimposeaﬁxedsetofpossibleminibatchesofconsecutive\nexamplesthatallmodelstrainedthereafterwilluse,andeachindividualmodel\nwillbeforcedtoreusethisorderingeverytimeitpassesthroughthetraining\ndata.However,thisdeviationfromtruerandomselectiondoesnotseemtohavea\nsigniﬁcantdetrimentaleﬀect.Failingtoevershuﬄetheexamplesinanywaycan\nseriouslyreducetheeﬀectivenessofthealgorithm.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 688, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1877}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0689_b389d283", "text": "Manyoptimization problemsinmachinelearningdecomposeoverexamples\nwellenoughthatwecancomputeentireseparateupdatesoverdiﬀerentexamples\ninparallel.Inotherwords,wecancomputetheupdatethatminimizes J(X)for\noneminibatchofexamplesXatthesametimethatwecomputetheupdatefor\nseveralotherminibatches.Suchasynchronousparalleldistributedapproachesare\ndiscussedfurtherinsection.12.1.3\nAninterestingmotivationforminibatchstochasticgradientdescentisthatit\nfollowsthegradientofthetruegeneralizationerror(equation)solongasno 8.2\nexamplesarerepeated.Mostimplementations ofminibatchstochasticgradient\n2 8 0\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\ndescentshuﬄethedatasetonceandthenpassthroughitmultipletimes.Onthe\nﬁrstpass,eachminibatchisusedtocomputeanunbiasedestimateofthetrue\ngeneralization error.Onthesecondpass,theestimatebecomesbiasedbecauseitis\nformedbyre-samplingvaluesthathavealreadybeenused,ratherthanobtaining\nnewfairsamplesfromthedatageneratingdistribution.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 689, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 948}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0690_0876a23f", "text": "Thefactthatstochasticgradientdescentminimizesgeneralization erroris\neasiesttoseeintheonlinelearningcase,whereexamplesorminibatchesaredrawn\nfromastreamofdata.Inotherwords,insteadofreceivingaﬁxed-sizetraining\nset,thelearnerissimilartoalivingbeingwhoseesanewexampleateachinstant,\nwitheveryexample (x , y)comingfromthedatageneratingdistribution p da t a(x , y). Inthisscenario,examplesareneverrepeated;everyexperienceisafairsample\nfrom p da t a. Theequivalenceiseasiesttoderivewhenbothxand yarediscrete. Inthis\ncase,thegeneralization error(equation)canbewrittenasasum 8.2\nJ∗() =θ\nx\nyp da t a()((;)) x , y L fxθ , y , (8.7)\nwiththeexactgradient\ng= ∇ θ J∗() =θ\nx\nyp da t a()x , y∇ θ L f , y . ((;)xθ)(8.8)\nWehavealreadyseenthesamefactdemonstratedforthelog-likelihoodinequa-\ntionandequation;weobservenowthatthisholdsforotherfunctions 8.5 8.6 L\nbesidesthelikelihood.Asimilarresultcanbederivedwhenxand yarecontinuous,\nundermildassumptionsregarding p da t aand.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 690, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 955}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0691_6fe1f109", "text": "L\nHence, wecanobtainanunbiasedestimatoroftheexactgradientof the\ngeneralization errorbysamplingaminibatchofexamples {x( 1 ), . . .x( ) m}withcor-\nrespondingtargets y( ) ifromthedatageneratingdistribution p da t a,andcomputing\nthegradientofthelosswithrespecttotheparametersforthatminibatch:\nˆg=1\nm∇ θ\niL f((x( ) i;)θ , y( ) i) . (8.9)\nUpdatinginthedirectionof θ ˆgperformsSGDonthegeneralization error. Ofcourse, thisinterpretation only applies whenexamplesarenotreused. Nonetheless,itisusuallybesttomakeseveralpassesthroughthetrainingset,\nunlessthetrainingsetisextremelylarge. When multiplesuchepochsareused,\nonlytheﬁrstepochfollowstheunbiasedgradientofthegeneralization error,but\n2 8 1\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nofcourse,theadditionalepochsusuallyprovideenoughbeneﬁtduetodecreased\ntrainingerrortooﬀsettheharmtheycausebyincreasingthegapbetweentraining\nerrorandtesterror.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 691, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 887}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0692_5552a212", "text": "Withsomedatasetsgrowingrapidlyinsize,fasterthancomputingpower,it\nisbecomingmorecommonformachinelearningapplicationstouseeachtraining\nexampleonlyonceoreventomakeanincompletepassthroughthetraining\nset.Whenusinganextremelylargetrainingset,overﬁttingisnotanissue,so\nunderﬁttingandcomputational eﬃciencybecomethepredominant concerns.See\nalso ()foradiscussionoftheeﬀectofcomputational BottouandBousquet2008\nbottlenecksongeneralization error,asthenumberoftrainingexamplesgrows. 8.2ChallengesinNeuralNetworkOptimization\nOptimization ingeneralisanextremelydiﬃculttask.Traditionally,machine\nlearninghasavoidedthediﬃcultyofgeneraloptimization bycarefullydesigning\ntheobjectivefunctionandconstraintstoensurethattheoptimization problemis\nconvex.Whentrainingneuralnetworks,wemustconfrontthegeneralnon-convex\ncase.Evenconvexoptimization isnotwithoutitscomplications. Inthissection,\nwesummarizeseveralofthemostprominentchallengesinvolvedinoptimization\nfortrainingdeepmodels.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 692, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 958}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0693_27a87517", "text": "Inthissection,\nwesummarizeseveralofthemostprominentchallengesinvolvedinoptimization\nfortrainingdeepmodels. 8.2.1Ill-Conditioning\nSomechallengesariseevenwhenoptimizingconvexfunctions.Ofthese,themost\nprominentisill-conditioning oftheHessianmatrixH.Thisisaverygeneral\nprobleminmostnumericaloptimization, convexorotherwise,andisdescribedin\nmoredetailinsection.4.3.1\nTheill-conditioning problemisgenerallybelievedtobepresentinneural\nnetworktrainingproblems.Ill-conditioningcanmanifestbycausingSGDtoget\n“stuck”inthesensethatevenverysmallstepsincreasethecostfunction. Recallfromequationthatasecond-orderTaylorseriesexpansionofthe 4.9\ncostfunctionpredictsthatagradientdescentstepofwilladd − g\n1\n22gHgg− g (8.10)\ntothecost.Ill-conditioningofthegradientbecomesaproblemwhen1\n22gHg\nexceeds gg.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 693, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 790}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0694_cccd4dab", "text": "Todeterminewhetherill-conditioning isdetrimentaltoaneural\nnetwork training task, one canmonitorthe squaredgradientnormggand\n2 8 2\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n−50050100150200250\nTrainingtime(epochs)−20246810121416Gradient norm\n0 50100150200250\nTrainingtime(epochs)01 .02 .03 .04 .05 .06 .07 .08 .09 .10 .Classiﬁcationerrorrate\nFigure8.1:Gradientdescentoftendoesnotarriveatacriticalpointofanykind.Inthis\nexample,thegradientnormincreasesthroughouttrainingofaconvolutionalnetworkused\nforobjectdetection. ( L e f t )Ascatterplotshowinghowthenormsofindividualgradient\nevaluationsaredistributedovertime.Toimprovelegibility,onlyonegradientnorm\nisplottedperepoch.Therunningaverageofallgradientnormsisplottedasasolid\ncurve.Thegradientnormclearlyincreasesovertime,ratherthandecreasingaswewould\nexpectifthetrainingprocessconvergedtoacriticalpoint.Despitetheincreasing ( R i g h t )\ngradient,thetrainingprocessisreasonablysuccessful.Thevalidationsetclassiﬁcation\nerrordecreasestoalowlevel.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 694, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 993}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0695_efff405d", "text": "thegHgterm.Inmanycases,thegradientnormdoesnotshrinksigniﬁcantly\nthroughoutlearning,butthegHgtermgrowsbymorethananorderofmagnitude. Theresultisthatlearningbecomesveryslowdespitethepresenceofastrong\ngradientbecausethelearningratemustbeshrunktocompensateforevenstronger\ncurvature.Figureshowsanexampleofthegradientincreasingsigniﬁcantly 8.1\nduringthesuccessfultrainingofaneuralnetwork. Thoughill-conditioning ispresentinothersettingsbesidesneuralnetwork\ntraining,someofthetechniquesusedtocombatitinothercontextsareless\napplicabletoneuralnetworks.Forexample,Newton’smethodisanexcellenttool\nforminimizingconvexfunctionswithpoorlyconditionedHessianmatrices,butin\nthesubsequentsectionswewillarguethatNewton’smethodrequiressigniﬁcant\nmodiﬁcationbeforeitcanbeappliedtoneuralnetworks.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 695, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 775}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0696_ef3cc79c", "text": "8.2.2LocalMinima\nOneofthemostprominentfeaturesofaconvexoptimization problemisthatit\ncanbereducedtotheproblemofﬁndingalocalminimum.Anylocalminimumis\n2 8 3\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nguaranteedtobeaglobalminimum.Someconvexfunctionshaveaﬂatregionat\nthebottomratherthanasingleglobalminimumpoint,butanypointwithinsuch\naﬂatregionisanacceptablesolution.Whenoptimizingaconvexfunction,we\nknowthatwehavereachedagoodsolutionifweﬁndacriticalpointofanykind. Withnon-convexfunctions,suchasneuralnets,itispossibletohavemany\nlocalminima.Indeed,nearlyanydeepmodelisessentiallyguaranteedtohave\nanextremelylargenumberoflocalminima.However,aswewillsee,thisisnot\nnecessarilyamajorproblem.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 696, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 684}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0697_85056644", "text": "Neuralnetworksandanymodelswithmultipleequivalentlyparametrized latent\nvariablesallhavemultiplelocalminimabecauseofthemodelidentiﬁability\nproblem.Amodelissaidtobeidentiﬁableifasuﬃcientlylargetrainingsetcan\nruleoutallbutonesettingofthemodel’sparameters.Modelswithlatentvariables\nareoftennotidentiﬁablebecausewecanobtainequivalentmodelsbyexchanging\nlatentvariableswitheachother.Forexample,wecouldtakeaneuralnetworkand\nmodifylayer1byswappingtheincomingweightvectorforunit iwiththeincoming\nweightvectorforunit j,thendoingthesamefortheoutgoingweightvectors.Ifwe\nhave mlayerswith nunitseach,thenthereare n!mwaysofarrangingthehidden\nunits.Thiskindofnon-identiﬁabilit yisknownasweightspacesymmetry.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 697, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 689}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0698_70fec5a1", "text": "Inadditiontoweightspacesymmetry,manykindsofneuralnetworkshave\nadditionalcausesofnon-identiﬁabilit y.Forexample,inanyrectiﬁedlinearor\nmaxoutnetwork,wecanscalealloftheincomingweightsandbiasesofaunitby\nαifwealsoscaleallofitsoutgoingweightsby1\nα.Thismeansthat—ifthecost\nfunctiondoesnotincludetermssuchasweightdecaythatdependdirectlyonthe\nweightsratherthanthemodels’outputs—everylocalminimumofarectiﬁedlinear\normaxoutnetworkliesonan( m n×)-dimensionalhyperbolaofequivalentlocal\nminima. Thesemodelidentiﬁabilityissuesmeanthattherecanbeanextremelylarge\norevenuncountablyinﬁniteamountoflocalminimainaneuralnetworkcost\nfunction.However,alloftheselocalminimaarisingfromnon-identiﬁabilit yare\nequivalenttoeachotherincostfunctionvalue.Asaresult,theselocalminimaare\nnotaproblematicformofnon-convexity.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 698, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 788}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0699_9ab4f9e8", "text": "Localminimacanbeproblematiciftheyhavehighcostincomparisontothe\nglobalminimum.Onecanconstructsmallneuralnetworks,evenwithouthidden\nunits,thathavelocalminimawithhighercostthantheglobalminimum(Sontag\nandSussman1989Brady1989GoriandTesi1992 ,; etal.,; ,).Iflocalminima\nwithhighcostarecommon,thiscouldposeaseriousproblemforgradient-based\noptimization algorithms. Itremainsanopenquestionwhethertherearemanylocalminimaofhighcost\n2 8 4\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nfornetworksofpracticalinterestandwhetheroptimization algorithmsencounter\nthem.Formanyyears,mostpractitioners believedthatlocalminimawerea\ncommonproblemplaguingneuralnetworkoptimization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 699, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 656}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0700_58d80ead", "text": "Today,thatdoesnot\nappeartobethecase.Theproblemremainsanactiveareaofresearch,butexperts\nnowsuspectthat,forsuﬃcientlylargeneuralnetworks,mostlocalminimahavea\nlowcostfunctionvalue,andthatitisnotimportanttoﬁndatrueglobalminimum\nratherthantoﬁndapointinparameterspacethathaslowbutnotminimalcost\n(,; ,; ,; Saxeetal.2013Dauphinetal.2014Goodfellow etal.2015Choromanska\netal.,).2014\nManypractitioners attributenearlyalldiﬃcultywithneuralnetworkoptimiza-\ntiontolocalminima.Weencouragepractitioners tocarefullytestforspeciﬁc\nproblems.Atestthatcanruleoutlocalminimaastheproblemistoplotthe\nnormofthegradientovertime.Ifthenormofthegradientdoesnotshrinkto\ninsigniﬁcantsize,theproblemisneitherlocalminimanoranyotherkindofcritical\npoint.Thiskindofnegativetestcanruleoutlocalminima.Inhighdimensional\nspaces,itcanbeverydiﬃculttopositivelyestablishthatlocalminimaarethe\nproblem.Manystructuresotherthanlocalminimaalsohavesmallgradients.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 700, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 914}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0701_99e41c48", "text": "8.2.3Plateaus,SaddlePointsandOtherFlatRegions\nFormanyhigh-dimensionalnon-convexfunctions,localminima(andmaxima)\nareinfactrarecomparedtoanotherkindofpointwithzerogradient:asaddle\npoint.Somepointsaroundasaddlepointhavegreatercostthanthesaddlepoint,\nwhileothershavealowercost. Atasaddlepoint,theHessianmatrixhasboth\npositiveandnegativeeigenvalues.Pointslyingalongeigenvectorsassociatedwith\npositiveeigenvalueshavegreatercostthanthesaddlepoint,whilepointslying\nalongnegativeeigenvalueshavelowervalue.Wecanthinkofasaddlepointas\nbeingalocalminimumalongonecross-sectionofthecostfunctionandalocal\nmaximumalonganothercross-section.Seeﬁgureforanillustration.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 701, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 648}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0702_f99200dc", "text": "4.5\nManyclasses ofrandomfunctionsexhibitthefollowingbehavior:inlow-\ndimensionalspaces,localminimaarecommon.Inhigherdimensionalspaces,local\nminimaarerareandsaddlepointsaremorecommon.Forafunction f: Rn→ Rof\nthistype,theexpectedratioofthenumberofsaddlepointstolocalminimagrows\nexponentiallywith n.Tounderstandtheintuitionbehindthisbehavior,observe\nthattheHessianmatrixatalocalminimumhasonlypositiveeigenvalues. The\nHessianmatrixatasaddlepointhasamixtureofpositiveandnegativeeigenvalues. Imaginethatthesignofeacheigenvalueisgeneratedbyﬂippingacoin.Inasingle\ndimension,itiseasytoobtainalocalminimumbytossingacoinandgettingheads\nonce.In n-dimensionalspace,itisexponentiallyunlikelythatall ncointosseswill\n2 8 5\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nbeheads.See ()forareviewoftherelevanttheoreticalwork. Dauphinetal.2014\nAnamazingpropertyofmanyrandomfunctionsisthattheeigenvaluesofthe\nHessianbecomemorelikelytobepositiveaswereachregionsoflowercost.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 702, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 947}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0703_87f23388", "text": "Dauphinetal.2014\nAnamazingpropertyofmanyrandomfunctionsisthattheeigenvaluesofthe\nHessianbecomemorelikelytobepositiveaswereachregionsoflowercost. In\nourcointossinganalogy,thismeanswearemorelikelytohaveourcoincomeup\nheads ntimesifweareatacriticalpointwithlowcost. Thismeansthatlocal\nminimaaremuchmorelikelytohavelowcostthanhighcost.Criticalpointswith\nhighcostarefarmorelikelytobesaddlepoints.Criticalpointswithextremely\nhighcostaremorelikelytobelocalmaxima. Thishappensformanyclassesofrandomfunctions.Doesithappenforneural\nnetworks? ()showedtheoreticallythatshallowautoencoders BaldiandHornik1989\n(feedforwardnetworkstrainedtocopytheirinputtotheiroutput,describedin\nchapter)withnononlinearities haveglobalminimaandsaddlepointsbutno 14\nlocalminimawithhighercostthantheglobalminimum.Theyobservedwithout\nproofthattheseresultsextendtodeepernetworkswithoutnonlinearities.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 703, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 864}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0704_48848e98", "text": "The\noutputofsuchnetworksisalinearfunctionoftheirinput,buttheyareuseful\ntostudyasamodelofnonlinearneuralnetworksbecausetheirlossfunctionis\nanon-convexfunctionoftheirparameters.Suchnetworksareessentiallyjust\nmultiplematricescomposedtogether. ()providedexactsolutions Saxeetal.2013\ntothecompletelearningdynamicsinsuchnetworksandshowedthatlearningin\nthesemodelscapturesmanyofthequalitativefeaturesobservedinthetrainingof\ndeepmodelswithnonlinearactivationfunctions. ()showed Dauphinetal.2014\nexperimentallythatrealneuralnetworksalsohavelossfunctionsthatcontainvery\nmanyhigh-costsaddlepoints.Choromanska2014etal.()providedadditional\ntheoreticalarguments,showingthatanotherclassofhigh-dimensionalrandom\nfunctionsrelatedtoneuralnetworksdoessoaswell.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 704, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 741}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0705_75a8561e", "text": "Whataretheimplicationsoftheproliferationofsaddlepointsfortrainingalgo-\nrithms?Forﬁrst-orderoptimization algorithmsthatuseonlygradientinformation,\nthesituationisunclear.Thegradientcanoftenbecomeverysmallnearasaddle\npoint.Ontheotherhand,gradientdescentempiricallyseemstobeabletoescape\nsaddlepointsinmanycases. ()providedvisualizationsof Goodfellowetal.2015\nseverallearningtrajectoriesofstate-of-the-art neuralnetworks,withanexample\ngiveninﬁgure.Thesevisualizationsshowaﬂatteningofthecostfunctionnear 8.2\naprominentsaddlepointwheretheweightsareallzero,buttheyalsoshowthe\ngradientdescenttrajectoryrapidlyescapingthisregion. () Goodfellowetal.2015\nalsoarguethatcontinuous-timegradientdescentmaybeshownanalyticallytobe\nrepelledfrom,ratherthanattractedto,anearbysaddlepoint,butthesituation\nmaybediﬀerentformorerealisticusesofgradientdescent. ForNewton’smethod, itisclearthatsaddlepointsconstituteaproblem.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 705, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 898}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0706_e24706a5", "text": "ForNewton’smethod, itisclearthatsaddlepointsconstituteaproblem. 2 8 6\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nP r o j e c t i o n2o f θ\nP r o j e c t i o n 1 o f θJ(\n)θ\nFigure8.2:Avisualizationofthecostfunctionofaneuralnetwork.Imageadapted\nwithpermissionfromGoodfellow2015 e t a l .(). Thesevisualizationsappearsimilarfor\nfeedforwardneuralnetworks,convolutionalnetworks,andrecurrentnetworksapplied\ntorealobjectrecognition andnaturallanguageprocessingtasks.Surprisingly,these\nvisualizationsusuallydonotshowmanyconspicuousobstacles. Priortothesuccessof\nstochasticgradientdescentfortrainingverylargemodelsbeginninginroughly2012,\nneuralnetcostfunctionsurfacesweregenerallybelievedtohavemuchmorenon-convex\nstructurethanisrevealedbytheseprojections. Theprimaryobstaclerevealedbythis\nprojectionisasaddlepointofhighcostnearwheretheparametersareinitialized,but,as\nindicatedbythebluepath,theSGDtrainingtrajectoryescapesthissaddlepointreadily.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 706, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 936}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0707_56e5faa0", "text": "Theprimaryobstaclerevealedbythis\nprojectionisasaddlepointofhighcostnearwheretheparametersareinitialized,but,as\nindicatedbythebluepath,theSGDtrainingtrajectoryescapesthissaddlepointreadily. Mostoftrainingtimeisspenttraversingtherelativelyﬂatvalleyofthecostfunction,\nwhichmaybeduetohighnoiseinthegradient,poorconditioningoftheHessianmatrix\ninthisregion,orsimplytheneedtocircumnavigatethetall“mountain”visibleinthe\nﬁgureviaanindirectarcingpath. 2 8 7\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nGradientdescentisdesignedtomove“downhill”andisnotexplicitlydesigned\ntoseekacriticalpoint.Newton’smethod,however,isdesignedtosolvefora\npointwherethegradientiszero.Withoutappropriatemodiﬁcation,itcanjump\ntoasaddlepoint.Theproliferation ofsaddlepointsinhighdimensionalspaces\npresumablyexplainswhysecond-ordermethodshavenotsucceededinreplacing\ngradientdescentforneuralnetworktraining.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 707, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 872}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0708_c4176269", "text": "()introduceda Dauphinetal.2014\nsaddle-freeNewtonmethodforsecond-orderoptimization andshowedthatit\nimprovessigniﬁcantlyoverthetraditionalversion.Second-order methodsremain\ndiﬃculttoscaletolargeneuralnetworks,butthissaddle-freeapproachholds\npromiseifitcouldbescaled. Thereareotherkindsofpointswithzerogradientbesidesminimaandsaddle\npoints.Therearealsomaxima, whic haremuchlikesaddlepointsfromthe\nperspectiveofoptimization—many algorithmsarenotattractedtothem, but\nunmodiﬁedNewton’smethodis.Maximaofmanyclassesofrandomfunctions\nbecomeexponentiallyrareinhighdimensionalspace,justlikeminimado. Theremayalsobewide,ﬂatregionsofconstantvalue.Intheselocations,the\ngradientandalsotheHessianareallzero.Suchdegeneratelocationsposemajor\nproblemsforallnumericaloptimization algorithms.Inaconvexproblem,awide,\nﬂatregionmustconsistentirelyofglobalminima,butinageneraloptimization\nproblem,sucharegioncouldcorrespondtoahighvalueoftheobjectivefunction.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 708, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 933}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0709_5eea6542", "text": "8.2.4CliﬀsandExplodingGradients\nNeuralnetworkswithmanylayersoftenhaveextremelysteepregionsresembling\ncliﬀs,asillustratedinﬁgure.Theseresultfromthemultiplicationofseveral 8.3\nlargeweightstogether.Onthefaceofanextremelysteepcliﬀstructure,the\ngradientupdatestepcanmovetheparametersextremelyfar,usuallyjumpingoﬀ\nofthecliﬀstructurealtogether. 2 8 8\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n\n \n\nFigure8.3:Theobjectivefunctionforhighlynonlineardeepneuralnetworksorfor\nrecurrentneuralnetworksoftencontainssharpnonlinearitiesinparameterspaceresulting\nfromthemultiplicationofseveralparameters.Thesenonlinearitiesgiverisetovery\nhighderivativesinsomeplaces.Whentheparametersgetclosetosuchacliﬀregion,a\ngradientdescentupdatecancatapulttheparametersveryfar,possiblylosingmostofthe\noptimizationworkthathadbeendone.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 709, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 810}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0710_493482a4", "text": "FigureadaptedwithpermissionfromPascanu\ne t a l .().2013\nThecliﬀcanbedangerouswhetherweapproachitfromaboveorfrombelow,\nbutfortunatelyitsmostseriousconsequencescanbeavoidedusingthegradient\nclippingheuristicdescribedinsection.Thebasicideaistorecallthat 10.11.1\nthegradientdoesnotspecifytheoptimalstepsize,butonlytheoptimaldirection\nwithinaninﬁnitesimalregion.Whenthetraditionalgradientdescentalgorithm\nproposestomakeaverylargestep,thegradientclippingheuristicintervenesto\nreducethestepsizetobesmallenoughthatitislesslikelytogooutsidetheregion\nwherethegradientindicatesthedirectionofapproximately steepestdescent.Cliﬀ\nstructuresaremostcommoninthecostfunctionsforrecurrentneuralnetworks,\nbecausesuchmodelsinvolveamultiplication ofmanyfactors,withonefactor\nforeachtimestep.Longtemporalsequencesthusincuranextremeamountof\nmultiplication.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 710, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 830}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0711_7e8ff743", "text": "8.2.5Long-TermDependencies\nAnotherdiﬃcultythatneuralnetworkoptimization algorithmsmustovercome\narises when thecomputational gra ph becomes extremely deep.Feedforward\nnetworkswithmanylayershavesuchdeepcomputational graphs.Sodorecurrent\nnetworks,describedinchapter,whichconstructverydeepcomputational graphs 10\n2 8 9\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nbyrepeatedlyapplyingthesameoperationateachtimestepofalongtemporal\nsequence.Repeatedapplicationofthesameparametersgivesrisetoespecially\npronounceddiﬃculties. Forexample,supposethatacomputational graphcontainsapaththatconsists\nofrepeatedlymultiplyingbyamatrixW.After tsteps,thisisequivalenttomul-\ntiplyingbyWt.SupposethatWhasaneigendecompositionW=Vdiag(λ)V− 1. Inthissimplecase,itisstraightforwardtoseethat\nWt=\nVλVdiag()− 1t= ()VdiagλtV− 1.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 711, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 799}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0712_d05fe89b", "text": "Inthissimplecase,itisstraightforwardtoseethat\nWt=\nVλVdiag()− 1t= ()VdiagλtV− 1. (8.11)\nAnyeigenvalues λ ithatarenotnearanabsolutevalueofwilleitherexplodeifthey 1\naregreaterthaninmagnitudeorvanishiftheyarelessthaninmagnitude.The 1 1\nvanishingandexplodinggradientproblemreferstothefactthatgradients\nthroughsuchagrapharealsoscaledaccordingtodiag(λ)t.Vanishinggradients\nmakeitdiﬃculttoknowwhichdirectiontheparametersshouldmovetoimprove\nthecostfunction,whileexplodinggradientscanmakelearningunstable.Thecliﬀ\nstructuresdescribedearlierthatmotivategradientclippingareanexampleofthe\nexplodinggradientphenomenon.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 712, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 605}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0713_588edb5b", "text": "Therepeatedmultiplication byWateachtimestepdescribedhereisvery\nsimilartothepowermethodalgorithmusedtoﬁndthelargesteigenvalueof\namatrixWandthecorrespondingeigenvector.Fromthispointofviewitis\nnotsurprisingthatxWtwilleventuallydiscardallcomponentsofxthatare\northogonaltotheprincipaleigenvectorof.W\nRecurrentnetworksusethesamematrixWateachtimestep,butfeedforward\nnetworksdonot,soevenverydeepfeedforwardnetworkscanlargelyavoidthe\nvanishingandexplodinggradientproblem(,). Sussillo2014\nWedeferafurtherdiscussionofthechallengesoftrainingrecurrentnetworks\nuntilsection,afterrecurrentnetworkshavebeendescribedinmoredetail. 10.7\n8.2.6InexactGradients\nMostoptimization algorithmsaredesignedwiththeassumptionthatwehave\naccesstotheexactgradientorHessianmatrix.Inpractice,weusuallyonlyhave\nanoisyorevenbiasedestimateofthesequantities. Nearlyeverydeeplearning\nalgorithmreliesonsampling-basedestimatesatleastinsofarasusingaminibatch\noftrainingexamplestocomputethegradient.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 713, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 956}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0714_b68acea3", "text": "Nearlyeverydeeplearning\nalgorithmreliesonsampling-basedestimatesatleastinsofarasusingaminibatch\noftrainingexamplestocomputethegradient. Inothercases,theobjectivefunctionwewanttominimizeisactuallyintractable. Whentheobjectivefunctionisintractable,typicallyitsgradientisintractableas\nwell.Insuchcaseswecanonlyapproximatethegradient.Theseissuesmostlyarise\n2 9 0\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nwiththemoreadvancedmodelsinpart.Forexample,contrastivedivergence III\ngivesatechniqueforapproximatingthegradientoftheintractablelog-likelihood\nofaBoltzmannmachine. Variousneuralnetworkoptimization algorithmsaredesignedtoaccountfor\nimperfectionsinthegradientestimate.Onecanalsoavoidtheproblembychoosing\nasurrogatelossfunctionthatiseasiertoapproximate thanthetrueloss.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 714, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 768}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0715_d5836fd4", "text": "8.2.7PoorCorrespondencebetweenLocalandGlobalStructure\nManyoftheproblemswehavediscussedsofarcorrespondtopropertiesofthe\nlossfunctionatasinglepoint—itcanbediﬃculttomakeasinglestepif J(θ)is\npoorlyconditionedatthecurrentpointθ,orifθliesonacliﬀ,orifθisasaddle\npointhidingtheopportunitytomakeprogressdownhillfromthegradient. Itispossibletoovercomealloftheseproblemsatasinglepointandstill\nperformpoorlyifthedirectionthatresultsinthemostimprovementlocallydoes\nnotpointtowarddistantregionsofmuchlowercost. Goodfellow2015etal.()arguethatmuchoftheruntimeoftrainingisdueto\nthelengthofthetrajectoryneededtoarriveatthesolution.Figureshowsthat8.2\nthelearningtrajectoryspendsmostofitstimetracingoutawidearcarounda\nmountain-shapedstructure.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 715, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 723}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0716_0e2fddb3", "text": "Muchofresearchintothediﬃcultiesofoptimization hasfocusedonwhether\ntrainingarrivesataglobalminimum,alocalminimum,orasaddlepoint,butin\npracticeneuralnetworksdonotarriveatacriticalpointofanykind.Figure8.1\nshowsthatneuralnetworksoftendonotarriveataregionofsmallgradient.Indeed,\nsuchcriticalpointsdonotevennecessarilyexist.Forexample,thelossfunction\n−log p( y|x;θ)canlackaglobalminimumpointandinsteadasymptotically\napproachsomevalueasthemodelbecomesmoreconﬁdent.Foraclassiﬁerwith\ndiscrete yand p( y|x)providedbyasoftmax,thenegativelog-likelihoodcan\nbecomearbitrarilyclosetozeroifthemodelisabletocorrectlyclassifyevery\nexampleinthetrainingset,butitisimpossibletoactuallyreachthevalueof\nzero.Likewise,amodelofrealvalues p( y|x) =N( y; f(θ) , β− 1)canhavenegative\nlog-likelihoodthatasymptotestonegativeinﬁnity—if f(θ)isabletocorrectly\npredictthevalueofalltrainingset ytargets,thelearningalgorithmwillincrease\nβwithoutbound.Seeﬁgureforanexampleofafailureoflocaloptimization to 8.4\nﬁndagoodcostfunctionvalueevenintheabsenceofanylocalminimaorsaddle\npoints.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 716, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1045}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0717_0726231b", "text": "Futureresearchwillneedtodevelopfurtherunderstandingofthefactorsthat\ninﬂuencethelengthofthelearningtrajectoryandbettercharacterizetheoutcome\n2 9 1\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nθJ ( ) θ\nFigure8.4:Optimizationbasedonlocaldownhillmovescanfailifthelocalsurfacedoes\nnotpointtowardtheglobalsolution.Hereweprovideanexampleofhowthiscanoccur,\neveniftherearenosaddlepointsandnolocalminima.Thisexamplecostfunction\ncontainsonlyasymptotestowardlowvalues,notminima.Themaincauseofdiﬃcultyin\nthiscaseisbeinginitializedonthewrongsideofthe“mountain”andnotbeingableto\ntraverseit. Inhigherdimensionalspace,learningalgorithmscanoftencircumnavigate\nsuchmountainsbutthetrajectoryassociatedwithdoingsomaybelongandresultin\nexcessivetrainingtime,asillustratedinﬁgure.8.2\noftheprocess. Manyexistingresearchdirectionsareaimedatﬁndinggoodinitialpointsfor\nproblemsthathavediﬃcultglobalstructure,ratherthandevelopingalgorithms\nthatusenon-localmoves.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 717, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 932}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0718_3eb0e2e3", "text": "Manyexistingresearchdirectionsareaimedatﬁndinggoodinitialpointsfor\nproblemsthathavediﬃcultglobalstructure,ratherthandevelopingalgorithms\nthatusenon-localmoves. Gradientdescentandessentiallyalllearningalgorithmsthatareeﬀectivefor\ntrainingneuralnetworksarebasedonmakingsmall,localmoves.Theprevious\nsectionshaveprimarilyfocusedonhowthecorrectdirectionoftheselocalmoves\ncanbediﬃculttocompute.Wemaybeabletocomputesomepropertiesofthe\nobjectivefunction,suchasitsgradient,onlyapproximately ,withbiasorvariance\ninourestimateofthecorrectdirection.Inthesecases,localdescentmayormay\nnotdeﬁneareasonablyshortpathtoavalidsolution,butwearenotactually\nabletofollowthelocaldescentpath.Theobjectivefunctionmayhaveissues\nsuchaspoorconditioningordiscontinuousgradients,causingtheregionwhere\nthegradientprovidesagoodmodeloftheobjectivefunctiontobeverysmall.In\nthesecases,localdescentwithstepsofsize maydeﬁneareasonablyshortpath\ntothesolution,butweareonlyabletocomputethelocaldescentdirectionwith\nstepsofsize δ .Inthesecases,localdescentmayormaynotdeﬁneapath\ntothesolution,butthepathcontainsmanysteps,sofollowingthepathincursa\n2 9 2\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nhighcomputational cost.Sometimeslocalinformationprovidesusnoguide,when\nthefunctionhasawideﬂatregion,orifwemanagetolandexactlyonacritical\npoint(usuallythislatterscenarioonlyhappenstomethodsthatsolveexplicitly\nforcriticalpoints,suchasNewton’smethod).Inthesecases,localdescentdoes\nnotdeﬁneapathtoasolutionatall.Inothercases,localmovescanbetoogreedy\nandleadusalongapaththatmovesdownhillbutawayfromanysolution,asin\nﬁgure,oralonganunnecessarilylongtrajectorytothesolution,asinﬁgure.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 718, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1634}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0719_5ad44dfb", "text": "8.4 8.2\nCurrently,wedonotunderstandwhichoftheseproblemsaremostrelevantto\nmakingneuralnetworkoptimization diﬃcult,andthisisanactiveareaofresearch. Regardlessofwhichoftheseproblemsaremostsigniﬁcant,allofthemmightbe\navoidedifthereexistsaregionofspaceconnectedreasonablydirectlytoasolution\nbyapaththatlocaldescentcanfollow,andifweareabletoinitializelearning\nwithinthatwell-behavedregion. Thislastviewsuggestsresearchintochoosing\ngoodinitialpointsfortraditionaloptimization algorithmstouse. 8.2.8TheoreticalLimitsofOptimization\nSeveraltheoreticalresultsshowthattherearelimitsontheperformanceofany\noptimization algorithmwemightdesignforneuralnetworks(BlumandRivest,\n1992Judd1989WolpertandMacReady1997 ;,; ,).Typicallytheseresultshave\nlittlebearingontheuseofneuralnetworksinpractice.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 719, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 776}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0720_91aaae3a", "text": "Sometheoreticalresultsapplyonlytothecasewheretheunitsofaneural\nnetworkoutput discretevalues.However, most neuralnetworkunitsoutput\nsmoothlyincreasingvaluesthatmakeoptimization vialocalsearchfeasible.Some\ntheoreticalresultsshowthatthereexistproblemclassesthatareintractable,but\nitcanbediﬃculttotellwhetheraparticularproblemfallsintothatclass.Other\nresultsshowthatﬁndingasolutionforanetworkofagivensizeisintractable,but\ninpracticewecanﬁndasolutioneasilybyusingalargernetworkforwhichmany\nmoreparametersettingscorrespondtoanacceptablesolution.Moreover,inthe\ncontextofneuralnetworktraining,weusuallydonotcareaboutﬁndingtheexact\nminimumofafunction,butseekonlytoreduceitsvaluesuﬃcientlytoobtaingood\ngeneralization error. Theoretical analysisofwhetheranoptimization algorithm\ncanaccomplishthisgoalisextremelydiﬃcult.Developingmorerealisticbounds\nontheperformanceofoptimization algorithmsthereforeremainsanimportant\ngoalformachinelearningresearch.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 720, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 938}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0721_05c711b6", "text": "2 9 3\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.3BasicAlgorithms\nWehavepreviouslyintroducedthegradientdescent(section)algorithmthat 4.3\nfollowsthegradientofanentiretrainingsetdownhill.Thismaybeaccelerated\nconsiderablybyusingstochasticgradientdescenttofollowthegradientofrandomly\nselectedminibatchesdownhill,asdiscussedinsectionandsection. 5.9 8.1.3\n8.3.1StochasticGradientDescent\nStochasticgradientdescent(SGD)anditsvariantsareprobablythemostused\noptimization algorithmsformachinelearningingeneralandfordeeplearning\ninparticular. As discussedinsection,itispossibletoobtainanunbiased 8.1.3\nestimateofthegradientbytakingtheaveragegradientonaminibatchof m\nexamplesdrawni.i.dfromthedatageneratingdistribution. Algorithmshowshowtofollowthisestimateofthegradientdownhill. 8.1\nAlgorithm8.1Stochasticgradientdescent(SGD)updateattrainingiteration k\nRequire:Learningrate  k. Require:Initialparameterθ\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 721, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 989}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0722_2f1484f7", "text": "Require:Initialparameterθ\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i. Computegradientestimate: ˆg←+1\nm∇ θ\ni L f((x( ) i;)θ ,y( ) i)\nApplyupdate:θθ← − ˆg\nendwhile\nAcrucialparameterfortheSGDalgorithmisthelearningrate.Previously,we\nhavedescribedSGDasusingaﬁxedlearningrate .Inpractice,itisnecessaryto\ngraduallydecreasethelearningrateovertime,sowenowdenotethelearningrate\natiterationas k  k.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 722, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 482}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0723_f287f90a", "text": "ThisisbecausetheSGDgradientestimatorintroducesasourceofnoise(the\nrandomsamplingof mtrainingexamples)thatdoesnotvanishevenwhenwearrive\nataminimum.Bycomparison,thetruegradientofthetotalcostfunctionbecomes\nsmallandthen 0whenweapproachandreachaminimumusingbatchgradient\ndescent,sobatchgradientdescentcanuseaﬁxedlearningrate.Asuﬃcient\nconditiontoguaranteeconvergenceofSGDisthat\n∞\nk = 1 k= and ∞ , (8.12)\n2 9 4\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n∞\nk = 12\nk < .∞ (8.13)\nInpractice,itiscommontodecaythelearningratelinearlyuntiliteration: τ\n k= (1 )− α  0+ α  τ (8.14)\nwith α=k\nτ.Afteriteration,itiscommontoleaveconstant.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 723, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 628}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0724_b6144982", "text": "τ \nThelearningratemaybechosenbytrialanderror,butitisusuallybest\ntochooseitbymonitoringlearningcurvesthatplottheobjectivefunctionasa\nfunctionoftime.Thisismoreofanartthanascience,andmostguidanceonthis\nsubjectshouldberegardedwithsomeskepticism.Whenusingthelinearschedule,\ntheparameterstochooseare  0,  τ,and τ.Usually τmaybesettothenumberof\niterationsrequiredtomakeafewhundredpassesthroughthetrainingset.Usually\n τshouldbesettoroughlythevalueof 1%  0.Themainquestionishowtoset  0. Ifitistoolarge,thelearningcurvewillshowviolentoscillations,withthecost\nfunctionoftenincreasingsigniﬁcantly.Gentleoscillationsareﬁne,especiallyif\ntrainingwithastochasticcostfunctionsuchasthecostfunctionarisingfromthe\nuseofdropout.Ifthelearningrateistoolow,learningproceedsslowly,andifthe\ninitiallearningrateistoolow,learningmaybecomestuckwithahighcostvalue.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 724, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 840}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0725_8ac61e93", "text": "Typically,theoptimalinitiallearningrate,intermsoftotaltrainingtimeandthe\nﬁnalcostvalue,ishigherthanthelearningratethatyieldsthebestperformance\naftertheﬁrst100iterationsorso.Therefore,itisusuallybesttomonitortheﬁrst\nseveraliterationsandusealearningratethatishigherthanthebest-performing\nlearningrateatthistime,butnotsohighthatitcausessevereinstability. ThemostimportantpropertyofSGDandrelatedminibatchoronlinegradient-\nbasedoptimization isthatcomputationtimeperupdatedoesnotgrowwiththe\nnumberoftrainingexamples.Thisallowsconvergenceevenwhenthenumber\noftrainingexamplesbecomesverylarge.Foralargeenoughdataset,SGDmay\nconvergetowithinsomeﬁxedtoleranceofitsﬁnaltestseterrorbeforeithas\nprocessedtheentiretrainingset.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 725, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 710}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0726_a860d32c", "text": "Tostudytheconvergencerateofanoptimization algorithmitiscommonto\nmeasuretheexcesserror J(θ)−min θ J(θ),whichistheamountthatthecurrent\ncostfunctionexceedstheminimumpossiblecost.WhenSGDisappliedtoaconvex\nproblem,theexcesserroris O(1√\nk)after kiterations,whileinthestronglyconvex\ncaseitis O(1\nk).Theseboundscannotbeimprovedunlessextraconditionsare\nassumed.Batchgradientdescentenjoysbetterconvergenceratesthanstochastic\ngradientdescentintheory.However,theCramér-Raobound(,;, Cramér1946Rao\n1945)statesthatgeneralization errorcannotdecreasefasterthan O(1\nk).Bottou\n2 9 5\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nandBousquet2008()arguethatitthereforemaynotbeworthwhiletopursue\nanoptimization algorithmthatconvergesfasterthan O(1\nk)formachinelearning\ntasks—fasterconvergencepresumablycorrespondstooverﬁtting.Moreover,the\nasymptoticanalysisobscuresmanyadvantagesthatstochasticgradientdescent\nhasafterasmallnumberofsteps.Withlargedatasets,theabilityofSGDtomake\nrapidinitialprogresswhileevaluatingthegradientforonlyveryfewexamples\noutweighsitsslowasymptoticconvergence.Mostofthealgorithmsdescribedin\ntheremainderofthischapterachievebeneﬁtsthatmatterinpracticebutarelost\nintheconstantfactorsobscuredbythe O(1\nk)asymptoticanalysis.Onecanalso\ntradeoﬀthebeneﬁtsofbothbatchandstochasticgradientdescentbygradually\nincreasingtheminibatchsizeduringthecourseoflearning.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 726, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1351}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0727_06d0237f", "text": "FormoreinformationonSGD,see(). Bottou1998\n8.3.2Momentum\nWhilestochasticgradientdescentremainsaverypopularoptimization strategy,\nlearningwithitcansometimesbeslow.Themethodofmomentum(Polyak1964,)\nisdesignedtoacceleratelearning,especiallyinthefaceofhighcurvature,smallbut\nconsistentgradients,ornoisygradients.Themomentumalgorithmaccumulates\nanexponentiallydecayingmovingaverageofpastgradientsandcontinuestomove\nintheirdirection.Theeﬀectofmomentumisillustratedinﬁgure.8.5\nFormally,themomentumalgorithmintroducesavariablevthatplaystherole\nofvelocity—itisthedirectionandspeedatwhichtheparametersmovethrough\nparameterspace.Thevelocityissettoanexponentiallydecayingaverageofthe\nnegativegradient.Thenamemomentumderivesfromaphysicalanalogy,in\nwhichthenegativegradientisaforcemovingaparticlethroughparameterspace,\naccordingtoNewton’slawsofmotion.Momentuminphysicsismasstimesvelocity.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 727, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 872}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0728_1318b02d", "text": "Inthemomentumlearningalgorithm,weassumeunitmass,sothevelocityvectorv\nmayalsoberegardedasthemomentumoftheparticle.Ahyperparameter α∈[0 ,1)\ndetermineshowquicklythecontributionsofpreviousgradientsexponentiallydecay. Theupdateruleisgivenby:\nvv← α−∇  θ\n1\nmm \ni = 1L((fx( ) i;)θ ,y( ) i)\n, (8.15)\nθθv ← + . (8.16)\nThevelocityvaccumulatesthegradientelements∇ θ1\nmm\ni = 1 L((fx( ) i;)θ ,y( ) i)\n. Thelarger αisrelativeto ,themorepreviousgradientsaﬀectthecurrentdirection.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 728, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 471}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0729_bf6d08fc", "text": "(8.16)\nThevelocityvaccumulatesthegradientelements∇ θ1\nmm\ni = 1 L((fx( ) i;)θ ,y( ) i)\n. Thelarger αisrelativeto ,themorepreviousgradientsaﬀectthecurrentdirection. TheSGDalgorithmwithmomentumisgiveninalgorithm .8.2\n2 9 6\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n− − − 3 0 2 0 1 0 0 1 0 2 0− 3 0− 2 0− 1 001 02 0\nFigure8.5:Momentumaimsprimarilytosolvetwoproblems:poorconditioningofthe\nHessianmatrixandvarianceinthestochasticgradient.Here,weillustratehowmomentum\novercomestheﬁrstofthesetwoproblems.Thecontourlinesdepictaquadraticloss\nfunctionwithapoorlyconditionedHessianmatrix.Theredpathcuttingacrossthe\ncontoursindicatesthepathfollowedbythemomentumlearningruleasitminimizesthis\nfunction.Ateachstepalongtheway,wedrawanarrowindicatingthestepthatgradient\ndescentwouldtakeatthatpoint.Wecanseethatapoorlyconditionedquadraticobjective\nlookslikealong,narrowvalleyorcanyonwithsteepsides.Momentumcorrectlytraverses\nthecanyonlengthwise,whilegradientstepswastetimemovingbackandforthacrossthe\nnarrowaxisofthecanyon.Comparealsoﬁgure,whichshowsthebehaviorofgradient 4.6\ndescentwithoutmomentum.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 729, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1085}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0730_3652071d", "text": "2 9 7\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nPreviously,thesizeofthestepwassimplythenormofthegradientmultiplied\nbythelearningrate.Now,thesizeofthestepdependsonhowlargeandhow\nalignedasequenceofgradientsare.Thestepsizeislargestwhenmanysuccessive\ngradientspointinexactlythesamedirection.Ifthemomentumalgorithmalways\nobservesgradientg,thenitwillaccelerateinthedirectionof−g,untilreachinga\nterminalvelocitywherethesizeofeachstepis\n||||g\n1− α. (8.17)\nItisthushelpfultothinkofthemomentumhyperparameterintermsof1\n1 − α.For\nexample, α= .9correspondstomultiplyingthemaximumspeedbyrelativeto 10\nthegradientdescentalgorithm. Commonvaluesof αusedinpracticeinclude .5, .9,and .99.Likethelearning\nrate, αmayalsobeadaptedovertime.Typicallyitbeginswithasmallvalueand\nislaterraised.Itislessimportanttoadapt αovertimethantoshrink overtime. Algorithm8.2Stochasticgradientdescent(SGD)withmomentum\nRequire:Learningrate,momentumparameter.  α\nRequire:Initialparameter,initialvelocity.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 730, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 968}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0731_e3dd9d2f", "text": "Algorithm8.2Stochasticgradientdescent(SGD)withmomentum\nRequire:Learningrate,momentumparameter.  α\nRequire:Initialparameter,initialvelocity. θ v\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i. Computegradientestimate:g←1\nm∇ θ\ni L f((x( ) i;)θ ,y( ) i)\nComputevelocityupdate:vvg ← α− \nApplyupdate:θθv ← +\nendwhile\nWecanviewthemomentumalgorithmassimulatingaparticlesubjectto\ncontinuous-timeNewtoniandynamics.Thephysicalanalogycanhelptobuild\nintuitionforhowthemomentumandgradientdescentalgorithmsbehave. Thepositionoftheparticleatanypointintimeisgivenbyθ( t).Theparticle\nexperiencesnetforce.Thisforcecausestheparticletoaccelerate: f() t\nf() = t∂2\n∂ t2θ() t .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 731, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 744}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0732_2e0729ef", "text": "Thepositionoftheparticleatanypointintimeisgivenbyθ( t).Theparticle\nexperiencesnetforce.Thisforcecausestheparticletoaccelerate: f() t\nf() = t∂2\n∂ t2θ() t . (8.18)\nRatherthanviewingthisasasecond-orderdiﬀerentialequationoftheposition,\nwecanintroducethevariablev( t)representingthevelocityoftheparticleattime\ntandrewritetheNewtoniandynamicsasaﬁrst-orderdiﬀerentialequation:\nv() = t∂\n∂ tθ() t , (8.19)\n2 9 8\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nf() = t∂\n∂ tv() t . (8.20)\nThemomentumalgorithmthenconsistsofsolvingthediﬀerentialequationsvia\nnumericalsimulation.Asimplenumericalmethodforsolvingdiﬀerentialequations\nisEuler’smethod,whichsimplyconsistsofsimulatingthedynamicsdeﬁnedby\ntheequationbytakingsmall,ﬁnitestepsinthedirectionofeachgradient. Thisexplainsthebasicformofthemomentumupdate,butwhatspeciﬁcallyare\ntheforces?Oneforceisproportionaltothenegativegradientofthecostfunction:\n−∇ θ J(θ).Thisforcepushestheparticledownhillalongthecostfunctionsurface.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 732, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 957}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0733_b9eeb8aa", "text": "Thegradientdescentalgorithmwouldsimplytakeasinglestepbasedoneach\ngradient,buttheNewtonianscenariousedbythemomentumalgorithminstead\nusesthisforcetoalterthevelocityoftheparticle.Wecanthinkoftheparticle\nasbeinglikeahockeypuckslidingdownanicysurface.Wheneveritdescendsa\nsteeppartofthesurface,itgathersspeedandcontinuesslidinginthatdirection\nuntilitbeginstogouphillagain. Oneotherforceisnecessary.Iftheonlyforceisthegradientofthecostfunction,\nthentheparticlemightnevercometorest.Imagineahockeypuckslidingdown\nonesideofavalleyandstraightuptheotherside,oscillatingbackandforthforever,\nassumingtheiceisperfectlyfrictionless.Toresolvethisproblem,weaddone\notherforce,proportionalto−v( t).Inphysicsterminology,thisforcecorresponds\ntoviscousdrag,asiftheparticlemustpushthrougharesistantmediumsuchas\nsyrup.Thiscausestheparticletograduallyloseenergyovertimeandeventually\nconvergetoalocalminimum. Whydoweuse−v( t)andviscousdraginparticular?", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 733, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 925}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0734_16d7846f", "text": "Whydoweuse−v( t)andviscousdraginparticular? Partofthereasonto\nuse−v( t)ismathematical convenience—anintegerpowerofthevelocityiseasy\ntoworkwith.However,otherphysicalsystemshaveotherkindsofdragbased\nonotherintegerpowersofthevelocity.Forexample,aparticletravelingthrough\ntheairexperiencesturbulentdrag,withforceproportionaltothesquareofthe\nvelocity,whileaparticlemovingalongthegroundexperiencesdryfriction,witha\nforceofconstantmagnitude.Wecanrejecteachoftheseoptions.Turbulentdrag,\nproportionaltothesquareofthevelocity,becomesveryweakwhenthevelocityis\nsmall.Itisnotpowerfulenoughtoforcetheparticletocometorest.Aparticle\nwithanon-zeroinitialvelocitythatexperiencesonlytheforceofturbulentdrag\nwillmoveawayfromitsinitialpositionforever,withthedistancefromthestarting\npointgrowinglike O(log t).Wemustthereforeusealowerpowerofthevelocity. Ifweuseapowerofzero,representingdryfriction,thentheforceistoostrong.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 734, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 899}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0735_f9d552f6", "text": "Ifweuseapowerofzero,representingdryfriction,thentheforceistoostrong. Whentheforceduetothegradientofthecostfunctionissmallbutnon-zero,the\nconstantforceduetofrictioncancausetheparticletocometorestbeforereaching\nalocalminimum.Viscousdragavoidsbothoftheseproblems—itisweakenough\n2 9 9\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthatthegradientcancontinuetocausemotionuntilaminimumisreached,but\nstrongenoughtopreventmotionifthegradientdoesnotjustifymoving.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 735, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 452}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0736_917789d6", "text": "8.3.3NesterovMomentum\nSutskever2013etal.()introducedavariantofthemomentumalgorithmthatwas\ninspiredbyNesterov’sacceleratedgradientmethod(,,).The Nesterov19832004\nupdaterulesinthiscasearegivenby:\nvv← α−∇  θ\n1\nmm \ni = 1L\nfx(( ) i;+ )θ αv ,y( ) i\n,(8.21)\nθθv ← + , (8.22)\nwheretheparameters αand playasimilarroleasinthestandardmomentum\nmethod.ThediﬀerencebetweenNesterovmomentumandstandardmomentumis\nwherethegradientisevaluated.WithNesterovmomentumthegradientisevaluated\nafterthecurrentvelocityisapplied.ThusonecaninterpretNesterovmomentum\nasattemptingtoaddacorrectionfactortothestandardmethodofmomentum. ThecompleteNesterovmomentumalgorithmispresentedinalgorithm .8.3\nIntheconvexbatchgradientcase,Nesterovmomentumbringstherateof\nconvergenceoftheexcesserrorfrom O(1 /k)(after ksteps)to O(1 /k2)asshown\nbyNesterov1983().Unfortunately, inthestochasticgradientcase,Nesterov\nmomentumdoesnotimprovetherateofconvergence.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 736, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 917}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0737_b377a893", "text": "Algorithm8.3Stochasticgradientdescent(SGD)withNesterovmomentum\nRequire:Learningrate,momentumparameter.  α\nRequire:Initialparameter,initialvelocity. θ v\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondinglabelsy( ) i. Applyinterimupdate: ˜θθv ← + α\nComputegradient(atinterimpoint):g←1\nm∇ ˜ θ\ni L f((x( ) i;˜θy) ,( ) i)\nComputevelocityupdate:vvg ← α− \nApplyupdate:θθv ← +\nendwhile\n3 0 0\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.4ParameterInitializationStrategies\nSomeoptimization algorithmsarenotiterativebynatureandsimplysolvefora\nsolutionpoint.Otheroptimization algorithmsareiterativebynaturebut,when\nappliedtotherightclassofoptimization problems,convergetoacceptablesolutions\ninanacceptableamountoftimeregardlessofinitialization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 737, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 807}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0738_dbfd44ce", "text": "Deeplearningtraining\nalgorithmsusuallydonothaveeitheroftheseluxuries.Trainingalgorithmsfordeep\nlearningmodelsareusuallyiterativeinnatureandthusrequiretheusertospecify\nsomeinitialpointfromwhichtobegintheiterations.Moreover,trainingdeep\nmodelsisasuﬃcientlydiﬃculttaskthatmostalgorithmsarestronglyaﬀectedby\nthechoiceofinitialization. Theinitialpointcandeterminewhetherthealgorithm\nconvergesatall,withsomeinitialpointsbeingsounstablethatthealgorithm\nencountersnumericaldiﬃcultiesandfailsaltogether.Whenlearningdoesconverge,\ntheinitialpointcandeterminehowquicklylearningconvergesandwhetherit\nconvergestoapointwithhigh orlowcost.Also, pointsofcomparablecost\ncanhavewildlyvaryinggeneralization error,andtheinitialpointcanaﬀectthe\ngeneralization aswell.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 738, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 745}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0739_69885501", "text": "Moderninitialization strategiesaresimpleandheuristic.Designingimproved\ninitialization strategiesisadiﬃculttaskbecauseneuralnetworkoptimization is\nnotyetwellunderstood.Mostinitialization strategiesarebasedonachievingsome\nnicepropertieswhenthenetworkisinitialized.However,wedonothaveagood\nunderstandingofwhichofthesepropertiesarepreservedunderwhichcircumstances\nafterlearningbeginstoproceed.Afurtherdiﬃcultyisthatsomeinitialpoints\nmaybebeneﬁcialfromtheviewpointofoptimization butdetrimentalfromthe\nviewpointofgeneralization. Ourunderstandingofhowtheinitialpointaﬀects\ngeneralization isespeciallyprimitive,oﬀeringlittletonoguidanceforhowtoselect\ntheinitialpoint. Perhapstheonlypropertyknownwithcompletecertaintyisthattheinitial\nparametersneedto“breaksymmetry” betweendiﬀerentunits.Iftwohidden\nunitswiththesameactivationfunctionareconnectedtothesameinputs,then\ntheseunitsmusthavediﬀerentinitialparameters.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 739, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 901}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0740_f0acf3f9", "text": "Iftheyhavethesameinitial\nparameters,thenadeterministiclearningalgorithmappliedtoadeterministiccost\nandmodelwillconstantlyupdatebothoftheseunitsinthesameway.Evenifthe\nmodelortrainingalgorithmiscapableofusingstochasticitytocomputediﬀerent\nupdatesfordiﬀerentunits(forexample,ifonetrainswithdropout),itisusually\nbesttoinitializeeachunittocomputeadiﬀerentfunctionfromalloftheother\nunits.Thismayhelptomakesurethatnoinputpatternsarelostinthenull\nspaceofforwardpropagationandnogradientpatternsarelostinthenullspace\nofback-propagation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 740, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 526}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0741_8fbe5c5f", "text": "Thegoalofhavingeachunitcomputeadiﬀerentfunction\n3 0 1\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nmotivatesrandominitialization oftheparameters.Wecouldexplicitlysearch\nforalargesetofbasisfunctionsthatareallmutuallydiﬀerentfromeachother,\nbutthisoftenincursanoticeablecomputational cost.Forexample,ifwehaveat\nmostasmanyoutputsasinputs,wecoulduseGram-Schmidtorthogonalization\nonaninitialweightmatrix,andbeguaranteedthateachunitcomputesavery\ndiﬀerentfunctionfromeachotherunit.Randominitialization fromahigh-entropy\ndistributionoverahigh-dimensionalspaceiscomputationally cheaperandunlikely\ntoassignanyunitstocomputethesamefunctionaseachother. Typically,wesetthebiasesforeachunittoheuristicallychosenconstants,and\ninitializeonlytheweightsrandomly.Extraparameters,forexample,parameters\nencodingtheconditionalvarianceofaprediction,areusuallysettoheuristically\nchosenconstantsmuchlikethebiasesare.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 741, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 889}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0742_41b58826", "text": "Wealmostalwaysinitializealltheweightsin themodel tovalues drawn\nrandomly froma Gaussian oruniform distribution.Thechoiceof Gaussian\noruniformdistributiondoesnotseemtomatterverymuch,buthasnotbeen\nexhaustivelystudied.Thescaleoftheinitialdistribution,however,doeshavea\nlargeeﬀectonboththeoutcomeoftheoptimization procedureandontheability\nofthenetworktogeneralize. Largerinitialweightswillyieldastrongersymmetrybreakingeﬀect,helping\ntoavoidredundantunits.Theyalsohelptoavoidlosingsignalduringforwardor\nback-propagationthroughthelinearcomponentofeachlayer—largervaluesinthe\nmatrixresultinlargeroutputsofmatrixmultiplication. Initialweightsthatare\ntoolargemay,however,resultinexplodingvaluesduringforwardpropagationor\nback-propagation.Inrecurrentnetworks,largeweightscanalsoresultinchaos\n(suchextremesensitivitytosmallperturbationsoftheinputthatthebehavior\nofthedeterministicforwardpropagationprocedureappearsrandom).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 742, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 911}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0743_de8c0f37", "text": "Tosome\nextent,theexplodinggradientproblemcanbemitigatedbygradientclipping\n(thresholdingthevaluesofthegradientsbeforeperformingagradientdescentstep). Largeweightsmayalsoresultinextremevaluesthatcausetheactivationfunction\ntosaturate,causingcompletelossofgradientthroughsaturatedunits.These\ncompetingfactorsdeterminetheidealinitialscaleoftheweights.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 743, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 346}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0744_2c3f248f", "text": "Largeweightsmayalsoresultinextremevaluesthatcausetheactivationfunction\ntosaturate,causingcompletelossofgradientthroughsaturatedunits.These\ncompetingfactorsdeterminetheidealinitialscaleoftheweights. Theperspectivesofregularizationandoptimization cangiveverydiﬀerent\ninsightsintohowweshouldinitializeanetwork.Theoptimization perspective\nsuggeststhattheweightsshouldbelargeenoughtopropagateinformationsuccess-\nfully,butsomeregularizationconcernsencouragemakingthemsmaller.Theuse\nofanoptimization algorithmsuchasstochasticgradientdescentthatmakessmall\nincrementalchangestotheweightsandtendstohaltinareasthatarenearerto\ntheinitialparameters(whetherduetogettingstuckinaregionoflowgradient,or\n3 0 2\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nduetotriggeringsomeearlystoppingcriterionbasedonoverﬁtting)expressesa\npriorthattheﬁnalparametersshouldbeclosetotheinitialparameters.Recall\nfromsectionthatgradientdescentwithearlystoppingisequivalenttoweight 7.8\ndecayforsomemodels.Inthegeneralcase,gradientdescentwithearlystoppingis\nnotthesameasweightdecay,butdoesprovidealooseanalogyforthinkingabout\ntheeﬀectofinitialization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 744, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1111}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0745_a1c22f8a", "text": "Wecanthinkofinitializingtheparametersθtoθ 0as\nbeingsimilartoimposingaGaussianprior p(θ)withmeanθ 0.Fromthispoint\nofview,itmakessensetochooseθ 0tobenear0.Thispriorsaysthatitismore\nlikelythatunitsdonotinteractwitheachotherthanthattheydointeract.Units\ninteractonlyifthelikelihoodtermoftheobjectivefunctionexpressesastrong\npreferenceforthemtointeract.Ontheotherhand,ifweinitializeθ 0tolarge\nvalues,thenourpriorspeciﬁeswhichunitsshouldinteractwitheachother,and\nhowtheyshouldinteract. Someheuristicsareavailableforchoosingtheinitialscaleoftheweights.One\nheuristicistoinitializetheweightsofafullyconnectedlayerwith minputsand\nnoutputsbysamplingeachweightfrom U(−1√m,1√m),whileGlorotandBengio\n()suggestusingthe 2010 normalizedinitialization\nW i , j∼ U\n−\n6\nm n+,\n6\nm n+\n.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 745, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 766}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0746_70eabffe", "text": "(8.23)\nThislatterheuristicisdesignedtocompromisebetweenthegoalofinitializing\nalllayerstohavethesameactivationvarianceandthegoalofinitializingall\nlayerstohavethesamegradientvariance.Theformulaisderivedusingthe\nassumptionthatthenetworkconsistsonlyofachainofmatrixmultiplications,\nwithnononlinearities. Realneuralnetworksobviouslyviolatethisassumption,\nbutmanystrategiesdesignedforthelinearmodelperformreasonablywellonits\nnonlinearcounterparts. Saxe2013etal.()recommendinitializingtorandomorthogonalmatrices,with\nacarefullychosenscalingorgainfactor gthataccountsforthenonlinearityapplied\nateachlayer.Theyderivespeciﬁcvaluesofthescalingfactorfordiﬀerenttypesof\nnonlinearactivationfunctions.Thisinitialization schemeisalsomotivatedbya\nmodelofadeepnetworkasasequenceofmatrixmultiplieswithoutnonlinearities. Undersuchamodel,thisinitialization schemeguaranteesthatthetotalnumberof\ntrainingiterationsrequiredtoreachconvergenceisindependentofdepth.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 746, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 938}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0747_5ba4d598", "text": "Undersuchamodel,thisinitialization schemeguaranteesthatthetotalnumberof\ntrainingiterationsrequiredtoreachconvergenceisindependentofdepth. Increasingthescalingfactor gpushesthenetworktowardtheregimewhere\nactivationsincreaseinnormastheypropagateforwardthroughthenetworkand\ngradientsincreaseinnormastheypropagatebackward. ()showed Sussillo2014\nthatsettingthegainfactorcorrectlyissuﬃcienttotrainnetworksasdeepas\n3 0 3\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n1,000layers,withoutneedingtouseorthogonalinitializations.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 747, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 515}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0748_f008dd21", "text": "()showed Sussillo2014\nthatsettingthegainfactorcorrectlyissuﬃcienttotrainnetworksasdeepas\n3 0 3\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n1,000layers,withoutneedingtouseorthogonalinitializations. A keyinsightof\nthisapproachisthatinfeedforwardnetworks,activationsandgradientscangrow\norshrinkoneachstepofforwardorback-propagation, followingarandomwalk\nbehavior.Thisisbecausefeedforwardnetworksuseadiﬀerentweightmatrixat\neachlayer.Ifthisrandomwalkistunedtopreservenorms,thenfeedforward\nnetworkscanmostlyavoidthevanishingandexplodinggradientsproblemthat\nariseswhenthesameweightmatrixisusedateachstep,describedinsection.8.2.5\nUnfortunately,theseoptimalcriteriaforinitialweightsoftendonotleadto\noptimalperformance.Thismaybeforthreediﬀerentreasons.First,wemay\nbeusingthewrongcriteria—itmaynotactuallybebeneﬁcialtopreservethe\nnormofasignalthroughouttheentirenetwork.Second,thepropertiesimposed\natinitialization maynotpersistafterlearninghasbeguntoproceed.Third,the\ncriteriamightsucceedatimprovingthespeedofoptimization butinadvertently\nincreasegeneralization error.Inpractice,weusuallyneedtotreatthescaleofthe\nweightsasahyperparameter whoseoptimalvalueliessomewhereroughlynearbut\nnotexactlyequaltothetheoreticalpredictions.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 748, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1216}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0749_da59365a", "text": "Onedrawbacktoscalingrulesthatsetalloftheinitialweightstohavethe\nsamestandarddeviation,suchas1√m,isthateveryindividualweightbecomes\nextremelysmallwhenthelayersbecomelarge. ()introducedan Martens2010\nalternativeinitialization schemecalledsparseinitializationinwhicheachunitis\ninitializedtohaveexactly knon-zeroweights.Theideaistokeepthetotalamount\nofinputtotheunitindependentfromthenumberofinputs mwithoutmakingthe\nmagnitudeofindividualweightelementsshrinkwith m.Sparseinitialization helps\ntoachievemorediversityamongtheunitsatinitialization time.However,italso\nimposesaverystrongpriorontheweightsthatarechosentohavelargeGaussian\nvalues.Becauseittakesalongtimeforgradientdescenttoshrink“incorrect”large\nvalues,thisinitialization schemecancauseproblemsforunitssuchasmaxoutunits\nthathaveseveralﬁltersthatmustbecarefullycoordinatedwitheachother.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 749, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 840}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0750_c27e7bbb", "text": "Whencomputational resourcesallowit,itisusuallyagoodideatotreatthe\ninitialscaleoftheweightsforeachlayerasahyperparameter, andtochoosethese\nscalesusingahyperparametersearchalgorithmdescribedinsection,such11.4.2\nasrandomsearch.Thechoiceofwhethertousedenseorsparseinitialization\ncanalsobemadeahyperparameter.Alternately,onecanmanuallysearchfor\nthebestinitialscales.Agoodruleofthumbforchoosingtheinitialscalesisto\nlookattherangeorstandarddeviationofactivationsorgradientsonasingle\nminibatchofdata.Iftheweightsaretoosmall,therangeofactivationsacrossthe\nminibatchwillshrinkastheactivationspropagateforwardthroughthenetwork. Byrepeatedlyidentifyingtheﬁrstlayerwithunacceptably smallactivationsand\n3 0 4\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nincreasingitsweights,itispossibletoeventuallyobtainanetworkwithreasonable\ninitialactivationsthroughout.Iflearningisstilltooslowatthispoint,itcanbe\nusefultolookattherangeorstandarddeviationofthegradientsaswellasthe\nactivations.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 750, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 965}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0751_9a4c4ddc", "text": "Thisprocedurecaninprinciplebeautomatedandisgenerallyless\ncomputationally costlythanhyperparameter optimization basedonvalidationset\nerrorbecauseitisbasedonfeedbackfromthebehavioroftheinitialmodelona\nsinglebatchofdata,ratherthanonfeedbackfromatrainedmodelonthevalidation\nset.Whilelongusedheuristically,thisprotocolhasrecentlybeenspeciﬁedmore\nformallyandstudiedby (). MishkinandMatas2015\nSo far we have focused on the initialization ofthe weights.Fortunately,\ninitialization ofotherparametersistypicallyeasier.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 751, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 508}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0752_6ac86f56", "text": "MishkinandMatas2015\nSo far we have focused on the initialization ofthe weights.Fortunately,\ninitialization ofotherparametersistypicallyeasier. Theapproachforsettingthebiasesmustbecoordinatedwiththeapproach\nforsettingstheweights.Settingthebiasestozeroiscompatiblewithmostweight\ninitialization schemes.Thereareafewsituationswherewemaysetsomebiasesto\nnon-zerovalues:\n•Ifabiasisforanoutputunit,thenitisoftenbeneﬁcialtoinitializethebiasto\nobtaintherightmarginalstatisticsoftheoutput.Todothis,weassumethat\ntheinitialweightsaresmallenoughthattheoutputoftheunitisdetermined\nonlybythebias.Thisjustiﬁessettingthebiastotheinverseoftheactivation\nfunctionappliedtothemarginalstatisticsoftheoutputinthetrainingset.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 752, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 700}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0753_705c51d0", "text": "Forexample,iftheoutputisadistributionoverclassesandthisdistribution\nisahighlyskeweddistributionwiththemarginalprobabilityofclass igiven\nbyelement c iofsomevectorc,thenwecansetthebiasvectorbbysolving\ntheequationsoftmax (b) =c.Thisappliesnotonlytoclassiﬁersbutalsoto\nmodelswewillencounterinPart,suchasautoencodersandBoltzmann III\nmachines.Thesemodelshavelayerswhoseoutputshouldresembletheinput\ndatax,anditcanbeveryhelpfultoinitializethebiasesofsuchlayersto\nmatchthemarginaldistributionover.x\n•Sometimeswemay wanttochoosethebiastoavoidcausingtoo much\nsaturationatinitialization. Forexample,wemaysetthebiasofaReLU\nhiddenunitto0.1ratherthan0toavoidsaturatingtheReLUatinitialization. Thisapproachisnotcompatiblewithweightinitialization schemesthatdo\nnotexpectstronginputfromthebiasesthough.Forexample, itisnot\nrecommendedforusewithrandomwalkinitialization (,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 753, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 854}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0754_d070a81e", "text": "Thisapproachisnotcompatiblewithweightinitialization schemesthatdo\nnotexpectstronginputfromthebiasesthough.Forexample, itisnot\nrecommendedforusewithrandomwalkinitialization (,). Sussillo2014\n•Sometimesaunitcontrolswhetherotherunitsareabletoparticipateina\nfunction.Insuchsituations,wehaveaunitwithoutput uandanotherunit\nh∈[0 ,1],andtheyaremultipliedtogethertoproduceanoutput u h. We\n3 0 5\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\ncanview hasagatethatdetermineswhether u h u≈or u h≈0. Inthese\nsituations,wewanttosetthebiasfor hsothat h≈1mostofthetimeat\ninitialization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 754, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 568}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0755_26c0a434", "text": "We\n3 0 5\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\ncanview hasagatethatdetermineswhether u h u≈or u h≈0. Inthese\nsituations,wewanttosetthebiasfor hsothat h≈1mostofthetimeat\ninitialization. Otherwise udoesnothaveachancetolearn.Forexample,\nJozefowicz2015etal.()advocatesettingthebiastofortheforgetgateof 1\ntheLSTMmodel,describedinsection.10.10\nAnothercommontypeofparameterisavarianceorprecisionparameter.For\nexample,wecanperformlinearregressionwithaconditionalvarianceestimate\nusingthemodel\np y y (| Nx) = (|wTx+1) b , /β (8.24)\nwhere βisaprecisionparameter.Wecanusuallyinitializevarianceorprecision\nparametersto1safely.Anotherapproachistoassumetheinitialweightsareclose\nenoughtozerothatthebiasesmaybesetwhileignoringtheeﬀectoftheweights,\nthensetthebiasestoproducethecorrectmarginalmeanoftheoutput,andset\nthevarianceparameterstothemarginalvarianceoftheoutputinthetrainingset.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 755, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 875}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0756_c0ba74a6", "text": "Besidesthesesimpleconstantorrandommethodsofinitializingmodelparame-\nters,itispossibletoinitializemodelparametersusingmachinelearning.Acommon\nstrategydiscussedinpartofthisbookistoinitializeasupervisedmodelwith III\ntheparameterslearnedbyanunsupervisedmodeltrainedonthesameinputs. Onecanalsoperformsupervisedtrainingonarelatedtask.Evenperforming\nsupervisedtrainingonanunrelatedtaskcansometimesyieldaninitialization that\noﬀersfasterconvergencethanarandominitialization. Someoftheseinitialization\nstrategiesmayyieldfasterconvergenceandbettergeneralization becausethey\nencodeinformationaboutthedistributionintheinitialparametersofthemodel. Othersapparentlyperformwellprimarilybecausetheysettheparameterstohave\ntherightscaleorsetdiﬀerentunitstocomputediﬀerentfunctionsfromeachother.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 756, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 775}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0757_da7bb061", "text": "Othersapparentlyperformwellprimarilybecausetheysettheparameterstohave\ntherightscaleorsetdiﬀerentunitstocomputediﬀerentfunctionsfromeachother. 8.5AlgorithmswithAdaptiveLearningRates\nNeuralnetworkresearchershavelongrealizedthatthelearningratewasreliablyone\nofthehyperparameters thatisthemostdiﬃculttosetbecauseithasasigniﬁcant\nimpactonmodelperformance.Aswehavediscussedinsectionsand,the 4.38.2\ncostisoftenhighlysensitivetosomedirectionsinparameterspaceandinsensitive\ntoothers.Themomentumalgorithmcanmitigatetheseissuessomewhat,but\ndoessoattheexpenseofintroducinganotherhyperparameter. Inthefaceofthis,\nitisnaturaltoaskifthereisanotherway.Ifwebelievethatthedirectionsof\nsensitivityaresomewhataxis-aligned,itcanmakesensetouseaseparatelearning\n3 0 6\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nrateforeachparameter,andautomatically adapttheselearningratesthroughout\nthecourseoflearning.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 757, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 881}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0758_1151a9f5", "text": "The algorithm(,)isanearlyheuristicapproach delta-bar-delta Jacobs1988\ntoadaptingindividuallearningratesformodelparametersduringtraining.The\napproachisbasedonasimpleidea:ifthepartialderivativeoftheloss,withrespect\ntoagivenmodelparameter,remainsthesamesign,thenthelearningrateshould\nincrease.Ifthepartialderivativewithrespecttothatparameterchangessign,\nthenthelearningrateshoulddecrease. Ofcourse,thiskindofrulecanonlybe\nappliedtofullbatchoptimization. Morerecently,anumberofincremental(ormini-batch-bas ed)methodshave\nbeenintroducedthatadaptthelearningratesofmodelparameters.Thissection\nwillbrieﬂyreviewafewofthesealgorithms.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 758, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 624}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0759_05c06a9b", "text": "Morerecently,anumberofincremental(ormini-batch-bas ed)methodshave\nbeenintroducedthatadaptthelearningratesofmodelparameters.Thissection\nwillbrieﬂyreviewafewofthesealgorithms. 8.5.1AdaGrad\nTheAdaGradalgorithm,showninalgorithm ,individuallyadaptsthelearning 8.4\nratesofallmodelparametersbyscalingtheminverselyproportionaltothesquare\nrootofthesumofalloftheirhistoricalsquaredvalues(,).The Duchietal.2011\nparameterswiththelargestpartialderivativeofthelosshaveacorrespondingly\nrapiddecreaseintheirlearningrate,whileparameterswithsmallpartialderivatives\nhavearelativelysmalldecreaseintheirlearningrate.Theneteﬀectisgreater\nprogressinthemoregentlyslopeddirectionsofparameterspace.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 759, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 672}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0760_53d182fa", "text": "Inthecontextofconvexoptimization, theAdaGradalgorithmenjoyssome\ndesirabletheoreticalproperties.However,empiricallyithasbeenfoundthat—for\ntrainingdeepneuralnetworkmodels—theaccumulation ofsquaredgradientsfrom\nthebeginningoftrainingcanresultinaprematureandexcessivedecreaseinthe\neﬀectivelearningrate.AdaGradperformswellforsomebutnotalldeeplearning\nmodels. 8.5.2RMSProp\nTheRMSPropalgorithm(,)modiﬁesAdaGradtoperformbetterin Hinton2012\nthenon-convexsettingbychangingthegradientaccumulation intoanexponentially\nweightedmovingaverage.AdaGradisdesignedtoconvergerapidlywhenapplied\ntoaconvexfunction.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 760, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 592}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0761_a65e859a", "text": "When appliedtoanon-convexfunctiontotrainaneural\nnetwork,thelearningtrajectorymaypassthroughmanydiﬀerentstructuresand\neventuallyarriveataregionthatisalocallyconvexbowl.AdaGradshrinksthe\nlearningrateaccordingtotheentirehistoryofthesquaredgradientandmay\n3 0 7\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.4TheAdaGradalgorithm\nRequire:Globallearningrate \nRequire:Initialparameterθ\nRequire:Smallconstant,perhaps δ 10− 7,fornumericalstability\nInitializegradientaccumulationvariabler= 0\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i. Computegradient:g←1\nm∇ θ\ni L f((x( ) i;)θ ,y( ) i)\nAccumulatesquaredgradient:rrgg ←+\nComputeupdate: ∆θ←−\nδ +√rg.(Divisionandsquarerootapplied\nelement-wise)\nApplyupdate:θθθ ← +∆\nendwhile\nhavemadethelearningratetoosmallbeforearrivingatsuchaconvexstructure.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 761, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 884}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0762_8f476cd9", "text": "RMSPropusesanexponentiallydecayingaveragetodiscardhistoryfromthe\nextremepastsothatitcanconvergerapidlyafterﬁndingaconvexbowl,asifit\nwereaninstanceoftheAdaGradalgorithminitializedwithinthatbowl. RMSPropisshowninitsstandardforminalgorithm andcombinedwith 8.5\nNesterovmomentuminalgorithm .ComparedtoAdaGrad,theuseofthe 8.6\nmovingaverageintroducesanewhyperparameter, ρ,thatcontrolsthelengthscale\nofthemovingaverage. Empirically,RMSProphasbeenshowntobeaneﬀectiveandpracticalop-\ntimizationalgorithmfordeepneuralnetworks.Itiscurrentlyoneofthego-to\noptimization methodsbeingemployedroutinelybydeeplearningpractitioners.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 762, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 611}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0763_e035d177", "text": "Empirically,RMSProphasbeenshowntobeaneﬀectiveandpracticalop-\ntimizationalgorithmfordeepneuralnetworks.Itiscurrentlyoneofthego-to\noptimization methodsbeingemployedroutinelybydeeplearningpractitioners. 8.5.3Adam\nAdam( ,)isyetanotheradaptivelearningrateoptimization KingmaandBa2014\nalgorithmandispresentedinalgorithm .Thename“Adam” derivesfrom 8.7\nthephrase“adaptivemoments.”Inthecontextoftheearlieralgorithms,itis\nperhapsbestseenasavariantonthecombinationofRMSPropandmomentum\nwithafewimportantdistinctions.First,inAdam,momentumisincorporated\ndirectlyasanestimateoftheﬁrstordermoment(withexponentialweighting)of\nthegradient.ThemoststraightforwardwaytoaddmomentumtoRMSPropisto\napplymomentumtotherescaledgradients.Theuseofmomentumincombination\nwithrescalingdoesnothaveacleartheoreticalmotivation.Second,Adamincludes\n3 0 8\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.5TheRMSPropalgorithm\nRequire:Globallearningrate,decayrate.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 763, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 930}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0764_525c2f46", "text": " ρ\nRequire:Initialparameterθ\nRequire:Smallconstant δ, usually 10− 6, usedtostabilizedivision bysmall\nnumbers. Initializeaccumulation variablesr= 0\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i. Computegradient:g←1\nm∇ θ\ni L f((x( ) i;)θ ,y( ) i)\nAccumulatesquaredgradient:rrgg ← ρ+(1 )− ρ\nComputeparameterupdate: ∆θ=−√\nδ + rg.(1√\nδ + rappliedelement-wise)\nApplyupdate:θθθ ← +∆\nendwhile\nbiascorrectionstotheestimatesofboththeﬁrst-ordermoments(themomentum\nterm)andthe(uncentered)second-ordermomentstoaccountfortheirinitialization\nattheorigin(seealgorithm ).RMSPropalsoincorporatesanestimateofthe 8.7\n(uncentered)second-ordermoment,howeveritlacksthecorrectionfactor.Thus,\nunlikeinAdam,theRMSPropsecond-ordermomentestimatemayhavehighbias\nearlyintraining.Adamisgenerallyregardedasbeingfairlyrobusttothechoice\nofhyperparameters ,thoughthelearningratesometimesneedstobechangedfrom\nthesuggesteddefault.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 764, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0765_bb6a7b45", "text": "8.5.4ChoosingtheRightOptimizationAlgorithm\nInthissection,wediscussedaseriesofrelatedalgorithmsthateachseektoaddress\nthechallengeofoptimizingdeepmodelsbyadaptingthelearningrateforeach\nmodelparameter.Atthispoint,anaturalquestionis:whichalgorithmshouldone\nchoose? Unfortunately,thereiscurrentlynoconsensusonthispoint. () Schauletal.2014\npresentedavaluablecomparisonofalargenumberofoptimization algorithms\nacrossawiderangeoflearningtasks.Whiletheresultssuggestthatthefamilyof\nalgorithmswithadaptivelearningrates(representedbyRMSPropandAdaDelta)\nperformedfairlyrobustly,nosinglebestalgorithmhasemerged. Currently,themostpopularoptimization algorithmsactivelyinuseinclude\nSGD,SGDwithmomentum,RMSProp,RMSPropwithmomentum,AdaDelta\nandAdam.Thechoiceofwhichalgorithmtouse,atthispoint,seemstodepend\n3 0 9\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.6RMSPropalgorithmwithNesterovmomentum\nRequire:Globallearningrate,decayrate,momentumcoeﬃcient.  ρ α\nRequire:Initialparameter,initialvelocity.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 765, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 990}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0766_ab614b95", "text": " ρ α\nRequire:Initialparameter,initialvelocity. θ v\nInitializeaccumulation variabler= 0\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i. Computeinterimupdate: ˜θθv ← + α\nComputegradient:g←1\nm∇ ˜ θ\ni L f((x( ) i;˜θy) ,( ) i)\nAccumulategradient:rrgg ← ρ+(1 )− ρ\nComputevelocityupdate:vv← α−√rg.(1√rappliedelement-wise)\nApplyupdate:θθv ← +\nendwhile\nlargelyontheuser’sfamiliaritywiththealgorithm(foreaseofhyperparameter\ntuning). 8.6ApproximateSecond-OrderMethods\nInthissectionwediscusstheapplicationofsecond-ordermethodstothetraining\nofdeepnetworks.See ()foranearliertreatmentofthissubject.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 766, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 676}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0767_da5350c6", "text": "8.6ApproximateSecond-OrderMethods\nInthissectionwediscusstheapplicationofsecond-ordermethodstothetraining\nofdeepnetworks.See ()foranearliertreatmentofthissubject. LeCunetal.1998a\nForsimplicityofexposition,theonlyobjectivefunctionweexamineistheempirical\nrisk:\nJ() = θ E x ,y ∼ ˆ pdata ( ) x , y[((;))] = L fxθ , y1\nmm \ni = 1L f((x( ) i;)θ , y( ) i) .(8.25)\nHoweverthemethodswediscusshereextendreadilytomoregeneralobjective\nfunctionsthat,forinstance,includeparameterregularizationtermssuchasthose\ndiscussedinchapter.7\n8.6.1Newton’sMethod\nInsection,weintroducedsecond-ordergradientmethods.Incontrasttoﬁrst- 4.3\nordermethods,second-ordermethodsmakeuseofsecondderivativestoimprove\noptimization. Themostwidelyusedsecond-ordermethodisNewton’smethod.We\nnowdescribeNewton’smethodinmoredetail,withemphasisonitsapplicationto\nneuralnetworktraining. 3 1 0\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.7TheAdamalgorithm\nRequire:Stepsize(Suggesteddefault: )  0001 .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 767, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 961}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0768_84c913dd", "text": "3 1 0\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.7TheAdamalgorithm\nRequire:Stepsize(Suggesteddefault: )  0001 . Require:Exponentialdecayratesformomentestimates, ρ 1and ρ 2in[0 ,1). (Suggesteddefaults:andrespectively) 09 . 0999 . Require:Smallconstant δusedfornumericalstabilization.(Suggesteddefault:\n10− 8)\nRequire:Initialparametersθ\nInitialize1stand2ndmomentvariables ,s= 0r= 0\nInitializetimestep t= 0\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 768, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 551}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0769_b462e676", "text": ". . ,x( ) m}with\ncorrespondingtargetsy( ) i. Computegradient:g←1\nm∇ θ\ni L f((x( ) i;)θ ,y( ) i)\nt t←+1\nUpdatebiasedﬁrstmomentestimate:s← ρ 1s+(1− ρ 1)g\nUpdatebiasedsecondmomentestimate:r← ρ 2r+(1− ρ 2)gg\nCorrectbiasinﬁrstmoment:ˆs←s\n1 − ρt\n1\nCorrectbiasinsecondmoment:ˆr←r\n1 − ρt\n2\nComputeupdate: ∆= θ− ˆs√\nˆ r + δ(operationsappliedelement-wise)\nApplyupdate:θθθ ← +∆\nendwhile\nNewton’smethodisanoptimization schemebasedonusingasecond-orderTay-\nlorseriesexpansiontoapproximate J(θ)nearsomepointθ 0,ignoringderivatives\nofhigherorder:\nJ J () θ≈(θ 0)+(θθ− 0)∇ θ J(θ 0)+1\n2(θθ− 0)Hθθ (− 0) ,(8.26)\nwhereHistheHessianof Jwithrespecttoθevaluatedatθ 0.Ifwethensolvefor\nthecriticalpointofthisfunction,weobtaintheNewtonparameterupdaterule:\nθ∗= θ 0−H− 1∇ θ J(θ 0) (8.27)\nThusforalocallyquadraticfunction(withpositivedeﬁniteH),byrescaling\nthegradientbyH− 1,Newton’smethodjumpsdirectlytotheminimum.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 769, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 889}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0770_1576c675", "text": "If the\nobjectivefunctionisconvexbutnotquadratic(therearehigher-orderterms),this\nupdatecanbeiterated,yieldingthetrainingalgorithmassociatedwithNewton’s\nmethod,giveninalgorithm .8.8\n3 1 1\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.8Newton’smethodwithobjective J(θ) =\n1\nmm\ni = 1 L f((x( ) i;)θ , y( ) i). Require:Initialparameterθ 0\nRequire:Trainingsetofexamples m\nwhile do stoppingcriterionnotmet\nComputegradient:g←1\nm∇ θ\ni L f((x( ) i;)θ ,y( ) i)\nComputeHessian:H←1\nm∇2\nθ\ni L f((x( ) i;)θ ,y( ) i)\nComputeHessianinverse:H− 1\nComputeupdate: ∆= θ−H− 1g\nApplyupdate:θθθ = +∆\nendwhile\nForsurfacesthatarenotquadratic,aslongastheHessianremainspositive\ndeﬁnite,Newton’smethodcanbeappliediteratively.Thisimpliesatwo-step\niterativeprocedure.First,updateorcomputetheinverseHessian(i.e.byupdat-\ningthequadraticapproximation).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 770, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 828}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0771_dbcc9906", "text": "Second, updatetheparametersaccordingto\nequation.8.27\nInsection,wediscussedhowNewton’smethodisappropriateonlywhen 8.2.3\ntheHessianispositivedeﬁnite.Indeeplearning,thesurfaceoftheobjective\nfunctionistypicallynon-convexwithmanyfeatures,suchassaddlepoints,that\nareproblematicforNewton’smethod. IftheeigenvaluesoftheHessianarenot\nallpositive,forexample,nearasaddlepoint,thenNewton’smethodcanactually\ncauseupdatestomoveinthewrongdirection.Thissituationcanbeavoided\nbyregularizingtheHessian.Commonregularizationstrategiesincludeaddinga\nconstant,,alongthediagonaloftheHessian.Theregularizedupdatebecomes α\nθ∗= θ 0−[(( H fθ 0))+ ] αI− 1∇ θ f(θ 0) . (8.28)\nThisregularizationstrategyisusedinapproximations toNewton’smethod,such\nastheLevenberg–Marquardt algorithm(Levenberg1944Marquardt1963 ,;,),and\nworksfairlywellaslongasthenegativeeigenvaluesoftheHessianarestillrelatively\nclosetozero.Incaseswheretherearemoreextremedirectionsofcurvature,the\nvalueof αwouldhavetobesuﬃcientlylargetooﬀsetthenegativeeigenvalues.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 771, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1001}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0772_3df53c47", "text": "However,as αincreasesinsize,theHessianbecomesdominatedbythe αIdiagonal\nandthedirectionchosenbyNewton’smethodconvergestothestandardgradient\ndividedby α. Whenstrongnegativecurvatureispresent, αmayneedtobeso\nlargethatNewton’smethodwouldmakesmallerstepsthangradientdescentwith\naproperlychosenlearningrate.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 772, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 301}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0773_5c40a3cf", "text": "Whenstrongnegativecurvatureispresent, αmayneedtobeso\nlargethatNewton’smethodwouldmakesmallerstepsthangradientdescentwith\naproperlychosenlearningrate. Beyondthechallengescreatedbycertainfeaturesoftheobjectivefunction,\n3 1 2\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nsuchassaddlepoints,theapplicationofNewton’smethodfortraininglargeneural\nnetworksislimitedbythesigniﬁcantcomputational burdenitimposes.The\nnumberofelementsintheHessianissquaredinthenumberofparameters,sowith\nkparameters(andforevenverysmallneuralnetworksthenumberofparameters\nkcanbeinthemillions),Newton’smethodwouldrequiretheinversionofa k k×\nmatrix—with computational complexityof O( k3).Also,sincetheparameterswill\nchangewitheveryupdate,theinverseHessianhastobecomputed ateverytraining\niteration.Asaconsequence,onlynetworkswithaverysmallnumberofparameters\ncanbepracticallytrainedviaNewton’smethod.Intheremainderofthissection,\nwewilldiscussalternativesthatattempttogainsomeoftheadvantagesofNewton’s\nmethodwhileside-steppingthecomputational hurdles.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 773, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1014}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0774_25ba41bf", "text": "8.6.2ConjugateGradients\nConjugategradientsisamethodtoeﬃcientlyavoidthecalculationoftheinverse\nHessianbyiterativelydescendingconjugatedirections.Theinspirationforthis\napproachfollowsfromacarefulstudyoftheweaknessofthemethodofsteepest\ndescent(seesectionfordetails),wherelinesearchesareappliediterativelyin 4.3\nthedirectionassociatedwiththegradient.Figureillustrateshowthemethodof 8.6\nsteepestdescent,whenappliedinaquadraticbowl,progressesinaratherineﬀective\nback-and-forth,zig-zagpattern.Thishappensbecauseeachlinesearchdirection,\nwhengivenbythegradient,isguaranteedtobeorthogonaltothepreviousline\nsearchdirection.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 774, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 612}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0775_56e056b3", "text": "Lettheprevioussearchdirectionbed t − 1.Attheminimum,wheretheline\nsearchterminates,thedirectionalderivativeiszeroindirectiond t − 1:∇ θ J(θ)·\nd t − 1=0.Sincethegradientatthispointdeﬁnesthecurrentsearchdirection,\nd t=∇ θ J(θ) willhavenocontributioninthedirectiond t − 1.Thusd tisorthogonal\ntod t − 1.Thisrelationshipbetweend t − 1andd tisillustratedinﬁgurefor8.6\nmultipleiterationsofsteepestdescent.Asdemonstratedintheﬁgure,thechoiceof\northogonaldirectionsofdescentdonotpreservetheminimumalongtheprevious\nsearchdirections.Thisgivesrisetothezig-zagpatternofprogress,whereby\ndescendingtotheminimuminthecurrentgradientdirection,wemustre-minimize\ntheobjectiveinthepreviousgradientdirection.Thus,byfollowingthegradientat\ntheendofeachlinesearchweare,inasense,undoingprogresswehavealready\nmadeinthedirectionofthepreviouslinesearch.Themethodofconjugategradients\nseekstoaddressthisproblem.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 775, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 878}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0776_8cb2bc4d", "text": "Inthemethodofconjugategradients,weseektoﬁndasearchdirectionthat\nisconjugatetothepreviouslinesearchdirection,i.e.itwillnotundoprogress\nmadeinthatdirection.Attrainingiteration t,thenextsearchdirectiond ttakes\n3 1 3\nCHAPTER8. OPTIMIZATIONFORTRAININGDEEPMODELS\n\u0000 \u0000 \u0000   \u0000\u0000\u0000\nFigure 8.6: The method of steepest descent applied to a quadratic cost surface. The\nmethod of steepest descent involves jumping to the point of lowest cost along the line\ndeﬁnedbythegradientattheinitialpointoneachstep. Thisresolvessomeoftheproblems\nseen with using a ﬁxed learning rate in ﬁgure , but evenwith the optimal step size 4.6\nthealgorithm stillmakesback-and-forthprogress toward theoptimum. Bydeﬁnition, at\ntheminimumof theobjective alongagivendirection, thegradientatthe ﬁnalpointis\northogonalto thatdirection. theform:\nd t= ∇ θ J β( ) +θ td t−1 (8.29)\nwhere β tisacoeﬃcientwhosemagnitudecontrolshowmuchofthedirection, d t−1,\nweshouldaddbacktothecurrentsearchdirection.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 776, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 971}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0777_d75e6ba8", "text": "theform:\nd t= ∇ θ J β( ) +θ td t−1 (8.29)\nwhere β tisacoeﬃcientwhosemagnitudecontrolshowmuchofthedirection, d t−1,\nweshouldaddbacktothecurrentsearchdirection. Twodirections,d tandd t−1,aredeﬁnedasconjugateif d\ntHd t−1= 0,where\nHistheHessianmatrix. Thestraightforwardwaytoimposeconjugacywouldinvolvecalculationofthe\neigenvectorsofHtochoose β t,whichwouldnotsatisfyourgoalofdeveloping\namethodthatismorecomputationallyviablethanNewton’smethodforlarge\nproblems. Canwecalculatetheconjugatedirectionswithoutresortingtothese\ncalculations? Fortunatelytheanswertothatisyes. Twopopularmethodsforcomputingthe β tare:\n1. Fletcher-Reeves:\nβ t=∇ θ J(θ t)∇ θ J(θ t)\n∇ θ J(θ t−1)∇ θ J(θ t−1)(8.30)\n314\nCHAPTER8. OPTIMIZATIONFORTRAININGDEEPMODELS\n2. Polak-Ribière:\nβ t=(∇ θ J(θ t)− ∇ θ J(θ t−1))∇ θ J(θ t)\n∇ θ J(θ t−1)∇ θ J(θ t−1)(8.31)\nForaquadraticsurface,theconjugatedirectionsensurethatthegradientalong\nthepreviousdirectiondoesnotincreaseinmagnitude. Wethereforestayatthe\nminimumalong thepreviousdirections.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 777, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 999}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0778_b79fdaf2", "text": "Wethereforestayatthe\nminimumalong thepreviousdirections. As aconsequence, in a k-dimensional\nparameterspace,theconjugategradientmethodrequiresatmost klinesearchesto\nachievetheminimum. Theconjugategradientalgorithmisgiveninalgorithm .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 778, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 233}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0779_84164c93", "text": "As aconsequence, in a k-dimensional\nparameterspace,theconjugategradientmethodrequiresatmost klinesearchesto\nachievetheminimum. Theconjugategradientalgorithmisgiveninalgorithm . 8.9\nAlgorithm8.9 Theconjugategradientmethod\nRequire:Initialparameters θ 0\nRequire:Trainingsetof examples m\nInitializeρ 0= 0\nInitialize g 0= 0\nInitialize t= 1\nwhile do stoppingcriterionnotmet\nInitializethegradient g t= 0\nComputegradient: g t←1\nm∇ θ\ni L f( (x( ) i; )θ ,y( ) i)\nCompute β t=(g t −g t−1 )g t\ng\nt−1g t−1(Polak-Ribière)\n(Nonlinearconjugategradient: optionallyreset β ttozero,forexampleif tis\namultipleofsomeconstant ,suchas ) k k = 5\nComputesearchdirection: ρ t= −g t+ β tρ t−1\nPerformlinesearchtoﬁnd: ∗= argmin 1\nmm\ni=1 L f( (x( ) i;θ t+ ρ t),y( ) i)\n(On a truly quadratic cost function, analytically solve for ∗rather than\nexplicitlysearchingforit)\nApplyupdate:θ t+1= θ t+ ∗ρ t\nt t←+ 1\nendwhile\nNonlinearConjugateGradients: So far wehave discussed the method of\nconjugategradientsasitisappliedtoquadraticobjectivefunctions.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 779, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1022}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0780_ad451735", "text": "Ofcourse,\nourprimaryinterestinthischapteristoexploreoptimizationmethodsfortraining\nneuralnetworksandotherrelateddeeplearningmodelswherethecorresponding\nobjective function is far from quadratic. Perhaps surprisingly, the method of\nconjugategradientsisstillapplicableinthissetting,thoughwithsomemodiﬁcation. Withoutanyassurancethattheobjectiveisquadratic,theconjugatedirections\n315\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\narenolongerassuredtoremainattheminimumoftheobjectiveforprevious\ndirections.Asaresult,thenonlinearconjugategradientsalgorithmincludes\noccasionalresetswherethemethodofconjugategradientsisrestartedwithline\nsearchalongtheunalteredgradient.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 780, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 659}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0781_ad9da7bc", "text": "Practitionersreportreasonableresultsinapplicationsofthenonlinearconjugate\ngradientsalgorithmtotrainingneuralnetworks,thoughitisoftenbeneﬁcialto\ninitializetheoptimizationwithafewiterationsofstochasticgradientdescentbefore\ncommencingnonlinearconjugategradients.Also,whilethe(nonlinear)conjugate\ngradientsalgorithmhastraditionallybeencastasabatchmethod,minibatch\nversionshavebeenusedsuccessfullyforthetrainingofneuralnetworks(,Leetal. 2011). Adaptationsofconjugategradientsspeciﬁcallyforneuralnetworkshave\nbeenproposedearlier,suchasthescaledconjugategradientsalgorithm(,Moller\n1993). 8.6.3BFGS\nTheBroyden–Fletcher–Goldfarb–Shanno(BFGS)algorithmattemptsto\nbringsomeoftheadvantagesofNewton’smethodwithoutthecomputational\nburden.In thatrespect, BFGSissimilartotheconjugategradientmethod.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 781, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 781}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0782_8aa1b375", "text": "8.6.3BFGS\nTheBroyden–Fletcher–Goldfarb–Shanno(BFGS)algorithmattemptsto\nbringsomeoftheadvantagesofNewton’smethodwithoutthecomputational\nburden.In thatrespect, BFGSissimilartotheconjugategradientmethod. However,BFGStakesamoredirectapproachtotheapproximation ofNewton’s\nupdate.RecallthatNewton’supdateisgivenby\nθ∗= θ 0−H− 1∇ θ J(θ 0) , (8.32)\nwhereHistheHessianof Jwithrespecttoθevaluatedatθ 0.Theprimary\ncomputational diﬃcultyinapplyingNewton’supdateisthecalculationofthe\ninverseHessianH− 1.Theapproachadoptedbyquasi-Newtonmethods(ofwhich\ntheBFGSalgorithmisthemostprominent)istoapproximate theinversewith\namatrixM tthatisiterativelyreﬁnedbylowrankupdatestobecomeabetter\napproximationofH− 1. ThespeciﬁcationandderivationoftheBFGSapproximationisgiveninmany\ntextbooksonoptimization, includingLuenberger1984().", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 782, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 804}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0783_30167bc9", "text": "ThespeciﬁcationandderivationoftheBFGSapproximationisgiveninmany\ntextbooksonoptimization, includingLuenberger1984(). OncetheinverseHessianapproximationM tisupdated,thedirectionofdescent\nρ tisdeterminedbyρ t=M tg t.Alinesearchisperformedinthisdirectionto\ndeterminethesizeofthestep, ∗,takeninthisdirection.Theﬁnalupdatetothe\nparametersisgivenby:\nθ t + 1= θ t+ ∗ρ t . (8.33)\nLikethemethodofconjugategradients,theBFGSalgorithmiteratesaseriesof\nlinesearcheswiththedirectionincorporatingsecond-orderinformation. However\n3 1 6\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nunlikeconjugategradients,thesuccessoftheapproachisnotheavilydependent\nonthelinesearchﬁndingapointveryclosetothetrueminimumalongtheline. Thus,relativetoconjugategradients,BFGShastheadvantagethatitcanspend\nlesstimereﬁningeachlinesearch.Ontheotherhand,theBFGSalgorithmmust\nstoretheinverseHessianmatrix,M,thatrequires O( n2)memory,makingBFGS\nimpracticalformostmoderndeeplearningmodelsthattypicallyhavemillionsof\nparameters.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 783, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 984}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0784_cacfa9d8", "text": "Limited Memory BFGS (or L-BFGS)The memory costs ofthe BFGS\nalgorithmcanbesigniﬁcantlydecreasedbyavoidingstoringthecompleteinverse\nHessianapproximationM.TheL-BFGSalgorithmcomputestheapproximationM\nusingthesamemethodastheBFGSalgorithm,butbeginningwiththeassumption\nthatM( 1 ) t −istheidentitymatrix,ratherthanstoringtheapproximation fromone\nsteptothenext.Ifusedwithexactlinesearches,thedirectionsdeﬁnedbyL-BFGS\naremutuallyconjugate.However,unlikethemethodofconjugategradients,this\nprocedureremainswellbehavedwhentheminimumofthelinesearchisreached\nonlyapproximately .TheL-BFGSstrategywithnostoragedescribedherecanbe\ngeneralizedtoincludemoreinformationabouttheHessianbystoringsomeofthe\nvectorsusedtoupdateateachtimestep,whichcostsonlyperstep. M O n()\n8.7OptimizationStrategiesandMeta-Algorithms\nManyoptimization techniquesarenotexactlyalgorithms, butrathergeneral\ntemplatesthatcanbespecializedtoyieldalgorithms,orsubroutinesthatcanbe\nincorporatedintomanydiﬀerentalgorithms.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 784, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 969}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0785_fe77c889", "text": "8.7.1BatchNormalization\nBatchnormalization ( ,)isoneofthemostexcitingrecent IoﬀeandSzegedy2015\ninnovationsinoptimizingdeepneuralnetworksanditisactuallynotanoptimization\nalgorithmatall.Instead,itisamethodofadaptivereparametrization, motivated\nbythediﬃcultyoftrainingverydeepmodels. Verydeepmodelsinvolvethecompositionofseveralfunctionsorlayers.The\ngradienttellshowtoupdateeachparameter,undertheassumptionthattheother\nlayersdonotchange.Inpractice,weupdateallofthelayerssimultaneously. Whenwemaketheupdate,unexpectedresultscanhappenbecausemanyfunctions\ncomposedtogetherarechangedsimultaneously,usingupdatesthatwerecomputed\nundertheassumptionthattheotherfunctionsremainconstant.Asasimple\n3 1 7\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nexample,supposewehaveadeepneuralnetworkthathasonlyoneunitperlayer\nanddoesnotuseanactivationfunctionateachhiddenlayer:ˆ y= x w 1 w 2 w 3 . . . w l. Here, w iprovidestheweightusedbylayer i.Theoutputoflayer iis h i= h i − 1 w i.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 785, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 959}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0786_3bd84ebe", "text": ". . w l. Here, w iprovidestheweightusedbylayer i.Theoutputoflayer iis h i= h i − 1 w i. Theoutput ˆ yisalinearfunctionoftheinput x,butanonlinearfunctionofthe\nweights w i.Supposeourcostfunctionhasputagradientofon1 ˆ y,sowewishto\ndecreaseˆ yslightly.Theback-propagationalgorithmcanthencomputeagradient\ng=∇ wˆ y.Considerwhathappenswhenwemakeanupdatewwg ← − .The\nﬁrst-orderTaylorseriesapproximation ofˆ ypredictsthatthevalueofˆ ywilldecrease\nby gg.Ifwewantedtodecreaseˆ yby .1,thisﬁrst-orderinformationavailablein\nthegradientsuggestswecouldsetthelearningrate to. 1\ngg.However,theactual\nupdatewillincludesecond-orderandthird-ordereﬀects,onuptoeﬀectsoforder l. Thenewvalueofˆ yisgivenby\nx w( 1−  g 1)( w 2−  g 2)( . . . w l−  g l) . (8.34)\nAnexampleofonesecond-ordertermarisingfromthisupdateis 2g 1 g 2l\ni = 3 w i.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 786, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 818}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0787_2a15323d", "text": "Thenewvalueofˆ yisgivenby\nx w( 1−  g 1)( w 2−  g 2)( . . . w l−  g l) . (8.34)\nAnexampleofonesecond-ordertermarisingfromthisupdateis 2g 1 g 2l\ni = 3 w i. Thistermmightbenegligibleifl\ni = 3 w iissmall,ormightbeexponentiallylarge\niftheweightsonlayersthrough3 laregreaterthan.Thismakesitveryhard 1\ntochooseanappropriatelearningrate,becausetheeﬀectsofanupdatetothe\nparametersforonelayerdependssostronglyonalloftheotherlayers.Second-order\noptimizationalgorithmsaddressthisissuebycomputinganupdatethattakesthese\nsecond-orderinteractionsintoaccount,butwecanseethatinverydeepnetworks,\nevenhigher-orderinteractionscanbesigniﬁcant.Evensecond-orderoptimization\nalgorithmsareexpensiveandusuallyrequirenumerousapproximations thatprevent\nthemfromtrulyaccountingforallsigniﬁcantsecond-orderinteractions. Building\nan n-thorderoptimization algorithmfor n >2thusseemshopeless.Whatcanwe\ndoinstead?", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 787, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 884}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0788_5d64963b", "text": "Building\nan n-thorderoptimization algorithmfor n >2thusseemshopeless.Whatcanwe\ndoinstead? Batchnormalization providesanelegantwayofreparametrizing almostanydeep\nnetwork.Thereparametrization signiﬁcantlyreducestheproblemofcoordinating\nupdatesacrossmanylayers.Batchnormalization canbeappliedtoanyinput\norhiddenlayerinanetwork.LetHbeaminibatchofactivationsofthelayer\ntonormalize,arrangedasadesignmatrix,withtheactivationsforeachexample\nappearinginarowofthematrix.Tonormalize,wereplaceitwith H\nH=Hµ−\nσ, (8.35)\nwhereµisavectorcontainingthemeanofeachunitandσisavectorcontaining\nthestandarddeviationofeachunit.Thearithmetichereisbasedonbroadcasting\nthevectorµandthevectorσtobeappliedtoeveryrowofthematrixH.Within\neachrow,thearithmeticiselement-wise,so H i , jisnormalizedbysubtracting µ j\n3 1 8\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nanddividingby σ j.TherestofthenetworkthenoperatesonHinexactlythe\nsamewaythattheoriginalnetworkoperatedon.H\nAttrainingtime,\nµ=1\nm\niH i , : (8.36)\nand\nσ=\nδ+1\nm\ni( )Hµ−2\ni , (8.37)\nwhere δisasmallpositivevaluesuchas10− 8imposedtoavoidencountering\ntheundeﬁnedgradientof√zat z=0.Crucially, weback-propagatethrough\ntheseoperationsforcomputingthemeanandthestandarddeviation,andfor\napplyingthemtonormalizeH.Thismeansthatthegradientwillneverpropose\nanoperation that actssimplytoincreasethestandard deviationormeanof\nh i;thenormalization operationsremovetheeﬀectofsuchanactionandzero\noutitscomponentinthegradient.Thiswasamajorinnovationofthebatch\nnormalization approach.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 788, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1497}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0789_498079f2", "text": "Previous approacheshadinvolvedaddingpenaltiesto\nthecostfunctiontoencourageunitstohavenormalizedactivationstatisticsor\ninvolvedinterveningtorenormalizeunitstatisticsaftereachgradientdescentstep. Theformerapproachusuallyresultedinimperfectnormalization andthelatter\nusuallyresultedinsigniﬁcantwastedtimeasthelearningalgorithmrepeatedly\nproposedchangingthemeanandvarianceandthenormalization steprepeatedly\nundidthischange.Batchnormalization reparametrizes themodeltomakesome\nunitsalwaysbestandardizedbydeﬁnition,deftlysidesteppingbothproblems. Attesttime,µandσmaybereplacedbyrunningaveragesthatwerecollected\nduringtrainingtime.Thisallowsthemodeltobeevaluatedonasingleexample,\nwithoutneedingtousedeﬁnitionsofµandσthatdependonanentireminibatch. Revisitingtheˆ y= x w 1 w 2 . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 789, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 773}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0790_c2dec21f", "text": "Revisitingtheˆ y= x w 1 w 2 . . . w lexample,weseethatwecanmostlyresolvethe\ndiﬃcultiesinlearningthismodelbynormalizing h l − 1.Supposethat xisdrawn\nfromaunitGaussian.Then h l − 1willalsocomefromaGaussian,becausethe\ntransformationfrom xto h lislinear.However, h l − 1willnolongerhavezeromean\nandunitvariance.Afterapplyingbatchnormalization, weobtainthenormalized\nˆh l − 1thatrestoresthezeromeanandunitvarianceproperties.Foralmostany\nupdatetothelowerlayers,ˆh l − 1willremainaunitGaussian.Theoutput ˆ ymay\nthenbelearnedasasimplelinearfunction ˆ y= w lˆ h l − 1.Learninginthismodelis\nnowverysimplebecausetheparametersatthelowerlayerssimplydonothavean\neﬀectinmostcases;theiroutputisalwaysrenormalizedtoaunitGaussian. In\nsomecornercases,thelowerlayerscanhaveaneﬀect.Changingoneofthelower\nlayerweightstocanmaketheoutputbecomedegenerate,andchangingthesign 0\n3 1 9\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nofoneofthelowerweightscanﬂiptherelationshipbetweenˆ h l − 1and y.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 790, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 966}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0791_a1defacc", "text": "These\nsituationsareveryrare.Withoutnormalization, nearlyeveryupdatewouldhave\nanextremeeﬀectonthestatisticsof h l − 1.Batchnormalization hasthusmade\nthismodelsigniﬁcantlyeasiertolearn. Inthisexample,theeaseoflearningof\ncoursecameatthecostofmakingthelowerlayersuseless.Inourlinearexample,\nthelowerlayersnolongerhaveanyharmfuleﬀect,buttheyalsonolongerhave\nanybeneﬁcialeﬀect.Thisisbecausewehavenormalizedouttheﬁrstandsecond\norderstatistics,whichisallthatalinearnetworkcaninﬂuence.Inadeepneural\nnetworkwithnonlinearactivationfunctions,thelowerlayerscanperformnonlinear\ntransformationsofthedata,sotheyremainuseful.Batchnormalization actsto\nstandardizeonlythemeanandvarianceofeachunitinordertostabilizelearning,\nbutallowstherelationshipsbetweenunitsandthenonlinearstatisticsofasingle\nunittochange.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 791, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 790}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0792_f751874d", "text": "Becausetheﬁnallayerofthenetworkisabletolearnalineartransformation,\nwemayactuallywishtoremovealllinearrelationshipsbetweenunitswithina\nlayer.Indeed,thisistheapproachtakenby (),whoprovided Desjardinsetal.2015\ntheinspirationforbatchnormalization. Unfortunately, eliminating alllinear\ninteractionsismuchmoreexpensivethanstandardizingthemeanandstandard\ndeviationofeachindividualunit,andsofarbatchnormalization remainsthemost\npracticalapproach.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 792, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 438}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0793_45934660", "text": "Unfortunately, eliminating alllinear\ninteractionsismuchmoreexpensivethanstandardizingthemeanandstandard\ndeviationofeachindividualunit,andsofarbatchnormalization remainsthemost\npracticalapproach. Normalizingthemeanandstandarddeviationofaunitcanreducetheexpressive\npowerofthe neuralnetworkcontainingthatunit.Inordertomaintainthe\nexpressivepowerofthenetwork,itiscommontoreplacethebatchofhiddenunit\nactivationsHwithγH+βratherthansimplythenormalizedH.Thevariables\nγandβarelearnedparametersthatallowthenewvariabletohaveanymean\nandstandarddeviation.Atﬁrstglance,thismayseemuseless—whydidweset\nthemeanto 0,andthenintroduceaparameterthatallowsittobesetbackto\nanyarbitraryvalueβ?Theansweristhatthenewparametrization canrepresent\nthesamefamilyoffunctionsoftheinputastheoldparametrization, butthenew\nparametrization hasdiﬀerentlearningdynamics.Intheoldparametrization, the\nmeanofHwasdeterminedbyacomplicatedinteractionbetweentheparameters\ninthelayersbelowH.Inthenewparametrization, themeanofγH+βis\ndeterminedsolelybyβ.Thenewparametrization ismucheasiertolearnwith\ngradientdescent.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 793, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1071}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0794_e135b75f", "text": "Mostneuralnetworklayerstaketheformof φ(XW+b)where φissome\nﬁxednonlinearactivationfunctionsuchastherectiﬁedlineartransformation.It\nisnaturaltowonderwhetherweshouldapplybatchnormalization totheinput\nX,ortothetransformedvalueXW+b. ()recommend IoﬀeandSzegedy2015\n3 2 0\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthelatter.Morespeciﬁcally,XW+bshouldbereplacedbyanormalizedversion\nofXW.Thebiastermshouldbeomittedbecauseitbecomesredundantwith\nthe βparameterappliedbythebatchnormalization reparametrization. Theinput\ntoalayerisusuallytheoutputofanonlinearactivationfunctionsuchasthe\nrectiﬁedlinearfunctioninapreviouslayer. Thestatisticsoftheinputarethus\nmorenon-Gaussianandlessamenabletostandardizationbylinearoperations. Inconvolutionalnetworks,describedinchapter,itisimportanttoapplythe 9\nsamenormalizing µand σateveryspatiallocationwithinafeaturemap,sothat\nthestatisticsofthefeaturemapremainthesameregardlessofspatiallocation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 794, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 922}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0795_a9453632", "text": "8.7.2CoordinateDescent\nInsomecases,itmaybepossibletosolveanoptimization problemquicklyby\nbreakingitintoseparatepieces.Ifweminimize f(x)withrespecttoasingle\nvariable x i, then minimize it with respect to another variable x jand soon,\nrepeatedlycyclingthroughallvariables,weareguaranteedtoarriveata(local)\nminimum.Thispracticeisknownascoordinatedescent,becauseweoptimize\nonecoordinateatatime. Moregenerally,blockcoordinatedescentrefersto\nminimizingwithrespecttoasubsetofthevariablessimultaneously.Theterm\n“coordinatedescent”isoftenusedtorefertoblockcoordinatedescentaswellas\nthestrictlyindividualcoordinatedescent. Coordinatedescentmakesthemostsensewhenthediﬀerentvariablesinthe\noptimization problemcanbeclearlyseparatedintogroupsthatplayrelatively\nisolatedroles,orwhenoptimization withrespecttoonegroupofvariablesis\nsigniﬁcantlymoreeﬃcientthanoptimization withrespecttoallofthevariables.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 795, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 886}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0796_1f974b5d", "text": "Forexample,considerthecostfunction\nJ ,(HW) =\ni , j| H i , j|+\ni , j\nXW−H2\ni , j.(8.38)\nThisfunctiondescribesalearningproblemcalledsparsecoding,wherethegoalis\ntoﬁndaweightmatrixWthatcanlinearlydecodeamatrixofactivationvalues\nHtoreconstructthetrainingsetX.Mostapplicationsofsparsecodingalso\ninvolveweightdecayoraconstraintonthenormsofthecolumnsofW,inorder\ntopreventthepathologicalsolutionwithextremelysmallandlarge.HW\nThefunction Jisnotconvex.However, wecandividetheinputstothe\ntrainingalgorithmintotwosets:thedictionaryparametersWandthecode\nrepresentationsH.Minimizingtheobjectivefunctionwithrespecttoeitheroneof\nthesesetsofvariablesisaconvexproblem.Blockcoordinatedescentthusgives\n3 2 1\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nusanoptimization strategythatallowsustouseeﬃcientconvexoptimization\nalgorithms,byalternatingbetweenoptimizingWwithHﬁxed,thenoptimizing\nHWwithﬁxed.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 796, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 882}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0797_4d94c9f4", "text": "Coordinatedescentisnotaverygoodstrategywhenthevalueofonevariable\nstronglyinﬂuencestheoptimalvalueofanothervariable,asinthefunction f(x) =\n( x 1− x 2)2+ α\nx2\n1+ x2\n2\nwhere αisapositiveconstant.Theﬁrsttermencourages\nthetwovariablestohavesimilarvalue,whilethesecondtermencouragesthem\ntobenearzero.Thesolutionistosetbothtozero.Newton’smethodcansolve\ntheprobleminasinglestepbecauseitisapositivedeﬁnitequadraticproblem. However,forsmall α,coordinatedescentwillmakeveryslowprogressbecausethe\nﬁrsttermdoesnotallowasinglevariabletobechangedtoavaluethatdiﬀers\nsigniﬁcantlyfromthecurrentvalueoftheothervariable. 8.7.3PolyakAveraging\nPolyakaveraging(PolyakandJuditsky1992,)consistsofaveragingtogetherseveral\npoints inthe trajectory through parameter spacevisited by anoptimization\nalgorithm. If titerationsofgradientdescentvisitpointsθ( 1 ), . . . ,θ( ) t,thenthe\noutputofthePolyakaveragingalgorithmisˆθ( ) t=1\nt\niθ( ) i.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 797, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 912}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0798_51118b0b", "text": "If titerationsofgradientdescentvisitpointsθ( 1 ), . . . ,θ( ) t,thenthe\noutputofthePolyakaveragingalgorithmisˆθ( ) t=1\nt\niθ( ) i. Onsomeproblem\nclasses,suchasgradientdescentappliedtoconvexproblems,thisapproachhas\nstrongconvergenceguarantees.Whenappliedtoneuralnetworks,itsjustiﬁcation\nismoreheuristic,butitperformswellinpractice.Thebasicideaisthatthe\noptimization algorithmmayleapbackandforthacrossavalleyseveraltimes\nwithoutevervisitingapointnearthebottomofthevalley.Theaverageofallof\nthelocationsoneithersideshouldbeclosetothebottomofthevalleythough. Innon-convexproblems,thepathtakenbytheoptimization trajectorycanbe\nverycomplicatedandvisitmanydiﬀerentregions.Includingpointsinparameter\nspacefromthedistantpastthatmaybeseparatedfromthecurrentpointbylarge\nbarriersinthecostfunctiondoesnotseemlikeausefulbehavior.Asaresult,\nwhenapplyingPolyakaveragingtonon-convexproblems,itistypicaltousean\nexponentiallydecayingrunningaverage:\nˆθ( ) t= αˆθ( 1 ) t −+(1 )− αθ( ) t.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 798, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 966}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0799_26983fde", "text": "(8.39)\nTherunningaverageapproachisusedinnumerousapplications.SeeSzegedy\netal.()forarecentexample. 2015\n3 2 2\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.7.4SupervisedPretraining\nSometimes,directlytrainingamodeltosolveaspeciﬁctaskcanbetooambitious\nifthemodeliscomplexandhardtooptimizeorifthetaskisverydiﬃcult.Itis\nsometimesmoreeﬀectivetotrainasimplermodeltosolvethetask,thenmake\nthemodelmorecomplex.Itcanalsobemoreeﬀectivetotrainthemodeltosolve\nasimplertask,thenmoveontoconfronttheﬁnaltask.Thesestrategiesthat\ninvolvetrainingsimplemodelsonsimpletasksbeforeconfrontingthechallengeof\ntrainingthedesiredmodeltoperformthedesiredtaskarecollectivelyknownas\npretraining.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 799, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 664}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0800_51e5fcd6", "text": "Greedyalgorithmsbreakaproblemintomanycomponents,thensolvefor\ntheoptimalversionofeachcomponentinisolation.Unfortunately,combiningthe\nindividuallyoptimalcomponentsisnotguaranteedtoyieldanoptimalcomplete\nsolution.However,greedyalgorithmscanbecomputationally muchcheaperthan\nalgorithmsthatsolveforthebestjointsolution,andthequalityofagreedysolution\nisoftenacceptableifnotoptimal.Greedyalgorithmsmayalsobefollowedbya\nﬁne-tuningstageinwhichajointoptimization algorithmsearchesforanoptimal\nsolutiontothefullproblem.Initializingthejointoptimization algorithmwitha\ngreedysolutioncangreatlyspeeditupandimprovethequalityofthesolutionit\nﬁnds. Pretraining,andespeciallygreedypretraining,algorithmsareubiquitousin\ndeeplearning.Inthissection,wedescribespeciﬁcallythosepretrainingalgorithms\nthatbreaksupervisedlearningproblemsintoothersimplersupervisedlearning\nproblems.Thisapproachisknownas .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 800, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 877}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0801_a4c41233", "text": "greedysupervisedpretraining\nIntheoriginal( ,)versionofgreedysupervisedpretraining, Bengioetal.2007\neachstageconsistsofasupervisedlearningtrainingtaskinvolvingonlyasubsetof\nthelayersintheﬁnalneuralnetwork.Anexampleofgreedysupervisedpretraining\nisillustratedinﬁgure,inwhicheachaddedhiddenlayerispretrainedaspart 8.7\nofashallowsupervisedMLP,takingasinputtheoutputofthepreviouslytrained\nhiddenlayer.Insteadofpretrainingonelayeratatime,SimonyanandZisserman\n()pretrainadeepconvolutionalnetwork(elevenweightlayers)andthenuse 2015\ntheﬁrstfourandlastthreelayersfromthisnetworktoinitializeevendeeper\nnetworks(withuptonineteenlayersofweights).Themiddlelayersofthenew,\nverydeepnetworkareinitializedrandomly.Thenewnetworkisthenjointlytrained. Anotheroption,exploredbyYu2010etal.()istousetheofthepreviously outputs\ntrainedMLPs,aswellastherawinput,asinputsforeachaddedstage.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 801, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 859}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0802_485e43e8", "text": "Anotheroption,exploredbyYu2010etal.()istousetheofthepreviously outputs\ntrainedMLPs,aswellastherawinput,asinputsforeachaddedstage. Why would greedy sup ervised pretraining help?The hypothesis  initially\ndiscussedby ()isthatithelpstoprovidebetterguidancetothe Bengioetal.2007\n3 2 3\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\ny y\nh( 1 )h( 1 )\nx x\n( a )U( 1 )U( 1 )\nW( 1 )W( 1 ) y yh( 1 )h( 1 )\nx x\n( b )U( 1 )U( 1 )W( 1 )W( 1 )\ny yh( 1 )h( 1 )\nx x\n( c )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 )\ny y U( 2 )U( 2 ) W( 2 )W( 2 )\ny yh( 1 )h( 1 )\nx x\n( d )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 )y\nU( 2 )U( 2 )\nW( 2 )W( 2 )\nFigure8.7:Illustrationofoneformofgreedysupervisedpretraining( ,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 802, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 672}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0803_3e8ba228", "text": "Bengio e t a l .2007\n( a )Westartbytrainingasuﬃcientlyshallowarchitecture.Anotherdrawingofthe ( b )\nsamearchitecture.Wekeeponlytheinput-to-hiddenlayeroftheoriginalnetworkand ( c )\ndiscardthehidden-to-outputlayer.Wesendtheoutputoftheﬁrsthiddenlayerasinput\ntoanothersupervisedsinglehiddenlayerMLPthatistrainedwiththesameobjective\nastheﬁrstnetworkwas,thusaddingasecondhiddenlayer.Thiscanberepeatedforas\nmanylayersasdesired.Anotherdrawingoftheresult,viewedasafeedforwardnetwork. ( d )\nTofurtherimprovetheoptimization,wecanjointlyﬁne-tuneallthelayers,eitheronlyat\ntheendorateachstageofthisprocess. 3 2 4\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nintermediatelevelsofadeephierarchy.Ingeneral,pretrainingmayhelpbothin\ntermsofoptimization andintermsofgeneralization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 803, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 760}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0804_616167ad", "text": "3 2 4\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nintermediatelevelsofadeephierarchy.Ingeneral,pretrainingmayhelpbothin\ntermsofoptimization andintermsofgeneralization. Anapproachrelatedtosupervisedpretrainingextendstheideatothecontext\noftransferlearning:Yosinski2014etal.()pretrainadeepconvolutionalnetwith8\nlayersofweightsonasetoftasks(asubsetofthe1000ImageNetobjectcategories)\nandtheninitializeasame-sizenetworkwiththeﬁrst klayersoftheﬁrstnet.All\nthelayersofthesecondnetwork(withtheupperlayersinitializedrandomly)are\nthenjointlytrainedtoperformadiﬀerentsetoftasks(anothersubsetofthe1000\nImageNetobjectcategories),withfewertrainingexamplesthanfortheﬁrstsetof\ntasks.Otherapproachestotransferlearningwithneuralnetworksarediscussedin\nsection.15.2\nAnotherrelatedlineofworkistheFitNets( ,)approach.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 804, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 794}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0805_6b07583a", "text": "Romeroetal.2015\nThisapproachbeginsbytraininganetworkthathaslowenoughdepthandgreat\nenoughwidth(numberofunitsperlayer)tobeeasytotrain.Thisnetworkthen\nbecomesateacherforasecondnetwork,designatedthestudent.Thestudent\nnetworkismuchdeeperandthinner(eleventonineteenlayers)andwouldbe\ndiﬃculttotrainwithSGDundernormalcircumstances.Thetrainingofthe\nstudentnetworkismadeeasierbytrainingthestudentnetworknotonlytopredict\ntheoutputfortheoriginaltask,butalsotopredictthevalueofthemiddlelayer\noftheteachernetwork.Thisextrataskprovidesasetofhintsabouthowthe\nhiddenlayersshouldbeusedandcansimplifytheoptimizationproblem.Additional\nparametersareintroducedtoregressthemiddlelayerofthe5-layerteachernetwork\nfromthemiddlelayerofthedeeperstudentnetwork.However,insteadofpredicting\ntheﬁnalclassiﬁcationtarget,theobjectiveistopredictthemiddlehiddenlayer\noftheteachernetwork.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 805, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 851}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0806_55c79811", "text": "Thelowerlayersofthestudentnetworksthushavetwo\nobjectives:tohelptheoutputsofthestudentnetworkaccomplishtheirtask,as\nwellastopredicttheintermediatelayeroftheteachernetwork.Althoughathin\nanddeepnetworkappearstobemorediﬃculttotrainthanawideandshallow\nnetwork,thethinanddeepnetworkmaygeneralizebetterandcertainlyhaslower\ncomputational costifitisthinenoughtohavefarfewerparameters.Without\nthehintsonthehiddenlayer,thestudentnetworkperformsverypoorlyinthe\nexperiments,bothonthetrainingandtestset.Hintsonmiddlelayersmaythus\nbeoneofthetoolstohelptrainneuralnetworksthatotherwiseseemdiﬃcultto\ntrain,butotheroptimization techniquesorchangesinthearchitecturemayalso\nsolvetheproblem. 3 2 5\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.7.5DesigningModelstoAidOptimization\nToimproveoptimization, thebeststrategyisnotalwaystoimprovetheoptimization\nalgorithm.Instead,manyimprovementsintheoptimization ofdeepmodelshave\ncomefromdesigningthemodelstobeeasiertooptimize.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 806, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 949}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0807_890b7e24", "text": "Inprinciple,wecoulduseactivationfunctionsthatincreaseanddecreasein\njaggednon-monotonic patterns.However,thiswouldmakeoptimization extremely\ndiﬃcult.Inpractice, itismoreimportanttochooseamodelfamilythatiseasyto\noptimizethantouseapowerfuloptimizationalgorithm.Mostoftheadvancesin\nneuralnetworklearningoverthepast30yearshavebeenobtainedbychanging\nthemodelfamilyratherthanchangingtheoptimization procedure.Stochastic\ngradientdescentwithmomentum,whichwasusedtotrainneuralnetworksinthe\n1980s,remainsinuseinmodernstateoftheartneuralnetworkapplications. Speciﬁcally,modernneuralnetworksreﬂectadesignchoicetouselineartrans-\nformationsbetweenlayersandactivationfunctionsthatarediﬀerentiable almost\neverywhereandhavesigniﬁcantslopeinlargeportionsoftheirdomain.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 807, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 749}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0808_c0f9c875", "text": "Inpar-\nticular,modelinnovationsliketheLSTM,rectiﬁedlinearunitsandmaxoutunits\nhaveallmovedtowardusingmorelinearfunctionsthanpreviousmodelslikedeep\nnetworksbasedonsigmoidalunits.Thesemodelshavenicepropertiesthatmake\noptimization easier.Thegradientﬂowsthroughmanylayersprovidedthatthe\nJacobianofthelineartransformationhasreasonablesingularvalues. Moreover,\nlinearfunctionsconsistentlyincreaseinasingledirection,soevenifthemodel’s\noutputisveryfarfromcorrect,itisclearsimplyfromcomputingthegradient\nwhichdirectionitsoutputshouldmovetoreducethelossfunction.Inotherwords,\nmodernneuralnetshavebeendesignedsothattheirlocalgradientinformation\ncorrespondsreasonablywelltomovingtowardadistantsolution.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 808, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 689}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0809_379784a5", "text": "Othermodeldesignstrategiescanhelptomakeoptimization easier.For\nexample,linearpathsorskipconnectionsbetweenlayersreducethelengthof\ntheshortestpathfromthelower layer’sparameters totheoutput, and thus\nmitigatethevanishinggradientproblem(Srivastava2015etal.,).Arelatedidea\ntoskipconnectionsisaddingextracopiesoftheoutputthatareattachedtothe\nintermediatehiddenlayersofthenetwork,asinGoogLeNet( ,) Szegedy etal.2014a\nanddeeply-supervisednets(,).These“auxiliaryheads”aretrained Leeetal.2014\ntoperformthesametaskastheprimaryoutputatthetopofthenetworkinorder\ntoensurethatthelowerlayersreceivealargegradient.Whentrainingiscomplete\ntheauxiliaryheadsmaybediscarded.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 809, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 653}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0810_ed015eba", "text": "Thisisanalternativetothepretraining\nstrategies,whichwereintroducedintheprevioussection.Inthisway,onecan\ntrainjointlyallthelayersinasinglephasebutchangethearchitecture, sothat\nintermediatelayers(especiallythelowerones)cangetsomehintsaboutwhatthey\n3 2 6\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nshoulddo,viaashorterpath.Thesehintsprovideanerrorsignaltolowerlayers. 8.7.6ContinuationMethodsandCurriculumLearning\nAsarguedinsection,manyofthechallengesinoptimization arisefromthe 8.2.7\nglobalstructureofthecostfunctionandcannotberesolvedmerelybymakingbetter\nestimatesoflocalupdatedirections.Thepredominant strategyforovercomingthis\nproblemistoattempttoinitializetheparametersinaregionthatisconnected\ntothesolutionbyashortpaththroughparameterspacethatlocaldescentcan\ndiscover.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 810, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 772}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0811_c7b275d4", "text": "Continuationmethodsareafamilyofstrategiesthatcanmakeoptimization\neasierbychoosinginitialpointstoensurethatlocaloptimization spendsmostof\nitstimeinwell-behavedregionsofspace.Theideabehindcontinuationmethodsis\ntoconstructaseriesofobjectivefunctionsoverthesameparameters.Inorderto\nminimizeacostfunction J(θ),wewillconstructnewcostfunctions { J( 0 ), . . . , J( ) n}. Thesecostfunctionsaredesignedtobeincreasinglydiﬃcult,with J( 0 )beingfairly\neasytominimize,and J( ) n,themostdiﬃcult,being J(θ),thetruecostfunction\nmotivatingtheentireprocess.Whenwesaythat J( ) iiseasierthan J( + 1 ) i,we\nmeanthatitiswellbehavedovermoreofθspace.Arandominitialization ismore\nlikelytolandintheregionwherelocaldescentcanminimizethecostfunction\nsuccessfullybecausethisregionislarger.Theseriesofcostfunctionsaredesigned\nsothatasolutiontooneisagoodinitialpointofthenext.Wethusbeginby\nsolvinganeasyproblemthenreﬁnethesolutiontosolveincrementally harder\nproblemsuntilwearriveatasolutiontothetrueunderlyingproblem.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 811, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0812_65c3628a", "text": "Traditionalcontinuationmethods(predatingtheuseofcontinuationmethods\nforneuralnetworktraining)areusuallybasedonsmoothingtheobjectivefunction. SeeWu1997()foranexampleofsuchamethodandareviewofsomerelated\nmethods.Continuationmethodsarealsocloselyrelatedtosimulatedannealing,\nwhichaddsnoisetotheparameters(Kirkpatrick 1983etal.,).Continuation\nmethodshavebeenextremelysuccessfulinrecentyears.SeeMobahiandFisher\n()foranoverviewofrecentliterature,especiallyforAIapplications.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 812, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 467}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0813_63ba3913", "text": "2015\nContinuationmethodstraditionallyweremostlydesignedwiththegoalof\novercomingthechallengeoflocalminima.Speciﬁcally,theyweredesignedto\nreachaglobalminimumdespitethepresenceofmanylocalminima.Todoso,\nthesecontinuationmethodswouldconstructeasiercostfunctionsby“blurring”the\noriginalcostfunction.Thisblurringoperationcanbedonebyapproximating\nJ( ) i() = θ Eθ∼ N ( θ; θ , σ()2 i) J(θ) (8.40)\nviasampling.Theintuitionforthisapproachisthatsomenon-convexfunctions\n3 2 7\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nbecomeapproximately convexwhenblurred.Inmanycases,thisblurringpreserves\nenoughinformationaboutthelocationofaglobalminimumthatwecanﬁndthe\nglobalminimumbysolvingprogressivelylessblurredversionsoftheproblem.This\napproachcanbreakdowninthreediﬀerentways.First,itmightsuccessfullydeﬁne\naseriesofcostfunctionswheretheﬁrstisconvexandtheoptimumtracksfrom\nonefunctiontothenextarrivingattheglobalminimum,butitmightrequireso\nmanyincrementalcostfunctionsthatthecostoftheentireprocedureremainshigh.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 813, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 993}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0814_53f34443", "text": "NP-hardoptimization problemsremainNP-hard,evenwhencontinuationmethods\nareapplicable.Theothertwowaysthatcontinuationmethodsfailbothcorrespond\ntothemethodnotbeingapplicable.First,thefunctionmightnotbecomeconvex,\nnomatterhowmuchitisblurred.Considerforexamplethefunction J(θ) =−θθ. Second,thefunctionmaybecomeconvexasaresultofblurring,buttheminimum\nofthisblurredfunctionmaytracktoalocalratherthanaglobalminimumofthe\noriginalcostfunction. Thoughcontinuationmethodsweremostlyoriginallydesignedtodealwiththe\nproblemoflocalminima,localminimaarenolongerbelievedtobetheprimary\nproblemforneuralnetworkoptimization. Fortunately,continuationmethodscan\nstillhelp.Theeasierobjectivefunctionsintroducedbythecontinuationmethodcan\neliminateﬂatregions,decreasevarianceingradientestimates,improveconditioning\noftheHessianmatrix,ordoanythingelsethatwilleithermakelocalupdates\neasiertocomputeorimprovethecorrespondencebetweenlocalupdatedirections\nandprogresstowardaglobalsolution.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 814, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 959}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0815_97778943", "text": "Bengio2009etal.()observedthatanapproachcalledcurriculumlearning\norshapingcanbeinterpretedasacontinuationmethod.Curriculumlearningis\nbasedontheideaofplanningalearningprocesstobeginbylearningsimpleconcepts\nandprogresstolearningmorecomplexconceptsthatdependonthesesimpler\nconcepts.Thisbasicstrategywaspreviouslyknowntoaccelerateprogressinanimal\ntraining(,;,; Skinner1958Peterson2004KruegerandDayan2009,)andmachine\nlearning(,;,;,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 815, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 427}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0816_7e0d8ad4", "text": "() Solomonoﬀ1989Elman1993Sanger1994Bengioetal.2009\njustiﬁedthisstrategyasacontinuationmethod,whereearlier J( ) iaremadeeasierby\nincreasingtheinﬂuenceofsimplerexamples(eitherbyassigningtheircontributions\ntothecostfunctionlargercoeﬃcients,orbysamplingthemmorefrequently),and\nexperimentallydemonstratedthatbetterresultscouldbeobtainedbyfollowinga\ncurriculumonalarge-scaleneurallanguagemodelingtask.Curriculumlearning\nhasbeensuccessfulonawiderangeofnaturallanguage(Spitkovsky2010etal.,;\nCollobert2011aMikolov2011bTuandHonavar2011 etal.,; etal.,; ,)andcomputer\nvision( ,; ,; ,) Kumaretal.2010LeeandGrauman2011SupancicandRamanan2013\ntasks.Curriculumlearningwasalsoveriﬁedasbeingconsistentwiththewayin\nwhichhumans teach(,):teachersstartbyshowingeasierand Khanetal.2011\n3 2 8\nCHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nmoreprototypicalexamplesandthenhelpthelearnerreﬁnethedecisionsurface\nwiththelessobviouscases.Curriculum-based strategiesaremoreeﬀectivefor\nteachinghumansthanstrategiesbasedonuniformsamplingofexamples,andcan\nalsoincreasetheeﬀectivenessofotherteachingstrategies( , BasuandChristensen\n2013).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 816, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1100}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0817_335811f6", "text": "Anotherimportantcontributiontoresearchoncurriculumlearningaroseinthe\ncontextoftrainingrecurrentneuralnetworkstocapturelong-termdependencies:\nZarembaandSutskever2014()foundthatmuchbetterresultswereobtainedwitha\nstochasticcurriculum,inwhicharandommixofeasyanddiﬃcultexamplesisalways\npresentedtothelearner,butwheretheaverageproportionofthemorediﬃcult\nexamples(here,thosewithlonger-termdependencies)isgraduallyincreased.With\nadeterministiccurriculum,noimprovementoverthebaseline(ordinarytraining\nfromthefulltrainingset)wasobserved. Wehavenowdescribedthebasicfamilyofneuralnetworkmodelsandhowto\nregularizeandoptimizethem.Inthechaptersahead,weturntospecializationsof\ntheneuralnetworkfamily,thatallowneuralnetworkstoscaletoverylargesizesand\nprocessinputdatathathasspecialstructure.Theoptimization methodsdiscussed\ninthischapterareoftendirectlyapplicabletothesespecializedarchitectures with\nlittleornomodiﬁcation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 817, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 905}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0818_998c1b1c", "text": "3 2 9\nC h a p t e r 9\nC on v ol u t i on al N e t w orks\nCon v o l ut i o na l net w o r k s(,),alsoknownas LeCun1989 c o n v o l ut i o na l neur al\nnet w o r k sorCNNs,areaspecializedkindofneuralnetworkforprocessingdata\nthathasaknown,grid-liketopology.Examplesincludetime-seriesdata,whichcan\nbethoughtofasa1Dgridtakingsamplesatregulartimeintervals,andimagedata,\nwhichcanbethoughtofasa2Dgridofpixels.Convolutionalnetworkshavebeen\ntremendouslysuccessfulinpracticalapplications.Thename“convolutionalneural\nnetwork”indicatesthatthenetworkemploysamathematical operationcalled\nc o n v o l ut i o n.Convolutionisaspecializedkindoflinearoperation.Convolutional\nnetworksaresimplyneuralnetworksthatuseconvolutioninplaceofgeneralmatrix\nmultiplicationinatleastoneoftheirlayers.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 818, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 767}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0819_104a3514", "text": "Inthis chapter, wewillﬁrst describewhatconvolutionis.Next, wewill\nexplainthemotivationbehindusingconvolutioninaneuralnetwork.Wewillthen\ndescribeanoperationcalled p o o l i ng,whichalmostallconvolutionalnetworks\nemploy.Usually,theoperationusedinaconvolutionalneuralnetworkdoesnot\ncorrespondpreciselytothedeﬁnitionofconvolutionasusedinotherﬁeldssuch\nasengineeringorpuremathematics.Wewilldescribeseveralvariantsonthe\nconvolutionfunctionthatarewidelyusedinpracticeforneuralnetworks.We\nwillalso show how convolutionmaybeappliedtomanykindsofdata, with\ndiﬀerentnumbersofdimensions.Wethendiscussmeansofmakingconvolution\nmoreeﬃcient.Convolutionalnetworksstandoutasanexampleofneuroscientiﬁc\nprinciplesinﬂuencingdeeplearning.Wewilldiscusstheseneuroscientiﬁcprinciples,\nthenconcludewithcommentsabouttheroleconvolutionalnetworkshaveplayed\ninthehistoryofdeeplearning.Onetopicthischapterdoesnotaddressishowto\nchoosethearchitectureofyourconvolutionalnetwork.Thegoalofthischapteris\ntodescribethekindsoftoolsthatconvolutionalnetworksprovide,whilechapter11\n330\nCHAPTER9.CONVOLUTIONALNETWORKS\ndescribesgeneralguidelinesforchoosingwhichtoolstouseinwhichcircumstances.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 819, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1146}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0820_a802d5e6", "text": "Researchintoconvolutionalnetworkarchitecturesproceedssorapidlythatanew\nbestarchitectureforagivenbenchmarkisannouncedeveryfewweekstomonths,\nrenderingitimpracticaltodescribethebestarchitectureinprint.However,the\nbestarchitectureshaveconsistentlybeencomposedofthebuildingblocksdescribed\nhere. 9.1TheConvolutionOperation\nInitsmostgeneralform,convolutionisanoperationontwofunctionsofareal-\nvaluedargument.Tomotivatethedeﬁnitionofconvolution,westartwithexamples\noftwofunctionswemightuse. Supposewearetrackingthelocationofaspaceshipwithalasersensor.Our\nlasersensorprovidesasingleoutput x( t),thepositionofthespaceshipattime\nt.Both xand tarereal-valued,i.e.,wecangetadiﬀerentreadingfromthelaser\nsensoratanyinstantintime. Nowsupposethatourlasersensorissomewhatnoisy.Toobtainalessnoisy\nestimateofthespaceship’sposition,wewouldliketoaveragetogetherseveral\nmeasurements.Ofcourse,morerecentmeasurementsaremorerelevant,sowewill\nwantthistobeaweightedaveragethatgivesmoreweighttorecentmeasurements.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 820, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 982}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0821_dbfc2f8f", "text": "Wecandothiswithaweightingfunction w( a),where aistheageofameasurement. Ifweapplysuchaweightedaverageoperationateverymoment,weobtainanew\nfunctionprovidingasmoothedestimateofthepositionofthespaceship: s\ns t() =\nx a w t a d a ()( −) (9.1)\nThisoperationiscalled c o n v o l ut i o n.Theconvolutionoperationistypically\ndenotedwithanasterisk:\ns t x w t () = ( ∗)() (9.2)\nInourexample, wneedstobeavalidprobabilitydensityfunction,orthe\noutputisnotaweightedaverage.Also, wneedstobeforallnegativearguments, 0\noritwilllookintothefuture,whichispresumablybeyondourcapabilities.These\nlimitationsareparticulartoourexamplethough.Ingeneral,convolutionisdeﬁned\nforanyfunctionsforwhichtheaboveintegralisdeﬁned,andmaybeusedforother\npurposesbesidestakingweightedaverages.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 821, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 751}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0822_f59cfe2e", "text": "Inconvolutionalnetworkterminology,theﬁrstargument(inthisexample,the\nfunction x)totheconvolutionisoftenreferredtoasthe i nputandthesecond\n3 3 1\nCHAPTER9.CONVOLUTIONALNETWORKS\nargument(inthisexample,thefunction w)asthe k e r nel.Theoutputissometimes\nreferredtoasthe .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 822, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 265}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0823_f71d660c", "text": "f e at ur e m ap\nInourexample,theideaofalasersensorthatcanprovidemeasurements\nateveryinstantintimeisnotrealistic.Usually,whenweworkwithdataona\ncomputer,timewillbediscretized,andoursensorwillprovidedataatregular\nintervals.Inourexample,itmightbemorerealistictoassumethatourlaser\nprovidesameasurementoncepersecond.Thetimeindex tcanthentakeononly\nintegervalues.Ifwenowassumethat xand waredeﬁnedonlyoninteger t,we\ncandeﬁnethediscreteconvolution:\ns t x w t () = ( ∗)() =∞\na = − ∞x a w t a ()( −) (9.3)\nInmachinelearningapplications,theinputisusuallyamultidimensional array\nofdataandthekernelisusuallyamultidimensionalarrayofparametersthatare\nadaptedbythelearningalgorithm.Wewillrefertothesemultidimensional arrays\nastensors.Becauseeachelementoftheinputandkernelmustbeexplicitlystored\nseparately,weusuallyassumethatthesefunctionsarezeroeverywherebutthe\nﬁnitesetofpointsforwhichwestorethevalues.Thismeansthatinpracticewe\ncanimplementtheinﬁnitesummationasasummationoveraﬁnitenumberof\narrayelements.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 823, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 990}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0824_3fcce624", "text": "Finally,weoftenuseconvolutionsovermorethanoneaxisatatime.For\nexample,ifweuseatwo-dimensionalimage Iasourinput,weprobablyalsowant\ntouseatwo-dimensionalkernel: K\nS i , j I K i , j () = ( ∗)() =\nm\nnI m , n K i m , j n . ( )( − −)(9.4)\nConvolutioniscommutative,meaningwecanequivalentlywrite:\nS i , j K I i , j () = ( ∗)() =\nm\nnI i m , j n K m , n . ( − −)( )(9.5)\nUsuallythelatterformulaismorestraightforwardtoimplementinamachine\nlearninglibrary,becausethereislessvariationintherangeofvalidvaluesof m\nand.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 824, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 505}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0825_f6689c65", "text": "( − −)( )(9.5)\nUsuallythelatterformulaismorestraightforwardtoimplementinamachine\nlearninglibrary,becausethereislessvariationintherangeofvalidvaluesof m\nand. n\nThecommutativepropertyofconvolutionarisesbecausewehave ﬂi pp e dthe\nkernelrelativetotheinput,inthesensethatas mincreases,theindexintothe\ninputincreases,buttheindexintothekerneldecreases.Theonlyreasontoﬂip\nthekernelistoobtainthecommutativeproperty.Whilethecommutativeproperty\n3 3 2\nCHAPTER9.CONVOLUTIONALNETWORKS\nisusefulforwritingproofs,itisnotusuallyanimportantpropertyofaneural\nnetworkimplementation.Instead,manyneuralnetworklibrariesimplementa\nrelatedfunctioncalledthe c r o ss-c o r r e l a t i o n,whichisthesameasconvolution\nbutwithoutﬂippingthekernel:\nS i , j I K i , j () = ( ∗)() =\nm\nnI i m , j n K m , n . (+ +)( )(9.6)\nManymachinelearninglibrariesimplementcross-correlationbutcallitconvolution.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 825, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 866}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0826_eee9a39b", "text": "(+ +)( )(9.6)\nManymachinelearninglibrariesimplementcross-correlationbutcallitconvolution. Inthistextwewillfollowthisconventionofcallingbothoperationsconvolution,\nandspecifywhetherwemeantoﬂipthekernelornotincontextswherekernel\nﬂippingisrelevant.Inthecontextofmachinelearning,thelearningalgorithmwill\nlearntheappropriatevaluesofthekernelintheappropriateplace,soanalgorithm\nbasedonconvolutionwithkernelﬂippingwilllearnakernelthatisﬂippedrelative\ntothekernellearnedbyanalgorithmwithouttheﬂipping.Itisalsorarefor\nconvolutiontobeusedaloneinmachinelearning;insteadconvolutionisused\nsimultaneouslywithotherfunctions,andthecombinationofthesefunctionsdoes\nnotcommuteregardlessofwhethertheconvolutionoperationﬂipsitskernelor\nnot. Seeﬁgureforanexampleofconvolution(withoutkernelﬂipping)applied 9.1\ntoa2-Dtensor.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 826, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 799}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0827_5e281b71", "text": "Seeﬁgureforanexampleofconvolution(withoutkernelﬂipping)applied 9.1\ntoa2-Dtensor. Discreteconvolutioncanbeviewedasmultiplicationbyamatrix.However,the\nmatrixhasseveralentriesconstrainedtobeequaltootherentries.Forexample,\nforunivariatediscreteconvolution,eachrowofthematrixisconstrainedtobe\nequaltotherowaboveshiftedbyoneelement.Thisisknownasa T o e pl i t z\nm at r i x.Intwodimensions,a doubly bl o c k c i r c ul an t m at r i xcorrespondsto\nconvolution.Inadditiontotheseconstraintsthatseveralelementsbeequalto\neachother,convolutionusuallycorrespondstoaverysparsematrix(amatrix\nwhoseentriesaremostlyequaltozero).Thisisbecausethekernelisusuallymuch\nsmallerthantheinputimage.Anyneuralnetworkalgorithmthatworkswith\nmatrixmultiplication anddoesnotdependonspeciﬁcpropertiesofthematrix\nstructureshouldworkwithconvolution,withoutrequiringanyfurtherchanges\ntotheneuralnetwork.Typicalconvolutionalneuralnetworksdomakeuseof\nfurtherspecializationsinordertodealwithlargeinputseﬃciently,buttheseare\nnotstrictlynecessaryfromatheoreticalperspective.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 827, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1033}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0828_dfdbe65e", "text": "3 3 3\nCHAPTER9.CONVOLUTIONALNETWORKS\na b c d\ne f g h\ni j k lw x\ny z\na w + b x +\ne y + f za w + b x +\ne y + f zb w + c x +\nf y + g zb w + c x +\nf y + g zc w + d x +\ng y + h zc w + d x +\ng y + h z\ne w + f x +\ni y + j ze w + f x +\ni y + j zf w + g x +\nj y + k zf w + g x +\nj y + k zg w + h x +\nk y + l zg w + h x +\nk y + l zI nput\nK e r ne l\nO ut put\nFigure9.1:Anexampleof2-Dconvolutionwithoutkernel-ﬂipping.Inthiscasewerestrict\ntheoutputtoonlypositionswherethekernelliesentirelywithintheimage,called“valid”\nconvolutioninsomecontexts.Wedrawboxeswitharrowstoindicatehowtheupper-left\nelementoftheoutputtensorisformedbyapplyingthekerneltothecorresponding\nupper-leftregionoftheinputtensor.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 828, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 682}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0829_d1935148", "text": "3 3 4\nCHAPTER9.CONVOLUTIONALNETWORKS\n9.2Motivation\nConvolutionleveragesthreeimportantideasthatcanhelpimproveamachine\nlearningsystem: spar se i nt e r ac t i o n s, par ameter shar i ngand e q ui v ar i an t\nr e pr e se n t at i o ns.Moreover, convolutionprovidesameansforworkingwith\ninputsofvariablesize.Wenowdescribeeachoftheseideasinturn.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 829, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 340}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0830_6c47aba0", "text": "Traditionalneuralnetworklayersusematrixmultiplicationbyamatrixof\nparameterswithaseparateparameterdescribingtheinteractionbetweeneachinput\nunitandeachoutputunit.Thismeanseveryoutputunitinteractswitheveryinput\nunit.Convolutionalnetworks,however,typicallyhave spar se i n t e r ac t i o ns(also\nreferredtoas spar se c o nnec t i v i t yor spar se wei g h t s).Thisisaccomplishedby\nmakingthekernelsmallerthantheinput.Forexample,whenprocessinganimage,\ntheinputimagemighthavethousandsormillionsofpixels,butwecandetectsmall,\nmeaningfulfeaturessuchasedgeswithkernelsthatoccupyonlytensorhundredsof\npixels.Thismeansthatweneedtostorefewerparameters,whichbothreducesthe\nmemoryrequirementsofthemodelandimprovesitsstatisticaleﬃciency.Italso\nmeansthatcomputingtheoutputrequiresfeweroperations.Theseimprovements\nineﬃciencyareusuallyquitelarge.Ifthereare minputsand noutputs,then\nmatrixmultiplication requires m n ×parametersandthealgorithmsusedinpractice\nhave O( m n ×)runtime(perexample).Ifwelimitthenumberofconnections\neachoutputmayhaveto k,thenthesparselyconnectedapproachrequiresonly\nk n ×parametersand O( k n ×)runtime.Formanypracticalapplications,itis\npossibletoobtaingoodperformanceonthemachinelearningtaskwhilekeeping\nkseveralordersofmagnitudesmallerthan m.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 830, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1249}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0831_267c1929", "text": "Forgraphicaldemonstrationsof\nsparseconnectivity,seeﬁgureandﬁgure.Inadeepconvolutionalnetwork, 9.2 9.3\nunitsinthedeeperlayersmayindirectlyinteractwithalargerportionoftheinput,\nasshowninﬁgure.Thisallowsthenetworktoeﬃcientlydescribecomplicated 9.4\ninteractionsbetweenmanyvariablesbyconstructingsuchinteractionsfromsimple\nbuildingblocksthateachdescribeonlysparseinteractions.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 831, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 371}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0832_2e7eedd5", "text": "P ar amet e r shar i ngreferstousingthesameparameterformorethanone\nfunctioninamodel.Inatraditionalneuralnet,eachelementoftheweightmatrix\nisusedexactlyoncewhencomputingtheoutputofalayer.Itismultipliedby\noneelementoftheinputandthenneverrevisited.Asasynonymforparameter\nsharing,onecansaythatanetworkhas t i e d w e i g h t s,becausethevalueofthe\nweightappliedtooneinputistiedtothevalueofaweightappliedelsewhere.In\naconvolutionalneuralnet,eachmemberofthekernelisusedateveryposition\noftheinput(exceptperhapssomeoftheboundarypixels, dependingonthe\ndesigndecisionsregardingtheboundary).Theparametersharingusedbythe\nconvolutionoperationmeansthatratherthanlearningaseparatesetofparameters\n3 3 5\nCHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nFigure9.2: S p a r s e c o n n e c t i v i t y , v i e w e d f r o m b e l o w :Wehighlightoneinputunit, x 3,\nandalsohighlighttheoutputunitsin sthatareaﬀectedbythisunit.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 832, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1040}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0833_256849cd", "text": "( T o p )When sis\nformedbyconvolutionwithakernelofwidth,onlythreeoutputsareaﬀectedby 3 x. ( Bottom )Whenisformedbymatrixmultiplication,connectivityisnolongersparse,so s\nalloftheoutputsareaﬀectedby x 3. 3 3 6\nCHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nFigure9.3: S p a r s e c o n n e c t i v i t y , v i e w e d f r o m a b o v e :  Wehighlightoneoutputunit, s 3,\nandalsohighlighttheinputunitsin xthataﬀectthisunit.Theseunitsareknown\nasthereceptiveﬁeldof s 3. ( T o p )When sisformedbyconvolutionwithakernelof\nwidth,onlythreeinputsaﬀect 3 s 3.When ( Bottom ) sisformedbymatrixmultiplication,\nconnectivityisnolongersparse,soalloftheinputsaﬀect s 3.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 833, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 789}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0834_7d0a493e", "text": "( T o p )When sisformedbyconvolutionwithakernelof\nwidth,onlythreeinputsaﬀect 3 s 3.When ( Bottom ) sisformedbymatrixmultiplication,\nconnectivityisnolongersparse,soalloftheinputsaﬀect s 3. x 1 x 1 x 2 x 2 x 3 x 3h 2 h 2 h 1 h 1 h 3 h 3\nx 4 x 4h 4 h 4\nx 5 x 5h 5 h 5g 2 g 2 g 1 g 1 g 3 g 3 g 4 g 4 g 5 g 5\nFigure9.4:Thereceptiveﬁeldoftheunitsinthedeeperlayersofaconvolutionalnetwork\nislargerthanthereceptiveﬁeldoftheunitsintheshallowlayers.Thiseﬀectincreasesif\nthenetworkincludesarchitecturalfeatureslikestridedconvolution(ﬁgure)orpooling 9.12\n(section).Thismeansthateventhough 9.3 d i r e c tconnectionsinaconvolutionalnetare\nverysparse,unitsinthedeeperlayerscanbe i n d i r e c t l yconnectedtoallormostofthe\ninputimage.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 834, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 720}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0835_9e2a7ac2", "text": "3 3 7\nCHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4 x 5 x 5s 2 s 2 s 1 s 1 s 3 s 3 s 4 s 4 s 5 s 5\nFigure9.5:Parametersharing:Blackarrowsindicatetheconnectionsthatuseaparticular\nparameterintwodiﬀerentmodels. ( T o p )Theblackarrowsindicateusesofthecentral\nelementofa3-elementkernelinaconvolutionalmodel.Duetoparametersharing,this\nsingleparameterisusedatallinputlocations.Thesingleblackarrowindicates ( Bottom )\ntheuseofthecentralelementoftheweightmatrixinafullyconnectedmodel.Thismodel\nhasnoparametersharingsotheparameterisusedonlyonce.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 835, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 630}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0836_3774abbd", "text": "foreverylocation,welearnonlyoneset.Thisdoesnotaﬀecttheruntimeof\nforwardpropagation—it isstill O( k n ×)—butitdoesfurtherreducethestorage\nrequirementsofthemodelto kparameters.Recallthat kisusuallyseveralorders\nofmagnitudelessthan m.Since mand nareusuallyroughlythesamesize, kis\npracticallyinsigniﬁcantcomparedto m n ×.Convolutionisthusdramatically more\neﬃcientthandensematrixmultiplication intermsofthememoryrequirements\nandstatisticaleﬃciency.Foragraphicaldepictionofhowparametersharingworks,\nseeﬁgure.9.5\nAsanexampleofbothoftheseﬁrsttwoprinciplesinaction,ﬁgureshows9.6\nhowsparseconnectivityandparametersharingcandramatically improvethe\neﬃciencyofalinearfunctionfordetectingedgesinanimage. Inthecaseofconvolution,theparticularformofparametersharingcausesthe\nlayertohaveapropertycalled e q ui v ar i anc etotranslation.Tosayafunctionis\nequivariantmeansthatiftheinputchanges,theoutputchangesinthesameway. Speciﬁcally,afunction f( x)isequivarianttoafunction gif f( g( x))= g( f( x)).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 836, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 980}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0837_683d06e9", "text": "Speciﬁcally,afunction f( x)isequivarianttoafunction gif f( g( x))= g( f( x)). Inthecaseofconvolution,ifwelet gbeanyfunctionthattranslatestheinput,\ni.e.,shiftsit,thentheconvolutionfunctionisequivariantto g.Forexample,let I\nbeafunctiongivingimagebrightnessatintegercoordinates.Let gbeafunction\n3 3 8\nCHAPTER9.CONVOLUTIONALNETWORKS\nmappingoneimagefunctiontoanotherimagefunction,suchthat I= g( I)is\ntheimagefunctionwith I( x , y)= I( x −1 , y).Thisshiftseverypixelof Ione\nunittotheright.Ifweapplythistransformationto I,thenapplyconvolution,\ntheresultwillbethesameasifweappliedconvolutionto I,thenappliedthe\ntransformation gtotheoutput.Whenprocessingtimeseriesdata,thismeans\nthatconvolutionproducesasortoftimelinethatshowswhendiﬀerentfeatures\nappearintheinput.Ifwemoveaneventlaterintimeintheinput,theexact\nsamerepresentationofitwillappearintheoutput,justlaterintime.Similarly\nwithimages,convolutioncreatesa2-Dmapofwherecertainfeaturesappearin\ntheinput.Ifwemovetheobjectintheinput,itsrepresentationwillmovethe\nsameamountintheoutput.Thisisusefulforwhenweknowthatsomefunction\nofasmallnumberofneighboringpixelsisusefulwhenappliedtomultipleinput\nlocations.Forexample,whenprocessingimages,itisusefultodetectedgesin\ntheﬁrstlayerofaconvolutionalnetwork.Thesameedgesappearmoreorless\neverywhereintheimage,soitispracticaltoshareparametersacrosstheentire\nimage.Insomecases,wemaynotwishtoshareparametersacrosstheentire\nimage.Forexample,ifweareprocessingimagesthatarecroppedtobecentered\nonanindividual’sface,weprobablywanttoextractdiﬀerentfeaturesatdiﬀerent\nlocations—thepartofthenetworkprocessingthetopofthefaceneedstolookfor\neyebrows,whilethepartofthenetworkprocessingthebottomofthefaceneedsto\nlookforachin.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 837, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1693}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0838_8034f035", "text": "Convolutionisnotnaturallyequivarianttosomeothertransformations,such\naschangesinthescaleorrotationofanimage.Othermechanismsarenecessary\nforhandlingthesekindsoftransformations. Finally,somekindsofdatacannotbeprocessedbyneuralnetworksdeﬁnedby\nmatrixmultiplication withaﬁxed-shapematrix.Convolutionenablesprocessing\nofsomeofthesekindsofdata.Wediscussthisfurtherinsection.9.7\n9.3Pooling\nAtypicallayerofaconvolutionalnetworkconsistsofthreestages(seeﬁgure).9.7\nIntheﬁrststage,thelayerperformsseveralconvolutionsinparalleltoproducea\nsetoflinearactivations.Inthesecondstage,eachlinearactivationisrunthrough\nanonlinearactivationfunction,suchastherectiﬁedlinearactivationfunction. Thisstageissometimescalledthe det e c t o rstage. Inthethirdstage,weusea\np o o l i ng f unc t i o ntomodifytheoutputofthelayerfurther.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 838, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 804}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0839_5b25266e", "text": "Thisstageissometimescalledthe det e c t o rstage. Inthethirdstage,weusea\np o o l i ng f unc t i o ntomodifytheoutputofthelayerfurther. Apoolingfunctionreplacestheoutputofthenetatacertainlocationwitha\nsummarystatisticofthenearbyoutputs.Forexample,the m ax p o o l i ng(Zhou\n3 3 9\nCHAPTER9.CONVOLUTIONALNETWORKS\nFigure9.6: E ﬃ c i e n c y o f e d g e d e t e c t i o n. Theimageontherightwasformedbytaking\neachpixelintheoriginalimageandsubtractingthevalueofitsneighboringpixelonthe\nleft. Thisshowsthestrengthofalloftheverticallyorientededgesintheinputimage,\nwhichcanbeausefuloperationforobjectdetection.Bothimagesare280pixelstall.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 839, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 628}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0840_3c21a8b3", "text": "Thisshowsthestrengthofalloftheverticallyorientededgesintheinputimage,\nwhichcanbeausefuloperationforobjectdetection.Bothimagesare280pixelstall. Theinputimageis320pixelswidewhiletheoutputimageis319pixelswide.This\ntransformationcanbedescribedbyaconvolutionkernelcontainingtwoelements,and\nrequires319 ×280 ×3=267 ,960ﬂoatingpointoperations(twomultiplicationsand\noneadditionperoutputpixel)tocomputeusingconvolution.Todescribethesame\ntransformationwithamatrixmultiplicationwouldtake320 ×280 ×319 ×280,orover\neightbillion,entriesinthematrix,makingconvolutionfourbilliontimesmoreeﬃcientfor\nrepresentingthistransformation.Thestraightforwardmatrixmultiplicationalgorithm\nperformsoversixteenbillionﬂoatingpointoperations,makingconvolutionroughly60,000\ntimesmoreeﬃcientcomputationally.Ofcourse,mostoftheentriesofthematrixwouldbe\nzero.Ifwestoredonlythenonzeroentriesofthematrix,thenbothmatrixmultiplication\nandconvolutionwouldrequirethesamenumberofﬂoatingpointoperationstocompute.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 840, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 967}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0841_b0ddf117", "text": "Thematrixwouldstillneedtocontain2 ×319 ×280=178 ,640entries.Convolution\nisanextremelyeﬃcientwayofdescribingtransformationsthatapplythesamelinear\ntransformationofasmall,localregionacrosstheentireinput.(Photocredit:Paula\nGoodfellow)\n3 4 0\nCHAPTER9.CONVOLUTIONALNETWORKS\nConvolutional Layer\nInput to layerConvolution stage:\nAne transform ﬃDetector stage:\nNonlinearity\ne.g., rectiﬁed linearPooling stageNext layer\nInput to layersConvolution layer:\nAne transform  ﬃDetector layer: Nonlinearity\ne.g., rectiﬁed linearPooling layerNext layerComplex layer terminology Simple layer terminology\nFigure9.7:Thecomponentsofatypicalconvolutionalneuralnetworklayer.Therearetwo\ncommonlyusedsetsofterminologyfordescribingtheselayers. ( L e f t )Inthisterminology,\ntheconvolutionalnetisviewedasasmallnumberofrelativelycomplexlayers,with\neachlayerhavingmany“stages.”Inthisterminology,thereisaone-to-onemapping\nbetweenkerneltensorsandnetworklayers.Inthisbookwegenerallyusethisterminology.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 841, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 967}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0842_130ff0c5", "text": "( R i g h t )Inthisterminology,theconvolutionalnetisviewedasalargernumberofsimple\nlayers;everystepofprocessingisregardedasalayerinitsownright.Thismeansthat\nnotevery“layer”hasparameters. 3 4 1\nCHAPTER9.CONVOLUTIONALNETWORKS\nandChellappa1988,)operationreportsthemaximumoutputwithinarectangular\nneighborhood.Otherpopularpoolingfunctionsincludetheaverageofarectangular\nneighborhood,the L2normofarectangularneighborhood,oraweightedaverage\nbasedonthedistancefromthecentralpixel. Inallcases,poolinghelpstomaketherepresentationbecomeapproximately\ni n v ar i an ttosmalltranslationsoftheinput.Invariancetotranslationmeansthat\nifwetranslatetheinputbyasmallamount,thevaluesofmostofthepooled\noutputsdonotchange.Seeﬁgureforanexampleofhowthisworks.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 842, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 734}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0843_e041dc21", "text": "9.8 Invariance\ntolocaltranslationcanbeaveryusefulpropertyifwecaremoreaboutwhether\nsomefeatureispresentthanexactlywhereitis.Forexample,whendetermining\nwhetheranimagecontainsaface,weneednotknowthelocationoftheeyeswith\npixel-perfectaccuracy,wejustneedtoknowthatthereisaneyeontheleftside\nofthefaceandaneyeontherightsideoftheface.Inothercontexts,itismore\nimportanttopreservethelocationofafeature.Forexample,ifwewanttoﬁnda\ncornerdeﬁnedbytwoedgesmeetingataspeciﬁcorientation,weneedtopreserve\nthelocationoftheedgeswellenoughtotestwhethertheymeet. Theuseofpoolingcanbeviewedasaddinganinﬁnitelystrongpriorthat\nthefunctionthelayerlearnsmustbeinvarianttosmalltranslations.Whenthis\nassumptioniscorrect,itcangreatlyimprovethestatisticaleﬃciencyofthenetwork.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 843, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 743}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0844_f2072ebb", "text": "Poolingoverspatialregionsproducesinvariancetotranslation,butifwepool\novertheoutputsofseparatelyparametrized convolutions,thefeaturescanlearn\nwhichtransformationstobecomeinvariantto(seeﬁgure).9.9\nBecausepoolingsummarizestheresponsesoverawholeneighborhood,itis\npossibletousefewerpoolingunitsthandetectorunits,byreportingsummary\nstatisticsforpoolingregionsspaced kpixelsapartratherthan1pixelapart.See\nﬁgureforanexample.Thisimprovesthecomputational eﬃciencyofthe 9.10\nnetworkbecausethenextlayerhasroughly ktimesfewerinputstoprocess.When\nthenumberofparametersinthenextlayerisafunctionofitsinputsize(suchas\nwhenthenextlayerisfullyconnectedandbasedonmatrixmultiplication) this\nreductionintheinputsizecanalsoresultinimprovedstatisticaleﬃciencyand\nreducedmemoryrequirementsforstoringtheparameters. Formanytasks,poolingisessentialforhandlinginputsofvaryingsize.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 844, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 851}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0845_6f1523cb", "text": "Formanytasks,poolingisessentialforhandlinginputsofvaryingsize. For\nexample,ifwewanttoclassifyimagesofvariablesize,theinputtotheclassiﬁcation\nlayermusthaveaﬁxedsize.Thisisusuallyaccomplishedbyvaryingthesizeofan\noﬀsetbetweenpoolingregionssothattheclassiﬁcationlayeralwaysreceivesthe\nsamenumberofsummarystatisticsregardlessoftheinputsize.Forexample,the\nﬁnalpoolinglayerofthenetworkmaybedeﬁnedtooutputfoursetsofsummary\nstatistics,oneforeachquadrantofanimage,regardlessoftheimagesize. 3 4 2\nCHAPTER9.CONVOLUTIONALNETWORKS\n0. 1 1. 0. 21. 1. 1. 0. 10. 2\n. . . . . .. . . . . . 0. 3 0. 1 1.1. 0. 3 1. 0. 21. . . . . . .. . . . . .D E T E C T O R   S T A GEP O O L I N G  ST A GE\nP O O L I N G  ST A GE\nD E T E C T O R   S T A GE\nFigure9.8:Maxpoolingintroducesinvariance.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 845, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 762}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0846_b029b84c", "text": "1. 1. 0. 10. 2\n. . . . . .. . . . . . 0. 3 0. 1 1.1. 0. 3 1. 0. 21. . . . . . .. . . . . .D E T E C T O R   S T A GEP O O L I N G  ST A GE\nP O O L I N G  ST A GE\nD E T E C T O R   S T A GE\nFigure9.8:Maxpoolingintroducesinvariance. ( T o p )Aviewofthemiddleoftheoutput\nofaconvolutionallayer.Thebottomrowshowsoutputsofthenonlinearity.Thetop\nrowshowstheoutputsofmaxpooling,withastrideofonepixelbetweenpoolingregions\nandapoolingregionwidthofthreepixels.Aviewofthesamenetwork,after ( Bottom )\ntheinputhasbeenshiftedtotherightbyonepixel.Everyvalueinthebottomrowhas\nchanged,butonlyhalfofthevaluesinthetoprowhavechanged,becausethemaxpooling\nunitsareonlysensitivetothemaximumvalueintheneighborhood,notitsexactlocation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 846, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 709}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0847_6ead7507", "text": "3 4 3\nCHAPTER9.CONVOLUTIONALNETWORKS\nL ar ge   r e s pon s e\ni n  po ol i ng uni tL ar ge   r e s pon s e\ni n  po ol i ng uni t\nL ar ge\nr e s ponse\ni n  de t e c t or\nuni t   1L ar ge\nr e s ponse\ni n  de t e c t or\nuni t   3\nFigure9.9: E x a m p l e o f l e a r n e d i n v a r i a n c e s :Apoolingunitthatpoolsovermultiplefeatures\nthatarelearnedwithseparateparameterscanlearntobeinvarianttotransformationsof\ntheinput.Hereweshowhowasetofthreelearnedﬁltersandamaxpoolingunitcanlearn\ntobecomeinvarianttorotation.Allthreeﬁltersareintendedtodetectahand-written5.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 847, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 559}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0848_503971fa", "text": "Eachﬁlterattemptstomatchaslightlydiﬀerentorientationofthe5.Whena5appearsin\ntheinput,thecorrespondingﬁlterwillmatchitandcausealargeactivationinadetector\nunit.Themaxpoolingunitthenhasalargeactivationregardlessofwhichdetectorunit\nwasactivated.Weshowherehowthenetworkprocessestwodiﬀerentinputs,resulting\nintwodiﬀerentdetectorunitsbeingactivated.Theeﬀectonthepoolingunitisroughly\nthesameeitherway.Thisprincipleisleveragedbymaxoutnetworks(Goodfellow e t a l .,\n2013a)andotherconvolutionalnetworks.Maxpoolingoverspatialpositionsisnaturally\ninvarianttotranslation;thismulti-channelapproachisonlynecessaryforlearningother\ntransformations. 0. 1 1. 0. 21. 0. 2\n0. 10. 1\n0. 0 0.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 848, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 666}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0849_f89b1713", "text": "0. 1 1. 0. 21. 0. 2\n0. 10. 1\n0. 0 0. 1\nFigure9.10: P o o l i n g w i t h d o w n s a m p l i n g.Hereweusemax-poolingwithapoolwidthof\nthreeandastridebetweenpoolsoftwo.Thisreducestherepresentationsizebyafactor\noftwo,whichreducesthecomputationalandstatisticalburdenonthenextlayer.Note\nthattherightmostpoolingregionhasasmallersize,butmustbeincludedifwedonot\nwanttoignoresomeofthedetectorunits. 3 4 4\nCHAPTER9.CONVOLUTIONALNETWORKS\nSometheoreticalworkgivesguidanceastowhichkindsofpoolingoneshould\nuseinvarioussituations( ,).Itisalsopossibletodynamically Boureauetal.2010\npoolfeaturestogether,forexample,byrunningaclusteringalgorithmonthe\nlocationsofinterestingfeatures( ,).Thisapproachyieldsa Boureauetal.2011\ndiﬀerentsetofpoolingregionsforeachimage.Anotherapproachistolearna\nsinglepoolingstructurethatisthenappliedtoallimages(,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 849, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 826}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0850_0127550f", "text": "Jiaetal.2012\nPoolingcancomplicatesomekindsofneuralnetworkarchitecturesthatuse\ntop-downinformation, suchasBoltzmannmachinesandautoencoders.These\nissueswillbediscussedfurtherwhenwepresentthesetypesofnetworksinpart.III\nPoolinginconvolutionalBoltzmannmachinesispresentedinsection. The20.6\ninverse-likeoperationsonpoolingunitsneededinsomediﬀerentiablenetworkswill\nbecoveredinsection.20.10.6\nSomeexamplesofcompleteconvolutionalnetworkarchitecturesforclassiﬁcation\nusingconvolutionandpoolingareshowninﬁgure.9.11\n9.4Convolutionand Pooling asan InﬁnitelyStrong\nPrior\nRecalltheconceptofa pr i o r pr o babili t y di st r i but i o nfromsection.Thisis5.2\naprobabilitydistributionovertheparametersofamodelthatencodesourbeliefs\naboutwhatmodelsarereasonable,beforewehaveseenanydata.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 850, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 768}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0851_ce7df58b", "text": "Priorscanbeconsideredweakorstrongdependingonhowconcentratedthe\nprobabilitydensityintheprioris.Aweakpriorisapriordistributionwithhigh\nentropy,suchasaGaussiandistributionwithhighvariance.Suchapriorallows\nthedatatomovetheparametersmoreorlessfreely.Astrongpriorhasverylow\nentropy,suchasaGaussiandistributionwithlowvariance.Suchapriorplaysa\nmoreactiveroleindeterminingwheretheparametersendup. Aninﬁnitelystrongpriorplaceszeroprobabilityonsomeparametersandsays\nthattheseparametervaluesarecompletelyforbidden,regardlessofhowmuch\nsupportthedatagivestothosevalues.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 851, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 555}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0852_12140bb8", "text": "Aninﬁnitelystrongpriorplaceszeroprobabilityonsomeparametersandsays\nthattheseparametervaluesarecompletelyforbidden,regardlessofhowmuch\nsupportthedatagivestothosevalues. Wecanimagineaconvolutionalnetasbeingsimilartoafullyconnectednet,\nbutwithaninﬁnitelystrongprioroveritsweights.Thisinﬁnitelystrongprior\nsaysthattheweightsforonehiddenunitmustbeidenticaltotheweightsofits\nneighbor,butshiftedinspace.Theprioralsosaysthattheweightsmustbezero,\nexceptforinthesmall,spatiallycontiguousreceptiveﬁeldassignedtothathidden\nunit.Overall,wecanthinkoftheuseofconvolutionasintroducinganinﬁnitely\nstrongpriorprobabilitydistributionovertheparametersofalayer.Thisprior\n3 4 5\nCHAPTER9.CONVOLUTIONALNETWORKS\nInput image: \n256x256x3Output of \nconvolution + \nReLU: 256x256x64Output of pooling \nwith stride 4: \n64x64x64Output of \nconvolution + \nReLU: 64x64x64Output of pooling \nwith stride 4: \n16x16x64Output of reshape to \nvector:\n16,384 unitsOutput of matrix \nmultiply: 1,000 unitsOutput of softmax: \n1,000 class \nprobabilities\nInput image: \n256x256x3Output of \nconvolution + \nReLU: 256x256x64Output of pooling \nwith stride 4: \n64x64x64Output of \nconvolution + \nReLU: 64x64x64Output of pooling to \n3x3 grid: 3x3x64Output of reshape to \nvector:\n576 unitsOutput of matrix \nmultiply: 1,000 unitsOutput of softmax: \n1,000 class \nprobabilities\nInput image: \n256x256x3Output of \nconvolution + \nReLU: 256x256x64Output of pooling \nwith stride 4: \n64x64x64Output of \nconvolution + \nReLU: 64x64x64Output of \nconvolution:\n16x16x1,000Output of average \npooling: 1x1x1,000Output of softmax: \n1,000 class \nprobabilities\nOutput of pooling \nwith stride 4: \n16x16x64\nFigure9.11:Examplesofarchitecturesforclassiﬁcationwithconvolutionalnetworks.The\nspeciﬁcstridesanddepthsusedinthisﬁgurearenotadvisableforrealuse;theyare\ndesignedtobeveryshallowinordertoﬁtontothepage.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 852, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1826}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0853_4329b8b3", "text": "Realconvolutionalnetworks\nalsoofteninvolvesigniﬁcantamountsofbranching,unlikethechainstructuresused\nhereforsimplicity. ( L e f t )Aconvolutionalnetworkthatprocessesaﬁxedimagesize. Afteralternatingbetweenconvolutionandpoolingforafewlayers,thetensorforthe\nconvolutionalfeaturemapisreshapedtoﬂattenoutthespatialdimensions.Therest\nofthenetworkisanordinaryfeedforwardnetworkclassiﬁer,asdescribedinchapter.6\n( C e n t e r )Aconvolutionalnetworkthatprocessesavariable-sizedimage,butstillmaintains\nafullyconnectedsection.Thisnetworkusesapoolingoperationwithvariably-sizedpools\nbutaﬁxednumberofpools,inordertoprovideaﬁxed-sizevectorof576unitstothe\nfullyconnectedportionofthenetwork. Aconvolutionalnetworkthatdoesnot ( R i g h t )\nhaveanyfullyconnectedweightlayer.Instead,thelastconvolutionallayeroutputsone\nfeaturemapperclass.Themodelpresumablylearnsamapofhowlikelyeachclassisto\noccurateachspatiallocation.Averagingafeaturemapdowntoasinglevalueprovides\ntheargumenttothesoftmaxclassiﬁeratthetop.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 853, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 985}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0854_e12a4b37", "text": "3 4 6\nCHAPTER9.CONVOLUTIONALNETWORKS\nsaysthatthefunctionthelayershouldlearncontainsonlylocalinteractionsandis\nequivarianttotranslation.Likewise,theuseofpoolingisaninﬁnitelystrongprior\nthateachunitshouldbeinvarianttosmalltranslations. Ofcourse,implementing aconvolutionalnetasafullyconnectednetwithan\ninﬁnitelystrongpriorwouldbeextremelycomputationally wasteful.Butthinking\nofaconvolutionalnetasafullyconnectednetwithaninﬁnitelystrongpriorcan\ngiveussomeinsightsintohowconvolutionalnetswork. Onekeyinsightisthatconvolutionandpoolingcancauseunderﬁtting. Like\nanyprior,convolutionandpoolingareonlyusefulwhentheassumptionsmade\nbythepriorarereasonablyaccurate.Ifataskreliesonpreservingprecisespatial\ninformation, thenusingpoolingonallfeaturescanincreasethetrainingerror.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 854, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 764}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0855_9d8683aa", "text": "Someconvolutionalnetworkarchitectures ( ,)aredesignedto Szegedy etal.2014a\nusepoolingonsomechannelsbutnotonotherchannels,inordertogetboth\nhighlyinvariantfeaturesandfeaturesthatwillnotunderﬁtwhenthetranslation\ninvariancepriorisincorrect.Whenataskinvolvesincorporatinginformationfrom\nverydistantlocationsintheinput,thenthepriorimposedbyconvolutionmaybe\ninappropriate. Anotherkeyinsightfromthisviewisthatweshouldonlycompareconvolu-\ntionalmodelstootherconvolutionalmodelsinbenchmarksofstatisticallearning\nperformance.Modelsthatdonotuseconvolutionwouldbeabletolearneven\nifwepermutedallofthepixelsintheimage.Formanyimagedatasets,there\nareseparatebenchmarksformodelsthatare p e r m ut at i o n i nv ar i antandmust\ndiscovertheconceptoftopologyvialearning,andmodelsthathavetheknowledge\nofspatialrelationshipshard-codedintothembytheirdesigner.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 855, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 834}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0856_4dc31b32", "text": "9.5VariantsoftheBasicConvolutionFunction\nWhendiscussingconvolutioninthecontextofneuralnetworks,weusuallydo\nnotreferexactlytothestandarddiscreteconvolutionoperationasitisusually\nunderstoodinthemathematical literature.Thefunctionsusedinpracticediﬀer\nslightly.Herewedescribethesediﬀerencesindetail,andhighlightsomeuseful\npropertiesofthefunctionsusedinneuralnetworks. First,whenwerefertoconvolutioninthecontextofneuralnetworks,weusually\nactuallymeananoperationthatconsistsofmanyapplicationsofconvolutionin\nparallel.Thisisbecauseconvolutionwithasinglekernelcanonlyextractonekind\noffeature,albeitatmanyspatiallocations.Usuallywewanteachlayerofour\nnetworktoextractmanykindsoffeatures,atmanylocations. 3 4 7\nCHAPTER9.CONVOLUTIONALNETWORKS\nAdditionally,theinputisusuallynotjustagridofrealvalues.Rather,itisa\ngridofvector-valuedobservations.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 856, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 831}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0857_a0eb66d5", "text": "3 4 7\nCHAPTER9.CONVOLUTIONALNETWORKS\nAdditionally,theinputisusuallynotjustagridofrealvalues.Rather,itisa\ngridofvector-valuedobservations. Forexample,acolorimagehasared,green\nandblueintensityateachpixel.Inamultilayerconvolutionalnetwork,theinput\ntothesecondlayeristheoutputoftheﬁrstlayer,whichusuallyhastheoutput\nofmanydiﬀerentconvolutionsateachposition.Whenworkingwithimages,we\nusuallythinkoftheinputandoutputoftheconvolutionasbeing3-Dtensors,with\noneindexintothediﬀerentchannelsandtwoindicesintothespatialcoordinates\nofeachchannel.Softwareimplementationsusuallyworkinbatchmode,sothey\nwillactuallyuse4-Dtensors,withthefourthaxisindexingdiﬀerentexamplesin\nthebatch,butwewillomitthebatchaxisinourdescriptionhereforsimplicity. Becauseconvolutionalnetworksusuallyusemulti-channelconvolution,the\nlinearoperationstheyarebasedonarenotguaranteedtobecommutative,evenif\nkernel-ﬂippingisused.Thesemulti-channeloperationsareonlycommutativeif\neachoperationhasthesamenumberofoutputchannelsasinputchannels.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 857, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 991}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0858_d5beb009", "text": "Assumewehavea4-Dkerneltensor Kwithelement K i , j , k, lgivingtheconnection\nstrengthbetweenaunitinchannel ioftheoutputandaunitinchannel jofthe\ninput,withanoﬀsetof krowsand lcolumnsbetweentheoutputunitandthe\ninputunit.Assumeourinputconsistsofobserveddata Vwithelement V i , j , kgiving\nthevalueoftheinputunitwithinchannel iatrow jandcolumn k.Assumeour\noutputconsistsof Zwiththesameformatas V.If Zisproducedbyconvolving K\nacrosswithoutﬂipping,then V K\nZ i , j , k=\nl , m , nV l , j m , k n + − 1 + − 1 K i , l , m , n (9.7)\nwherethesummationover l, mand nisoverallvaluesforwhichthetensorindexing\noperationsinsidethesummationisvalid.Inlinearalgebranotation,weindexinto\narraysusingafortheﬁrstentry.Thisnecessitatesthe 1 −1intheaboveformula. ProgramminglanguagessuchasCandPythonindexstartingfrom,rendering0\ntheaboveexpressionevensimpler.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 858, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 833}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0859_0ec7f057", "text": "ProgramminglanguagessuchasCandPythonindexstartingfrom,rendering0\ntheaboveexpressionevensimpler. Wemaywanttoskipoversomepositionsofthekernelinordertoreducethe\ncomputational cost(attheexpenseofnotextractingourfeaturesasﬁnely).We\ncanthinkofthisasdownsamplingtheoutputofthefullconvolutionfunction.If\nwewanttosampleonlyevery spixelsineachdirectionintheoutput,thenwecan\ndeﬁneadownsampledconvolutionfunctionsuchthat c\nZ i , j , k= ( ) c K V , , s i , j , k=\nl , m , n\nVl , j s m , k s n ( − × 1 ) + ( − × 1 ) + K i , l , m , n\n.(9.8)\nWereferto sasthe st r i deofthisdownsampledconvolution.Itisalsopossible\n3 4 8\nCHAPTER9.CONVOLUTIONALNETWORKS\ntodeﬁneaseparatestrideforeachdirectionofmotion.Seeﬁgureforan9.12\nillustration. Oneessentialfeatureofanyconvolutionalnetworkimplementationistheability\ntoimplicitlyzero-padtheinput Vinordertomakeitwider.Withoutthisfeature,\nthewidthoftherepresentationshrinksbyonepixellessthanthekernelwidth\nateachlayer.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 859, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 939}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0860_10bfb1a0", "text": "Zeropaddingtheinputallowsustocontrolthekernelwidthand\nthesizeoftheoutputindependently.Withoutzeropadding,weareforcedto\nchoosebetweenshrinkingthespatialextentofthenetworkrapidlyandusingsmall\nkernels—bothscenariosthatsigniﬁcantlylimittheexpressivepowerofthenetwork. Seeﬁgureforanexample. 9.13\nThreespecialcasesofthezero-paddingsettingareworthmentioning.Oneis\ntheextremecaseinwhichnozero-paddingisusedwhatsoever,andtheconvolution\nkernelisonlyallowedtovisitpositionswheretheentirekerneliscontainedentirely\nwithintheimage.InMATLABterminology,thisiscalled v al i dconvolution.In\nthiscase,allpixelsintheoutputareafunctionofthesamenumberofpixelsin\ntheinput,sothebehaviorofanoutputpixelissomewhatmoreregular.However,\nthesizeoftheoutputshrinksateachlayer.Iftheinputimagehaswidth mand\nthekernelhaswidth k,theoutputwillbeofwidth m k −+1.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 860, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 825}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0861_8a8fc391", "text": "Therateofthis\nshrinkagecanbedramaticifthekernelsusedarelarge.Sincetheshrinkageis\ngreaterthan0,itlimitsthenumberofconvolutionallayersthatcanbeincluded\ninthenetwork.Aslayersareadded,thespatialdimensionofthenetworkwill\neventuallydropto1 ×1,atwhichpointadditionallayerscannotmeaningfully\nbeconsideredconvolutional.Anotherspecialcaseofthezero-paddingsettingis\nwhenjustenoughzero-paddingisaddedtokeepthesizeoftheoutputequalto\nthesizeoftheinput.MATLABcallsthis sameconvolution.Inthiscase,the\nnetworkcancontainasmanyconvolutionallayersastheavailablehardwarecan\nsupport,sincetheoperationofconvolutiondoesnotmodifythearchitectural\npossibilitiesavailabletothenextlayer.However,theinputpixelsneartheborder\ninﬂuencefeweroutputpixelsthantheinputpixelsnearthecenter.Thiscanmake\ntheborderpixelssomewhatunderrepresen tedinthemodel.Thismotivatesthe\notherextremecase,whichMATLABreferstoas f ul lconvolution,inwhichenough\nzeroesareaddedforeverypixeltobevisited ktimesineachdirection,resulting\ninanoutputimageofwidth m+ k −1.Inthiscase,theoutputpixelsnearthe\nborderareafunctionoffewerpixelsthantheoutputpixelsnearthecenter.This\ncanmakeitdiﬃculttolearnasinglekernelthatperformswellatallpositionsin\ntheconvolutionalfeaturemap.Usuallytheoptimalamountofzeropadding(in\ntermsoftestsetclassiﬁcationaccuracy)liessomewherebetween“valid”and“same”\nconvolution.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 861, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1328}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0862_0b97a2b0", "text": "3 4 9\nCHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 1 s 1 s 2 s 2\nx 4 x 4 x 5 x 5s 3 s 3\nx 1 x 1 x 2 x 2 x 3 x 3z 2 z 2 z 1 z 1 z 3 z 3\nx 4 x 4z 4 z 4\nx 5 x 5z 5 z 5s 1 s 1 s 2 s 2 s 3 s 3St r i de d\nc onv ol ut i on\nD ow nsampl i n g\nC onv ol ut i on\nFigure 9.12:Convolution witha stride.Inthisexample,we use astride oftwo. ( T o p )Convolutionwithastridelengthoftwoimplementedinasingleoperation. ( Bot-\nt o m )Convolutionwithastridegreaterthanonepixelismathematicallyequivalentto\nconvolutionwithunitstridefollowedbydownsampling.Obviously,thetwo-stepapproach\ninvolvingdownsamplingiscomputationallywasteful,becauseitcomputesmanyvalues\nthatarethendiscarded. 3 5 0\nCHAPTER9.CONVOLUTIONALNETWORKS\n. . . . . .. . . . . . . . .. . . . . .. . . . . . Figure9.13: T h e e ﬀ e c t o f z e r o p a d d i n g o n n e t w o r k s i z e:Consideraconvolutionalnetwork\nwithakernelofwidthsixateverylayer.Inthisexample,wedonotuseanypooling,so\nonlytheconvolutionoperationitselfshrinksthenetworksize.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 862, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0863_8e56a253", "text": "( T o p )Inthisconvolutional\nnetwork,wedonotuseanyimplicitzeropadding.Thiscausestherepresentationto\nshrinkbyﬁvepixelsateachlayer.Startingfromaninputofsixteenpixels,weareonly\nabletohavethreeconvolutionallayers,andthelastlayerdoesnotevermovethekernel,\nsoarguablyonlytwoofthelayersaretrulyconvolutional.Therateofshrinkingcan\nbemitigatedbyusingsmallerkernels,butsmallerkernelsarelessexpressiveandsome\nshrinkingisinevitableinthiskindofarchitecture. Byaddingﬁveimplicitzeroes ( Bottom )\ntoeachlayer,wepreventtherepresentationfromshrinkingwithdepth.Thisallowsusto\nmakeanarbitrarilydeepconvolutionalnetwork. 3 5 1\nCHAPTER9.CONVOLUTIONALNETWORKS\nInsomecases,wedonotactuallywanttouseconvolution,butratherlocally\nconnectedlayers(,,).Inthiscase,theadjacencymatrixinthe LeCun19861989\ngraphofourMLPisthesame,buteveryconnectionhasitsownweight,speciﬁed\nbya6-Dtensor W.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 863, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 852}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0864_94311699", "text": "Theindicesinto Warerespectively: i,theoutputchannel,\nj,theoutputrow, k,theoutputcolumn, l,theinputchannel, m,therowoﬀset\nwithintheinput,and n,thecolumnoﬀsetwithintheinput.Thelinearpartofa\nlocallyconnectedlayeristhengivenby\nZ i , j , k=\nl , m , n[ V l , j m , k n + − 1 + − 1 w i , j , k, l , m , n] . (9.9)\nThisissometimesalsocalled unshar e d c o nv o l ut i o n,becauseitisasimilaroper-\nationtodiscreteconvolutionwithasmallkernel,butwithoutsharingparameters\nacrosslocations.Figurecompareslocalconnections,convolution,andfull 9.14\nconnections. Locallyconnectedlayersareusefulwhenweknowthateachfeatureshouldbe\nafunctionofasmallpartofspace,butthereisnoreasontothinkthatthesame\nfeatureshouldoccuracrossallofspace.Forexample,ifwewanttotellifanimage\nisapictureofaface,weonlyneedtolookforthemouthinthebottomhalfofthe\nimage.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 864, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 819}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0865_70735aad", "text": "Itcanalsobeusefultomakeversionsofconvolutionorlocallyconnectedlayers\ninwhichtheconnectivityisfurtherrestricted,forexampletoconstraineachoutput\nchannel itobeafunctionofonlyasubsetoftheinputchannels l.Acommon\nwaytodothisistomaketheﬁrst moutputchannelsconnecttoonlytheﬁrst\nninputchannels,thesecond moutputchannelsconnecttoonlythesecond n\ninputchannels,andsoon.Seeﬁgureforanexample.Modelinginteractions 9.15\nbetweenfewchannelsallowsthenetworktohavefewerparametersinorderto\nreducememoryconsumptionandincreasestatisticaleﬃciency,andalsoreduces\ntheamountofcomputationneededtoperformforwardandback-propagation. It\naccomplishesthesegoalswithoutreducingthenumberofhiddenunits.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 865, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 666}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0866_4e3b352f", "text": "It\naccomplishesthesegoalswithoutreducingthenumberofhiddenunits. T i l e d c o n v o l ut i o n( ,;,)oﬀersacom- GregorandLeCun2010aLeetal.2010\npromisebetweenaconvolutionallayerandalocallyconnectedlayer.Ratherthan\nlearningaseparatesetofweightsatspatiallocation,welearnasetofkernels every\nthatwerotatethroughaswemovethroughspace.Thismeansthatimmediately\nneighboringlocationswillhavediﬀerentﬁlters,likeinalocallyconnectedlayer,\nbutthememoryrequirementsforstoringtheparameterswillincreaseonlybya\nfactorofthesizeofthissetofkernels,ratherthanthesizeoftheentireoutput\nfeaturemap.Seeﬁgureforacomparisonoflocallyconnectedlayers,tiled 9.16\nconvolution,andstandardconvolution. 3 5 2\nCHAPTER9.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 866, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 680}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0867_51934dc0", "text": "3 5 2\nCHAPTER9. CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2s 1 s 1 s 3 s 3\nx 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\na   b a   b a   b a   b a a   b c   d e   f g   h  i \nx 4 x 4 x 3 x 3s 4 s 4 s 2 s 2\nFigure 9.14: Comparison of localconnections, convolution, andfull connections. (T op)A locally connected layer with a patch size of two pixels. Each edge islabeled with\na unique letterto show thateach edge isassociated with itsown weight parameter. (Center)A convolutional layerwith a kernel width of two pixels. This model hasexactly\nthesame connectivityas thelocallyconnected layer. Thediﬀerence liesnot inwhichunits\ninteractwitheachother,butinhowtheparametersareshared. Thelocallyconnectedlayer\nhas no parameter sharing. The convolutionallayer uses thesame two weights repeatedly\nacross the entire input,as indicated bythe repetition ofthe letters labeling each edge.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 867, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0868_8960f6a2", "text": "Thelocallyconnectedlayer\nhas no parameter sharing. The convolutionallayer uses thesame two weights repeatedly\nacross the entire input,as indicated bythe repetition ofthe letters labeling each edge. (Bottom)A fullyconnected layer resemblesa locallyconnected layer in the sensethat each\nedge has its own parameter (there are too many to label explicitly with letters in this\ndiagram). However,it does nothave the restrictedconnectivity ofthe locally connected\nlayer. 353\nCHAPTER9.CONVOLUTIONALNETWORKS\nI nput T e nsorO ut put T e nsor\nS p a t i a l   c o o r d i n a t e sC h a n n e l   c o o r d i n a t e s\nFigure9.15: Aconvolutionalnetworkwiththeﬁrsttwooutputchannelsconnectedto\nonlytheﬁrsttwoinputchannels,andthesecondtwooutputchannelsconnectedtoonly\nthesecondtwoinputchannels. 3 5 4\nCHAPTER9.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 868, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 796}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0869_c9df3d54", "text": "3 5 4\nCHAPTER9. CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\na   b a   b a   b a   b a a   b c   d e   f g   h  i \nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\na   b c   d a   b c   d a \nFigure 9.16: A comparison of locally connected layers,tiled convolution, and standard\nconvolution. All three have thesame sets of connectionsbetweenunits, when thesame\nsize of kernel is used. This diagram illustrates the use of a kernel that is two pixels wide. The diﬀerences between the methodslies in how they shareparameters. (T op)A locally\nconnectedlayerhasno sharingatall. Weindicate thateachconnection hasits ownweight\nby labelingeach connectionwith a unique letter. Tiled convolution has a set of (Center)\ntdiﬀerent kernels. Here we illustrate the case of t= 2.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 869, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 919}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0870_74aecd75", "text": "Weindicate thateachconnection hasits ownweight\nby labelingeach connectionwith a unique letter. Tiled convolution has a set of (Center)\ntdiﬀerent kernels. Here we illustrate the case of t= 2. One of these kernels has edges\nlabeled“a” and “b,” whilethe other hasedges labeled“c” and “d.” Each timewemove one\npixel to the right in theoutput, we move on to usinga diﬀerent kernel. This means that,\nlike the locally connected layer, neighboring units in the outputhave diﬀerent parameters. Unlike the locally connected layer, after we have gone through all tavailable kernels,\nwe cycle back to the ﬁrst kernel. If two output units are separated by a multiple of t\nsteps, then they share parameters. Traditional convolution is equivalent to tiled (Bottom)\nconvolutionwith t= 1. Thereis onlyone kerneland itis appliedeverywhere,as indicated\nin the diagramby using thekernel with weights labeled “a” and “b” everywhere.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 870, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 911}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0871_0994f9ff", "text": "Thereis onlyone kerneland itis appliedeverywhere,as indicated\nin the diagramby using thekernel with weights labeled “a” and “b” everywhere. 355\nCHAPTER9.CONVOLUTIONALNETWORKS\nTodeﬁnetiledconvolutionalgebraically,let kbea6-Dtensor,wheretwoof\nthedimensionscorrespondtodiﬀerentlocationsintheoutputmap.Ratherthan\nhavingaseparateindexforeachlocationintheoutputmap,outputlocationscycle\nthroughasetof tdiﬀerentchoicesofkernelstackineachdirection.If tisequalto\ntheoutputwidth,thisisthesameasalocallyconnectedlayer. Z i , j , k=\nl , m , nV l , j m , k n + − 1 + − 1 K i , l , m , n , j t , k t % + 1 % + 1 ,(9.10)\nwhereis themodulooperation,with % t% t=0(, t+1)% t=1,etc.It is\nstraightforwardtogeneralizethisequationtouseadiﬀerenttilingrangeforeach\ndimension.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 871, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 751}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0872_4be9547d", "text": "Bothlocallyconnectedlayersandtiledconvolutionallayershaveaninteresting\ninteractionwithmax-pooling:thedetectorunitsoftheselayersaredrivenby\ndiﬀerentﬁlters.Iftheseﬁlterslearntodetectdiﬀerenttransformedversionsof\nthesameunderlyingfeatures,thenthemax-pooledunitsbecomeinvarianttothe\nlearnedtransformation(seeﬁgure).Convolutionallayersarehard-codedtobe 9.9\ninvariantspeciﬁcallytotranslation. Otheroperationsbesidesconvolutionareusuallynecessarytoimplementa\nconvolutionalnetwork.Toperformlearning,onemustbeabletocomputethe\ngradientwithrespecttothekernel,giventhegradientwithrespecttotheoutputs. Insomesimplecases, thisoperationcanbeperformedusingtheconvolution\noperation,butmanycasesofinterest,includingthecaseofstridegreaterthan1,\ndonothavethisproperty.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 872, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 748}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0873_eded70de", "text": "Insomesimplecases, thisoperationcanbeperformedusingtheconvolution\noperation,butmanycasesofinterest,includingthecaseofstridegreaterthan1,\ndonothavethisproperty. Recallthatconvolutionisalinearoperationandcanthusbedescribedasa\nmatrixmultiplication (ifweﬁrstreshapetheinputtensorintoaﬂatvector).The\nmatrixinvolvedisafunctionoftheconvolutionkernel.Thematrixissparseand\neachelementofthekerneliscopiedtoseveralelementsofthematrix.Thisview\nhelpsustoderivesomeoftheotheroperationsneededtoimplementaconvolutional\nnetwork. Multiplication bythetransposeofthematrixdeﬁnedbyconvolutionisone\nsuchoperation.Thisistheoperationneededtoback-propagate errorderivatives\nthroughaconvolutionallayer,soitisneededtotrainconvolutionalnetworks\nthathavemorethanonehiddenlayer.Thissameoperationisalsoneededifwe\nwishtoreconstructthevisibleunitsfromthehiddenunits( ,). Simard etal.1992\nReconstructingthevisibleunitsisanoperationcommonlyusedinthemodels\ndescribedinpartofthisbook,suchasautoencoders,RBMs,andsparsecoding.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 873, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 987}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0874_d667c365", "text": "Simard etal.1992\nReconstructingthevisibleunitsisanoperationcommonlyusedinthemodels\ndescribedinpartofthisbook,suchasautoencoders,RBMs,andsparsecoding. III\nTransposeconvolutionisnecessarytoconstructconvolutionalversionsofthose\nmodels.Likethekernelgradientoperation,thisinputgradientoperationcanbe\n3 5 6\nCHAPTER9.CONVOLUTIONALNETWORKS\nimplementedusingaconvolutioninsomecases,butinthegeneralcaserequires\nathirdoperationtobeimplemented.Caremustbetakentocoordinatethis\ntransposeoperationwiththeforwardpropagation. Thesizeoftheoutputthatthe\ntransposeoperationshouldreturndependsonthezeropaddingpolicyandstrideof\ntheforwardpropagationoperation,aswellasthesizeoftheforwardpropagation’s\noutputmap.Insomecases,multiplesizesofinputtoforwardpropagationcan\nresultinthesamesizeofoutputmap,sothetransposeoperationmustbeexplicitly\ntoldwhatthesizeoftheoriginalinputwas.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 874, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 851}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0875_4dd55ca6", "text": "Thesethreeoperations—convolution,backpropfromoutputtoweights,and\nbackpropfromoutputtoinputs—aresuﬃcienttocomputeallofthegradients\nneededtotrainanydepthoffeedforwardconvolutionalnetwork,aswellastotrain\nconvolutionalnetworkswithreconstructionfunctionsbasedonthetransposeof\nconvolution. See ()forafullderivationoftheequationsinthe Goodfellow2010\nfullygeneralmulti-dimensional,multi-example case.Togiveasenseofhowthese\nequationswork,wepresentthetwodimensional,singleexampleversionhere. Supposewewanttotrainaconvolutionalnetworkthatincorporatesstrided\nconvolutionofkernelstack Kappliedtomulti-channelimage Vwithstride sas\ndeﬁnedby c( K V , , s)asinequation.Supposewewanttominimizesomeloss 9.8\nfunction J( V K ,).Duringforwardpropagation, wewillneedtouse citselfto\noutput Z,whichisthenpropagatedthroughtherestofthenetworkandusedto\ncomputethecostfunction J.Duringback-propagation, wewillreceiveatensor G\nsuchthat G i , j , k=∂\n∂ Z i , j , kJ , .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 875, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 938}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0876_0faead10", "text": "( V K)\nTotrainthenetwork,weneedtocomputethederivativeswithrespecttothe\nweightsinthekernel.Todoso,wecanuseafunction\ng , , s ( G V) i , j , k, l=∂\n∂ K i , j , k, lJ ,( V K) =\nm , nG i , m , n V j , m s k, n s l ( − × 1 ) + ( − × 1 ) + .(9.11)\nIfthislayerisnotthebottomlayerofthenetwork,wewillneedtocompute\nthegradientwithrespectto Vinordertoback-propagate theerrorfartherdown. Todoso,wecanuseafunction\nh , , s ( K G) i , j , k=∂\n∂ V i , j , kJ ,( V K) (9.12)\n=\nl , m\ns . t . ( 1 ) + = l − × s m j\nn , p\ns . t . ( 1 ) + = n − × s p k\nqK q , i , m , p G q , l , n .(9.13)\nAutoencodernetworks, describedinchapter, arefeedforwardnetworks 14\ntrainedtocopytheirinputtotheiroutput.AsimpleexampleisthePCAalgorithm,\n3 5 7\nCHAPTER9.CONVOLUTIONALNETWORKS\nthatcopiesitsinput xtoanapproximatereconstruction rusingthefunction\nWW x.Itiscommonformore general autoencoders tousemultiplication\nbythetransposeoftheweightmatrixjustasPCAdoes.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 876, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 924}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0877_2a7017ba", "text": "Tomakesuchmodels\nconvolutional,wecanusethefunction htoperformthetransposeoftheconvolution\noperation.Supposewehavehiddenunits Hinthesameformatas Zandwedeﬁne\nareconstruction\nR K H = ( h , , s .) (9.14)\nInordertotraintheautoencoder,wewillreceivethegradientwithrespect\nto Rasatensor E.Totrainthedecoder,weneedtoobtainthegradientwith\nrespectto K.Thisisgivenby g( H E , , s).Totraintheencoder,weneedtoobtain\nthegradientwithrespectto H.Thisisgivenby c( K E , , s).Itisalsopossibleto\ndiﬀerentiatethrough gusing cand h,buttheseoperationsarenotneededforthe\nback-propagationalgorithmonanystandardnetworkarchitectures. Generally,wedonotuseonlyalinearoperationinordertotransformfrom\ntheinputstotheoutputsinaconvolutionallayer.Wegenerallyalsoaddsome\nbiastermtoeachoutputbeforeapplyingthenonlinearity.Thisraisesthequestion\nofhowtoshareparametersamongthebiases.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 877, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 845}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0878_e29b2894", "text": "Forlocallyconnectedlayersitis\nnaturaltogiveeachunititsownbias,andfortiledconvolution,itisnaturalto\nsharethebiaseswiththesametilingpatternasthekernels.Forconvolutional\nlayers,itistypicaltohaveonebiasperchanneloftheoutputandshareitacross\nalllocationswithineachconvolutionmap.However,iftheinputisofknown,ﬁxed\nsize,itisalsopossibletolearnaseparatebiasateachlocationoftheoutputmap. Separatingthebiasesmayslightlyreducethestatisticaleﬃciencyofthemodel,but\nalsoallowsthemodeltocorrectfordiﬀerencesintheimagestatisticsatdiﬀerent\nlocations.Forexample,whenusingimplicitzeropadding,detectorunitsatthe\nedgeoftheimagereceivelesstotalinputandmayneedlargerbiases.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 878, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 648}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0879_756fa70b", "text": "9.6StructuredOutputs\nConvolutionalnetworkscanbeusedtooutputahigh-dimensional,structured\nobject,ratherthanjustpredictingaclasslabelforaclassiﬁcationtaskorareal\nvalueforaregressiontask.Typicallythisobjectisjustatensor,emittedbya\nstandardconvolutionallayer.Forexample,themodelmightemitatensor S,where\nS i , j , kistheprobabilitythatpixel ( j , k)oftheinputtothenetworkbelongstoclass\ni.Thisallowsthemodeltolabeleverypixelinanimageanddrawprecisemasks\nthatfollowtheoutlinesofindividualobjects.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 879, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 487}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0880_294ce90b", "text": "Oneissuethatoftencomesupisthattheoutputplanecanbesmallerthanthe\n3 5 8\nCHAPTER9.CONVOLUTIONALNETWORKS\nˆ Y( 1 )ˆ Y( 1 )ˆ Y( 2 )ˆ Y( 2 )ˆ Y( 3 )ˆ Y( 3 )\nH( 1 )H( 1 )H( 2 )H( 2 )H( 3 )H( 3 )\nXXU U UV V V W W\nFigure9.17:Anexampleofarecurrentconvolutionalnetworkforpixellabeling.The\ninputisanimagetensor,withaxescorrespondingtoimagerows,imagecolumns,and X\nchannels(red,green,blue).Thegoalistooutputatensoroflabelsˆ Y,withaprobability\ndistributionoverlabelsforeachpixel.Thistensorhasaxescorrespondingtoimagerows,\nimagecolumns,andthediﬀerentclasses.Ratherthanoutputtingˆ Yinasingleshot,the\nrecurrentnetworkiterativelyreﬁnesitsestimateˆ Ybyusingapreviousestimateofˆ Y\nasinputforcreatinganewestimate.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 880, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 690}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0881_17db69e3", "text": "Thesameparametersareusedforeachupdated\nestimate,andtheestimatecanbereﬁnedasmanytimesaswewish.Thetensorof\nconvolutionkernels Uisusedoneachsteptocomputethehiddenrepresentationgiventhe\ninputimage.Thekerneltensor Visusedtoproduceanestimateofthelabelsgiventhe\nhiddenvalues.Onallbuttheﬁrststep,thekernels Wareconvolvedoverˆ Ytoprovide\ninputtothehiddenlayer.Ontheﬁrsttimestep,thistermisreplacedbyzero.Because\nthesameparametersareusedoneachstep,thisisanexampleofarecurrentnetwork,as\ndescribedinchapter.10\ninputplane,asshowninﬁgure.Inthekindsofarchitectures typicallyusedfor 9.13\nclassiﬁcationofasingleobjectinanimage,thegreatestreductioninthespatial\ndimensionsofthenetworkcomesfromusingpoolinglayerswithlargestride.In\nordertoproduceanoutputmapofsimilarsizeastheinput,onecanavoidpooling\naltogether(,).Anotherstrategyistosimplyemitalower-resolution Jainetal.2007\ngridoflabels( ,,).Finally,inprinciple,onecould PinheiroandCollobert20142015\nuseapoolingoperatorwithunitstride.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 881, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 963}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0882_4f6ef0e1", "text": "Onestrategyforpixel-wiselabelingofimagesistoproduceaninitialguess\noftheimagelabels,thenreﬁnethisinitialguessusingtheinteractionsbetween\nneighboringpixels.Repeatingthisreﬁnementstepseveraltimescorrespondsto\nusingthesameconvolutionsateachstage,sharingweightsbetweenthelastlayersof\nthedeepnet(,).Thismakesthesequenceofcomputationsperformed Jainetal.2007\nbythesuccessiveconvolutionallayerswithweightssharedacrosslayersaparticular\nkindofrecurrentnetwork( ,,).Figureshows PinheiroandCollobert20142015 9.17\nthearchitectureofsucharecurrentconvolutionalnetwork. 3 5 9\nCHAPTER9.CONVOLUTIONALNETWORKS\nOnceapredictionforeachpixelismade,variousmethodscanbeusedto\nfurtherprocessthesepredictionsinordertoobtainasegmentationoftheimage\nintoregions( ,; Briggman etal.2009Turaga 2010Farabet2013 etal.,; etal.,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 882, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 792}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0883_dac4b030", "text": "Thegeneralideaistoassumethatlargegroupsofcontiguouspixelstendtobe\nassociatedwiththesamelabel.Graphicalmodelscandescribetheprobabilistic\nrelationshipsbetweenneighboringpixels.Alternatively,theconvolutionalnetwork\ncanbetrainedtomaximizeanapproximation ofthegraphicalmodeltraining\nobjective(,; ,). Ningetal.2005Thompsonetal.2014\n9.7DataTypes\nThedatausedwithaconvolutionalnetworkusuallyconsistsofseveralchannels,\neachchannelbeingtheobservationofadiﬀerentquantityatsomepointinspace\nortime.Seetableforexamplesofdatatypeswithdiﬀerentdimensionalities 9.1\nandnumberofchannels. Foranexampleofconvolutionalnetworksappliedtovideo,seeChenetal.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 883, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 630}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0884_e0b32d6a", "text": "Foranexampleofconvolutionalnetworksappliedtovideo,seeChenetal. ().2010\nSofarwehavediscussedonlythecasewhereeveryexampleinthetrainandtest\ndatahasthesamespatialdimensions.Oneadvantagetoconvolutionalnetworks\nisthattheycanalsoprocessinputswithvaryingspatialextents.Thesekindsof\ninputsimplycannotberepresentedbytraditional,matrixmultiplication-based\nneuralnetworks.Thisprovidesacompellingreasontouseconvolutionalnetworks\nevenwhencomputational costandoverﬁttingarenotsigniﬁcantissues. Forexample,consideracollectionofimages,whereeachimagehasadiﬀerent\nwidthandheight.Itisunclearhowtomodelsuchinputswithaweightmatrixof\nﬁxedsize.Convolutionisstraightforwardtoapply;thekernelissimplyapplieda\ndiﬀerentnumberoftimesdependingonthesizeoftheinput,andtheoutputofthe\nconvolutionoperationscalesaccordingly.Convolutionmaybeviewedasmatrix\nmultiplication; thesameconvolutionkernelinducesadiﬀerentsizeofdoublyblock\ncirculantmatrixforeachsizeofinput.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 884, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 927}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0885_c93c1be7", "text": "Sometimes theoutputofthenetworkis\nallowedtohavevariablesizeaswellastheinput,forexampleifwewanttoassign\naclasslabeltoeachpixeloftheinput.Inthiscase,nofurtherdesignworkis\nnecessary.Inothercases,thenetworkmustproducesomeﬁxed-sizeoutput,for\nexampleifwewanttoassignasingleclasslabeltotheentireimage.Inthiscase\nwemustmakesomeadditionaldesignsteps,likeinsertingapoolinglayerwhose\npoolingregionsscaleinsizeproportionaltothesizeoftheinput,inorderto\nmaintainaﬁxednumberofpooledoutputs.Someexamplesofthiskindofstrategy\nareshowninﬁgure.9.11\n3 6 0\nCHAPTER9.CONVOLUTIONALNETWORKS\nSinglechannel Multi-channel\n1-DAudio waveform:The axis we\nconvolveovercorrespondsto\ntime.Wediscretizetimeand\nmeasuretheamplitudeofthe\nwaveformoncepertimestep.Skeletonanimationdata:Anima-\ntionsof3-Dcomputer-rendered\ncharactersaregeneratedbyalter-\ningtheposeofa“skeleton”over\ntime.Ateachpointintime,the\nposeofthecharacterisdescribed\nbyaspeciﬁcationoftheanglesof\neachofthejointsinthecharac-\nter’sskeleton.Eachchannelin\nthedatawefeedtotheconvolu-\ntionalmodelrepresentstheangle\naboutoneaxisofonejoint.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 885, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1062}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0886_8a8f0d1c", "text": "2-DAudiodatathathasbeenprepro-\ncessedwithaFouriertransform:\nWecantransformtheaudiowave-\nformintoa2Dtensorwithdif-\nferentrowscorrespondingtodif-\nferentfrequencies anddiﬀerent\ncolumnscorrespondingtodiﬀer-\nentpointsintime.Usingconvolu-\ntioninthetimemakesthemodel\nequivarianttoshiftsintime.Us-\ningconvolutionacrossthefre-\nquencyaxismakesthemodel\nequivarianttofrequency,sothat\nthesamemelodyplayedinadif-\nferentoctaveproducesthesame\nrepresentationbutatadiﬀerent\nheightinthenetwork’soutput.Colorimagedata:Onechannel\ncontainstheredpixels,onethe\ngreen pixels, and one theblue\npixels.Theconvolutionkernel\nmovesoverboththehorizontal\nandverticalaxesofthe image,\nconferringtranslationequivari-\nanceinbothdirections. 3-DVolumetricdata:Acommon\nsourceofthiskindofdataismed-\nicalimagingtechnology,suchas\nCTscans.Colorvideodata:Oneaxiscorre-\nspondstotime,onetotheheight\nofthevideoframe,andoneto\nthewidthofthevideoframe. Table9.1:Examplesofdiﬀerentformatsofdatathatcanbeusedwithconvolutional\nnetworks.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 886, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 982}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0887_7578c490", "text": "Table9.1:Examplesofdiﬀerentformatsofdatathatcanbeusedwithconvolutional\nnetworks. 3 6 1\nCHAPTER9.CONVOLUTIONALNETWORKS\nNotethattheuseofconvolutionforprocessingvariablesizedinputsonlymakes\nsenseforinputsthathavevariablesizebecausetheycontainvaryingamounts\nofobservationofthesamekindofthing—diﬀeren tlengthsofrecordingsover\ntime,diﬀerentwidthsofobservationsoverspace,etc.Convolutiondoesnotmake\nsenseiftheinputhasvariablesizebecauseitcanoptionallyincludediﬀerent\nkindsofobservations.Forexample,ifweareprocessingcollegeapplications,and\nourfeaturesconsistofbothgradesandstandardizedtestscores,butnotevery\napplicanttookthestandardizedtest,thenitdoesnotmakesensetoconvolvethe\nsameweightsoverboththefeaturescorrespondingtothegradesandthefeatures\ncorrespondingtothetestscores. 9.8EﬃcientConvolutionAlgorithms\nModernconvolutionalnetworkapplicationsofteninvolvenetworkscontainingmore\nthanonemillionunits.Powerfulimplementations exploitingparallelcomputation\nresources,asdiscussedinsection,areessential.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 887, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 990}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0888_6ea0292a", "text": "However,inmanycasesit 12.1\nisalsopossibletospeedupconvolutionbyselectinganappropriateconvolution\nalgorithm. Convolutionisequivalenttoconvertingboththeinputandthekerneltothe\nfrequencydomainusingaFouriertransform,performingpoint-wisemultiplication\nofthetwosignals, andconvertingbacktothetimedomainusinganinverse\nFouriertransform.Forsomeproblemsizes,thiscanbefasterthanthenaive\nimplementationofdiscreteconvolution. Whena d-dimensionalkernelcanbeexpressedas theouterproductof d\nvectors,onevectorperdimension,thekerneliscalled se par abl e.Whenthe\nkernelisseparable,naiveconvolutionisineﬃcient.Itisequivalenttocompose d\none-dimensional convolutionswitheachofthesevectors.Thecomposedapproach\nissigniﬁcantlyfasterthanperformingone d-dimensionalconvolutionwiththeir\nouterproduct.Thekernelalsotakesfewerparameterstorepresentasvectors.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 888, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 825}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0889_c1f4b9e3", "text": "Ifthekernelis welementswideineachdimension,thennaivemultidimensional\nconvolutionrequires O( wd)runtimeandparameterstoragespace,whileseparable\nconvolutionrequires O( w d ×)runtimeandparameterstoragespace.Ofcourse,\nnoteveryconvolutioncanberepresentedinthisway. Devisingfasterwaysofperformingconvolutionorapproximateconvolution\nwithoutharmingtheaccuracyofthemodelisanactiveareaofresearch.Eventech-\nniquesthatimprovetheeﬃciencyofonlyforwardpropagationareusefulbecause\ninthecommercialsetting,itistypicaltodevotemoreresourcestodeploymentof\nanetworkthantoitstraining.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 889, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 560}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0890_029423d9", "text": "3 6 2\nCHAPTER9.CONVOLUTIONALNETWORKS\n9.9RandomorUnsupervisedFeatures\nTypically,themostexpensivepartofconvolutionalnetworktrainingislearningthe\nfeatures.Theoutputlayerisusuallyrelativelyinexpensiveduetothesmallnumber\noffeaturesprovidedasinputtothislayerafterpassingthroughseverallayersof\npooling.Whenperformingsupervisedtrainingwithgradientdescent,everygradient\nsteprequiresacompleterunofforwardpropagationandbackwardpropagation\nthroughtheentirenetwork.Onewaytoreducethecostofconvolutionalnetwork\ntrainingistousefeaturesthatarenottrainedinasupervisedfashion. Therearethreebasicstrategiesforobtaining con volutionkernelswithout\nsupervisedtraining.Oneistosimplyinitializethemrandomly.Anotheristo\ndesignthembyhand,forexamplebysettingeachkerneltodetectedgesata\ncertainorientationorscale.Finally,onecanlearnthekernelswithanunsupervised\ncriterion.Forexample, ()apply Coatesetal.2011 k-meansclusteringtosmall\nimagepatches,thenuseeachlearnedcentroidasaconvolutionkernel.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 890, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 961}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0891_deede5ec", "text": "PartIII\ndescribesmanymoreunsupervisedlearningapproaches.Learningthefeatures\nwithanunsupervisedcriterionallowsthemtobedeterminedseparatelyfromthe\nclassiﬁerlayeratthetopofthearchitecture.Onecanthenextractthefeaturesfor\ntheentiretrainingsetjustonce,essentiallyconstructinganewtrainingsetforthe\nlastlayer.Learningthelastlayeristhentypicallyaconvexoptimization problem,\nassumingthelastlayerissomethinglikelogisticregressionoranSVM. Randomﬁltersoftenworksurprisinglywellinconvolutionalnetworks(Jarrett\netal. etal. etal. ,;2009Saxe,;2011Pinto,;2011CoxandPinto2011Saxe,).etal. ()showedthatlayersconsistingofconvolutionfollowingbypoolingnaturally 2011\nbecomefrequencyselectiveandtranslationinvariantwhenassignedrandomweights. Theyarguethatthisprovidesaninexpensivewaytochoosethearchitectureof\naconvolutionalnetwork:ﬁrstevaluatetheperformanceofseveralconvolutional\nnetworkarchitecturesbytrainingonlythelastlayer,thentakethebestofthese\narchitecturesandtraintheentirearchitectureusingamoreexpensiveapproach.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 891, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 995}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0892_fd336352", "text": "Anintermediate approachistolearnthefeatures,butusingmethodsthatdo\nnotrequirefullforwardandback-propagationateverygradientstep.Aswith\nmultilayerperceptrons,weusegreedylayer-wisepretraining,totraintheﬁrstlayer\ninisolation,thenextractallfeaturesfromtheﬁrstlayeronlyonce,thentrainthe\nsecondlayerinisolationgiventhosefeatures,andsoon.Chapterhasdescribed 8\nhowtoperformsupervisedgreedylayer-wisepretraining,andpartextendsthisIII\ntogreedylayer-wisepretrainingusinganunsupervisedcriterionateachlayer.The\ncanonicalexampleofgreedylayer-wisepretrainingofaconvolutionalmodelisthe\nconvolutionaldeepbeliefnetwork(,).Convolutionalnetworksoﬀer Leeetal.2009\n3 6 3\nCHAPTER9.CONVOLUTIONALNETWORKS\nustheopportunitytotakethepretrainingstrategyonestepfurtherthanispossible\nwithmultilayerperceptrons.Insteadoftraininganentireconvolutionallayerata\ntime,wecantrainamodelofasmallpatch,as ()dowith Coatesetal.2011 k-means.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 892, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 895}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0893_915211d5", "text": "Wecanthenusetheparametersfromthispatch-basedmodeltodeﬁnethekernels\nofaconvolutionallayer.Thismeansthatitispossibletouseunsupervisedlearning\ntotrainaconvolutionalnetworkwithouteverusingconvolutionduringthetraining\nprocess.Usingthisapproach,wecantrainverylargemodelsandincurahigh\ncomputational costonlyatinferencetime( ,; , Ranzatoetal.2007bJarrettetal. 2009Kavukcuoglu2010Coates 2013 ; etal.,; etal.,).Thisapproachwaspopular\nfromroughly2007–2013,whenlabeleddatasetsweresmallandcomputational\npowerwasmorelimited.Today,mostconvolutionalnetworksaretrainedina\npurelysupervisedfashion,usingfullforwardandback-propagation throughthe\nentirenetworkoneachtrainingiteration. Aswithotherapproachestounsupervisedpretraining,itremainsdiﬃcultto\nteaseapartthecauseofsomeofthebeneﬁtsseenwiththisapproach.Unsupervised\npretrainingmayoﬀersomeregularizationrelativetosupervisedtraining,oritmay\nsimplyallowustotrainmuchlargerarchitectures duetothereducedcomputational\ncostofthelearningrule.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 893, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 968}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0894_0c842a0f", "text": "9.10TheNeuroscientiﬁcBasisforConvolutionalNet-\nworks\nConvolutional networksare perhaps the greatest successstory ofbiologically\ninspiredartiﬁcialintelligence.Thoughconvolutionalnetworkshavebeenguided\nbymanyotherﬁelds,someofthekeydesignprinciplesofneuralnetworkswere\ndrawnfromneuroscience.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 894, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 288}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0895_cb40b0fa", "text": "Thehistoryofconvolutionalnetworksbeginswithneuroscientiﬁcexperiments\nlongbeforetherelevantcomputational modelsweredeveloped.Neurophysiologists\nDavidHubelandTorstenWieselcollaboratedforseveralyearstodeterminemany\nofthemostbasicfactsabouthowthemammalianvisionsystemworks(Hubeland\nWiesel195919621968 ,,,).Theiraccomplishmentswereeventuallyrecognizedwith\naNobelprize.Theirﬁndingsthathavehadthegreatestinﬂuenceoncontemporary\ndeeplearningmodelswerebasedonrecordingtheactivityofindividualneuronsin\ncats.Theyobservedhowneuronsinthecat’sbrainrespondedtoimagesprojected\ninpreciselocationsonascreeninfrontofthecat.Theirgreatdiscoverywas\nthatneuronsintheearlyvisualsystemrespondedmoststronglytoveryspeciﬁc\npatternsoflight,suchaspreciselyorientedbars,butrespondedhardlyatallto\notherpatterns. 3 6 4\nCHAPTER9.CONVOLUTIONALNETWORKS\nTheirworkhelpedtocharacterizemanyaspectsofbrainfunctionthatare\nbeyondthescopeofthisbook.Fromthepointofviewofdeeplearning,wecan\nfocusonasimpliﬁed,cartoonviewofbrainfunction.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 895, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 988}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0896_e3456e62", "text": "Inthissimpliﬁedview,wefocusonapartofthebraincalledV1,alsoknown\nasthe pr i m ar y v i sual c o r t e x. V1istheﬁrstareaofthebrainthatbeginsto\nperformsigniﬁcantlyadvancedprocessingofvisualinput. Inthiscartoonview,\nimagesareformedbylightarrivingintheeyeandstimulatingtheretina,the\nlight-sensitivetissueinthebackoftheeye.Theneuronsintheretinaperform\nsomesimplepreprocessingoftheimagebutdonotsubstantiallyalterthewayitis\nrepresented.Theimagethenpassesthroughtheopticnerveandabrainregion\ncalledthelateralgeniculatenucleus. Themainrole,asfarasweareconcerned\nhere,ofbothoftheseanatomicalregionsisprimarilyjusttocarrythesignalfrom\ntheeyetoV1,whichislocatedatthebackofthehead.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 896, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 666}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0897_050b0812", "text": "Themainrole,asfarasweareconcerned\nhere,ofbothoftheseanatomicalregionsisprimarilyjusttocarrythesignalfrom\ntheeyetoV1,whichislocatedatthebackofthehead. AconvolutionalnetworklayerisdesignedtocapturethreepropertiesofV1:\n1.V1isarrangedinaspatialmap.Itactuallyhasatwo-dimensionalstructure\nmirroring the structure of theimage in the retina.For example, light\narrivingatthelowerhalfoftheretinaaﬀectsonlythecorrespondinghalfof\nV1.Convolutionalnetworkscapturethispropertybyhavingtheirfeatures\ndeﬁnedintermsoftwodimensionalmaps. 2.V1containsmany si m pl e c e l l s.Asimplecell’sactivitycantosomeextent\nbecharacterizedbyalinear function oftheimagein asmall, spatially\nlocalizedreceptiveﬁeld.Thedetectorunitsofaconvolutionalnetworkare\ndesignedtoemulatethesepropertiesofsimplecells.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 897, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 769}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0898_54ae5fe6", "text": "3.V1alsocontainsmany c o m pl e x c e l l s.Thesecellsrespondtofeaturesthat\naresimilartothosedetectedbysimplecells,butcomplexcellsareinvariant\ntosmallshiftsinthepositionofthefeature.Thisinspiresthepoolingunits\nofconvolutionalnetworks.Complexcellsarealsoinvarianttosomechanges\ninlightingthatcannotbecapturedsimplybypoolingoverspatiallocations. Theseinvarianceshaveinspiredsomeofthecross-channelpoolingstrategies\ninconvolutionalnetworks,suchasmaxoutunits( ,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 898, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 457}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0899_1f2145bd", "text": "Theseinvarianceshaveinspiredsomeofthecross-channelpoolingstrategies\ninconvolutionalnetworks,suchasmaxoutunits( ,). Goodfellow etal.2013a\nThoughweknowthemostaboutV1,itisgenerallybelievedthatthesame\nbasicprinciplesapplytootherareasofthevisualsystem.Inourcartoonviewof\nthevisualsystem,thebasicstrategyofdetectionfollowedbypoolingisrepeatedly\nappliedaswemovedeeperintothebrain.Aswepassthroughmultipleanatomical\nlayersofthebrain,weeventuallyﬁndcellsthatrespondtosomespeciﬁcconcept\nandareinvarianttomanytransformationsoftheinput.Thesecellshavebeen\n3 6 5\nCHAPTER9.CONVOLUTIONALNETWORKS\nnicknamed“grandmother cells”—theideaisthatapersoncouldhaveaneuronthat\nactivateswhenseeinganimageoftheirgrandmother, regardlessofwhethershe\nappearsintheleftorrightsideoftheimage,whethertheimageisaclose-upof\nherfaceorzoomedoutshotofherentirebody,whethersheisbrightlylit,orin\nshadow,etc.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 899, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 863}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0900_c4cc7823", "text": "Thesegrandmother cellshavebeenshowntoactuallyexistinthehumanbrain,\ninaregioncalledthemedialtemporallobe( ,).Researchers Quiroga etal.2005\ntestedwhetherindividualneuronswouldrespondtophotosoffamousindividuals. Theyfoundwhathascometobecalledthe“HalleBerryneuron”:anindividual\nneuronthatisactivatedbytheconceptofHalleBerry.Thisneuronﬁreswhena\npersonseesaphotoofHalleBerry,adrawingofHalleBerry,oreventextcontaining\nthewords“HalleBerry.”Ofcourse,thishasnothingtodowithHalleBerryherself;\notherneuronsrespondedtothepresenceofBillClinton,JenniferAniston,etc. Thesemedialtemporallobeneuronsaresomewhatmoregeneralthanmodern\nconvolutionalnetworks,whichwouldnotautomatically generalizetoidentifying\napersonorobjectwhenreadingitsname.Theclosestanalogtoaconvolutional\nnetwork’slastlayeroffeaturesisabrainareacalledtheinferotemporal cortex\n(IT).Whenviewinganobject,informationﬂowsfromtheretina,throughthe\nLGN,toV1,thenonwardtoV2,thenV4,thenIT.Thishappenswithintheﬁrst\n100msofglimpsinganobject.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 900, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 978}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0901_996bb68e", "text": "Ifapersonisallowedtocontinuelookingatthe\nobjectformoretime,theninformationwillbegintoﬂowbackwardsasthebrain\nusestop-downfeedbacktoupdatetheactivationsinthelowerlevelbrainareas. However,ifweinterrupttheperson’sgaze,andobserveonlytheﬁringratesthat\nresultfromtheﬁrst100msofmostlyfeedforwardactivation,thenITprovestobe\nverysimilartoaconvolutionalnetwork.ConvolutionalnetworkscanpredictIT\nﬁringrates,andalsoperformverysimilarlyto(timelimited)humansonobject\nrecognitiontasks(,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 901, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 472}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0902_cff1fb2a", "text": "DiCarlo2013\nThatbeingsaid,therearemanydiﬀerencesbetweenconvolutionalnetworks\nandthemammalianvisionsystem.Someofthesediﬀerencesarewellknown\ntocomputational neuroscientists,butoutsidethescopeofthisbook.Someof\nthesediﬀerencesarenotyetknown,becausemanybasicquestionsabouthowthe\nmammalianvisionsystemworksremainunanswered.Asabrieflist:\n•Thehumaneyeismostlyverylowresolution,exceptforatinypatchcalledthe\nf o v e a.Thefoveaonlyobservesanareaaboutthesizeofathumbnailheldat\narmslength.Thoughwefeelasifwecanseeanentiresceneinhighresolution,\nthisisanillusioncreatedbythesubconsciouspartofourbrain,asitstitches\ntogetherseveralglimpsesofsmallareas.Mostconvolutionalnetworksactually\nreceivelargefullresolutionphotographsasinput.Thehumanbrainmakes\n3 6 6\nCHAPTER9.CONVOLUTIONALNETWORKS\nseveraleyemovementscalled sac c adestoglimpsethemostvisuallysalient\nortask-relevantpartsofascene.Incorporatingsimilarattentionmechanisms\nintodeeplearningmodelsisanactiveresearchdirection.Inthecontextof\ndeeplearning,attentionmechanismshavebeenmostsuccessfulfornatural\nlanguageprocessing,asdescribedinsection.Severalvisualmodels 12.4.5.1\nwithfoveationmechanismshavebeendevelopedbutsofarhavenotbecome\nthedominantapproach(LarochelleandHinton2010Denil2012 ,;etal.,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 902, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1231}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0903_f284a505", "text": "•Thehumanvisualsystemisintegratedwithmanyothersenses,suchas\nhearing,andfactorslikeourmoodsandthoughts.Convolutionalnetworks\nsofararepurelyvisual. •Thehumanvisualsystemdoesmuchmorethanjustrecognizeobjects.Itis\nabletounderstandentirescenesincludingmanyobjectsandrelationships\nbetweenobjects,andprocessesrich3-Dgeometricinformationneededfor\nourbodiestointerfacewiththeworld.Convolutionalnetworkshavebeen\nappliedtosomeoftheseproblemsbuttheseapplicationsareintheirinfancy. •EvensimplebrainareaslikeV1areheavilyimpactedbyfeedbackfromhigher\nlevels.Feedbackhasbeenexploredextensivelyinneuralnetworkmodelsbut\nhasnotyetbeenshowntooﬀeracompellingimprovement.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 903, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 647}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0904_0d4f7135", "text": "•EvensimplebrainareaslikeV1areheavilyimpactedbyfeedbackfromhigher\nlevels.Feedbackhasbeenexploredextensivelyinneuralnetworkmodelsbut\nhasnotyetbeenshowntooﬀeracompellingimprovement. •WhilefeedforwardITﬁringratescapturemuchofthesameinformationas\nconvolutionalnetworkfeatures,itisnotclearhowsimilartheintermediate\ncomputations are.Thebrainprobablyusesverydiﬀerentactivationand\npoolingfunctions.Anindividualneuron’sactivationprobablyisnotwell-\ncharacterizedbyasinglelinearﬁlterresponse.ArecentmodelofV1involves\nmultiplequadraticﬁltersforeachneuron(,).Indeedour Rustetal.2005\ncartoonpictureof“simplecells” and“complexcells” mightcreateanon-\nexistentdistinction;simplecellsandcomplexcellsmightbothbethesame\nkindofcellbutwiththeir“parameters”enablingacontinuumofbehaviors\nrangingfromwhatwecall“simple”towhatwecall“complex.”\nItis alsoworthmentioningthatneuroscience hastold usrelativelylittle\nabouthowtotrainconvolutionalnetworks.Modelstructureswithparameter\nsharingacrossmultiplespatiallocationsdatebacktoearlyconnectionistmodels\nofvision( ,),butthesemodelsdidnotusethemodern MarrandPoggio1976\nback-propagationalgorithmandgradientdescent.Forexample,theNeocognitron\n(Fukushima1980,)incorporatedmostofthemodelarchitecturedesignelementsof\nthemodernconvolutionalnetworkbutreliedonalayer-wiseunsupervisedclustering\nalgorithm.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 904, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1312}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0905_d9d6ac8f", "text": "3 6 7\nCHAPTER9.CONVOLUTIONALNETWORKS\nLang andHinton 1988()introducedthe use ofback-propagation totrain\nt i m e - del a y neur al net w o r k s(TDNNs).Tousecontemporary terminology,\nTDNNsareone-dimensional convolutionalnetworksappliedtotimeseries.Back-\npropagationappliedtothesemodelswasnotinspiredbyanyneuroscientiﬁcobserva-\ntionandisconsideredbysometobebiologicallyimplausible.Followingthesuccess\nofback-propagation-based trainingofTDNNs,( ,)developed LeCunetal.1989\nthemodernconvolutionalnetworkbyapplyingthesametrainingalgorithmto2-D\nconvolutionappliedtoimages. Sofarwehavedescribedhowsimplecellsareroughlylinearandselectivefor\ncertainfeatures,complexcellsaremorenonlinearandbecomeinvarianttosome\ntransformationsofthesesimplecellfeatures,andstacksoflayersthatalternate\nbetweenselectivityandinvariancecanyieldgrandmother cellsforveryspeciﬁc\nphenomena.Wehavenotyetdescribedpreciselywhattheseindividualcellsdetect.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 905, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 914}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0906_44a90078", "text": "Inadeep,nonlinearnetwork,itcanbediﬃculttounderstandthefunctionof\nindividualcells.Simplecellsintheﬁrstlayerareeasiertoanalyze,becausetheir\nresponsesaredrivenbyalinearfunction.Inanartiﬁcialneuralnetwork,wecan\njustdisplayanimageoftheconvolutionkerneltoseewhatthecorresponding\nchannelofaconvolutionallayerrespondsto.Inabiologicalneuralnetwork,we\ndonothaveaccesstotheweightsthemselves.Instead,weputanelectrodeinthe\nneuronitself,displayseveralsamplesofwhitenoiseimagesinfrontoftheanimal’s\nretina,andrecordhoweachofthesesamplescausestheneurontoactivate.Wecan\nthenﬁtalinearmodeltotheseresponsesinordertoobtainanapproximation of\ntheneuron’sweights.Thisapproachisknownas r e v e r se c o r r e l at i o n(Ringach\nandShapley2004,). ReversecorrelationshowsusthatmostV1cellshaveweightsthataredescribed\nby G ab o r f unc t i o ns.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 906, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 816}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0907_5b857a10", "text": "ReversecorrelationshowsusthatmostV1cellshaveweightsthataredescribed\nby G ab o r f unc t i o ns. TheGaborfunctiondescribestheweightata2-Dpoint\nintheimage.Wecanthinkofanimageasbeingafunctionof2-Dcoordinates,\nI( x , y).Likewise,wecanthinkofasimplecellassamplingtheimageatasetof\nlocations,deﬁnedbyasetof xcoordinates Xandasetof ycoordinates, Y,and\napplyingweightsthatarealsoafunctionofthelocation, w( x , y).Fromthispoint\nofview,theresponseofasimplecelltoanimageisgivenby\ns I() =\nx ∈ X\ny ∈ Yw x , y I x , y . ()() (9.15)\nSpeciﬁcally,takestheformofaGaborfunction: w x , y()\nw x , y α , β (; x , β y , f , φ , x 0 , y 0 , τ α) = exp− β x x 2− β y y 2\ncos( f x+) φ ,(9.16)\nwhere\nx= ( x x − 0)cos()+( τ y y − 0)sin() τ (9.17)\n3 6 8\nCHAPTER9.CONVOLUTIONALNETWORKS\nand\ny= ( − x x − 0)sin()+( τ y y − 0)cos() τ . (9.18)\nHere, α, β x, β y, f, φ, x 0, y 0,and τareparametersthatcontroltheproperties\noftheGaborfunction.FigureshowssomeexamplesofGaborfunctionswith 9.18\ndiﬀerentsettingsoftheseparameters.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 907, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 997}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0908_1b8185c6", "text": "(9.18)\nHere, α, β x, β y, f, φ, x 0, y 0,and τareparametersthatcontroltheproperties\noftheGaborfunction.FigureshowssomeexamplesofGaborfunctionswith 9.18\ndiﬀerentsettingsoftheseparameters. Theparameters x 0, y 0,and τdeﬁneacoordinatesystem. Wetranslateand\nrotate xand ytoform xand y.Speciﬁcally,thesimplecellwillrespondtoimage\nfeaturescenteredatthepoint( x 0, y 0),anditwillrespondtochangesinbrightness\naswemovealongalinerotatedradiansfromthehorizontal. τ\nViewedasafunctionof xand y,thefunction wthenrespondstochangesin\nbrightnessaswemovealongthe xaxis. Ithastwoimportantfactors:oneisa\nGaussianfunctionandtheotherisacosinefunction. TheGaussianfactor αexp\n− β x x 2− β y y 2\ncanbeseenasagatingtermthat\nensuresthesimplecellwillonlyrespondtovaluesnearwhere xand yareboth\nzero,inotherwords,nearthecenterofthecell’sreceptiveﬁeld.Thescalingfactor\nαadjuststhetotalmagnitudeofthesimplecell’sresponse,while β xand β ycontrol\nhowquicklyitsreceptiveﬁeldfallsoﬀ.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 908, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 959}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0909_ebe8c502", "text": "Thecosinefactor cos( f x+ φ) controlshowthesimplecellrespondstochanging\nbrightnessalongthe xaxis.Theparameter fcontrolsthefrequencyofthecosine\nandcontrolsitsphaseoﬀset. φ\nAltogether,thiscartoonviewofsimplecellsmeansthatasimplecellresponds\ntoaspeciﬁcspatialfrequencyofbrightnessinaspeciﬁcdirectionataspeciﬁc\nlocation.Simplecellsaremostexcitedwhenthewaveofbrightnessintheimage\nhasthesamephaseastheweights.Thisoccurswhentheimageisbrightwherethe\nweightsarepositiveanddarkwheretheweightsarenegative.Simplecellsaremost\ninhibitedwhenthewaveofbrightnessisfullyoutofphasewiththeweights—when\ntheimageisdarkwheretheweightsarepositiveandbrightwheretheweightsare\nnegative. Thecartoonviewofacomplexcellisthatitcomputesthe L2normofthe\n2-Dvectorcontainingtwosimplecells’responses: c( I)=\ns 0() I2+ s 1() I2.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 909, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 794}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0910_09016f5a", "text": "Thecartoonviewofacomplexcellisthatitcomputesthe L2normofthe\n2-Dvectorcontainingtwosimplecells’responses: c( I)=\ns 0() I2+ s 1() I2. An\nimportantspecialcaseoccurswhen s 1hasallofthesameparametersas s 0except\nfor φ,and φissetsuchthat s 1isonequartercycleoutofphasewith s 0.Inthis\ncase, s 0and s 1forma q uadr at u r e pai r.Acomplexcelldeﬁnedinthisway\nrespondswhentheGaussianreweightedimage I( x , y)exp( − β x x 2− β y y 2) contains\nahighamplitudesinusoidalwavewithfrequency findirection τnear ( x 0 , y 0),\nregardlessofthephaseoﬀsetofthiswave.Inotherwords,thecomplexcellis\ninvarianttosmalltranslationsoftheimageindirection τ,ortonegatingtheimage\n3 6 9\nCHAPTER9.CONVOLUTIONALNETWORKS\nFigure9.18:Gaborfunctionswithavarietyofparametersettings.Whiteindicates\nlargepositiveweight,blackindicateslargenegativeweight,andthebackgroundgray\ncorrespondstozeroweight. ( L e f t )Gaborfunctionswithdiﬀerentvaluesoftheparameters\nthatcontrolthecoordinatesystem: x 0, y 0,and τ.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 910, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 964}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0911_78e3eed8", "text": "( L e f t )Gaborfunctionswithdiﬀerentvaluesoftheparameters\nthatcontrolthecoordinatesystem: x 0, y 0,and τ. EachGaborfunctioninthisgridis\nassignedavalueof x 0and y 0proportionaltoitspositioninitsgrid,and τischosenso\nthateachGaborﬁlterissensitivetothedirectionradiatingoutfromthecenterofthegrid. Fortheothertwoplots, x 0, y 0,and τareﬁxedtozero. Gaborfunctionswith ( C e n t e r )\ndiﬀerentGaussianscaleparameters β xand β y.Gaborfunctionsarearrangedinincreasing\nwidth(decreasing β x)aswemovelefttorightthroughthegrid,andincreasingheight\n(decreasing β y)aswemovetoptobottom.Fortheothertwoplots,the βvaluesareﬁxed\nto1.5 ×theimagewidth.Gaborfunctionswithdiﬀerentsinusoidparameters ( R i g h t ) f\nand φ.Aswemovetoptobottom, fincreases,andaswemovelefttoright, φincreases. Fortheothertwoplots,isﬁxedto0andisﬁxedto5theimagewidth. φ f ×\n(replacingblackwithwhiteandviceversa).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 911, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 866}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0912_2baebead", "text": "Fortheothertwoplots,isﬁxedto0andisﬁxedto5theimagewidth. φ f ×\n(replacingblackwithwhiteandviceversa). Someofthemoststrikingcorrespondencesbetweenneuroscienceandmachine\nlearningcomefromvisuallycomparingthefeatureslearnedbymachinelearning\nmodelswiththoseemployedbyV1.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 912, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 264}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0913_c06ee841", "text": "Someofthemoststrikingcorrespondencesbetweenneuroscienceandmachine\nlearningcomefromvisuallycomparingthefeatureslearnedbymachinelearning\nmodelswiththoseemployedbyV1. ()showedthat OlshausenandField1996\nasimpleunsupervisedlearningalgorithm, sparse coding,learnsfeatureswith\nreceptiveﬁeldssimilartothoseofsimplecells.Sincethen,wehavefoundthat\nanextremelywidevarietyofstatisticallearningalgorithmslearnfeatureswith\nGabor-likefunctionswhenappliedtonaturalimages.Thisincludesmostdeep\nlearningalgorithms,whichlearnthesefeaturesintheirﬁrstlayer.Figure9.19\nshowssomeexamples.Becausesomanydiﬀerentlearningalgorithmslearnedge\ndetectors,itisdiﬃculttoconcludethatanyspeciﬁclearningalgorithmisthe\n“right”modelofthebrainjustbasedonthefeaturesthatitlearns(thoughitcan\ncertainlybeabadsignifanalgorithmdoeslearnsomesortofedgedetector not\nwhenappliedtonaturalimages).Thesefeaturesareanimportantpartofthe\nstatisticalstructureofnaturalimagesandcanberecoveredbymanydiﬀerent\napproachestostatisticalmodeling.SeeHyvärinen 2009etal.()forareviewofthe\nﬁeldofnaturalimagestatistics.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 913, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1051}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0914_eac8bd68", "text": "3 7 0\nCHAPTER9.CONVOLUTIONALNETWORKS\nFigure9.19:Manymachinelearningalgorithmslearnfeaturesthatdetectedgesorspeciﬁc\ncolorsofedgeswhenappliedtonaturalimages.Thesefeaturedetectorsarereminiscentof\ntheGaborfunctionsknowntobepresentinprimaryvisualcortex. ( L e f t )Weightslearned\nbyanunsupervisedlearningalgorithm(spikeandslabsparsecoding)appliedtosmall\nimagepatches. ( R i g h t )Convolutionkernelslearnedbytheﬁrstlayerofafullysupervised\nconvolutionalmaxoutnetwork.Neighboringpairsofﬁltersdrivethesamemaxoutunit.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 914, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 508}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0915_099f6aeb", "text": "( R i g h t )Convolutionkernelslearnedbytheﬁrstlayerofafullysupervised\nconvolutionalmaxoutnetwork.Neighboringpairsofﬁltersdrivethesamemaxoutunit. 9.11ConvolutionalNetworksandtheHistoryofDeep\nLearning\nConvolutionalnetworkshaveplayedanimportantroleinthehistoryofdeep\nlearning.Theyareakeyexampleofasuccessfulapplicationofinsightsobtained\nbystudyingthebraintomachinelearningapplications.Theywerealsosomeof\ntheﬁrstdeepmodelstoperformwell,longbeforearbitrarydeepmodelswere\nconsideredviable.Convolutionalnetworkswerealsosomeoftheﬁrstneural\nnetworkstosolveimportantcommercialapplicationsandremainattheforefront\nofcommercialapplicationsofdeeplearningtoday.Forexample,inthe1990s,the\nneuralnetworkresearchgroupatAT&Tdevelopedaconvolutionalnetworkfor\nreadingchecks(,).Bytheendofthe1990s,thissystemdeployed LeCunetal.1998b\nbyNECwasreadingover10%ofallthechecksintheUS.Later,severalOCR\nandhandwritingrecognitionsystemsbasedonconvolutionalnetsweredeployedby\nMicrosoft( ,).Seechapterformoredetailsonsuchapplications Simardetal.2003 12\nandmoremodernapplicationsofconvolutionalnetworks.See () LeCunetal.2010\nforamorein-depthhistoryofconvolutionalnetworksupto2010.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 915, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1144}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0916_18204ba1", "text": "Convolutionalnetworkswerealsousedtowinmanycontests.Thecurrent\nintensityofcommercialinterestindeeplearningbeganwhenKrizhevskyetal. ()wontheImageNetobjectrecognitionchallenge,butconvolutionalnetworks 2012\n3 7 1\nCHAPTER9.CONVOLUTIONALNETWORKS\nhadbeenusedtowinothermachinelearningandcomputervisioncontestswith\nlessimpactforyearsearlier.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 916, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 332}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0917_0f555f0a", "text": "Convolutionalnetsweresomeoftheﬁrstworkingdeepnetworkstrainedwith\nback-propagation.Itisnotentirelyclearwhyconvolutionalnetworkssucceeded\nwhengeneralback-propagationnetworkswereconsideredtohavefailed.Itmay\nsimplybethatconvolutionalnetworksweremorecomputationally eﬃcientthan\nfullyconnectednetworks,soitwaseasiertorunmultipleexperimentswiththem\nandtunetheirimplementation andhyperparameters.Largernetworksalsoseem\ntobeeasiertotrain.Withmodernhardware,largefullyconnectednetworks\nappeartoperformreasonablyonmanytasks,evenwhenusingdatasetsthatwere\navailableandactivationfunctionsthatwerepopularduringthetimeswhenfully\nconnectednetworkswerebelievednottoworkwell.Itmaybethattheprimary\nbarrierstothesuccessofneuralnetworkswerepsychological(practitioners did\nnotexpectneuralnetworkstowork,sotheydidnotmakeaseriouseﬀorttouse\nneuralnetworks).Whateverthecase,itisfortunatethatconvolutionalnetworks\nperformedwelldecadesago.Inmanyways,theycarriedthetorchfortherestof\ndeeplearningandpavedthewaytotheacceptanceofneuralnetworksingeneral.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 917, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1020}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0918_e69917a3", "text": "Convolutionalnetworksprovideawaytospecializeneuralnetworkstowork\nwithdatathathasacleargrid-structuredtopologyandtoscalesuchmodelsto\nverylargesize.Thisapproachhasbeenthemostsuccessfulonatwo-dimensional,\nimagetopology.Toprocessone-dimensional, sequentialdata,weturnnextto\nanotherpowerfulspecializationoftheneuralnetworksframework:recurrentneural\nnetworks. 3 7 2\nC h a p t e r 1 0\nS e qu e n ce Mo d e l i n g: Recurren t\nan d Recursiv e N e t s\nRecurrentneuralnetworksorRNNs( ,)areafamilyof Rumelhart e t a l .1986a\nneuralnetworksforprocessingsequentialdata.Muchasaconvolutionalnetwork\nisaneuralnetworkthatisspecializedforprocessingagridofvalues Xsuchas\nanimage,arecurrentneuralnetworkisaneuralnetworkthatisspecializedfor\nprocessingasequenceofvaluesx( 1 ), . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 918, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 760}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0919_e50c0b1a", "text": ". . ,x( ) τ.Justasconvolutionalnetworks\ncanreadilyscaletoimageswithlargewidthandheight,andsomeconvolutional\nnetworkscanprocessimagesofvariablesize,recurrentnetworkscanscaletomuch\nlongersequencesthanwouldbepracticalfornetworkswithoutsequence-based\nspecialization.Mostrecurrentnetworkscanalsoprocesssequencesofvariable\nlength. Togofrommulti-layernetworkstorecurrentnetworks,weneedtotakeadvan-\ntageofoneoftheearlyideasfoundinmachinelearningandstatisticalmodelsof\nthe1980s:sharingparametersacrossdiﬀerentpartsofamodel.Parametersharing\nmakesitpossibletoextendandapplythemodeltoexamplesofdiﬀerentforms\n(diﬀerentlengths,here)andgeneralizeacrossthem.Ifwehadseparateparameters\nforeachvalueofthetimeindex,wecouldnotgeneralizetosequencelengthsnot\nseenduringtraining,norsharestatisticalstrengthacrossdiﬀerentsequencelengths\nandacrossdiﬀerentpositionsintime.Suchsharingisparticularlyimportantwhen\naspeciﬁcpieceofinformationcanoccuratmultiplepositionswithinthesequence.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 919, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 955}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0920_b3bc7983", "text": "Forexample,considerthetwosentences“IwenttoNepalin2009”and“In2009,\nIwenttoNepal.”Ifweaskamachinelearningmodeltoreadeachsentenceand\nextracttheyearinwhichthenarratorwenttoNepal,wewouldlikeittorecognize\ntheyear2009astherelevantpieceofinformation,whetheritappearsinthesixth\n373\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwordorthesecondwordofthesentence.Supposethatwetrainedafeedforward\nnetworkthatprocessessentencesofﬁxedlength.Atraditionalfullyconnected\nfeedforwardnetworkwouldhaveseparateparametersforeachinputfeature,soit\nwouldneedtolearnalloftherulesofthelanguageseparatelyateachpositionin\nthesentence.Bycomparison,arecurrentneuralnetworksharesthesameweights\nacrossseveraltimesteps.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 920, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 692}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0921_726a3357", "text": "Arelatedideaistheuseofconvolutionacrossa1-Dtemporalsequence.This\nconvolutionalapproachisthebasisfortime-delayneuralnetworks(Langand\nHinton1988Waibel1989Lang1990 ,; e t a l .,; e t a l .,).Theconvolutionoperation\nallowsanetworktoshareparametersacrosstime,butisshallow.Theoutput\nofconvolutionisasequencewhereeachmemberoftheoutputisafunctionof\nasmallnumberofneighboringmembersoftheinput.Theideaofparameter\nsharingmanifestsintheapplicationofthesameconvolutionkernelateachtime\nstep.Recurrentnetworksshareparametersinadiﬀerentway.Eachmemberofthe\noutputisafunctionofthepreviousmembersoftheoutput.Eachmemberofthe\noutputisproducedusingthesameupdateruleappliedtothepreviousoutputs. Thisrecurrentformulationresultsinthesharingofparametersthroughavery\ndeepcomputational graph.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 921, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 764}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0922_f978406a", "text": "Thisrecurrentformulationresultsinthesharingofparametersthroughavery\ndeepcomputational graph. Forthesimplicityofexposition,werefertoRNNsasoperatingonasequence\nthatcontainsvectorsx( ) twiththetimestepindex trangingfromto1 τ.In\npractice,recurrentnetworksusuallyoperateonminibatchesofsuchsequences,\nwithadiﬀerentsequencelength τforeachmemberoftheminibatch.Wehave\nomittedtheminibatchindicestosimplifynotation.Moreover,thetimestepindex\nneednotliterallyrefertothepassageoftimeintherealworld.Sometimesitrefers\nonlytothepositioninthesequence.RNNsmayalsobeappliedintwodimensions\nacrossspatialdatasuchasimages,andevenwhenappliedtodatainvolvingtime,\nthenetworkmayhaveconnectionsthatgobackwardsintime,providedthatthe\nentiresequenceisobservedbeforeitisprovidedtothenetwork.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 922, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 759}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0923_af75d2d7", "text": "Thischapterextendstheideaofacomputational graphtoincludecycles.These\ncyclesrepresenttheinﬂuenceofthepresentvalueofavariableonitsownvalue\natafuturetimestep.Suchcomputational graphsallowustodeﬁnerecurrent\nneuralnetworks.Wethendescribemanydiﬀerentwaystoconstruct,train,and\nuserecurrentneuralnetworks. Formoreinformationonrecurrentneuralnetworksthanisavailableinthis\nchapter,wereferthereadertothetextbookofGraves2012(). 3 7 4\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.1UnfoldingComputationalGraphs\nAcomputational graphisawaytoformalizethestructureofasetofcomputations,\nsuchasthoseinvolvedinmappinginputsandparameterstooutputsandloss. Pleaserefertosectionforageneralintroduction. Inthissectionweexplain 6.5.1\ntheideaofunfoldingarecursiveorrecurrentcomputationintoacomputational\ngraphthathasarepetitivestructure,typicallycorrespondingtoachainofevents. Unfoldingthisgraphresultsinthesharingofparametersacrossadeepnetwork\nstructure.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 923, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 938}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0924_e88f6d1e", "text": "Unfoldingthisgraphresultsinthesharingofparametersacrossadeepnetwork\nstructure. Forexample,considertheclassicalformofadynamicalsystem:\ns( ) t= ( fs( 1 ) t −;)θ , (10.1)\nwheres( ) tiscalledthestateofthesystem. Equationisrecurrentbecausethedeﬁnitionof 10.1 sattime trefersbackto\nthesamedeﬁnitionattime. t−1\nForaﬁnitenumberoftimesteps τ,thegraphcanbeunfoldedbyapplying\nthedeﬁnition τ−1times.Forexample,ifweunfoldequationfor10.1 τ= 3time\nsteps,weobtain\ns( 3 )=( fs( 2 );)θ (10.2)\n=(( f fs( 1 ););)θθ (10.3)\nUnfoldingtheequationbyrepeatedlyapplyingthedeﬁnitioninthiswayhas\nyieldedanexpressionthatdoesnotinvolverecurrence.Suchanexpressioncan\nnowberepresentedbyatraditionaldirectedacycliccomputational graph. The\nunfoldedcomputational graphofequationandequationisillustratedin 10.1 10.3\nﬁgure.10.1\ns( t − 1 )s( t − 1 )s( ) ts( ) ts( + 1 ) ts( + 1 ) t\nf fs( ) . . .s( ) . . .s( ) . . .s( ) . . . f f f f f f\nFigure10.1:Theclassicaldynamicalsystemdescribedbyequation,illustratedasan 10.1\nunfoldedcomputationalgraph.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 924, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1005}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0925_0c70cedd", "text": ". .s( ) . . .s( ) . . .s( ) . . . f f f f f f\nFigure10.1:Theclassicaldynamicalsystemdescribedbyequation,illustratedasan 10.1\nunfoldedcomputationalgraph. Eachnoderepresentsthestateatsometime tandthe\nfunction fmapsthestateat ttothestateat t+1.Thesameparameters(thesamevalue\nofusedtoparametrize)areusedforalltimesteps. θ f\nAsanotherexample,letusconsideradynamicalsystemdrivenbyanexternal\nsignalx( ) t,\ns( ) t= ( fs( 1 ) t −,x( ) t;)θ , (10.4)\n3 7 5\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwhereweseethatthestatenowcontainsinformationaboutthewholepastsequence. Recurrentneuralnetworkscanbebuiltinmanydiﬀerentways.Muchas\nalmostanyfunctioncanbeconsideredafeedforwardneuralnetwork,essentially\nanyfunctioninvolvingrecurrencecanbeconsideredarecurrentneuralnetwork. Manyrecurrentneuralnetworksuseequationorasimilarequationto 10.5\ndeﬁnethevaluesoftheirhiddenunits.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 925, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 866}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0926_1c2a0ed9", "text": "Manyrecurrentneuralnetworksuseequationorasimilarequationto 10.5\ndeﬁnethevaluesoftheirhiddenunits. Toindicatethatthestateisthehidden\nunitsofthenetwork,wenowrewriteequationusingthevariable 10.4 htorepresent\nthestate:\nh( ) t= ( fh( 1 ) t −,x( ) t;)θ , (10.5)\nillustratedinﬁgure,typicalRNNswilladdextraarchitecturalfeaturessuch 10.2\nasoutputlayersthatreadinformationoutofthestatetomakepredictions.h\nWhentherecurrentnetworkistrainedtoperformataskthatrequirespredicting\nthefuturefromthepast,thenetworktypicallylearnstouseh( ) tasakindoflossy\nsummaryofthetask-relevantaspectsofthepastsequenceofinputsupto t.This\nsummaryisingeneralnecessarilylossy,sinceitmapsanarbitrarylengthsequence\n(x( ) t,x( 1 ) t −,x( 2 ) t −, . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 926, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 713}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0927_0258f208", "text": ". . ,x( 2 ),x( 1 ))toaﬁxedlengthvectorh( ) t.Dependingonthe\ntrainingcriterion,thissummarymightselectivelykeepsomeaspectsofthepast\nsequencewithmoreprecisionthanotheraspects.Forexample,iftheRNNisused\ninstatisticallanguagemodeling,typicallytopredictthenextwordgivenprevious\nwords,itmaynotbenecessarytostorealloftheinformationintheinputsequence\nuptotime t,butratheronlyenoughinformationtopredicttherestofthesentence. Themostdemandingsituationiswhenweaskh( ) ttoberichenoughtoallow\nonetoapproximately recovertheinputsequence,asinautoencoderframeworks\n(chapter).14\nf fhh\nx xh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) th( ) . . .h( ) . . .h( ) . . .h( ) . . . f f\nU nf ol df f f f f\nFigure10.2:Arecurrentnetworkwithnooutputs.Thisrecurrentnetworkjustprocesses\ninformationfromtheinputxbyincorporatingitintothestatehthatispassedforward\nthroughtime.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 927, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 899}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0928_d960ceb5", "text": ". .h( ) . . . f f\nU nf ol df f f f f\nFigure10.2:Arecurrentnetworkwithnooutputs.Thisrecurrentnetworkjustprocesses\ninformationfromtheinputxbyincorporatingitintothestatehthatispassedforward\nthroughtime. ( L e f t )Circuitdiagram.Theblacksquareindicatesadelayofasingletime\nstep.Thesamenetworkseenasanunfoldedcomputationalgraph,whereeach ( R i g h t )\nnodeisnowassociatedwithoneparticulartimeinstance.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 928, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 396}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0929_acd78714", "text": "( L e f t )Circuitdiagram.Theblacksquareindicatesadelayofasingletime\nstep.Thesamenetworkseenasanunfoldedcomputationalgraph,whereeach ( R i g h t )\nnodeisnowassociatedwithoneparticulartimeinstance. Equationcanbedrawnintwodiﬀerentways.OnewaytodrawtheRNN 10.5\niswithadiagramcontainingonenodeforeverycomponentthatmightexistina\n3 7 6\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nphysicalimplementationofthemodel,suchasabiologicalneuralnetwork.Inthis\nview,thenetworkdeﬁnesacircuitthatoperatesinrealtime,withphysicalparts\nwhosecurrentstatecaninﬂuencetheirfuturestate,asintheleftofﬁgure.10.2\nThroughoutthischapter,weuseablacksquareinacircuitdiagramtoindicate\nthataninteractiontakesplacewithadelayofasingletimestep,fromthestate\nattime ttothestateattime t+1.TheotherwaytodrawtheRNNisasan\nunfoldedcomputational graph,inwhicheachcomponentisrepresentedbymany\ndiﬀerentvariables,withonevariablepertimestep,representingthestateofthe\ncomponentatthatpointintime.Eachvariableforeachtimestepisdrawnasa\nseparatenodeofthecomputational graph,asintherightofﬁgure.Whatwe10.2\ncallunfoldingistheoperationthatmapsacircuitasintheleftsideoftheﬁgure\ntoacomputational graphwithrepeatedpiecesasintherightside.Theunfolded\ngraphnowhasasizethatdependsonthesequencelength.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 929, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1243}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0930_0779ddcc", "text": "Wecanrepresenttheunfoldedrecurrenceafterstepswithafunction t g( ) t:\nh( ) t= g( ) t(x( ) t,x( 1 ) t −,x( 2 ) t −, . . . ,x( 2 ),x( 1 )) (10.6)\n=( fh( 1 ) t −,x( ) t;)θ (10.7)\nThefunction g( ) ttakesthewholepastsequence (x( ) t,x( 1 ) t −,x( 2 ) t −, . . . ,x( 2 ),x( 1 ))\nasinputandproducesthecurrentstate,buttheunfoldedrecurrentstructure\nallowsustofactorize g( ) tintorepeatedapplicationofafunction f.Theunfolding\nprocessthusintroducestwomajoradvantages:\n1.Regardlessofthesequencelength,thelearnedmodelalwayshasthesame\ninputsize,becauseitisspeciﬁedintermsoftransitionfromonestateto\nanotherstate,ratherthanspeciﬁedintermsofavariable-length historyof\nstates. 2.Itispossibletousethetransitionfunction s a m e fwiththesameparameters\nateverytimestep.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 930, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 746}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0931_53ab4234", "text": "2.Itispossibletousethetransitionfunction s a m e fwiththesameparameters\nateverytimestep. Thesetwofactorsmakeitpossibletolearnasinglemodel fthatoperateson\nalltimestepsandallsequencelengths,ratherthanneedingtolearnaseparate\nmodel g( ) tforallpossibletimesteps.Learningasingle,sharedmodelallows\ngeneralization tosequencelengthsthatdidnotappearinthetrainingset,and\nallowsthemodeltobeestimatedwithfarfewertrainingexamplesthanwouldbe\nrequiredwithoutparametersharing. Boththerecurrentgraphandtheunrolledgraphhavetheiruses.Therecurrent\ngraphissuccinct.Theunfoldedgraphprovidesanexplicitdescriptionofwhich\ncomputations toperform.Theunfoldedgraphalsohelpstoillustratetheideaof\n3 7 7\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ninformationﬂowforwardintime(computingoutputsandlosses)andbackward\nintime(computinggradients)byexplicitlyshowingthepathalongwhichthis\ninformationﬂows.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 931, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 875}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0932_f37b65b6", "text": "10.2RecurrentNeuralNetworks\nArmedwiththegraphunrollingandparametersharingideasofsection,we10.1\ncandesignawidevarietyofrecurrentneuralnetworks. UUV V\nWWo( t − 1 )o( t − 1 )\nhhooy y\nLL\nx xo( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tWW WW WW WW\nh( ) . . .h( ) . . .h( ) . . .h( ) . . .V V V V V V\nUU UU UUU nf ol d\nFigure10.3:Thecomputationalgraphtocomputethetraininglossofarecurrentnetwork\nthatmapsaninputsequenceofxvaluestoacorrespondingsequenceofoutputovalues.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 932, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 656}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0933_37cfa219", "text": ". .h( ) . . .h( ) . . .V V V V V V\nUU UU UUU nf ol d\nFigure10.3:Thecomputationalgraphtocomputethetraininglossofarecurrentnetwork\nthatmapsaninputsequenceofxvaluestoacorrespondingsequenceofoutputovalues. Aloss Lmeasureshowfareachoisfromthecorrespondingtrainingtargety.Whenusing\nsoftmaxoutputs,weassumeoistheunnormalizedlogprobabilities.Theloss Linternally\ncomputesˆy=softmax(o) andcomparesthistothetargety.TheRNNhasinputtohidden\nconnectionsparametrizedbyaweightmatrixU,hidden-to-hiddenrecurrentconnections\nparametrizedbyaweightmatrixW,andhidden-to-outputconnectionsparametrizedby\naweightmatrixV.Equationdeﬁnesforwardpropagationinthismodel. 10.8 ( L e f t )The\nRNNanditslossdrawnwithrecurrentconnections. ( R i g h t )Thesameseenasantime-\nunfoldedcomputationalgraph,whereeachnodeisnowassociatedwithoneparticular\ntimeinstance.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 933, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 822}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0934_471880be", "text": "10.8 ( L e f t )The\nRNNanditslossdrawnwithrecurrentconnections. ( R i g h t )Thesameseenasantime-\nunfoldedcomputationalgraph,whereeachnodeisnowassociatedwithoneparticular\ntimeinstance. Someexamplesofimportantdesignpatternsforrecurrentneuralnetworks\nincludethefollowing:\n3 7 8\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n•Recurrentnetworksthatproduceanoutputateachtimestepandhave\nrecurrentconnectionsbetweenhiddenunits,illustratedinﬁgure.10.3\n•Recurrentnetworksthatproduceanoutputateachtimestepandhave\nrecurrentconnectionsonlyfromtheoutputatonetimesteptothehidden\nunitsatthenexttimestep,illustratedinﬁgure10.4\n•Recurrentnetworkswithrecurrentconnectionsbetweenhiddenunits,that\nreadanentiresequenceandthenproduceasingleoutput,illustratedin\nﬁgure.10.5\nﬁgureisareasonablyrepresentativeexamplethatwereturntothroughout 10.3\nmostofthechapter.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 934, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 843}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0935_ae67d832", "text": "Therecurrentneuralnetworkofﬁgureandequationisuniversalinthe 10.3 10.8\nsensethatanyfunctioncomputablebyaTuringmachinecanbecomputedbysuch\narecurrentnetworkofaﬁnitesize.TheoutputcanbereadfromtheRNNafter\nanumberoftimestepsthatisasymptoticallylinearinthenumberoftimesteps\nusedbytheTuringmachineandasymptoticallylinearinthelengthoftheinput\n(SiegelmannandSontag1991Siegelmann1995SiegelmannandSontag1995 ,;,; ,;\nHyotyniemi1996,).ThefunctionscomputablebyaTuringmachinearediscrete,\nsotheseresultsregardexactimplementation ofthefunction,notapproximations .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 935, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 545}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0936_09ea1050", "text": "TheRNN,whenusedasaTuringmachine,takesabinarysequenceasinputandits\noutputsmustbediscretizedtoprovideabinaryoutput.Itispossibletocomputeall\nfunctionsinthissettingusingasinglespeciﬁcRNNofﬁnitesize(Siegelmannand\nSontag1995()use886units).The“input”oftheTuringmachineisaspeciﬁcation\nofthefunctiontobecomputed,sothesamenetworkthatsimulatesthisTuring\nmachineissuﬃcientforallproblems.ThetheoreticalRNNusedfortheproof\ncansimulateanunboundedstackbyrepresentingitsactivationsandweightswith\nrationalnumbersofunboundedprecision. WenowdeveloptheforwardpropagationequationsfortheRNNdepictedin\nﬁgure.Theﬁguredoesnotspecifythechoiceofactivationfunctionforthe 10.3\nhiddenunits.Hereweassumethehyperbolictangentactivationfunction.Also,\ntheﬁguredoesnotspecifyexactlywhatformtheoutputandlossfunctiontake.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 936, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 781}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0937_49c95718", "text": "Hereweassumethattheoutputisdiscrete,asiftheRNNisusedtopredictwords\norcharacters.Anaturalwaytorepresentdiscretevariablesistoregardtheoutput\noasgivingtheunnormalized logprobabilitiesofeachpossiblevalueofthediscrete\nvariable.Wecanthenapplythesoftmaxoperationasapost-processingstepto\nobtainavectorˆyofnormalizedprobabilitiesovertheoutput.Forwardpropagation\nbeginswithaspeciﬁcationoftheinitialstateh( 0 ).Then,foreachtimestepfrom\n3 7 9\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nUV\nWo( t − 1 )o( t − 1 )\nhhooy y\nLL\nx xo( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tW W W Wo( ) . . .o( ) . . . h( ) . . .h( ) . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 937, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 809}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0938_af4f395f", "text": ". .o( ) . . . h( ) . . .h( ) . . .V V V\nU U UU nf ol d\nFigure10.4:AnRNNwhoseonlyrecurrenceisthefeedbackconnectionfromtheoutput\ntothehiddenlayer.Ateachtimestep t,theinputisxt,thehiddenlayeractivationsare\nh( ) t,theoutputsareo( ) t,thetargetsarey( ) tandthelossis L( ) t. ( L e f t )Circuitdiagram. ( R i g h t )Unfoldedcomputationalgraph.SuchanRNNislesspowerful(canexpressa\nsmallersetoffunctions)thanthoseinthefamilyrepresentedbyﬁgure.TheRNN 10.3\ninﬁgurecanchoosetoputanyinformationitwantsaboutthepastintoitshidden 10.3\nrepresentationhandtransmithtothefuture.TheRNNinthisﬁgureistrainedto\nputaspeciﬁcoutputvalueintoo,andoistheonlyinformationitisallowedtosend\ntothefuture.Therearenodirectconnectionsfromhgoingforward.Theprevioush\nisconnectedtothepresentonlyindirectly,viathepredictionsitwasusedtoproduce.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 938, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 801}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0939_16cc42aa", "text": "Unlessoisveryhigh-dimensionalandrich,itwillusuallylackimportantinformation\nfromthepast.ThismakestheRNNinthisﬁgurelesspowerful,butitmaybeeasierto\ntrainbecauseeachtimestepcanbetrainedinisolationfromtheothers,allowinggreater\nparallelizationduringtraining,asdescribedinsection.10.2.1\n3 8 0\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nt t τ = 1to= ,weapplythefollowingupdateequations:\na( ) t= +bWh( 1 ) t −+Ux( ) t(10.8)\nh( ) t=tanh(a( ) t) (10.9)\no( ) t= +cVh( ) t(10.10)\nˆy( ) t=softmax(o( ) t) (10.11)\nwheretheparametersarethebiasvectorsbandcalongwiththeweightmatrices\nU,VandW,respectivelyforinput-to-hidden, hidden-to-output andhidden-to-\nhiddenconnections.Thisisanexampleofarecurrentnetworkthatmapsan\ninputsequencetoanoutputsequenceofthesamelength.Thetotallossfora\ngivensequenceofvaluespairedwithasequenceofvalueswouldthenbejust x y\nthesumofthelossesoverallthetimesteps.Forexample,if L( ) tisthenegative\nlog-likelihoodof y( ) tgivenx( 1 ), . . . ,x( ) t,then\nL\n{x( 1 ), . . . ,x( ) τ}{ ,y( 1 ), .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 939, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1006}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0940_f5de303f", "text": ". . ,x( ) t,then\nL\n{x( 1 ), . . . ,x( ) τ}{ ,y( 1 ), . . . ,y( ) τ}\n(10.12)\n=\ntL( ) t(10.13)\n=−\ntlog p m o de l\ny( ) t|{x( 1 ), . . . ,x( ) t}\n, (10.14)\nwhere p m o de l\ny( ) t|{x( 1 ), . . . ,x( ) t}\nisgivenbyreadingtheentryfor y( ) tfromthe\nmodel’soutputvectorˆy( ) t.Computingthegradientofthislossfunctionwithrespect\ntotheparametersisanexpensiveoperation.Thegradientcomputationinvolves\nperformingaforwardpropagationpassmovinglefttorightthroughourillustration\noftheunrolledgraphinﬁgure,followedbyabackwardpropagationpass 10.3\nmovingrighttoleftthroughthegraph.Theruntimeis O( τ) andcannotbereduced\nbyparallelization becausetheforwardpropagationgraphisinherentlysequential;\neachtimestepmayonlybecomputedafterthepreviousone.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 940, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 731}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0941_605a12fb", "text": "Statescomputed\nintheforwardpassmustbestoreduntiltheyarereusedduringthebackward\npass,sothememorycostisalso O( τ).Theback-propagation algorithmapplied\ntotheunrolledgraphwith O( τ)costiscalledback-propagationthroughtime\norBPTTandisdiscussedfurtherinsection.Thenetworkwithrecurrence 10.2.2\nbetweenhiddenunitsisthusverypowerfulbutalsoexpensivetotrain.Istherean\nalternative?", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 941, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 368}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0942_c6fbcfdb", "text": "10.2.1TeacherForcingandNetworkswithOutputRecurrence\nThenetworkwithrecurrentconnectionsonlyfromtheoutputatonetimestepto\nthehiddenunitsatthenexttimestep(showninﬁgure)isstrictlylesspowerful 10.4\n3 8 1\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nbecauseitlackshidden-to-hidden recurrentconnections.Forexample,itcannot\nsimulateauniversalTuringmachine.Becausethisnetworklackshidden-to-hidden\nrecurrence,itrequiresthattheoutputunitscapturealloftheinformationabout\nthepastthatthenetworkwillusetopredictthefuture.Becausetheoutputunits\nareexplicitlytrainedtomatchthetrainingsettargets,theyareunlikelytocapture\nthenecessaryinformationaboutthepasthistoryoftheinput,unlesstheuser\nknowshowtodescribethefullstateofthesystemandprovidesitaspartofthe\ntrainingsettargets.Theadvantageofeliminatinghidden-to-hidden recurrence\nisthat,foranylossfunctionbasedoncomparingthepredictionattime ttothe\ntrainingtargetattime t,allthetimestepsaredecoupled.Trainingcanthusbe\nparallelized,withthegradientforeachstep tcomputedinisolation.Thereisno\nneedtocomputetheoutputfortheprevioustimestepﬁrst,becausethetraining\nsetprovidestheidealvalueofthatoutput.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 942, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1127}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0943_873efdd8", "text": "h( t − 1 )h( t − 1 )\nWh( ) th( ) t . . . . . . x( t − 1 )x( t − 1 )x( ) tx( ) tx( ) . . .x( ) . . .W W\nU U Uh( ) τh( ) τ\nx( ) τx( ) τW\nUo( ) τo( ) τy( ) τy( ) τL( ) τL( ) τ\nV\n. . . . . . Figure10.5:Time-unfoldedrecurrentneuralnetworkwithasingleoutputattheend\nofthesequence.Suchanetworkcanbeusedtosummarizeasequenceandproducea\nﬁxed-sizerepresentationusedasinputforfurtherprocessing. Theremightbeatarget\nrightattheend(asdepictedhere)orthegradientontheoutputo( ) tcanbeobtainedby\nback-propagatingfromfurtherdownstreammodules. Modelsthathaverecurrentconnectionsfromtheiroutputsleadingbackinto\nthemodelmaybetrainedwithteacherforcing.Teacherforcingisaprocedure\nthatemergesfromthemaximumlikelihoodcriterion,inwhichduringtrainingthe\nmodelreceivesthegroundtruthoutput y( ) tasinputattime t+1.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 943, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 783}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0944_7c55ac9d", "text": "Wecansee\nthisbyexaminingasequencewithtwotimesteps.Theconditionalmaximum\n3 8 2\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\no( t − 1 )o( t − 1 )o( ) to( ) t\nh( t − 1 )h( t − 1 )h( ) th( ) t\nx( t − 1 )x( t − 1 )x( ) tx( ) tW\nV V\nU Uo( t − 1 )o( t − 1 )o( ) to( ) tL( t − 1 )L( t − 1 )L( ) tL( ) ty( t − 1 )y( t − 1 )y( ) ty( ) t\nh( t − 1 )h( t − 1 )h( ) th( ) t\nx( t − 1 )x( t − 1 )x( ) tx( ) tW\nV V\nU U\nT r ai n  t i m e T e s t   t i m e\nFigure10.6:Illustrationofteacherforcing.Teacherforcingisatrainingtechniquethatis\napplicabletoRNNsthathaveconnectionsfromtheiroutputtotheirhiddenstatesatthe\nnexttimestep. ( L e f t )Attraintime,wefeedthe c o r r e c t o u t p u ty( ) tdrawnfromthetrain\nsetasinputtoh( + 1 ) t.Whenthemodelisdeployed,thetrueoutputisgenerally ( R i g h t )\nnotknown.Inthiscase,weapproximatethecorrectoutputy( ) twiththemodel’soutput\no( ) t,andfeedtheoutputbackintothemodel.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 944, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 899}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0945_26bc92cf", "text": "3 8 3\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nlikelihoodcriterionis\nlog p\ny( 1 ),y( 2 )|x( 1 ),x( 2 )\n(10.15)\n=log p\ny( 2 )|y( 1 ),x( 1 ),x( 2 )\n+log p\ny( 1 )|x( 1 ),x( 2 )\n(10.16)\nInthisexample,weseethatattime t= 2,themodelistrainedtomaximizethe\nconditionalprobabilityofy( 2 )given b o t hthexsequencesofarandthepreviousy\nvaluefromthetrainingset.Maximumlikelihoodthusspeciﬁesthatduringtraining,\nratherthanfeedingthemodel’sownoutputbackintoitself,theseconnections\nshouldbefedwiththetargetvaluesspecifyingwhatthecorrectoutputshouldbe.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 945, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 552}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0946_f636416e", "text": "Thisisillustratedinﬁgure.10.6\nWeoriginallymotivatedteacherforcingasallowingustoavoidback-propagation\nthroughtimeinmodelsthatlackhidden-to-hidden connections.Teacherforcing\nmaystillbeappliedtomodelsthathavehidden-to-hidden connectionssolongas\ntheyhaveconnectionsfromtheoutputatonetimesteptovaluescomputedinthe\nnexttimestep.However,assoonasthehiddenunitsbecomeafunctionofearlier\ntimesteps,theBPTTalgorithmisnecessary.Somemodelsmaythusbetrained\nwithbothteacherforcingandBPTT. Thedisadvantageofstrictteacherforcingarisesifthenetworkisgoingtobe\nlaterusedinanopen-loopmode,withthenetworkoutputs(orsamplesfrom\ntheoutputdistribution)fedbackasinput. Inthiscase,thekindofinputsthat\nthenetworkseesduringtrainingcouldbequitediﬀerentfromthekindofinputs\nthatitwillseeattesttime.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 946, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 764}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0947_025ade91", "text": "Inthiscase,thekindofinputsthat\nthenetworkseesduringtrainingcouldbequitediﬀerentfromthekindofinputs\nthatitwillseeattesttime. Onewaytomitigatethisproblemistotrainwith\nbothteacher-forcedinputsandwithfree-runninginputs,forexamplebypredicting\nthecorrecttargetanumberofstepsinthefuturethroughtheunfoldedrecurrent\noutput-to-input paths.Inthisway,thenetworkcanlearntotakeintoaccount\ninputconditions(suchasthoseitgeneratesitselfinthefree-runningmode)not\nseenduringtrainingandhowtomapthestatebacktowardsonethatwillmake\nthenetworkgenerateproperoutputsafterafewsteps.Anotherapproach(Bengio\ne t a l .,)tomitigatethegapbetweentheinputsseenattraintimeandthe 2015b\ninputsseenattesttimerandomlychoosestousegeneratedvaluesoractualdata\nvaluesasinput.Thisapproachexploitsacurriculumlearningstrategytogradually\nusemoreofthegeneratedvaluesasinput. 10.2.2ComputingtheGradientinaRecurrentNeuralNetwork\nComputingthegradientthrougharecurrentneuralnetworkisstraightforward.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 947, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 946}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0948_353c4a34", "text": "10.2.2ComputingtheGradientinaRecurrentNeuralNetwork\nComputingthegradientthrougharecurrentneuralnetworkisstraightforward. Onesimplyappliesthegeneralizedback-propagationalgorithmofsection6.5.6\n3 8 4\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ntotheunrolledcomputational graph.Nospecializedalgorithmsarenecessary. Gradientsobtainedbyback-propagation maythenbeusedwithanygeneral-purpose\ngradient-basedtechniquestotrainanRNN. TogainsomeintuitionforhowtheBPTTalgorithmbehaves,weprovidean\nexampleofhowtocomputegradientsbyBPTTfortheRNNequationsabove\n(equationandequation).Thenodesofourcomputational graphinclude 10.8 10.12\ntheparametersU,V,W,bandcaswellasthesequenceofnodesindexedby\ntforx( ) t,h( ) t,o( ) tand L( ) t. Foreachnode Nweneedtocomputethegradient\n∇ N Lrecursively,basedonthegradientcomputedatnodesthatfollowitinthe\ngraph.Westarttherecursionwiththenodesimmediatelyprecedingtheﬁnalloss\n∂ L\n∂ L( ) t= 1 .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 948, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 914}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0949_4bc6273f", "text": "Foreachnode Nweneedtocomputethegradient\n∇ N Lrecursively,basedonthegradientcomputedatnodesthatfollowitinthe\ngraph.Westarttherecursionwiththenodesimmediatelyprecedingtheﬁnalloss\n∂ L\n∂ L( ) t= 1 . (10.17)\nInthisderivationweassumethattheoutputso( ) tareusedastheargumenttothe\nsoftmaxfunctiontoobtainthevectorˆyofprobabilitiesovertheoutput.Wealso\nassumethatthelossisthenegativelog-likelihoodofthetruetarget y( ) tgiventhe\ninputsofar.Thegradient∇o( ) t Lontheoutputsattimestep t,forall i , t,isas\nfollows:\n(∇o( ) t L)i=∂ L\n∂ o( ) t\ni=∂ L\n∂ L( ) t∂ L( ) t\n∂ o( ) t\ni=ˆ y( ) t\ni− 1i , y( ) t .(10.18)\nWeworkourwaybackwards,startingfromtheendofthesequence.Attheﬁnal\ntimestep, τh( ) τonlyhaso( ) τasadescendent,soitsgradientissimple:\n∇h( ) τ L= V∇o( ) τ L.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 949, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 748}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0950_fbd8d2c8", "text": "(10.19)\nWecantheniteratebackwardsintimetoback-propagate gradientsthroughtime,\nfrom t= τ−1downto t= 1,notingthath( ) t(for t < τ)hasasdescendentsboth\no( ) tandh( + 1 ) t.Itsgradientisthusgivenby\n∇h( ) t L=\n∂h( + 1 ) t\n∂h( ) t\n(∇h( +1) t L)+\n∂o( ) t\n∂h( ) t\n(∇o( ) t L) (10.20)\n= W(∇h( +1) t L)diag\n1−\nh( + 1 ) t2\n+V(∇o( ) t L)(10.21)\nwhere diag\n1−\nh( + 1 ) t2\nindicatesthediagonalmatrixcontainingtheelements\n1−( h( + 1 ) t\ni)2.ThisistheJacobianofthehyperbolictangentassociatedwiththe\nhiddenunitattime.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 950, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 516}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0951_064e173e", "text": "i t+1\n3 8 5\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nOncethegradientsonthe internalnodesofthe computational graphare\nobtained, wecanobtainthegradientsontheparameternodes.Becausethe\nparametersaresharedacrossmanytimesteps,wemusttakesomecarewhen\ndenotingcalculusoperationsinvolvingthesevariables.Theequationswewishto\nimplementusethebpropmethodofsection,thatcomputesthecontribution 6.5.6\nofasingleedgeinthecomputational graphtothegradient.However,the∇ W f\noperatorusedincalculustakesintoaccountthecontributionofWtothevalue\nof fduetoedgesinthecomputational graph.Toresolvethisambiguity,we a l l\nintroducedummyvariablesW( ) tthataredeﬁnedtobecopiesofWbutwitheach\nW( ) tusedonlyattimestep t.Wemaythenuse∇W( ) ttodenotethecontribution\noftheweightsattimesteptothegradient.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 951, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 775}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0952_d76a023d", "text": "t\nUsingthisnotation,thegradientontheremainingparametersisgivenby:\n∇ c L=\nt\n∂o( ) t\n∂c\n∇o( ) t L=\nt∇o( ) t L (10.22)\n∇ b L=\nt\n∂h( ) t\n∂b( ) t\n∇h( ) t L=\ntdiag\n1−\nh( ) t2\n∇h( ) t L(10.23)\n∇ V L=\nt\ni\n∂ L\n∂ o( ) t\ni\n∇ V o( ) t\ni=\nt(∇o( ) t L)h( ) t(10.24)\n∇ W L=\nt\ni\n∂ L\n∂ h( ) t\ni\n∇W( ) t h( ) t\ni (10.25)\n=\ntdiag\n1−\nh( ) t2\n(∇h( ) t L)h( 1 ) t −(10.26)\n∇ U L=\nt\ni\n∂ L\n∂ h( ) t\ni\n∇U( ) t h( ) t\ni (10.27)\n=\ntdiag\n1−\nh( ) t2\n(∇h( ) t L)x( ) t(10.28)\nWedonotneedtocomputethegradientwithrespecttox( ) tfortrainingbecause\nitdoesnothaveanyparametersasancestorsinthecomputational graphdeﬁning\ntheloss. 3 8 6\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.2.3RecurrentNetworksasDirectedGraphicalModels\nIntheexamplerecurrentnetworkwehavedevelopedsofar,thelosses L( ) twere\ncross-entropiesbetweentrainingtargetsy( ) tandoutputso( ) t.Aswithafeedforward\nnetwork,itisinprinciplepossibletousealmostanylosswitharecurrentnetwork.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 952, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 961}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0953_620ba0a5", "text": "Thelossshouldbechosenbasedonthetask.Aswithafeedforwardnetwork,we\nusuallywishtointerprettheoutputoftheRNNasaprobabilitydistribution,and\nweusuallyusethecross-entropyassociatedwiththatdistributiontodeﬁnetheloss. Meansquarederroristhecross-entropylossassociatedwithanoutputdistribution\nthatisaunitGaussian,forexample,justaswithafeedforwardnetwork. When we use apredictivelog-likelihood trainingobjective,such asequa-\ntion,wetraintheRNNtoestimatetheconditionaldistributionofthenext 10.12\nsequenceelementy( ) tgiventhepastinputs.Thismaymeanthatwemaximize\nthelog-likelihood\nlog( py( ) t|x( 1 ), . . . ,x( ) t) , (10.29)\nor,ifthemodelincludesconnectionsfromtheoutputatonetimesteptothenext\ntimestep,\nlog( py( ) t|x( 1 ), . . . ,x( ) t,y( 1 ), . . . ,y( 1 ) t −) .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 953, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 754}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0954_ae95689d", "text": ". . ,x( ) t) , (10.29)\nor,ifthemodelincludesconnectionsfromtheoutputatonetimesteptothenext\ntimestep,\nlog( py( ) t|x( 1 ), . . . ,x( ) t,y( 1 ), . . . ,y( 1 ) t −) . (10.30)\nDecomposingthejointprobabilityoverthesequenceofyvaluesasaseriesof\none-stepprobabilisticpredictionsisonewaytocapturethefulljointdistribution\nacrossthewholesequence.Whenwedonotfeedpastyvaluesasinputsthat\nconditionthenextstepprediction,thedirectedgraphicalmodelcontainsnoedges\nfromanyy( ) iinthepasttothecurrenty( ) t.Inthiscase,theoutputsyare\nconditionallyindependentgiventhesequenceofxvalues.Whenwedofeedthe\nactualyvalues(nottheirprediction,buttheactualobservedorgeneratedvalues)\nbackintothenetwork,thedirectedgraphicalmodelcontainsedgesfromally( ) i\nvaluesinthepasttothecurrent y( ) tvalue. Asasimpleexample,letusconsiderthecasewheretheRNNmodelsonlya\nsequenceofscalarrandomvariables Y={y( 1 ), . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 954, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 872}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0955_fd2560f8", "text": "Asasimpleexample,letusconsiderthecasewheretheRNNmodelsonlya\nsequenceofscalarrandomvariables Y={y( 1 ), . . . ,y( ) τ},withnoadditionalinputs\nx.Theinputattimestep tissimplytheoutputattimestep t−1.TheRNNthen\ndeﬁnesadirectedgraphicalmodelovertheyvariables.Weparametrizethejoint\ndistributionoftheseobservationsusingthechainrule(equation)forconditional3.6\nprobabilities:\nP P () = Y ( y( 1 ), . . . , y( ) τ) =τ\nt = 1P( y( ) t| y( 1 ) t −, y( 2 ) t −, . . . , y( 1 ))(10.31)\nwheretheright-handsideofthebarisemptyfor t=1,ofcourse.Hencethe\nnegativelog-likelihoodofasetofvalues { y( 1 ), . . . , y( ) τ}accordingtosuchamodel\n3 8 7\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ny( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) . . .y( ) . . . Figure10.7:Fullyconnectedgraphicalmodelforasequence y( 1 ), y( 2 ), . . . , y( ) t, . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 955, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 844}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0956_89e6cc1f", "text": ". .y( ) . . . Figure10.7:Fullyconnectedgraphicalmodelforasequence y( 1 ), y( 2 ), . . . , y( ) t, . . .:every\npastobservation y( ) imayinﬂuencetheconditionaldistributionofsome y( ) t(for t > i),\ngiventhepreviousvalues.Parametrizingthegraphicalmodeldirectlyaccordingtothis\ngraph(asinequation)mightbeveryineﬃcient,withanevergrowingnumberof 10.6\ninputsandparametersforeachelementofthesequence.RNNsobtainthesamefull\nconnectivitybuteﬃcientparametrization,asillustratedinﬁgure.10.8\nis\nL=\ntL( ) t(10.32)\nwhere\nL( ) t= log( − Py( ) t= y( ) t| y( 1 ) t −, y( 2 ) t −, . . . , y( 1 )) .(10.33)\ny( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) . . .y( ) . . .h( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )h( 4 )h( 4 )h( 5 )h( 5 )h( ) . . .h( ) . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 956, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 745}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0957_1362ef80", "text": ". . , y( 1 )) .(10.33)\ny( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) . . .y( ) . . .h( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )h( 4 )h( 4 )h( 5 )h( 5 )h( ) . . .h( ) . . . Figure10.8:IntroducingthestatevariableinthegraphicalmodeloftheRNN,even\nthoughitisadeterministicfunctionofitsinputs,helpstoseehowwecanobtainavery\neﬃcientparametrization,basedonequation.Everystageinthesequence(for 10.5 h( ) t\nandy( ) t)involvesthesamestructure(thesamenumberofinputsforeachnode)andcan\nsharethesameparameterswiththeotherstages. Theedgesinagraphicalmodelindicatewhichvariablesdependdirectlyonother\nvariables.Manygraphicalmodelsaimtoachievestatisticalandcomputational\neﬃciencybyomittingedgesthatdonotcorrespondtostronginteractions.For\n3 8 8\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nexample,itiscommontomaketheMarkovassumptionthatthegraphicalmodel\nshouldonlycontainedgesfrom{y( ) t k −, . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 957, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 898}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0958_ffd1ad45", "text": ". . ,y( 1 ) t −}toy( ) t,ratherthancontaining\nedgesfromtheentirepasthistory.However,insomecases,webelievethatallpast\ninputsshouldhaveaninﬂuenceonthenextelementofthesequence.RNNsare\nusefulwhenwebelievethatthedistributionovery( ) tmaydependonavalueofy( ) i\nfromthedistantpastinawaythatisnotcapturedbytheeﬀectofy( ) iony( 1 ) t −. OnewaytointerpretanRNNasagraphicalmodelistoviewtheRNNas\ndeﬁningagraphicalmodelwhosestructureisthecompletegraph,abletorepresent\ndirectdependenciesbetweenanypairofyvalues.Thegraphicalmodeloverthey\nvalueswiththecompletegraphstructureisshowninﬁgure.Thecomplete10.7\ngraphinterpretationoftheRNNisbasedonignoringthehiddenunitsh( ) tby\nmarginalizing themoutofthemodel.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 958, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 688}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0959_9746b84b", "text": "ItismoreinterestingtoconsiderthegraphicalmodelstructureofRNNsthat\nresultsfromregardingthehiddenunitsh( ) tasrandomvariables.1Includingthe\nhiddenunitsinthegraphicalmodelrevealsthattheRNNprovidesaveryeﬃcient\nparametrization ofthejointdistributionovertheobservations.Supposethatwe\nrepresentedanarbitraryjointdistributionoverdiscretevalueswithatabular\nrepresentation—anarraycontainingaseparateentryforeachpossibleassignment\nofvalues,withthevalueofthatentrygivingtheprobabilityofthatassignment\noccurring.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 959, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 499}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0960_f565c8d7", "text": "If ycantakeon kdiﬀerentvalues,thetabularrepresentationwould\nhave O( kτ)parameters.Bycomparison,duetoparametersharing,thenumberof\nparametersintheRNNis O(1)asafunctionofsequencelength.Thenumberof\nparametersintheRNNmaybeadjustedtocontrolmodelcapacitybutisnotforced\ntoscalewithsequencelength.EquationshowsthattheRNNparametrizes 10.5\nlong-termrelationshipsbetweenvariableseﬃciently,usingrecurrentapplications\nofthesamefunction fandsameparametersθateachtimestep.Figure10.8\nillustratesthegraphicalmodelinterpretation.Incorporating theh( ) tnodesin\nthegraphicalmodeldecouplesthepastandthefuture,actingasanintermediate\nquantitybetweenthem.Avariable y( ) iinthedistantpastmayinﬂuenceavariable\ny( ) tviaitseﬀectonh.Thestructureofthisgraphshowsthatthemodelcanbe\neﬃcientlyparametrized byusingthesameconditionalprobabilitydistributionsat\neachtimestep,andthatwhenthevariablesareallobserved,theprobabilityofthe\njointassignmentofallvariablescanbeevaluatedeﬃciently.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 960, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 948}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0961_eda9a9f4", "text": "Evenwiththeeﬃcientparametrization ofthegraphicalmodel,someoperations\nremaincomputationally challenging.Forexample,itisdiﬃculttopredictmissing\n1Th e c o n d i t i o n a l d i s t rib u t i o n o v e r t h e s e v a ria b l e s g i v e n t h e i r p a re n t s i s d e t e rm i n i s t i c . Th i s i s\np e rfe c t l y l e g i t i m a t e , t h o u g h i t i s s o m e wh a t ra re t o d e s i g n a g ra p h i c a l m o d e l with s u c h d e t e rm i n i s t i c\nh i d d e n u n i t s . 3 8 9\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nvaluesinthemiddleofthesequence. Thepricerecurrentnetworkspayfortheirreducednumberofparametersis\nthat theparametersmaybediﬃcult.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 961, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 673}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0962_6744e92e", "text": "3 8 9\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nvaluesinthemiddleofthesequence. Thepricerecurrentnetworkspayfortheirreducednumberofparametersis\nthat theparametersmaybediﬃcult. o p t i m i z i ng\nTheparametersharingusedinrecurrentnetworksreliesontheassumption\nthatthesameparameterscanbeusedfordiﬀerenttimesteps.Equivalently,the\nassumptionisthattheconditionalprobabilitydistributionoverthevariablesat\ntime t+1 giventhevariablesattime tisstationary,meaningthattherelationship\nbetweentheprevioustimestepandthenexttimestepdoesnotdependon t.In\nprinciple,itwouldbepossibletouse tasanextrainputateachtimestepandlet\nthelearnerdiscoveranytime-dependencewhilesharingasmuchasitcanbetween\ndiﬀerenttimesteps.Thiswouldalreadybemuchbetterthanusingadiﬀerent\nconditionalprobabilitydistributionforeach t,butthenetworkwouldthenhaveto\nextrapolatewhenfacedwithnewvaluesof.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 962, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 861}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0963_1ef33a8b", "text": "t\nTocompleteourviewofanRNNasagraphicalmodel,wemustdescribehow\ntodrawsamplesfromthemodel.Themainoperationthatweneedtoperformis\nsimplytosamplefromtheconditionaldistributionateachtimestep. However,\nthereisoneadditionalcomplication. The RNNmusthavesomemechanismfor\ndeterminingthelengthofthesequence.Thiscanbeachievedinvariousways. Inthecasewhentheoutputisasymboltakenfromavocabulary,onecan\naddaspecialsymbolcorrespondingtotheendofasequence(Schmidhuber2012,). Whenthatsymbolisgenerated,thesamplingprocessstops.Inthetrainingset,\nweinsertthissymbolasanextramemberofthesequence,immediatelyafterx( ) τ\nineachtrainingexample.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 963, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 615}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0964_a040f48d", "text": "Whenthatsymbolisgenerated,thesamplingprocessstops.Inthetrainingset,\nweinsertthissymbolasanextramemberofthesequence,immediatelyafterx( ) τ\nineachtrainingexample. AnotheroptionistointroduceanextraBernoullioutputtothemodelthat\nrepresentsthedecisiontoeithercontinuegenerationorhaltgenerationateach\ntimestep.Thisapproachismoregeneralthantheapproachofaddinganextra\nsymboltothevocabulary,becauseitmaybeappliedtoanyRNN,ratherthan\nonlyRNNsthatoutputasequenceofsymbols.Forexample,itmaybeappliedto\nanRNNthatemitsasequenceofrealnumbers.Thenewoutputunitisusuallya\nsigmoidunittrainedwiththecross-entropyloss.Inthisapproachthesigmoidis\ntrainedtomaximizethelog-probabilit yofthecorrectpredictionastowhetherthe\nsequenceendsorcontinuesateachtimestep.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 964, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 732}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0965_331e1557", "text": "Anotherwaytodeterminethesequencelength τistoaddanextraoutputto\nthemodelthatpredictstheinteger τitself.Themodelcansampleavalueof τ\nandthensample τstepsworthofdata.Thisapproachrequiresaddinganextra\ninputtotherecurrentupdateateachtimestepsothattherecurrentupdateis\nawareofwhetheritisneartheendofthegeneratedsequence.Thisextrainput\ncaneitherconsistofthevalueof τorcanconsistof τ t−,thenumberofremaining\n3 9 0\nCHAPTER10. SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ntime steps. Without this extra input, the RNN might generate sequences that\nendabruptly,suchasasentencethatendsbeforeitiscomplete. Thisapproachis\nbasedonthedecomposition\nP(x(1), . . . ,x( ) τ) = ( ) ( P τ Px(1), . . . ,x( ) τ| τ .)(10.34)\nThe strategy of predicting τdirectly is used for example by Goodfellow et al. ( ).2014d\n10.2.4 Modeling Sequences Conditioned on Context with RNNs\nIntheprevioussectionwedescribedhowanRNNcouldcorrespondtoadirected\ngraphicalmodelover asequence ofrandom variables y( ) twithno inputsx.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 965, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 980}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0966_60d97629", "text": "Of\ncourse, our development of RNNs as in equation included a sequence of 10.8\ninputsx(1),x(2), . . . ,x( ) τ. Ingeneral,RNNsallowtheextensionofthegraphical\nmodel view to represent not only a joint distribution over the yvariables but\nalso a conditional distribution over ygivenx. As discussed in the context of\nfeedforwardnetworksinsection ,anymodelrepresentingavariable 6.2.1.1 P(y;θ)\ncan be reinterpreted as amodel representinga conditional distribution P(yω|)\nwithω=θ. Wecanextendsuchamodeltorepresentadistribution P(y x|)by\nusingthesame P(y ω|)asbefore,butmaking ωafunctionofx. Inthecaseof\nanRNN,thiscanbeachievedindiﬀerentways. Wereviewherethemostcommon\nandobviouschoices. Previously, we havediscussed RNNsthattakeasequenceof vectors x( ) tfor\nt= 1 , . . . , τasinput. Anotheroption istotakeonly asinglevector xasinput. Whenxisaﬁxed-sizevector,wecansimplymakeitanextrainputoftheRNN\nthatgeneratesthe ysequence. Somecommonwaysofprovidinganextrainputto\nanRNNare:\n1. asanextrainputateachtimestep,or\n2.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 966, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1002}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0967_93c5b040", "text": "Whenxisaﬁxed-sizevector,wecansimplymakeitanextrainputoftheRNN\nthatgeneratesthe ysequence. Somecommonwaysofprovidinganextrainputto\nanRNNare:\n1. asanextrainputateachtimestep,or\n2. astheinitialstateh(0),or\n3. both. Theﬁrstandmostcommonapproachisillustratedinﬁgure . Theinteraction 10.9\nbetweentheinput xandeachhiddenunitvector h( ) tisparametrizedbyanewly\nintroducedweightmatrix Rthatwasabsentfromthemodelofonlythesequence\nof yvalues. Thesameproduct xRisaddedasadditionalinputtothehidden\nunitsateverytimestep. Wecanthinkofthechoiceof xasdeterminingthevalue\n391\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nofxRthatiseﬀectivelyanewbiasparameterusedforeachofthehiddenunits. Theweightsremainindependentoftheinput.Wecanthinkofthismodelastaking\ntheparametersθofthenon-conditional modelandturningthemintoω,where\nthebiasparameterswithinarenowafunctionoftheinput.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 967, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 862}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0968_4db62b05", "text": "Theweightsremainindependentoftheinput.Wecanthinkofthismodelastaking\ntheparametersθofthenon-conditional modelandturningthemintoω,where\nthebiasparameterswithinarenowafunctionoftheinput. ω\no( t − 1 )o( t − 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) tW W W W\ns( ) . . .s( ) . . .h( ) . . .h( ) . . .V V VU U U\nx xy( ) . . .y( ) . . . R R R R R\nFigure10.9:AnRNNthatmapsaﬁxed-lengthvectorxintoadistributionoversequences\nY.ThisRNNisappropriatefortaskssuchasimagecaptioning,whereasingleimageis\nusedasinputtoamodelthatthenproducesasequenceofwordsdescribingtheimage. Eachelementy( ) toftheobservedoutputsequenceservesbothasinput(forthecurrent\ntimestep)and,duringtraining,astarget(fortheprevioustimestep). Ratherthanreceivingonlyasinglevectorxasinput,theRNNmayreceive\nasequenceofvectorsx( ) tasinput.TheRNNdescribedinequationcorre-10.8\nspondstoaconditionaldistribution P(y( 1 ), . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 968, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1017}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0969_2647444f", "text": "Ratherthanreceivingonlyasinglevectorxasinput,theRNNmayreceive\nasequenceofvectorsx( ) tasinput.TheRNNdescribedinequationcorre-10.8\nspondstoaconditionaldistribution P(y( 1 ), . . . ,y( ) τ|x( 1 ), . . . ,x( ) τ)thatmakesa\nconditionalindependence assumptionthatthisdistributionfactorizesas\n\ntP(y( ) t|x( 1 ), . . . ,x( ) t) . (10.35)\nToremovetheconditionalindependenceassumption,wecanaddconnectionsfrom\ntheoutputattime ttothehiddenunitattime t+1,asshowninﬁgure.The10.10\nmodelcanthenrepresentarbitraryprobabilitydistributionsovertheysequence. Thiskindofmodelrepresentingadistributionoverasequencegivenanother\n3 9 2\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\no( t − 1 )o( t − 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) tW W W W\nh( ) . . .h( ) . . .h( ) . . .h( ) . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 969, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 918}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0970_ad28a40f", "text": ". .h( ) . . .h( ) . . .h( ) . . .V V V\nU U U\nx( t − 1 )x( t − 1 )R\nx( ) tx( ) tx( + 1 ) tx( + 1 ) tR R\nFigure10.10: Aconditionalrecurrentneuralnetworkmappingavariable-lengthsequence\nofxvaluesintoadistributionoversequencesofyvaluesofthesamelength.Comparedto\nﬁgure,thisRNNcontainsconnectionsfromthepreviousoutputtothecurrentstate.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 970, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 328}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0971_d8cc9771", "text": "10.3\nTheseconnectionsallowthisRNNtomodelanarbitrarydistributionoversequencesofy\ngivensequencesofxofthesamelength.TheRNNofﬁgureisonlyabletorepresent 10.3\ndistributionsinwhichtheyvaluesareconditionallyindependentfromeachothergiven\nthevalues.x\n3 9 3\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nsequencestillhasonerestriction,whichisthatthelengthofbothsequencesmust\nbethesame.Wedescribehowtoremovethisrestrictioninsection.10.4\no( t − 1 )o( t − 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tg( t − 1 )g( t − 1 )g( ) tg( ) tg( +1 ) tg( +1 ) t\nFigure10.11: Computation ofatypicalbidirectionalrecurrentneuralnetwork,meant\ntolearntomapinputsequencesxtotargetsequencesy,withloss L( ) tateachstep t.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 971, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 894}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0972_25e95f00", "text": "Thehrecurrencepropagatesinformationforwardintime(towardstheright)whilethe\ngrecurrencepropagatesinformationbackwardintime(towardstheleft).Thusateach\npoint t,theoutputunitso( ) tcanbeneﬁtfromarelevantsummaryofthepastinitsh( ) t\ninputandfromarelevantsummaryofthefutureinitsg( ) tinput. 10.3BidirectionalRNNs\nAlloftherecurrentnetworkswehaveconsidereduptonowhavea“causal”struc-\nture,meaningthatthestateattime tonlycapturesinformationfromthepast,\nx( 1 ), . . . ,x( 1 ) t −,andthepresentinputx( ) t.Someofthemodelswehavediscussed\nalsoallowinformationfrompastyvaluestoaﬀectthecurrentstatewhenthey\nvaluesareavailable.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 972, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 608}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0973_29455fd8", "text": ". . ,x( 1 ) t −,andthepresentinputx( ) t.Someofthemodelswehavediscussed\nalsoallowinformationfrompastyvaluestoaﬀectthecurrentstatewhenthey\nvaluesareavailable. However,inmanyapplicationswewanttooutputapredictionofy( ) twhichmay\n3 9 4\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ndependon t h e w h o l e i npu t s e q u e nc e.Forexample,inspeechrecognition,thecorrect\ninterpretationofthecurrentsoundasaphonememaydependonthenextfew\nphonemesbecauseofco-articulationandpotentiallymayevendependonthenext\nfewwordsbecauseofthelinguisticdependenciesbetweennearbywords:ifthere\naretwointerpretationsofthecurrentwordthatarebothacousticallyplausible,we\nmayhavetolookfarintothefuture(andthepast)todisambiguatethem.Thisis\nalsotrueofhandwritingrecognitionandmanyothersequence-to-sequencelearning\ntasks,describedinthenextsection.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 973, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 821}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0974_89db0314", "text": "Bidirectionalrecurrentneuralnetworks(orbidirectional RNNs)wereinvented\ntoaddressthatneed(SchusterandPaliwal1997,).Theyhavebeenextremelysuc-\ncessful(Graves2012,)inapplicationswherethatneedarises,suchashandwriting\nrecognition(Graves2008GravesandSchmidhuber2009 e t a l .,; ,),speechrecogni-\ntion(GravesandSchmidhuber2005Graves2013 Baldi ,; e t a l .,)andbioinformatics (\ne t a l .,).1999\nAsthenamesuggests,bidirectionalRNNscombineanRNNthatmovesforward\nthroughtimebeginningfromthestartofthesequencewithanotherRNNthat\nmovesbackwardthroughtimebeginningfromtheendofthesequence.Figure10.11\nillustratesthetypicalbidirectional RNN,withh( ) tstandingforthestateofthe\nsub-RNNthatmovesforwardthroughtimeandg( ) tstandingforthestateofthe\nsub-RNNthatmovesbackwardthroughtime.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 974, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 761}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0975_79b48780", "text": "Thisallowstheoutputunitso( ) t\ntocomputearepresentationthatdependson b o t h t h e p a s t a nd t h e f u t u r ebut\nismostsensitivetotheinputvaluesaroundtime t,withouthavingtospecifya\nﬁxed-sizewindowaround t(asonewouldhavetodowithafeedforwardnetwork,\naconvolutionalnetwork,oraregularRNNwithaﬁxed-sizelook-aheadbuﬀer). Thisideacanbenaturallyextendedto2-dimensionalinput,suchasimages,by\nhavingRNNs,eachonegoinginoneofthefourdirections: up, down,left, f o u r\nright.Ateachpoint ( i , j)ofa2-Dgrid,anoutput O i , jcouldthencomputea\nrepresentationthatwouldcapturemostlylocalinformationbutcouldalsodepend\non long-range inputs,ifthe RNN isable tolearn tocarry that information.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 975, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 671}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0976_1cfb7cc8", "text": "Comparedtoaconvolutionalnetwork,RNNsappliedtoimagesaretypicallymore\nexpensivebutallowforlong-rangelateralinteractionsbetweenfeaturesinthe\nsamefeaturemap(,; Visin e t a l .2015Kalchbrenner 2015 e t a l .,).Indeed,the\nforwardpropagationequationsforsuchRNNsmaybewritteninaformthatshows\ntheyuseaconvolutionthatcomputesthebottom-upinputtoeachlayer,prior\ntotherecurrentpropagationacrossthefeaturemapthatincorporatesthelateral\ninteractions. 3 9 5\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.4Encoder-DecoderSequence-to-SequenceArchitec-\ntures\nWehaveseeninﬁgurehowanRNNcanmapaninputsequencetoaﬁxed-size 10.5\nvector.WehaveseeninﬁgurehowanRNNcanmapaﬁxed-sizevectortoa 10.9\nsequence. Wehaveseeninﬁgures,,andhowanRNNcan 10.310.410.1010.11\nmapaninputsequencetoanoutputsequenceofthesamelength. E nc ode r\n…\nx( 1 )x( 1 )x( 2 )x( 2 )x( ) . . .x( ) . . .x( n x )x( n x )\nD e c ode r\n…\ny( 1 )y( 1 )y( 2 )y( 2 )y( ) . . .y( ) . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 976, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 922}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0977_9866bec8", "text": "E nc ode r\n…\nx( 1 )x( 1 )x( 2 )x( 2 )x( ) . . .x( ) . . .x( n x )x( n x )\nD e c ode r\n…\ny( 1 )y( 1 )y( 2 )y( 2 )y( ) . . .y( ) . . .y( n y )y( n y )CC\nFigure10.12: Exam pleofanencoder-decoderorsequence-to-sequenceRNNarchitecture,\nforlearningtogenerateanoutputsequence( y( 1 ), . . . , y( n y ))givenaninputsequence\n( x( 1 ), x( 2 ), . . . , x( n x )).ItiscomposedofanencoderRNNthatreadstheinputsequence\nandadecoderRNNthatgeneratestheoutputsequence(orcomputestheprobabilityofa\ngivenoutputsequence).TheﬁnalhiddenstateoftheencoderRNNisusedtocomputea\ngenerallyﬁxed-sizecontextvariable Cwhichrepresentsasemanticsummaryoftheinput\nsequenceandisgivenasinputtothedecoderRNN. HerewediscusshowanRNNcanbetrainedtomapaninputsequencetoan\noutputsequencewhichisnotnecessarilyofthesamelength.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 977, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 775}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0978_ad828f0a", "text": "HerewediscusshowanRNNcanbetrainedtomapaninputsequencetoan\noutputsequencewhichisnotnecessarilyofthesamelength. This comesupin\nmanyapplications,suchasspeechrecognition,machinetranslationorquestion\n3 9 6\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nanswering,wheretheinputandoutputsequencesinthetrainingsetaregenerally\nnotofthesamelength(althoughtheirlengthsmightberelated). WeoftencalltheinputtotheRNNthe“context.”Wewanttoproducea\nrepresentationofthiscontext, C.Thecontext Cmightbeavectororsequenceof\nvectorsthatsummarizetheinputsequenceXx= (( 1 ), . . . ,x( n x )). ThesimplestRNNarchitectureformappingavariable-length sequenceto\nanothervariable-length sequencewasﬁrstproposedby ()and Cho e t a l .2014a\nshortlyafterbySutskever2014 e t a l .(),whoindependentlydevelopedthatarchi-\ntectureandweretheﬁrsttoobtainstate-of-the-art translationusingthisapproach.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 978, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 862}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0979_7a430c60", "text": "Theformersystemisbasedonscoringproposalsgeneratedbyanothermachine\ntranslationsystem,whilethelatterusesastandalonerecurrentnetworktogenerate\nthetranslations. Theseauthorsrespectivelycalledthisarchitecture, illustrated\ninﬁgure,theencoder-decoderorsequence-to-sequencearchitecture.The 10.12\nideaisverysimple:(1)anencoderorreaderorinputRNNprocessestheinput\nsequence.Theencoderemitsthecontext C,usuallyasasimplefunctionofits\nﬁnalhiddenstate. (2)adecoderorwriteroroutputRNNisconditionedon\nthatﬁxed-lengthvector(justlikeinﬁgure)togeneratetheoutputsequence 10.9\nY=(y( 1 ), . . . ,y( n y )).Theinnovationofthiskindofarchitectureoverthose\npresentedinearliersectionsofthischapteristhatthelengths n xand n ycan\nvaryfromeachother,whilepreviousarchitectures constrained n x= n y= τ.Ina\nsequence-to-sequencearchitecture,thetwoRNNsaretrainedjointlytomaximize\ntheaverageoflog P(y( 1 ), . . . ,y( n y )|x( 1 ), . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 979, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 898}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0980_de946901", "text": ". . ,y( n y )|x( 1 ), . . . ,x( n x ))overallthepairsofxandy\nsequencesinthetrainingset.Thelaststateh n xoftheencoderRNNistypically\nusedasarepresentation Coftheinputsequencethatisprovidedasinputtothe\ndecoderRNN. Ifthecontext Cisavector,thenthedecoderRNNissimplyavector-to-\nsequenceRNNasdescribedinsection.Aswehaveseen,thereareatleast 10.2.4\ntwowaysforavector-to-sequenceRNNtoreceiveinput.Theinputcanbeprovided\nastheinitialstateoftheRNN,ortheinputcanbeconnectedtothehiddenunits\nateachtimestep.Thesetwowayscanalsobecombined. Thereisnoconstraintthattheencodermusthavethesamesizeofhiddenlayer\nasthedecoder.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 980, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 601}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0981_8951c5af", "text": "Thereisnoconstraintthattheencodermusthavethesamesizeofhiddenlayer\nasthedecoder. Oneclearlimitationofthisarchitectureiswhenthecontext Coutputbythe\nencoderRNNhasadimensionthatistoosmalltoproperlysummarizealong\nsequence.Thisphenomenon wasobservedby ()inthecontext Bahdanau e t a l .2015\nofmachinetranslation.Theyproposedtomake Cavariable-length sequencerather\nthanaﬁxed-sizevector.Additionally,theyintroducedanattentionmechanism\nthatlearnstoassociateelementsofthesequence Ctoelementsoftheoutput\n3 9 7\nCHAPTER10. SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nsequence. Seesection formoredetails. 12.4.5.1\n10.5 DeepRecurrentNetworks\nThecomputationinmostRNNscanbedecomposedintothreeblocksofparameters\nandassociatedtransformations:\n1. fromtheinputtothehiddenstate,\n2. fromtheprevioushiddenstatetothenexthiddenstate,and\n3. fromthehiddenstatetotheoutput. WiththeRNNarchitectureofﬁgure ,eachofthesethreeblocksisassociated 10.3\nwithasingleweightmatrix.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 981, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 938}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0982_b96ad2f8", "text": "fromtheprevioushiddenstatetothenexthiddenstate,and\n3. fromthehiddenstatetotheoutput. WiththeRNNarchitectureofﬁgure ,eachofthesethreeblocksisassociated 10.3\nwithasingleweightmatrix. Inotherwords,whenthenetworkisunfolded,each\nofthese correspondsto ashallowtransformation. By ashallowtransformation,\nwe mean a transformation that would be represented by a single layer within\na deepMLP. Typically thisis a transformationrepresentedby alearned aﬃnetransformationfollowedbyaﬁxednonlinearity. Would it be advantageous to introduce depth in each of these operations? Experimentalevidence(Graves 2013 Pascanu 2014a et al., ; et al., )stronglysuggests\nso. Theexperimentalevidenceisinagreementwiththeideathatweneedenough\ndepthinordertoperformtherequiredmappings. Seealso Schmidhuber 1992 ( ),\nElHihiandBengio 1996 Jaeger 2007a ( ),or ( )forearlierworkondeepRNNs. Graves 2013 et al.( )weretheﬁrsttoshowasigniﬁcantbeneﬁtofdecomposing\nthestateofanRNNintomultiplelayers asinﬁgure (left).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 982, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 973}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0983_e695108b", "text": "Graves 2013 et al.( )weretheﬁrsttoshowasigniﬁcantbeneﬁtofdecomposing\nthestateofanRNNintomultiplelayers asinﬁgure (left). Wecanthink 10.13\nof the lower layers in the hierarchy depicted in ﬁgure a as playing a role 10.13\nintransformingtherawinputintoarepresentationthatismoreappropriate,at\nthe higher levels of the hidden state. Pascanu 2014a et al.( ) go a step further\nandproposetohaveaseparateMLP(possiblydeep)foreachofthethreeblocks\nenumeratedabove,asillustratedinﬁgure b. Considerationsofrepresentational 10.13\ncapacitysuggesttoallocateenoughcapacityineachofthesethreesteps,butdoing\nsobyaddingdepthmayhurtlearningbymakingoptimizationdiﬃcult. Ingeneral,it is easier to optimize shallower architectures, and adding the extra depth of\nﬁgure bmakestheshortestpathfromavariableintimestep10.13 ttoavariable\nintimestep t+ 1becomelonger.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 983, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 832}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0984_76801618", "text": "Ingeneral,it is easier to optimize shallower architectures, and adding the extra depth of\nﬁgure bmakestheshortestpathfromavariableintimestep10.13 ttoavariable\nintimestep t+ 1becomelonger. Forexample,ifanMLPwithasinglehidden\nlayerisusedforthestate-to-statetransition,wehavedoubledthelengthofthe\nshortestpathbetweenvariablesinanytwodiﬀerenttimesteps,comparedwiththe\nordinaryRNNofﬁgure . However,asarguedby 10.3 Pascanu 2014a et al.( ),this\n398\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nhy\nxz\n( a) ( b) ( c )xhy\nxhy\nFigure10.13:Arecurrentneuralnetworkcanbemadedeepinmanyways(Pascanu\ne t a l .,).Thehiddenrecurrentstatecanbebrokendownintogroupsorganized 2014a ( a )\nhierarchically.Deepercomputation(e.g.,anMLP)canbeintroducedintheinput-to- ( b )\nhidden,hidden-to-hiddenandhidden-to-outputparts. Thismaylengthentheshortest\npathlinkingdiﬀerenttimesteps.Thepath-lengtheningeﬀectcanbemitigatedby ( c )\nintroducingskipconnections.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 984, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 932}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0985_fbc1c22b", "text": "Thismaylengthentheshortest\npathlinkingdiﬀerenttimesteps.Thepath-lengtheningeﬀectcanbemitigatedby ( c )\nintroducingskipconnections. 3 9 9\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ncanbemitigatedbyintroducingskipconnectionsinthehidden-to-hidden path,as\nillustratedinﬁgurec.10.13\n10.6RecursiveNeuralNetworks\nx( 1 )x( 1 )x( 2 )x( 2 )x( 3 )x( 3 )V V Vy yL L\nx( 4 )x( 4 )Voo\nU W U WUW\nFigure10.14:Arecursivenetworkhasacomputationalgraphthatgeneralizesthatofthe\nrecurrentnetworkfromachaintoatree.Avariable-sizesequencex( 1 ),x( 2 ), . . . ,x( ) tcan\nbemappedtoaﬁxed-sizerepresentation(theoutputo),withaﬁxedsetofparameters\n(theweightmatricesU,V,W).Theﬁgureillustratesasupervisedlearningcaseinwhich\nsometargetisprovidedwhichisassociatedwiththewholesequence.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 985, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 759}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0986_0e5a04cf", "text": "y\nRecursiveneuralnetworks2representyetanothergeneralization ofrecurrent\nnetworks,withadiﬀerentkindofcomputational graph,whichisstructuredasa\ndeeptree,ratherthanthechain-likestructureofRNNs.Thetypicalcomputational\ngraphforarecursivenetworkisillustratedinﬁgure.Recursiveneural 10.14\n2W e s u g g e s t t o n o t a b b re v i a t e “ re c u rs i v e n e u ra l n e t w o rk ” a s “ R NN” t o a v o i d c o n f u s i o n with\n“ re c u rre n t n e u ra l n e t w o rk . ”\n4 0 0\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nnetworkswereintroducedbyPollack1990()andtheirpotentialuseforlearningto\nreasonwasdescribedby().Recursivenetworkshavebeensuccessfully Bottou2011\nappliedtoprocessing d a t a s t r u c t u r e sasinputtoneuralnets(Frasconi1997 e t a l .,,\n1998 Socher2011ac2013a ),innaturallanguageprocessing( e t a l .,,,)aswellasin\ncomputervision( ,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 986, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 858}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0987_917242cf", "text": "Socher e t a l .2011b\nOneclearadvantageofrecursivenetsoverrecurrentnetsisthatforasequence\nofthesamelength τ,thedepth(measuredasthenumberofcompositionsof\nnonlinearoperations)canbedrasticallyreducedfrom τto O(log τ),whichmight\nhelpdealwithlong-termdependencies.Anopenquestionishowtobeststructure\nthetree.Oneoptionistohaveatreestructurewhichdoesnotdependonthedata,\nsuchasabalancedbinarytree.Insomeapplicationdomains,externalmethods\ncansuggesttheappropriatetreestructure.Forexample,whenprocessingnatural\nlanguagesentences,thetreestructurefortherecursivenetworkcanbeﬁxedto\nthestructureoftheparsetreeofthesentenceprovidedbyanaturallanguage\nparser( ,,). Ideally,onewouldlikethelearneritselfto Socher e t a l .2011a2013a\ndiscoverandinferthetreestructurethatisappropriateforanygiveninput,as\nsuggestedby().", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 987, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 796}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0988_3b3bccef", "text": "Ideally,onewouldlikethelearneritselfto Socher e t a l .2011a2013a\ndiscoverandinferthetreestructurethatisappropriateforanygiveninput,as\nsuggestedby(). Bottou2011\nManyvariantsoftherecursivenetideaarepossible.Forexample,Frasconi\ne t a l .()and1997Frasconi1998 e t a l .()associatethedatawithatreestructure,\nandassociatethe inputsandtargetswith individualnodesofthe tree.The\ncomputationperformedbyeachnodedoesnothavetobethetraditionalartiﬁcial\nneuroncomputation(aﬃnetransformationofallinputsfollowedbyamonotone\nnonlinearity).Forexample, ()proposeusingtensoroperations Socher e t a l .2013a\nandbilinearforms,whichhavepreviouslybeenfoundusefultomodelrelationships\nbetweenconcepts(Weston2010Bordes2012 e t a l .,; e t a l .,)whentheconceptsare\nrepresentedbycontinuousvectors(embeddings).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 988, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 780}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0989_15cd8996", "text": "10.7TheChallengeofLong-TermDependencies\nThemathematical challengeoflearninglong-termdependenciesinrecurrentnet-\nworkswasintroducedinsection.Thebasicproblemisthatgradientsprop- 8.2.5\nagatedovermanystagestendtoeithervanish(mostofthetime)orexplode\n(rarely,butwithmuchdamagetotheoptimization).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 989, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 289}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0990_d11c9d43", "text": "Evenifweassumethatthe\nparametersaresuchthattherecurrentnetworkisstable(canstorememories,\nwithgradientsnotexploding),thediﬃcultywithlong-termdependenciesarises\nfromtheexponentiallysmallerweightsgiventolong-terminteractions(involving\nthemultiplicationofmanyJacobians)comparedtoshort-termones.Manyother\nsourcesprovideadeepertreatment(,; Hochreiter1991Doya1993Bengio,; e t a l .,\n4 0 1\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n− − − 6 0 4 0 2 0 0 2 0 4 0 6 0\nI nput c o o r di na t e− 4− 3− 2− 101234P r o j e c t i o n o f o utput0\n1\n2\n3\n4\n5\nFigure10.15:Whencomposingmanynonlinearfunctions(likethelinear-tanhlayershown\nhere),theresultishighlynonlinear,typicallywithmostofthevaluesassociatedwithatiny\nderivative,somevalueswithalargederivative,andmanyalternationsbetweenincreasing\nanddecreasing.Inthisplot,weplotalinearprojectionofa100-dimensionalhiddenstate\ndowntoasingledimension,plottedonthe y-axis.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 990, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 909}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0991_37ef3c02", "text": "The x-axisisthecoordinateofthe\ninitialstatealongarandomdirectioninthe100-dimensionalspace.Wecanthusviewthis\nplotasalinearcross-sectionofahigh-dimensionalfunction.Theplotsshowthefunction\naftereachtimestep,orequivalently,aftereachnumberoftimesthetransitionfunction\nhasbeencomposed. 1994Pascanu2013 ; e t a l .,).Inthissection,wedescribetheprobleminmore\ndetail.Theremainingsectionsdescribeapproachestoovercomingtheproblem. Recurrentnetworksinvolvethecompositionofthesamefunctionmultiple\ntimes,oncepertimestep.Thesecompositionscanresultinextremelynonlinear\nbehavior,asillustratedinﬁgure.10.15\nInparticular,thefunctioncompositionemployedbyrecurrentneuralnetworks\nsomewhatresemblesmatrixmultiplication.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 991, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 696}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0992_fef5b720", "text": "Wecanthinkoftherecurrencerelation\nh( ) t= Wh( 1 ) t −(10.36)\nasaverysimplerecurrentneuralnetworklackinganonlinearactivationfunction,\nandlackinginputsx.As described insection , thisrecurrencerelation 8.2.5\nessentiallydescribesthepowermethod.Itmaybesimpliﬁedto\nh( ) t=\nWth( 0 ), (10.37)\nandifadmitsaneigendecompositionoftheform W\nWQQ = Λ, (10.38)\n4 0 2\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwithorthogonal ,therecurrencemaybesimpliﬁedfurtherto Q\nh( ) t= QΛtQh( 0 ). (10.39)\nTheeigenvaluesareraisedtothepowerof tcausingeigenvalueswithmagnitude\nlessthanonetodecaytozeroandeigenvalueswithmagnitudegreaterthanoneto\nexplode.Anycomponentofh( 0 )thatisnotalignedwiththelargesteigenvector\nwilleventuallybediscarded.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 992, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 726}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0993_30007011", "text": "Thisproblemisparticulartorecurrentnetworks.Inthescalarcase,imagine\nmultiplyingaweight wbyitselfmanytimes.Theproduct wtwilleithervanishor\nexplodedependingonthemagnitudeof w.However,ifwemakeanon-recurrent\nnetworkthathasadiﬀerentweight w( ) tateachtimestep,thesituationisdiﬀerent. Iftheinitialstateisgivenby,thenthestateattime 1 tisgivenby\nt w( ) t.Suppose\nthatthe w( ) tvaluesaregeneratedrandomly,independentlyfromoneanother,with\nzeromeanandvariance v.Thevarianceoftheproductis O( vn).Toobtainsome\ndesiredvariance v∗wemaychoosetheindividualweightswithvariance v=n√\nv∗. Verydeepfeedforwardnetworkswithcarefullychosenscalingcanthusavoidthe\nvanishingandexplodinggradientproblem,asarguedby(). Sussillo2014\nThevanishingandexplodinggradientproblemforRNNswasindependently\ndiscoveredbyseparateresearchers(,; ,,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 993, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 803}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0994_5b2cb679", "text": "Sussillo2014\nThevanishingandexplodinggradientproblemforRNNswasindependently\ndiscoveredbyseparateresearchers(,; ,,). Hochreiter1991Bengio e t a l .19931994\nOnemayhopethattheproblemcanbeavoidedsimplybystayinginaregionof\nparameterspacewherethegradientsdonotvanishorexplode.Unfortunately,in\nordertostorememoriesinawaythatisrobusttosmallperturbations,theRNN\nmustenteraregionofparameterspacewheregradientsvanish( ,, Bengio e t a l .1993\n1994).Speciﬁcally,wheneverthemodelisabletorepresentlongtermdependencies,\nthegradientofalongterminteractionhasexponentiallysmallermagnitudethan\nthegradientofashortterminteraction.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 994, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 609}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0995_2e8401a5", "text": "It doesnotmeanthatitisimpossible\ntolearn,butthatitmighttakeaverylongtimetolearnlong-termdependencies,\nbecausethesignalaboutthesedependencieswilltendtobehiddenbythesmallest\nﬂuctuationsarisingfromshort-termdependencies.Inpractice,theexperiments\nin ()showthatasweincreasethespanofthedependenciesthat Bengio e t a l .1994\nneedtobecaptured,gradient-basedoptimization becomesincreasinglydiﬃcult,\nwiththeprobabilityofsuccessfultrainingofatraditionalRNNviaSGDrapidly\nreaching0forsequencesofonlylength10or20.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 995, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 499}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0996_ae69bd7d", "text": "Foradeepertreatmentofrecurrentnetworksasdynamicalsystems,seeDoya\n(), ()and (),withareview 1993Bengio e t a l .1994SiegelmannandSontag1995\ninPascanu2013 e t a l .().Theremainingsectionsofthischapterdiscussvarious\napproachesthathavebeenproposedtoreducethediﬃcultyoflearninglong-\ntermdependencies(insomecasesallowinganRNNtolearndependenciesacross\n4 0 3\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nhundredsofsteps),buttheproblemoflearninglong-termdependenciesremains\noneofthemainchallengesindeeplearning.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 996, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 509}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0997_9537be59", "text": "10.8EchoStateNetworks\nTherecurrentweightsmappingfromh( 1 ) t −toh( ) tandtheinputweightsmapping\nfromx( ) ttoh( ) taresomeofthemostdiﬃcultparameterstolearninarecurrent\nnetwork.Oneproposed(,; ,; ,; Jaeger2003Maass e t a l .2002JaegerandHaas2004\nJaeger2007b,)approachtoavoidingthisdiﬃcultyistosettherecurrentweights\nsuchthattherecurrenthiddenunitsdoagoodjobofcapturingthehistoryofpast\ninputs,and l e a r n o nl y t h e o u t p u t w e i g h t s.Thisistheideathatwasindependently\nproposedforechostatenetworksorESNs( ,;,) JaegerandHaas2004Jaeger2007b\nandliquidstatemachines(,).Thelatterissimilar,except Maass e t a l .2002\nthatitusesspikingneurons(withbinaryoutputs)insteadofthecontinuous-valued\nhiddenunitsusedforESNs.BothESNsandliquidstatemachinesaretermed\nreservoircomputing(LukoševičiusandJaeger2009,)todenotethefactthat\nthehiddenunitsformofreservoiroftemporalfeatureswhichmaycapturediﬀerent\naspectsofthehistoryofinputs.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 997, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 919}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0998_0dbc13dc", "text": "Onewaytothinkaboutthesereservoircomputingrecurrentnetworksisthat\ntheyaresimilartokernelmachines:theymapanarbitrarylengthsequence(the\nhistoryofinputsuptotime t)intoaﬁxed-lengthvector(therecurrentstateh( ) t),\nonwhichalinearpredictor(typicallyalinearregression)canbeappliedtosolve\ntheproblemofinterest.Thetrainingcriterionmaythenbeeasilydesignedtobe\nconvexasafunctionoftheoutputweights.Forexample,iftheoutputconsists\noflinearregressionfromthehiddenunitstotheoutputtargets,andthetraining\ncriterionismeansquarederror,thenitisconvexandmaybesolvedreliablywith\nsimplelearningalgorithms(,). Jaeger2003\nTheimportantquestionistherefore:howdowesettheinputandrecurrent\nweightssothatarichsetofhistoriescanberepresentedintherecurrentneural\nnetworkstate? Theanswerproposedinthereservoircomputingliteratureisto\nviewtherecurrentnetasadynamicalsystem,andsettheinputandrecurrent\nweightssuchthatthedynamicalsystemisneartheedgeofstability.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 998, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 918}}
{"id": "computer_science_goodfellow_deep_learning_chunk_0999_eff8b9e6", "text": "Theanswerproposedinthereservoircomputingliteratureisto\nviewtherecurrentnetasadynamicalsystem,andsettheinputandrecurrent\nweightssuchthatthedynamicalsystemisneartheedgeofstability. TheoriginalideawastomaketheeigenvaluesoftheJacobianofthestate-to-\nstatetransitionfunctionbecloseto.Asexplainedinsection,animportant 1 8.2.5\ncharacteristicofarecurrentnetworkistheeigenvaluespectrumoftheJacobians\nJ( ) t=∂ s( ) t\n∂ s( 1 ) t −.OfparticularimportanceisthespectralradiusofJ( ) t,deﬁnedto\nbethemaximumoftheabsolutevaluesofitseigenvalues.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 999, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 526}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1000_18b15564", "text": "4 0 4\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nTounderstandtheeﬀectofthespectralradius,considerthesimplecaseof\nback-propagationwithaJacobianmatrixJthatdoesnotchangewith t.This\ncasehappens,forexample,whenthenetworkispurelylinear.SupposethatJhas\naneigenvectorvwithcorrespondingeigenvalue λ.Considerwhathappensaswe\npropagateagradientvectorbackwardsthroughtime.Ifwebeginwithagradient\nvectorg,thenafteronestepofback-propagation,wewillhaveJg,andafter n\nstepswewillhaveJng.Nowconsiderwhathappensifweinsteadback-propagate\naperturbedversionofg.Ifwebeginwithg+ δv,thenafteronestep,wewill\nhaveJ(g+ δv).After nsteps,wewillhaveJn(g+ δv).Fromthiswecansee\nthatback-propagationstartingfromgandback-propagationstartingfromg+ δv\ndivergeby δJnvafter nstepsofback-propagation.Ifvischosentobeaunit\neigenvectorofJwitheigenvalue λ,thenmultiplicationbytheJacobiansimply\nscalesthediﬀerenceateachstep.Thetwoexecutionsofback-propagationare\nseparatedbyadistanceof δ λ||n.Whenvcorrespondstothelargestvalueof|| λ,\nthisperturbationachievesthewidestpossibleseparationofaninitialperturbation\nofsize.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1000, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1078}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1001_e6e5b3bc", "text": "δ\nWhen || λ >1,thedeviationsize δ λ||ngrowsexponentiallylarge.When || λ <1,\nthedeviationsizebecomesexponentiallysmall. Ofcourse,thisexampleassumedthattheJacobianwasthesameatevery\ntimestep,correspondingtoarecurrentnetworkwithnononlinearity.Whena\nnonlinearityispresent,thederivativeofthenonlinearitywillapproachzeroon\nmanytimesteps,andhelptopreventtheexplosionresultingfromalargespectral\nradius. Indeed,themostrecentworkonechostatenetworksadvocatesusinga\nspectralradiusmuchlargerthanunity(,;,). Yildiz e t a l .2012Jaeger2012\nEverythingwehavesaidaboutback-propagation viarepeatedmatrixmultipli-\ncationappliesequallytoforwardpropagationinanetworkwithnononlinearity,\nwherethestateh( + 1 ) t= h( ) t W.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1001, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 698}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1002_10834567", "text": "WhenalinearmapWalwaysshrinkshasmeasuredbythe L2norm,then\nwesaythatthemapiscontractive.Whenthespectralradiusislessthanone,\nthemappingfromh( ) ttoh( + 1 ) tiscontractive,soasmallchangebecomessmaller\naftereachtimestep.Thisnecessarilymakesthenetworkforgetinformationabout\nthepastwhenweuseaﬁnitelevelofprecision(suchas32bitintegers)tostore\nthestatevector. TheJacobianmatrixtellsushowasmallchangeofh( ) tpropagatesonestep\nforward,orequivalently,howthegradientonh( + 1 ) tpropagatesonestepbackward,\nduringback-propagation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1002, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 516}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1003_12e98d9a", "text": "TheJacobianmatrixtellsushowasmallchangeofh( ) tpropagatesonestep\nforward,orequivalently,howthegradientonh( + 1 ) tpropagatesonestepbackward,\nduringback-propagation. NotethatneitherWnorJneedtobesymmetric(al-\nthoughtheyaresquareandreal),sotheycanhavecomplex-valuedeigenvaluesand\neigenvectors,withimaginarycomponentscorrespondingtopotentiallyoscillatory\n4 0 5\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nbehavior(ifthesameJacobianwasappliediteratively).Eventhoughh( ) tora\nsmallvariationofh( ) tofinterestinback-propagation arereal-valued,theycan\nbeexpressedinsuchacomplex-valuedbasis.Whatmattersiswhathappensto\nthemagnitude(complexabsolutevalue)ofthesepossiblycomplex-valuedbasis\ncoeﬃcients, whenwemultiplythematrixbythevector.Aneigenvaluewith\nmagnitudegreaterthanonecorrespondstomagniﬁcation (exponentialgrowth,if\nappliediteratively)orshrinking(exponentialdecay,ifappliediteratively).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1003, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 892}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1004_e37b9aa7", "text": "Withanonlinearmap, theJacobianisfreetochangeateachstep.The\ndynamicsthereforebecomemorecomplicated.However,itremainstruethata\nsmallinitialvariationcanturnintoalargevariationafterseveralsteps.One\ndiﬀerencebetweenthepurelylinearcaseandthenonlinearcaseisthattheuseof\nasquashingnonlinearitysuchastanhcancausetherecurrentdynamicstobecome\nbounded.Notethat itispossible forback-propagation to retainunbounded\ndynamicsevenwhenforwardpropagationhasboundeddynamics,forexample,\nwhenasequenceoftanhunitsareallinthemiddleoftheirlinearregimeandare\nconnectedbyweightmatriceswithspectralradiusgreaterthan.However,itis 1\nrareforalloftheunitstosimultaneouslylieattheirlinearactivationpoint. tanh\nThestrategyofechostatenetworksissimplytoﬁxtheweightstohavesome\nspectralradiussuchas,whereinformationiscarriedforwardthroughtimebut 3\ndoesnotexplodeduetothestabilizingeﬀectofsaturatingnonlinearities liketanh.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1004, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 884}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1005_c37ceeee", "text": "Morerecently,ithasbeenshownthatthetechniquesusedtosettheweights\ninESNscouldbeusedtotheweightsinafullytrainablerecurrentnet- i nit i a l i z e\nwork(withthehidden-to-hidden recurrentweightstrainedusingback-propagation\nthroughtime),helpingtolearnlong-termdependencies(Sutskever2012Sutskever ,;\ne t a l .,).Inthissetting,aninitialspectralradiusof1.2performswell,combined 2013\nwiththesparseinitialization schemedescribedinsection.8.4\n10.9LeakyUnitsandOtherStrategiesforMultiple\nTimeScales\nOnewaytodealwithlong-termdependencies istodesignamodelthatoperates\natmultipletimescales,sothatsomepartsofthemodeloperateatﬁne-grained\ntimescalesandcanhandlesmalldetails,whileotherpartsoperateatcoarsetime\nscalesandtransferinformationfromthedistantpasttothepresentmoreeﬃciently.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1005, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 760}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1006_6fe03379", "text": "Variousstrategiesforbuildingbothﬁneandcoarsetimescalesarepossible.These\nincludetheadditionofskipconnectionsacrosstime,“leakyunits”thatintegrate\nsignalswithdiﬀerenttimeconstants,andtheremovalofsomeoftheconnections\n4 0 6\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nusedtomodelﬁne-grainedtimescales. 10.9.1AddingSkipConnectionsthroughTime\nOnewaytoobtaincoarsetimescalesistoadddirectconnectionsfromvariablesin\nthedistantpasttovariablesinthepresent.Theideaofusingsuchskipconnections\ndatesbackto()andfollowsfromtheideaofincorporatingdelaysin Lin e t a l .1996\nfeedforwardneuralnetworks( ,).Inanordinaryrecurrent LangandHinton1988\nnetwork,arecurrentconnectiongoesfromaunitattime ttoaunitattime t+1. Itispossibletoconstructrecurrentnetworkswithlongerdelays(,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1006, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 761}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1007_5b39aa02", "text": "Itispossibletoconstructrecurrentnetworkswithlongerdelays(,). Bengio1991\nAswehaveseeninsection,gradientsmayvanishorexplodeexponentially 8.2.5\nw i t h r e s p e c t t o t h e nu m b e r o f t i m e s t e p s.()introducedrecurrent Lin e t a l .1996\nconnectionswithatime-delayof dtomitigatethisproblem.Gradientsnow\ndiminishexponentiallyasafunctionofτ\ndratherthan τ.Sincethereareboth\ndelayedandsinglestepconnections,gradientsmaystillexplodeexponentiallyin τ. Thisallowsthelearningalgorithmtocapturelongerdependenciesalthoughnotall\nlong-termdependencies mayberepresentedwellinthisway. 10.9.2LeakyUnitsandaSpectrumofDiﬀerentTimeScales\nAnotherwaytoobtainpathsonwhichtheproductofderivativesisclosetooneisto\nhaveunitswith l i ne a rself-connectionsandaweightnearoneontheseconnections.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1007, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 774}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1008_94965694", "text": "10.9.2LeakyUnitsandaSpectrumofDiﬀerentTimeScales\nAnotherwaytoobtainpathsonwhichtheproductofderivativesisclosetooneisto\nhaveunitswith l i ne a rself-connectionsandaweightnearoneontheseconnections. Whenweaccumulatearunningaverage µ( ) tofsomevalue v( ) tbyapplyingthe\nupdate µ( ) t← α µ( 1 ) t −+(1− α) v( ) tthe αparameterisanexampleofalinearself-\nconnectionfrom µ( 1 ) t −to µ( ) t.When αisnearone,therunningaverageremembers\ninformationaboutthepastforalongtime,andwhen αisnearzero,information\naboutthepastisrapidlydiscarded.Hiddenunitswithlinearself-connectionscan\nbehavesimilarlytosuchrunningaverages.Suchhiddenunitsarecalledleaky\nunits.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1008, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 638}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1009_a59c1bdd", "text": "Skipconnectionsthrough dtimestepsareawayofensuringthataunitcan\nalwayslearntobeinﬂuencedbyavaluefrom dtimestepsearlier.Theuseofa\nlinearself-connectionwithaweightnearoneisadiﬀerentwayofensuringthatthe\nunitcanaccessvaluesfromthepast.Thelinearself-connectionapproachallows\nthiseﬀecttobeadaptedmoresmoothlyandﬂexiblybyadjustingthereal-valued\nαratherthanbyadjustingtheinteger-valuedskiplength. Theseideaswereproposedby()andby (). Mozer1992 ElHihiandBengio1996\nLeakyunitswerealsofoundtobeusefulinthecontextofechostatenetworks\n(,). Jaeger e t a l .2007\n4 0 7\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nTherearetwobasicstrategiesforsettingthetimeconstantsusedbyleaky\nunits. Onestrategyistomanuallyﬁxthemtovaluesthatremainconstant,for\nexamplebysamplingtheirvaluesfromsomedistributiononceatinitialization time. Anotherstrategyistomakethetimeconstantsfreeparametersandlearnthem. Havingsuchleakyunitsatdiﬀerenttimescalesappearstohelpwithlong-term\ndependencies(,;Mozer1992Pascanu2013 e t a l .,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1009, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1010_0204c905", "text": "Anotherstrategyistomakethetimeconstantsfreeparametersandlearnthem. Havingsuchleakyunitsatdiﬀerenttimescalesappearstohelpwithlong-term\ndependencies(,;Mozer1992Pascanu2013 e t a l .,). 10.9.3RemovingConnections\nAnotherapproachtohandlelong-termdependenciesistheideaoforganizing\nthestateoftheRNNatmultipletime-scales( ,),with ElHihiandBengio1996\ninformationﬂowingmoreeasilythroughlongdistancesattheslowertimescales. Thisideadiﬀersfromtheskipconnectionsthroughtimediscussedearlier\nbecauseitinvolvesactively r e m o v i nglength-oneconnectionsandreplacingthem\nwithlongerconnections.Unitsmodiﬁedinsuchawayareforcedtooperateona\nlongtimescale.Skipconnectionsthroughtimeedges.Unitsreceivingsuch a d d\nnewconnectionsmaylearntooperateonalongtimescalebutmayalsochooseto\nfocusontheirothershort-termconnections. Therearediﬀerentwaysinwhichagroupofrecurrentunitscanbeforcedto\noperateatdiﬀerenttimescales.Oneoptionistomaketherecurrentunitsleaky,\nbuttohavediﬀerentgroupsofunitsassociatedwithdiﬀerentﬁxedtimescales.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1010, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 996}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1011_1606fc78", "text": "Therearediﬀerentwaysinwhichagroupofrecurrentunitscanbeforcedto\noperateatdiﬀerenttimescales.Oneoptionistomaketherecurrentunitsleaky,\nbuttohavediﬀerentgroupsofunitsassociatedwithdiﬀerentﬁxedtimescales. Thiswastheproposalin()andhasbeensuccessfullyusedin Mozer1992 Pascanu\ne t a l .().Anotheroptionistohaveexplicitanddiscreteupdatestakingplace 2013\natdiﬀerenttimes,withadiﬀerentfrequencyfordiﬀerentgroupsofunits.Thisis\ntheapproachof ()and ElHihiandBengio1996Koutnik 2014 e t a l .().Itworked\nwellonanumberofbenchmarkdatasets. 10.10TheLongShort-TermMemoryandOtherGated\nRNNs\nAsofthiswriting,themosteﬀectivesequencemodelsusedinpracticalapplications\narecalledgatedRNNs.Theseincludethelongshort-termmemoryand\nnetworksbasedonthe .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1011, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 720}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1012_1612239b", "text": "10.10TheLongShort-TermMemoryandOtherGated\nRNNs\nAsofthiswriting,themosteﬀectivesequencemodelsusedinpracticalapplications\narecalledgatedRNNs.Theseincludethelongshort-termmemoryand\nnetworksbasedonthe . gatedrecurrentunit\nLikeleakyunits,gatedRNNsarebasedontheideaofcreatingpathsthrough\ntimethathavederivativesthatneithervanishnorexplode.Leakyunits did\nthiswithconnectionweightsthatwereeithermanuallychosenconstantsorwere\nparameters.GatedRNNsgeneralizethistoconnectionweightsthatmaychange\n4 0 8\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nateachtimestep. ×\ni nput i nput gate f or ge t   gate output gateoutput\ns t at es e l f - l oop×\n+ ×\nFigure10.16:BlockdiagramoftheLSTMrecurrentnetwork“cell.”Cellsareconnected\nrecurrentlytoeachother,replacingtheusualhiddenunitsofordinaryrecurrentnetworks.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1012, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 797}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1013_45dc4cc7", "text": "Aninputfeatureiscomputedwitharegularartiﬁcialneuronunit.Itsvaluecanbe\naccumulatedintothestateifthesigmoidalinputgateallowsit.Thestateunithasa\nlinearself-loopwhoseweightiscontrolledbytheforgetgate.Theoutputofthecellcan\nbeshutoﬀbytheoutputgate.Allthegatingunitshaveasigmoidnonlinearity,whilethe\ninputunitcanhaveanysquashingnonlinearity. Thestateunitcanalsobeusedasan\nextrainputtothegatingunits.Theblacksquareindicatesadelayofasingletimestep.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1013, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 439}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1014_b40e0988", "text": "Thestateunitcanalsobeusedasan\nextrainputtothegatingunits.Theblacksquareindicatesadelayofasingletimestep. Leakyunitsallowthenetworkto a c c u m u l a t einformation(suchasevidence\nforaparticularfeatureorcategory)overalongduration.However,oncethat\ninformationhasbeenused,itmightbeusefulfortheneuralnetworkto f o r g e tthe\noldstate.Forexample,ifasequenceismadeofsub-sequencesandwewantaleaky\nunittoaccumulateevidenceinsideeachsub-subsequence,weneedamechanismto\nforgettheoldstatebysettingittozero.Insteadofmanuallydecidingwhento\nclearthestate,wewanttheneuralnetworktolearntodecidewhentodoit.This\n4 0 9\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\niswhatgatedRNNsdo.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1014, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 669}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1015_fc5256e5", "text": "10.10.1LSTM\nThecleverideaofintroducingself-loopstoproducepathswherethegradient\ncanﬂowforlongdurationsisacorecontributionoftheinitiallongshort-term\nmemory(LSTM)model(HochreiterandSchmidhuber1997,).Acrucialaddition\nhasbeentomaketheweightonthisself-loopconditionedonthecontext,ratherthan\nﬁxed(,).Bymakingtheweightofthisself-loopgated(controlled Gers e t a l .2000\nbyanotherhiddenunit),thetimescaleofintegrationcanbechangeddynamically. Inthiscase,wemeanthatevenforanLSTMwithﬁxedparameters,thetimescale\nofintegrationcanchangebasedontheinputsequence,becausethetimeconstants\nareoutputbythemodelitself.TheLSTMhasbeenfoundextremelysuccessful\ninmanyapplications, suchasunconstrainedhandwriting recognition(Graves\ne t a l .,),speechrecognition( 2009 Graves2013GravesandJaitly2014 e t a l .,; ,),\nhandwritinggeneration(Graves2013,),machinetranslation(Sutskever2014 e t a l .,),\nimagecaptioning(,; Kiros e t a l .2014bVinyals2014bXu2015 e t a l .,; e t a l .,)and\nparsing(Vinyals2014a e t a l .,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1015, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 984}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1016_dea9b49f", "text": "TheLSTMblockdiagramisillustratedinﬁgure.Thecorresponding 10.16\nforwardpropagationequationsaregivenbelow,inthecaseofashallowrecurrent\nnetworkarchitecture. Deeperarchitectures havealsobeensuccessfullyused(Graves\ne t a l .,;2013Pascanu2014a e t a l .,).Insteadofaunitthatsimplyappliesanelement-\nwisenonlinearitytotheaﬃnetransformationofinputsandrecurrentunits,LSTM\nrecurrentnetworkshave“LSTMcells”thathaveaninternalrecurrence(aself-loop),\ninadditiontotheouterrecurrenceoftheRNN.Eachcellhasthesameinputs\nandoutputsasanordinaryrecurrentnetwork,buthasmoreparametersanda\nsystemofgatingunitsthatcontrolstheﬂowofinformation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1016, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 615}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1017_70cbc0a6", "text": "Themostimportant\ncomponentisthestateunit s( ) t\nithathasalinearself-loopsimilartotheleaky\nunitsdescribedintheprevioussection.However,here,theself-loopweight(orthe\nassociatedtimeconstant)iscontrolledbyaforgetgateunit f( ) t\ni(fortimestep t\nandcell),thatsetsthisweighttoavaluebetween0and1viaasigmoidunit: i\nf( ) t\ni= σ\n bf\ni+\njUf\ni , j x( ) t\nj+\njWf\ni , j h( 1 ) t −\nj\n ,(10.40)\nwherex( ) tisthecurrentinputvectorandh( ) tisthecurrenthiddenlayervector,\ncontainingtheoutputsofalltheLSTMcells,andbf,Uf,Wfarerespectively\nbiases,inputweightsandrecurrentweightsfortheforgetgates.TheLSTMcell\n4 1 0\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ninternalstateisthusupdatedasfollows,butwithaconditionalself-loopweight\nf( ) t\ni:\ns( ) t\ni= f( ) t\ni s( 1 ) t −\ni + g( ) t\ni σ\n b i+\njU i , j x( ) t\nj+\njW i , j h( 1 ) t −\nj\n ,(10.41)\nwhereb,UandWrespectivelydenotethebiases,inputweightsandrecurrent\nweightsintotheLSTMcell.Theexternalinputgateunit g( ) t\niiscomputed\nsimilarlytotheforgetgate(withasigmoidunittoobtainagatingvaluebetween\n0and1),butwithitsownparameters:\ng( ) t\ni= σ\n bg\ni+\njUg\ni , j x( ) t\nj+\njWg\ni , j h( 1 ) t −\nj\n .(10.42)\nTheoutput h( ) t\nioftheLSTMcellcanalsobeshutoﬀ,viatheoutputgate q( ) t\ni,\nwhichalsousesasigmoidunitforgating:\nh( ) t\ni= tanh\ns( ) t\ni\nq( ) t\ni (10.43)\nq( ) t\ni= σ\n bo\ni+\njUo\ni , j x( ) t\nj+\njWo\ni , j h( 1 ) t −\nj\n (10.44)\nwhichhasparametersbo,Uo,Woforitsbiases,inputweightsandrecurrent\nweights,respectively.Amongthevariants,onecanchoosetousethecellstate s( ) t\ni\nasanextrainput(withitsweight)intothethreegatesofthe i-thunit,asshown\ninﬁgure.Thiswouldrequirethreeadditionalparameters.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1017, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1639}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1018_2c6d740c", "text": "10.16\nLSTMnetworkshavebeenshowntolearnlong-termdependenciesmoreeasily\nthanthesimplerecurrentarchitectures,ﬁrstonartiﬁcialdatasetsdesignedfor\ntestingtheabilitytolearnlong-termdependencies( ,; Bengio e t a l .1994Hochreiter\nandSchmidhuber1997Hochreiter 2001 ,; e t a l .,),thenonchallengingsequence\nprocessingtaskswherestate-of-the-art performance wasobtained(Graves2012,;\nGraves2013Sutskever2014 e t a l .,; e t a l .,).VariantsandalternativestotheLSTM\nhavebeenstudiedandusedandarediscussednext. 10.10.2OtherGatedRNNs\nWhichpieces ofthe LSTMarchitecture are actually necessary?Whatother\nsuccessfularchitecturescouldbedesignedthatallowthenetworktodynamically\ncontrolthetimescaleandforgettingbehaviorofdiﬀerentunits?", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1018, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 712}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1019_b3a7b2da", "text": "4 1 1\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nSomeanswerstothesequestionsaregivenwiththerecentworkongatedRNNs,\nwhoseunitsarealsoknownasgatedrecurrentunitsorGRUs(,; Cho e t a l .2014b\nChung20142015aJozefowicz2015Chrupala 2015 e t a l .,,; e t a l .,; e t a l .,).Themain\ndiﬀerencewiththeLSTMisthatasinglegatingunitsimultaneouslycontrolsthe\nforgettingfactorandthedecisiontoupdatethestateunit.Theupdateequations\narethefollowing:\nh( ) t\ni= u( 1 ) t −\ni h( 1 ) t −\ni+(1− u( 1 ) t −\ni) σ\n b i+\njU i , j x( 1 ) t −\nj +\njW i , j r( 1 ) t −\nj h( 1 ) t −\nj\n ,\n(10.45)\nwhereustandsfor“update”gateandrfor“reset”gate.Theirvalueisdeﬁnedas\nusual:\nu( ) t\ni= σ\n bu\ni+\njUu\ni , j x( ) t\nj+\njWu\ni , j h( ) t\nj\n (10.46)\nand\nr( ) t\ni= σ\n br\ni+\njUr\ni , j x( ) t\nj+\njWr\ni , j h( ) t\nj\n .(10.47)\nTheresetandupdatesgatescanindividually“ignore”partsofthestatevector.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1019, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 870}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1020_43773b91", "text": "Theupdategatesactlikeconditionalleakyintegratorsthatcanlinearlygateany\ndimension,thuschoosingtocopyit(atoneextremeofthesigmoid)orcompletely\nignoreit(attheotherextreme)byreplacingitbythenew“targetstate”value\n(towardswhichtheleakyintegratorwantstoconverge).Theresetgatescontrol\nwhichpartsofthestategetusedtocomputethenexttargetstate,introducingan\nadditionalnonlineareﬀectintherelationshipbetweenpaststateandfuturestate. Manymorevariantsaroundthisthemecanbedesigned.Forexamplethe\nresetgate(orforgetgate)outputcouldbesharedacrossmultiplehiddenunits. Alternately,theproductofaglobalgate(coveringawholegroupofunits,suchas\nanentirelayer)andalocalgate(perunit)couldbeusedtocombineglobalcontrol\nandlocalcontrol.However,severalinvestigationsoverarchitectural variations\noftheLSTMandGRUfoundnovariantthatwouldclearlybeatbothofthese\nacrossawiderangeoftasks(,; Greﬀ e t a l .2015Jozefowicz2015Greﬀ e t a l .,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1020, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 897}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1021_7ba00188", "text": "e t a l .()foundthatacrucialingredientistheforgetgate,while 2015 Jozefowicz\ne t a l .()foundthataddingabiasof1totheLSTMforgetgate,apractice 2015\nadvocatedby (),makestheLSTMasstrongasthebestofthe Gers e t a l .2000\nexploredarchitecturalvariants. 4 1 2\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.11OptimizationforLong-TermDependencies\nSection andsectionhavedescribedthevanishingandexplodinggradient 8.2.5 10.7\nproblemsthatoccurwhenoptimizingRNNsovermanytimesteps.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1021, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 474}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1022_a0d89d5c", "text": "AninterestingideaproposedbyMartensandSutskever2011()isthatsecond\nderivativesmayvanishatthesametimethatﬁrstderivativesvanish.Second-order\noptimization algorithmsmayroughlybeunderstoodasdividingtheﬁrstderivative\nbythesecondderivative(inhigherdimension,multiplyingthegradientbythe\ninverseHessian).Ifthesecondderivativeshrinksatasimilarratetotheﬁrst\nderivative,thentheratioofﬁrstandsecondderivativesmayremainrelatively\nconstant.Unfortunately,second-ordermethodshavemanydrawbacks,including\nhighcomputational cost,theneedforalargeminibatch,andatendencytobe\nattractedtosaddlepoints.MartensandSutskever2011()foundpromisingresults\nusingsecond-ordermethods.Later,Sutskever2013 e t a l .()foundthatsimpler\nmethodssuchasNesterovmomentumwithcarefulinitialization couldachieve\nsimilarresults.SeeSutskever2012()formoredetail.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1022, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 810}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1023_89862c20", "text": "Bothoftheseapproaches\nhavelargelybeenreplacedbysimplyusingSGD(evenwithoutmomentum)applied\ntoLSTMs.Thisispartofacontinuingthemeinmachinelearningthatitisoften\nmucheasiertodesignamodelthatiseasytooptimizethanitistodesignamore\npowerfuloptimization algorithm. 10.11.1ClippingGradients\nAsdiscussedinsection,stronglynonlinearfunctionssuchasthosecomputed 8.2.4\nbyarecurrentnetovermanytimestepstendtohavederivativesthatcanbe\neitherverylargeorverysmallinmagnitude.Thisisillustratedinﬁgureand8.3\nﬁgure,inwhichweseethattheobjectivefunction(asafunctionofthe 10.17\nparameters)hasa“landscape” inwhichoneﬁnds“cliﬀs”:wideandratherﬂat\nregionsseparatedbytinyregionswheretheobjectivefunctionchangesquickly,\nformingakindofcliﬀ.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1023, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 706}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1024_dd2bf8de", "text": "Thediﬃcultythatarisesisthatwhentheparametergradientisverylarge,a\ngradientdescentparameterupdatecouldthrowtheparametersveryfar,intoa\nregionwheretheobjectivefunctionislarger,undoingmuchoftheworkthathad\nbeendonetoreachthecurrentsolution.Thegradienttellsusthedirectionthat\ncorrespondstothesteepestdescentwithinaninﬁnitesimalregionsurroundingthe\ncurrentparameters.Outsideofthisinﬁnitesimalregion,thecostfunctionmay\nbegintocurvebackupwards.Theupdatemustbechosentobesmallenoughto\navoidtraversingtoomuchupwardcurvature.Wetypicallyuselearningratesthat\n4 1 3\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ndecayslowlyenoughthatconsecutivestepshaveapproximatelythesamelearning\nrate.Astepsizethatisappropriateforarelativelylinearpartofthelandscapeis\nofteninappropriate andcausesuphillmotionifweenteramorecurvedpartofthe\nlandscapeonthenextstep.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1024, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 837}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1025_96bf3759", "text": "\n \n               \n\n \n            \nFigure10.17:Exampleoftheeﬀectofgradientclippinginarecurrentnetworkwith\ntwoparameterswandb.Gradientclippingcanmakegradientdescentperformmore\nreasonablyinthevicinityofextremelysteepcliﬀs.Thesesteepcliﬀscommonlyoccur\ninrecurrentnetworksnearwherearecurrentnetworkbehavesapproximatelylinearly. Thecliﬀisexponentiallysteepinthenumberoftimestepsbecausetheweightmatrix\nismultipliedbyitselfonceforeachtimestep. ( L e f t )Gradientdescentwithoutgradient\nclippingovershootsthebottomofthissmallravine,thenreceivesaverylargegradient\nfromthecliﬀface.Thelargegradientcatastrophicallypropelstheparametersoutsidethe\naxesoftheplot.Gradientdescentwithgradientclippinghasamoremoderate ( R i g h t )\nreactiontothecliﬀ.Whileitdoesascendthecliﬀface,thestepsizeisrestrictedsothat\nitcannotbepropelledawayfromsteepregionnearthesolution.Figureadaptedwith\npermissionfromPascanu2013 e t a l .().", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1025, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 946}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1026_f38b1b40", "text": "Asimpletypeofsolutionhasbeeninusebypractitioners formanyyears:\nclippingthegradient.Therearediﬀerentinstancesofthisidea(Mikolov2012,;\nPascanu2013 e t a l .,).Oneoptionistocliptheparametergradientfromaminibatch\ne l e m e nt - w i s e(Mikolov2012,)justbeforetheparameterupdate.Anotheristo c l i p\nt h e norm ||||g o f t h e g r a d i e ntg(Pascanu2013 e t a l .,)justbeforetheparameter\nupdate:\nif||||g > v (10.48)\ng←g v\n||||g(10.49)\n4 1 4\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwhere visthenormthresholdandgisusedtoupdateparameters.Becausethe\ngradientofalltheparameters(includingdiﬀerentgroupsofparameters,suchas\nweightsandbiases)isrenormalizedjointlywithasinglescalingfactor,thelatter\nmethodhastheadvantagethatitguaranteesthateachstepisstillinthegradient\ndirection,butexperimentssuggestthatbothformsworksimilarly.Although\ntheparameterupdatehasthesamedirectionasthetruegradient,withgradient\nnormclipping,theparameterupdatevectornormisnowbounded.Thisbounded\ngradientavoidsperformingadetrimentalstepwhenthegradientexplodes.In\nfact,evensimplytakinga r a ndom s t e pwhenthegradientmagnitudeisabove\nathresholdtendstoworkalmostaswell.Iftheexplosionissoseverethatthe\ngradientisnumerically InforNan(consideredinﬁniteornot-a-number),then\narandomstepofsize vcanbetakenandwilltypicallymoveawayfromthe\nnumericallyunstableconﬁguration.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1026, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1334}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1027_ed7046ff", "text": "Clippingthegradientnormper-minibatchwill\nnotchangethedirectionofthegradientforanindividualminibatch.However,\ntakingtheaverageofthenorm-clippedgradientfrommanyminibatchesisnot\nequivalenttoclippingthenormofthetruegradient(thegradientformedfrom\nusingallexamples).Examplesthathavelargegradientnorm,aswellasexamples\nthatappearinthesameminibatchassuchexamples,willhavetheircontribution\ntotheﬁnaldirectiondiminished.Thisstandsincontrasttotraditionalminibatch\ngradientdescent,wherethetruegradientdirectionisequaltotheaverageoverall\nminibatchgradients.Putanotherway,traditionalstochasticgradientdescentuses\nanunbiasedestimateofthegradient,whilegradientdescentwithnormclipping\nintroducesaheuristicbiasthatweknowempiricallytobeuseful.Withelement-\nwiseclipping,thedirectionoftheupdateisnotalignedwiththetruegradient\northeminibatchgradient,butitisstilladescentdirection.Ithasalsobeen\nproposed(Graves2013,)tocliptheback-propagatedgradient(withrespectto\nhiddenunits)butnocomparisonhasbeenpublishedbetweenthesevariants;we\nconjecturethatallthesemethodsbehavesimilarly.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1027, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1051}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1028_79646a1e", "text": "10.11.2RegularizingtoEncourageInformationFlow\nGradientclippinghelpstodealwithexplodinggradients,butitdoesnothelpwith\nvanishinggradients.Toaddressvanishinggradientsandbettercapturelong-term\ndependencies,wediscussedtheideaofcreatingpathsinthecomputational graphof\ntheunfoldedrecurrentarchitecturealongwhichtheproductofgradientsassociated\nwitharcsisnear1.OneapproachtoachievethisiswithLSTMsandotherself-\nloopsandgatingmechanisms,describedaboveinsection.Anotherideais 10.10\ntoregularizeorconstraintheparameterssoastoencourage“informationﬂow.”\nInparticular,wewouldlikethegradientvector∇h( ) t Lbeingback-propagatedto\n4 1 5\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nmaintainitsmagnitude,evenifthelossfunctiononlypenalizestheoutputatthe\nendofthesequence.Formally,wewant\n(∇h( ) t L)∂h( ) t\n∂h( 1 ) t −(10.50)\ntobeaslargeas\n∇h( ) t L. (10.51)\nWiththisobjective,Pascanu2013 e t a l .()proposethefollowingregularizer:\nΩ =\nt\n|∇(h( ) t L)∂ h( ) t\n∂ h( 1 ) t −|\n||∇h( ) t L||−1\n2\n.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1028, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 989}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1029_ee4a7105", "text": "(10.51)\nWiththisobjective,Pascanu2013 e t a l .()proposethefollowingregularizer:\nΩ =\nt\n|∇(h( ) t L)∂ h( ) t\n∂ h( 1 ) t −|\n||∇h( ) t L||−1\n2\n. (10.52)\nComputingthegradientofthisregularizermayappeardiﬃcult,butPascanu\ne t a l .()proposeanapproximation inwhichweconsidertheback-propagated 2013\nvectors∇h( ) t Lasiftheywereconstants(forthepurposeofthisregularizer,so\nthatthereisnoneedtoback-propagatethroughthem).Theexperimentswith\nthisregularizersuggestthat,ifcombinedwiththenormclippingheuristic(which\nhandlesgradientexplosion),theregularizercanconsiderablyincreasethespanof\nthedependenciesthatanRNNcanlearn. BecauseitkeepstheRNNdynamics\nontheedgeofexplosivegradients,thegradientclippingisparticularlyimportant. Withoutgradientclipping,gradientexplosionpreventslearningfromsucceeding. AkeyweaknessofthisapproachisthatitisnotaseﬀectiveastheLSTMfor\ntaskswheredataisabundant,suchaslanguagemodeling.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1029, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 903}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1030_c8878f01", "text": "Withoutgradientclipping,gradientexplosionpreventslearningfromsucceeding. AkeyweaknessofthisapproachisthatitisnotaseﬀectiveastheLSTMfor\ntaskswheredataisabundant,suchaslanguagemodeling. 10.12ExplicitMemory\nIntelligencerequiresknowledgeandacquiringknowledgecanbedonevialearning,\nwhichhasmotivatedthedevelopmentoflarge-scaledeeparchitectures.However,\ntherearediﬀerentkindsofknowledge.Someknowledgecanbeimplicit,sub-\nconscious,anddiﬃculttoverbalize—suchashowtowalk,orhowadoglooks\ndiﬀerentfromacat.Otherknowledgecanbeexplicit,declarative,andrelatively\nstraightforwardtoputintowords—everydaycommonsense knowledge,like“acat\nisakindofanimal,”orveryspeciﬁcfactsthatyouneedtoknowtoaccomplish\nyourcurrentgoals,like“themeetingwiththesalesteamisat3:00PMinroom\n141.”\nNeuralnetworksexcelatstoringimplicitknowledge.However,theystruggleto\nmemorizefacts.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1030, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 835}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1031_8d32c571", "text": "Stochasticgradientdescentrequiresmanypresentationsofthe\n4 1 6\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nT ask   ne t w or k ,\nc ontrol l i ng th e   m e m o r yMe m or y   c e l l s\nW r i t i ng\nm e c hani s mR e adi ng\nm e c hani s m\nFigure10.18:Aschematicofanexampleofanetworkwithanexplicitmemory,capturing\nsomeofthekeydesignelementsoftheneuralTuringmachine.Inthisdiagramwe\ndistinguishthe“representation”partofthemodel(the“tasknetwork,”herearecurrent\nnetinthebottom)fromthe“memory”partofthemodel(thesetofcells),whichcan\nstorefacts.Thetasknetworklearnsto“control”thememory,decidingwheretoreadfrom\nandwheretowritetowithinthememory(throughthereadingandwritingmechanisms,\nindicatedbyboldarrowspointingatthereadingandwritingaddresses).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1031, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 743}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1032_0f02377a", "text": "4 1 7\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nsameinputbeforeitcanbestoredinaneuralnetworkparameters,andeventhen,\nthatinputwillnotbestoredespeciallyprecisely.Graves2014b e t a l .()hypothesized\nthatthisisbecauseneuralnetworkslacktheequivalentoftheworkingmemory\nsystemthatallowshumanbeingstoexplicitlyholdandmanipulatepiecesof\ninformationthat arerelevantto achieving some goal.Suchexplicit memory\ncomponentswouldallowoursystemsnotonlytorapidlyand“intentionally”store\nandretrievespeciﬁcfactsbutalsotosequentiallyreasonwiththem.Theneed\nforneuralnetworksthatcanprocessinformationinasequenceofsteps,changing\nthewaytheinputisfedintothenetworkateachstep,haslongbeenrecognized\nasimportantfortheabilitytoreasonratherthantomakeautomatic,intuitive\nresponsestotheinput(,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1032, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 773}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1033_c26bed44", "text": "Hinton1990\nToresolvethisdiﬃculty,Weston2014 e t a l .()introducedmemorynetworks\nthatincludeasetofmemorycellsthatcanbeaccessedviaanaddressingmecha-\nnism.Memorynetworksoriginallyrequiredasupervisionsignalinstructingthem\nhowtousetheirmemorycells.Graves2014b e t a l .()introducedtheneural\nTuringmachine,whichisabletolearntoreadfromandwritearbitrarycontent\ntomemorycellswithoutexplicitsupervisionaboutwhichactionstoundertake,\nandallowedend-to-endtrainingwithoutthissupervisionsignal,viatheuseof\nacontent-basedsoftattentionmechanism(see ()andsec- Bahdanau e t a l .2015\ntion).Thissoftaddressingmechanismhasbecomestandardwithother 12.4.5.1\nrelatedarchitecturesemulatingalgorithmicmechanismsinawaythatstillallows\ngradient-basedoptimization ( ,; Sukhbaatar e t a l .2015JoulinandMikolov2015,;\nKumar 2015Vinyals2015aGrefenstette2015 e t a l .,; e t a l .,; e t a l .,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1033, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 860}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1034_45b30ef3", "text": "Eachmemorycellcanbethoughtofasanextensionofthememorycellsin\nLSTMsandGRUs.Thediﬀerenceisthatthenetworkoutputsaninternalstate\nthatchooseswhichcelltoreadfromorwriteto,justasmemoryaccessesina\ndigitalcomputerreadfromorwritetoaspeciﬁcaddress. Itisdiﬃculttooptimizefunctionsthatproduceexact,integeraddresses.To\nalleviatethisproblem,NTMsactuallyreadtoorwritefrommanymemorycells\nsimultaneously.Toread,theytakeaweightedaverageofmanycells.Towrite,they\nmodifymultiplecellsbydiﬀerentamounts.Thecoeﬃcientsfortheseoperations\narechosentobefocusedonasmallnumberofcells,forexample,byproducing\nthemviaasoftmaxfunction.Usingtheseweightswithnon-zeroderivativesallows\nthefunctionscontrollingaccesstothememorytobeoptimizedusinggradient\ndescent.Thegradientonthesecoeﬃcientsindicateswhethereachofthemshould\nbeincreasedordecreased,butthegradientwilltypicallybelargeonlyforthose\nmemoryaddressesreceivingalargecoeﬃcient.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1034, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 892}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1035_564e80b1", "text": "Thesememorycellsaretypicallyaugmentedtocontainavector,ratherthan\n4 1 8\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nthesinglescalarstoredbyanLSTMorGRUmemorycell.Therearetworeasons\ntoincreasethesizeofthememorycell.Onereasonisthatwehaveincreasedthe\ncostofaccessingamemorycell.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1035, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 282}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1036_9d7c72d1", "text": "Wepaythecomputational costofproducinga\ncoeﬃcientformanycells,butweexpectthesecoeﬃcientstoclusteraroundasmall\nnumberofcells.Byreadingavectorvalue,ratherthanascalarvalue,wecan\noﬀsetsomeofthiscost.Anotherreasontousevector-valuedmemorycellsisthat\ntheyallowforcontent-basedaddressing,wheretheweightusedtoreadtoor\nwritefromacellisafunctionofthatcell.Vector-valuedcellsallowustoretrievea\ncompletevector-valuedmemoryifweareabletoproduceapatternthatmatches\nsomebutnotallofitselements.Thisisanalogoustothewaythatpeoplecan\nrecallthelyricsofasongbasedonafewwords.Wecanthinkofacontent-based\nreadinstructionassaying,“Retrievethelyricsofthesongthathasthechorus‘We\nallliveinayellowsubmarine.’”Content-basedaddressingismoreusefulwhenwe\nmaketheobjectstoberetrievedlarge—ifeveryletterofthesongwasstoredina\nseparatememorycell,wewouldnotbeabletoﬁndthemthisway.Bycomparison,\nlocation-basedaddressingisnotallowedtorefertothecontentofthememory.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1036, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 920}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1037_dfdc296d", "text": "Wecanthinkofalocation-basedreadinstructionassaying“Retrievethelyricsof\nthesonginslot347.”Location-basedaddressingcanoftenbeaperfectlysensible\nmechanismevenwhenthememorycellsaresmall. Ifthecontentofamemorycelliscopied(notforgotten)atmosttimesteps,then\ntheinformationitcontainscanbepropagatedforwardintimeandthegradients\npropagatedbackwardintimewithouteithervanishingorexploding. Theexplicitmemoryapproachisillustratedinﬁgure,whereweseethat 10.18\na“taskneuralnetwork” iscoupledwithamemory.Althoughthattaskneural\nnetworkcouldbefeedforwardorrecurrent,theoverallsystemisarecurrentnetwork. Thetasknetworkcanchoosetoreadfromorwritetospeciﬁcmemoryaddresses. ExplicitmemoryseemstoallowmodelstolearntasksthatordinaryRNNsorLSTM\nRNNscannotlearn.Onereasonforthisadvantagemaybebecauseinformationand\ngradientscanbepropagated(forwardintimeorbackwardsintime,respectively)\nforverylongdurations.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1037, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 876}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1038_ab704705", "text": "Asanalternativetoback-propagationthroughweightedaveragesofmemory\ncells,wecaninterpretthememoryaddressingcoeﬃcientsasprobabilities and\nstochasticallyreadjustonecell(ZarembaandSutskever2015,).Optimizingmodels\nthatmakediscretedecisionsrequiresspecializedoptimization algorithms,described\ninsection.Sofar,trainingthesestochasticarchitectures thatmakediscrete 20.9.1\ndecisionsremainsharderthantrainingdeterministicalgorithmsthatmakesoft\ndecisions. Whetheritissoft(allowingback-propagation) orstochasticandhard,the\n4 1 9\nCHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nmechanism forchoosing anaddress isin itsform identical totheattention\nmechanismwhichhadbeenpreviouslyintroducedinthecontextofmachine\ntranslation( ,)anddiscussedinsection.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1038, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 738}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1039_9fb2aa84", "text": "Theidea Bahdanau e t a l .2015 12.4.5.1\nofattentionmechanismsforneuralnetworkswasintroducedevenearlier,inthe\ncontextofhandwritinggeneration(Graves2013,),withanattentionmechanism\nthatwasconstrainedtomoveonlyforwardintimethroughthesequence.In\nthecaseofmachinetranslationandmemorynetworks,ateachstep,thefocusof\nattentioncanmovetoacompletelydiﬀerentplace,comparedtothepreviousstep. Recurrentneuralnetworksprovideawaytoextenddeeplearningtosequential\ndata.Theyarethelastmajortoolinourdeeplearningtoolbox.Ourdiscussionnow\nmovestohowtochooseandusethesetoolsandhowtoapplythemtoreal-world\ntasks.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1039, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 585}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1040_5952d1e7", "text": "4 2 0\nC h a p t e r 1 1\nPractical Methodology\nSuccessfullyapplyingdeeplearningtechniquesrequiresmorethanjustagood\nknowledgeofwhatalgorithmsexistandtheprinciplesthatexplainhowthey\nwork.Agoodmachinelearningpractitioneralsoneedstoknowhowtochoosean\nalgorithmforaparticularapplicationandhowtomonitorandrespondtofeedback\nobtainedfromexperimentsinordertoimproveamachinelearningsystem.During\ndaytodaydevelopmentofmachinelearningsystems,practitioners needtodecide\nwhethertogathermoredata,increaseordecreasemodelcapacity,addorremove\nregularizingfeatures,improvetheoptimization ofamodel,improveapproximate\ninferenceinamodel,ordebugthesoftwareimplementationofthemodel.Allof\ntheseoperationsareattheveryleasttime-consuming totryout,soitisimportant\ntobeabletodeterminetherightcourseofactionratherthanblindlyguessing.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1040, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 801}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1041_c2e76cda", "text": "Mostofthisbookisaboutdiﬀerentmachinelearningmodels,trainingalgo-\nrithms,andobjectivefunctions.Thismaygivetheimpressionthatthemost\nimportantingredienttobeingamachinelearningexpertisknowingawidevariety\nofmachinelearningtechniquesandbeinggoodatdiﬀerentkindsofmath.Inprac-\ntice,onecanusuallydomuchbetterwithacorrectapplicationofacommonplace\nalgorithmthanbysloppilyapplyinganobscurealgorithm.Correctapplicationof\nanalgorithmdependsonmasteringsomefairlysimplemethodology.Manyofthe\nrecommendations inthischapterareadaptedfrom().Ng2015\nWerecommendthefollowingpracticaldesignprocess:\n•Determineyourgoals—whaterrormetrictouse,andyourtargetvaluefor\nthiserrormetric.Thesegoalsanderrormetricsshouldbedrivenbythe\nproblemthattheapplicationisintendedtosolve. •Establishaworkingend-to-endpipelineassoonaspossible,includingthe\n421\nCHAPTER11.PRACTICALMETHODOLOGY\nestimationoftheappropriateperformancemetrics.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1041, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 889}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1042_763efb7b", "text": "•Establishaworkingend-to-endpipelineassoonaspossible,includingthe\n421\nCHAPTER11.PRACTICALMETHODOLOGY\nestimationoftheappropriateperformancemetrics. •Instrumentthesystemwelltodeterminebottlenecksinperformance.Diag-\nnosewhichcomponentsareperformingworsethanexpectedandwhetherit\nisduetooverﬁtting,underﬁtting, oradefectinthedataorsoftware. •Repeatedlymakeincrementalchangessuchasgatheringnewdata,adjusting\nhyperparameters,orchangingalgorithms,basedonspeciﬁcﬁndingsfrom\nyourinstrumentation. Asarunningexample,wewilluseStreetViewaddressnumbertranscription\nsystem( ,).Thepurposeofthisapplicationistoadd Goodfellow etal.2014d\nbuildingstoGoogleMaps.StreetViewcarsphotographthebuildingsandrecord\ntheGPScoordinatesassociatedwitheachphotograph. Aconvolutionalnetwork\nrecognizestheaddressnumberineachphotograph, allowingtheGoogleMaps\ndatabasetoaddthataddressinthecorrectlocation.Thestoryofhowthis\ncommercialapplicationwasdevelopedgivesanexampleofhowtofollowthedesign\nmethodologyweadvocate.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1042, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 976}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1043_3866f904", "text": "Wenowdescribeeachofthestepsinthisprocess. 11.1PerformanceMetrics\nDeterminingyourgoals,intermsofwhicherrormetrictouse,isanecessaryﬁrst\nstepbecauseyourerrormetricwillguideallofyourfutureactions. Youshould\nalsohaveanideaofwhatlevelofperformanceyoudesire. Keepinmindthatformostapplications,itisimpossibletoachieveabsolute\nzeroerror.TheBayeserrordeﬁnestheminimumerrorratethatyoucanhopeto\nachieve,evenifyouhaveinﬁnitetrainingdataandcanrecoverthetrueprobability\ndistribution.This isbecause your inputfeatures maynot contain complete\ninformationabouttheoutputvariable,orbecausethesystemmightbeintrinsically\nstochastic.Youwillalsobelimitedbyhavingaﬁniteamountoftrainingdata.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1043, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 665}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1044_d7fbe71b", "text": "Theamountoftrainingdatacanbelimitedforavarietyofreasons.Whenyour\ngoalistobuildthebestpossiblereal-worldproductorservice,youcantypically\ncollectmoredatabutmustdeterminethevalueofreducingerrorfurtherandweigh\nthisagainstthecostofcollectingmoredata.Datacollectioncanrequiretime,\nmoney,orhumansuﬀering(forexample,ifyourdatacollectionprocessinvolves\nperforminginvasivemedicaltests).Whenyourgoalistoanswerascientiﬁcquestion\naboutwhichalgorithmperformsbetteronaﬁxedbenchmark,thebenchmark\n4 2 2\nCHAPTER11.PRACTICALMETHODOLOGY\nspeciﬁcationusuallydeterminesthetrainingsetandyouarenotallowedtocollect\nmoredata.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1044, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 598}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1045_d6a4fa6b", "text": "Howcanonedetermineareasonablelevelofperformancetoexpect?Typically,\nintheacademicsetting,wehavesomeestimateoftheerrorratethatisattainable\nbasedonpreviouslypublishedbenchmarkresults.Inthereal-wordsetting,we\nhavesomeideaoftheerrorratethatisnecessaryforanapplicationtobesafe,\ncost-eﬀective,orappealingtoconsumers.Onceyouhavedeterminedyourrealistic\ndesirederrorrate,yourdesigndecisionswillbeguidedbyreachingthiserrorrate. Anotherimportantconsiderationbesidesthetargetvalueoftheperformance\nmetricisthechoiceofwhichmetrictouse.Severaldiﬀerentperformancemetrics\nmaybeusedtomeasuretheeﬀectivenessofacompleteapplicationthatincludes\nmachinelearningcomponents.Theseperformancemetricsareusuallydiﬀerent\nfromthecostfunctionusedtotrainthemodel.Asdescribedinsection,itis5.1.2\ncommontomeasuretheaccuracy,orequivalently,theerrorrate,ofasystem. However,manyapplicationsrequiremoreadvancedmetrics. Sometimesitismuchmorecostlytomakeonekindofamistakethananother.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1045, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 940}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1046_c12fd166", "text": "However,manyapplicationsrequiremoreadvancedmetrics. Sometimesitismuchmorecostlytomakeonekindofamistakethananother. Forexample,ane-mailspamdetectionsystemcanmaketwokindsofmistakes:\nincorrectlyclassifyingalegitimatemessageasspam,andincorrectlyallowinga\nspammessagetoappearintheinbox.Itismuchworsetoblockalegitimate\nmessagethantoallowaquestionablemessagetopassthrough.Ratherthan\nmeasuringtheerrorrateofaspamclassiﬁer,wemaywishtomeasuresomeform\noftotalcost,wherethecostofblockinglegitimatemessagesishigherthanthecost\nofallowingspammessages.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1046, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 536}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1047_a2c95313", "text": "Sometimeswewishtotrainabinaryclassiﬁerthatisintendedtodetectsome\nrareevent.Forexample,wemightdesignamedicaltestforararedisease.Suppose\nthatonlyoneineverymillionpeoplehasthisdisease.Wecaneasilyachieve\n99.9999%accuracyonthedetectiontask,bysimplyhard-codingtheclassiﬁer\ntoalwaysreportthatthediseaseisabsent.Clearly,accuracyisapoorwayto\ncharacterizetheperformanceofsuchasystem.Onewaytosolvethisproblemis\ntoinsteadmeasure pr e c i si o nand r e c al l.Precisionisthefractionofdetections\nreportedbythemodelthatwerecorrect,whilerecallisthefractionoftrueevents\nthatweredetected.Adetectorthatsaysnoonehasthediseasewouldachieve\nperfectprecision,butzerorecall.Adetectorthatsayseveryonehasthedisease\nwouldachieveperfectrecall,butprecisionequaltothepercentageofpeoplewho\nhavethedisease(0.0001%inourexampleofadiseasethatonlyonepeopleina\nmillionhave).Whenusingprecisionandrecall,itiscommontoplota P R c ur v e,\nwithprecisiononthe y-axisandrecallonthe x-axis.Theclassiﬁergeneratesascore\nthatishigheriftheeventtobedetectedoccurred.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1047, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1014}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1048_879636cd", "text": "Forexample,afeedforward\n4 2 3\nCHAPTER11.PRACTICALMETHODOLOGY\nnetworkdesignedtodetectadiseaseoutputs ˆ y= P( y=1| x),estimatingthe\nprobabilitythatapersonwhosemedicalresultsaredescribedbyfeatures xhas\nthedisease.Wechoosetoreportadetectionwheneverthisscoreexceedssome\nthreshold. Byvaryingthethreshold,wecantradeprecisionforrecall. Inmany\ncases,wewishtosummarizetheperformanceoftheclassiﬁerwithasinglenumber\nratherthanacurve.Todoso,wecanconvertprecision pandrecall rintoan\nF-scor egivenby\nF=2 pr\np r+. (11.1)\nAnotheroptionistoreportthetotalarealyingbeneaththePRcurve.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1048, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 563}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1049_95b9b4fd", "text": "(11.1)\nAnotheroptionistoreportthetotalarealyingbeneaththePRcurve. Insomeapplications,itispossibleforthemachinelearningsystemtorefuseto\nmakeadecision.Thisisusefulwhenthemachinelearningalgorithmcanestimate\nhowconﬁdentitshouldbeaboutadecision,especiallyifawrongdecisioncan\nbeharmfulandifahumanoperatorisabletooccasionallytakeover.TheStreet\nViewtranscriptionsystemprovidesanexampleofthissituation.Thetaskisto\ntranscribetheaddressnumberfromaphotographinordertoassociatethelocation\nwherethephotowastakenwiththecorrectaddressinamap.Becausethevalue\nofthemapdegradesconsiderablyifthemapisinaccurate,itisimportanttoadd\nanaddressonlyifthetranscriptioniscorrect.Ifthemachinelearningsystem\nthinksthatitislesslikelythanahumanbeingtoobtainthecorrecttranscription,\nthenthebestcourseofactionistoallowahumantotranscribethephotoinstead.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1049, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 817}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1050_e45c9e61", "text": "Ofcourse,themachinelearningsystemisonlyusefulifitisabletodramatically\nreducetheamountofphotosthatthehumanoperatorsmustprocess.Anatural\nperformancemetrictouseinthissituationis c o v e r age.Coverageisthefraction\nofexamplesforwhichthemachinelearningsystemisabletoproducearesponse. Itispossibletotradecoverageforaccuracy.Onecanalwaysobtain100%accuracy\nbyrefusingtoprocessanyexample,butthisreducesthecoverageto0%.Forthe\nStreetViewtask,thegoalfortheprojectwastoreachhuman-leveltranscription\naccuracywhilemaintaining95%coverage.Human-levelperformanceonthistask\nis98%accuracy. Manyothermetricsarepossible.Wecanforexample,measureclick-through\nrates,collectusersatisfactionsurveys,andsoon. Manyspecializedapplication\nareashaveapplication-speciﬁccriteriaaswell. Whatisimportantistodeterminewhichperformancemetrictoimproveahead\noftime,thenconcentrateonimprovingthismetric.Withoutclearlydeﬁnedgoals,\nitcanbediﬃculttotellwhetherchangestoamachinelearningsystemmake\nprogressornot.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1050, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 965}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1051_180f83d4", "text": "4 2 4\nCHAPTER11.PRACTICALMETHODOLOGY\n11.2DefaultBaselineModels\nAfterchoosingperformancemetricsandgoals, thenextstepinanypractical\napplicationistoestablishareasonableend-to-endsystemassoonaspossible.In\nthissection,weproviderecommendations forwhichalgorithmstouseastheﬁrst\nbaselineapproachinvarioussituations.Keepinmindthatdeeplearningresearch\nprogressesquickly,sobetterdefaultalgorithmsarelikelytobecomeavailablesoon\nafterthiswriting. Dependingonthecomplexityofyourproblem,youmayevenwanttobegin\nwithoutusingdeeplearning.Ifyourproblemhasachanceofbeingsolvedby\njustchoosingafewlinearweightscorrectly,youmaywanttobeginwithasimple\nstatisticalmodellikelogisticregression. Ifyouknowthatyourproblemfallsintoan“AI-complete”categorylikeobject\nrecognition,speechrecognition,machinetranslation,andsoon,thenyouarelikely\ntodowellbybeginningwithanappropriatedeeplearningmodel.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1051, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 861}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1052_e2bab8a4", "text": "Ifyouknowthatyourproblemfallsintoan“AI-complete”categorylikeobject\nrecognition,speechrecognition,machinetranslation,andsoon,thenyouarelikely\ntodowellbybeginningwithanappropriatedeeplearningmodel. First,choosethegeneralcategoryofmodelbasedonthestructureofyour\ndata.Ifyouwanttoperformsupervisedlearningwithﬁxed-sizevectorsasinput,\nuseafeedforwardnetworkwithfullyconnectedlayers.Iftheinputhasknown\ntopologicalstructure(forexample,iftheinputisanimage),useaconvolutional\nnetwork.Inthesecases,youshouldbeginbyusingsomekindofpiecewiselinear\nunit(ReLUsortheirgeneralizations likeLeakyReLUs,PreLusandmaxout).If\nyourinputoroutputisasequence,useagatedrecurrentnet(LSTMorGRU).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1052, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 664}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1053_05489b92", "text": "Areasonablechoiceofoptimization algorithmisSGDwithmomentumwitha\ndecayinglearningrate(populardecayschemesthatperformbetterorworseon\ndiﬀerentproblemsincludedecayinglinearlyuntilreachingaﬁxedminimumlearning\nrate,decayingexponentially,ordecreasingthelearningratebyafactorof2-10\neachtimevalidationerrorplateaus).AnotherveryreasonablealternativeisAdam. Batchnormalization canhaveadramaticeﬀectonoptimization performance,\nespeciallyforconvolutionalnetworksandnetworkswithsigmoidalnonlinearities. Whileitisreasonabletoomitbatchnormalization fromtheveryﬁrstbaseline,it\nshouldbeintroducedquicklyifoptimization appearstobeproblematic.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1053, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 623}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1054_22fb8730", "text": "Whileitisreasonabletoomitbatchnormalization fromtheveryﬁrstbaseline,it\nshouldbeintroducedquicklyifoptimization appearstobeproblematic. Unlessyourtrainingsetcontainstensofmillionsofexamplesormore,you\nshouldincludesomemildformsofregularizationfromthestart.Earlystopping\nshouldbeusedalmostuniversally.Dropoutisanexcellentregularizerthatiseasy\ntoimplementandcompatiblewithmanymodelsandtrainingalgorithms.Batch\nnormalization alsosometimesreducesgeneralization errorandallowsdropoutto\nbeomitted,duetothenoiseintheestimateofthestatisticsusedtonormalize\neachvariable. 4 2 5\nCHAPTER11.PRACTICALMETHODOLOGY\nIfyourtaskissimilartoanothertaskthathasbeenstudiedextensively,you\nwillprobablydowellbyﬁrstcopyingthemodelandalgorithmthatisalready\nknowntoperformbestonthepreviouslystudiedtask.Youmayevenwanttocopy\natrainedmodelfromthattask.Forexample,itiscommontousethefeatures\nfromaconvolutionalnetworktrainedonImageNettosolveothercomputervision\ntasks( ,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1054, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 937}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1055_d23feb1c", "text": "Girshicketal.2015\nAcommonquestioniswhethertobeginbyusingunsupervisedlearning,de-\nscribedfurtherinpart.Thisissomewhatdomainspeciﬁc.Somedomains,such III\nasnaturallanguageprocessing,areknowntobeneﬁttremendouslyfromunsuper-\nvisedlearningtechniquessuchaslearningunsupervisedwordembeddings.Inother\ndomains,suchascomputervision,currentunsupervisedlearningtechniquesdo\nnotbringabeneﬁt,exceptinthesemi-supervisedsetting,whenthenumberof\nlabeledexamplesisverysmall( ,; Kingma etal.2014Rasmus2015etal.,).Ifyour\napplicationisinacontextwhereunsupervisedlearningisknowntobeimportant,\nthenincludeitinyourﬁrstend-to-endbaseline.Otherwise,onlyuseunsupervised\nlearninginyourﬁrstattemptifthetaskyouwanttosolveisunsupervised.You\ncanalwaystryaddingunsupervisedlearninglaterifyouobservethatyourinitial\nbaselineoverﬁts.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1055, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 795}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1056_2e39aa94", "text": "11.3DeterminingWhethertoGatherMoreData\nAftertheﬁrstend-to-endsystemisestablished,itistimetomeasuretheperfor-\nmanceofthealgorithmanddeterminehowtoimproveit.Manymachinelearning\nnovicesaretemptedtomakeimprovementsbytryingoutmanydiﬀerentalgorithms. However,itisoftenmuchbettertogathermoredatathantoimprovethelearning\nalgorithm. Howdoesonedecidewhethertogathermoredata?First,determinewhether\ntheperformanceonthetrainingsetisacceptable.Ifperformanceonthetraining\nsetispoor,thelearningalgorithmisnotusingthetrainingdatathatisalready\navailable,sothereisnoreasontogathermoredata.Instead,tryincreasingthe\nsizeofthemodelbyaddingmorelayersoraddingmorehiddenunitstoeachlayer. Also,tryimprovingthelearningalgorithm,forexamplebytuningthelearning\nratehyperparameter.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1056, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 750}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1057_d8caa52a", "text": "Also,tryimprovingthelearningalgorithm,forexamplebytuningthelearning\nratehyperparameter. Iflargemodelsandcarefullytunedoptimization algorithms\ndonotworkwell,thentheproblemmightbetheofthetrainingdata.The quality\ndatamaybetoonoisyormaynotincludetherightinputsneededtopredictthe\ndesiredoutputs.Thissuggestsstartingover,collectingcleanerdataorcollectinga\nrichersetoffeatures. Iftheperformanceonthetrainingsetisacceptable,thenmeasuretheper-\n4 2 6\nCHAPTER11.PRACTICALMETHODOLOGY\nformanceonatestset.Iftheperformanceonthetestsetisalsoacceptable,\nthenthereisnothinglefttobedone.Iftestsetperformanceismuchworsethan\ntrainingsetperformance,thengatheringmoredataisoneofthemosteﬀective\nsolutions. Thekeyconsiderationsarethecostandfeasibilityofgatheringmore\ndata,thecostandfeasibilityofreducingthetesterrorbyothermeans,andthe\namountofdatathatisexpectedtobenecessarytoimprovetestsetperformance\nsigniﬁcantly.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1057, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 890}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1058_0ab6c603", "text": "Atlargeinternetcompanieswithmillionsorbillionsofusers,itis\nfeasibletogatherlargedatasets,andtheexpenseofdoingsocanbeconsiderably\nlessthantheotheralternatives,sotheanswerisalmostalwaystogathermore\ntrainingdata.Forexample,thedevelopmentoflargelabeleddatasetswasoneof\nthemostimportantfactorsinsolvingobjectrecognition.Inothercontexts,suchas\nmedicalapplications,itmaybecostlyorinfeasibletogathermoredata.Asimple\nalternativetogatheringmoredataistoreducethesizeofthemodelorimprove\nregularization, byadjustinghyperparameters suchasweightdecaycoeﬃcients,\norbyaddingregularizationstrategiessuchasdropout.Ifyouﬁndthatthegap\nbetweentrainandtestperformanceisstillunacceptable evenaftertuningthe\nregularizationhyperparameters ,thengatheringmoredataisadvisable.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1058, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 747}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1059_4ef51fd3", "text": "Whendecidingwhethertogathermoredata,itisalsonecessarytodecide\nhowmuchtogather.Itishelpfultoplotcurvesshowingtherelationshipbetween\ntrainingsetsizeandgeneralization error,likeinﬁgure.Byextrapolatingsuch 5.4\ncurves,onecanpredicthowmuchadditionaltrainingdatawouldbeneededto\nachieveacertainlevelofperformance.Usually,addingasmallfractionofthetotal\nnumberofexampleswillnothaveanoticeableimpactongeneralization error.Itis\nthereforerecommendedtoexperimentwithtrainingsetsizesonalogarithmicscale,\nforexampledoublingthenumberofexamplesbetweenconsecutiveexperiments. Ifgatheringmuchmoredataisnotfeasible,theonlyotherwaytoimprove\ngeneralization erroristoimprovethelearningalgorithmitself.Thisbecomesthe\ndomainofresearchandnotthedomainofadviceforappliedpractitioners.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1059, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 755}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1060_36476bdf", "text": "Ifgatheringmuchmoredataisnotfeasible,theonlyotherwaytoimprove\ngeneralization erroristoimprovethelearningalgorithmitself.Thisbecomesthe\ndomainofresearchandnotthedomainofadviceforappliedpractitioners. 11.4SelectingHyperparameters\nMostdeeplearningalgorithmscomewithmanyhyperparametersthatcontrolmany\naspectsofthealgorithm’sbehavior.Someofthesehyperparametersaﬀectthetime\nandmemorycostofrunningthealgorithm.Someofthesehyperparameters aﬀect\nthequalityofthemodelrecoveredbythetrainingprocessanditsabilitytoinfer\ncorrectresultswhendeployedonnewinputs. Therearetwobasicapproachestochoosingthesehyperparameters :choosing\nthemmanuallyandchoosingthemautomatically .Choosingthehyperparameters\n4 2 7\nCHAPTER11.PRACTICALMETHODOLOGY\nmanuallyrequiresunderstandingwhatthehyperparametersdoandhowmachine\nlearningmodelsachievegoodgeneralization. Automatichyperparameterselection\nalgorithmsgreatlyreducetheneedtounderstandtheseideas,buttheyareoften\nmuchmorecomputationally costly. 1 1 . 4 .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1060, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 969}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1061_d212186f", "text": "Automatichyperparameterselection\nalgorithmsgreatlyreducetheneedtounderstandtheseideas,buttheyareoften\nmuchmorecomputationally costly. 1 1 . 4 . 1 Ma n u a l Hyp erp a ra m et er T u n i n g\nTosethyperparameters manually,onemustunderstandtherelationshipbetween\nhyperparameters,trainingerror,generalization errorandcomputational resources\n(memoryandruntime).Thismeansestablishingasolidfoundationonthefun-\ndamentalideasconcerningtheeﬀectivecapacityofalearningalgorithmfrom\nchapter.5\nThegoalofmanualhyperparametersearchisusuallytoﬁndthelowestgeneral-\nizationerrorsubjecttosomeruntimeandmemorybudget.Wedonotdiscusshow\ntodeterminetheruntimeandmemoryimpactofvarioushyperparametershere\nbecausethisishighlyplatform-dependent.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1061, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 716}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1062_2b615f83", "text": "Theprimarygoalofmanualhyperparametersearchistoadjusttheeﬀective\ncapacityofthemodeltomatchthecomplexityofthetask.Eﬀectivecapacity\nisconstrainedbythreefactors: therepresentationalcapacityofthemodel,the\nabilityofthelearningalgorithmtosuccessfullyminimizethecostfunctionusedto\ntrainthemodel,andthedegreetowhichthecostfunctionandtrainingprocedure\nregularizethemodel.Amodelwithmorelayersandmorehiddenunitsperlayerhas\nhigherrepresentationalcapacity—itiscapableofrepresentingmorecomplicated\nfunctions.Itcannotnecessarilyactuallylearnallofthesefunctionsthough,if\nthetrainingalgorithmcannotdiscoverthatcertainfunctionsdoagoodjobof\nminimizingthetrainingcost,orifregularizationtermssuchasweightdecayforbid\nsomeofthesefunctions. Thegeneralization errortypicallyfollowsaU-shapedcurvewhenplottedas\nafunctionofoneofthehyperparameters ,asinﬁgure.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1062, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 829}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1063_bdd82c80", "text": "Thegeneralization errortypicallyfollowsaU-shapedcurvewhenplottedas\nafunctionofoneofthehyperparameters ,asinﬁgure. Atoneextreme,the 5.3\nhyperparametervaluecorrespondstolowcapacity,andgeneralization errorishigh\nbecausetrainingerrorishigh.Thisistheunderﬁttingregime.Attheotherextreme,\nthehyperparameter valuecorrespondstohighcapacity,andthegeneralization\nerrorishighbecausethegapbetweentrainingandtesterrorishigh.Somewhere\ninthemiddleliestheoptimalmodelcapacity,whichachievesthelowestpossible\ngeneralization error,byaddingamediumgeneralization gaptoamediumamount\noftrainingerror. Forsomehyperparameters,overﬁttingoccurswhenthevalueofthehyper-\nparameterislarge. Thenumberofhiddenunitsinalayerisonesuchexample,\n4 2 8\nCHAPTER11.PRACTICALMETHODOLOGY\nbecauseincreasingthenumberofhiddenunitsincreasesthecapacityofthemodel.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1063, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 813}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1064_4e86036a", "text": "Thenumberofhiddenunitsinalayerisonesuchexample,\n4 2 8\nCHAPTER11.PRACTICALMETHODOLOGY\nbecauseincreasingthenumberofhiddenunitsincreasesthecapacityofthemodel. Forsomehyperparameters ,overﬁttingoccurswhenthevalueofthehyperparame-\nterissmall.Forexample,thesmallestallowableweightdecaycoeﬃcientofzero\ncorrespondstothegreatesteﬀectivecapacityofthelearningalgorithm. NoteveryhyperparameterwillbeabletoexploretheentireU-shapedcurve.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1064, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 423}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1065_fe23b4dd", "text": "NoteveryhyperparameterwillbeabletoexploretheentireU-shapedcurve. Manyhyperparameters arediscrete,suchasthenumberofunitsinalayerorthe\nnumberoflinearpiecesinamaxoutunit,soitisonlypossibletovisitafewpoints\nalongthecurve.Somehyperparametersarebinary.Usuallythesehyperparameters\nareswitchesthat specify whetherornotto usesomeoptionalcomponentof\nthelearningalgorithm,suchasapreprocessingstepthatnormalizestheinput\nfeaturesbysubtractingtheirmeananddividingbytheirstandarddeviation.These\nhyperparameterscanonlyexploretwopointsonthecurve.Otherhyperparameters\nhavesomeminimumormaximumvaluethatpreventsthemfromexploringsome\npartofthecurve.Forexample,theminimumweightdecaycoeﬃcientiszero.This\nmeansthatifthemodelisunderﬁttingwhenweightdecayiszero,wecannotenter\ntheoverﬁttingregionbymodifyingtheweightdecaycoeﬃcient.Inotherwords,\nsomehyperparameters canonlysubtractcapacity. Thelearningrateisperhapsthemostimportanthyperparameter. Ifyou\nhave timeto tuneonly onehyperparameter, tune thelearning rate.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1065, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1066_596614c4", "text": "Thelearningrateisperhapsthemostimportanthyperparameter. Ifyou\nhave timeto tuneonly onehyperparameter, tune thelearning rate. It con-\ntrolstheeﬀectivecapacityofthemodelinamorecomplicatedwaythanother\nhyperparameters—theeﬀectivecapacityofthemodelishighestwhenthelearning\nrateiscorrectfortheoptimizationproblem,notwhenthelearningrateisespecially\nlargeorespeciallysmall.ThelearningratehasaU-shapedcurvefortrainingerror,\nillustratedinﬁgure.Whenthelearningrateistoolarge,gradientdescent 11.1\ncaninadvertentlyincreaseratherthandecreasethetrainingerror.Intheidealized\nquadraticcase,thisoccursifthelearningrateisatleasttwiceaslargeasits\noptimalvalue( ,).Whenthelearningrateistoosmall,training LeCunetal.1998a\nisnotonlyslower,butmaybecomepermanentlystuckwithahightrainingerror. Thiseﬀectispoorlyunderstood(itwouldnothappenforaconvexlossfunction).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1066, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 835}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1067_5cefaa60", "text": "Thiseﬀectispoorlyunderstood(itwouldnothappenforaconvexlossfunction). Tuningtheparametersotherthanthelearningraterequiresmonitoringboth\ntrainingandtesterrortodiagnosewhetheryourmodelisoverﬁttingorunderﬁtting,\nthenadjustingitscapacityappropriately . Ifyourerroronthetrainingsetishigherthanyourtargeterrorrate,youhave\nnochoicebuttoincreasecapacity.Ifyouarenotusingregularizationandyouare\nconﬁdentthatyouroptimization algorithmisperformingcorrectly,thenyoumust\naddmorelayerstoyournetworkoraddmorehiddenunits.Unfortunately,this\nincreasesthecomputational costsassociatedwiththemodel.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1067, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 577}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1068_4da28ec0", "text": "Ifyourerroronthetestsetishigherthanthanyourtargeterrorrate,youcan\n4 2 9\nCHAPTER11.PRACTICALMETHODOLOGY\n1 0− 21 0− 11 00\nL e a r ni ng r a t e ( l o g a r i t hm i c s c a l e )012345678T r a i ni ng e r r o r\nFigure11.1:Typicalrelationshipbetweenthelearningrateandthetrainingerror.Notice\nthesharpriseinerrorwhenthelearningisaboveanoptimalvalue.Thisisforaﬁxed\ntrainingtime,asasmallerlearningratemaysometimesonlyslowdowntrainingbya\nfactorproportionaltothelearningratereduction. Generalizationerrorcanfollowthis\ncurveorbecomplicatedbyregularizationeﬀectsarisingoutofhavingatoolargeor\ntoosmalllearningrates,sincepooroptimizationcan,tosomedegree,reduceorprevent\noverﬁtting,andevenpointswithequivalenttrainingerrorcanhavediﬀerentgeneralization\nerror.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1068, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 744}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1069_8804c8af", "text": "nowtaketwokindsofactions.Thetesterroristhesumofthetrainingerrorand\nthegapbetweentrainingandtesterror.Theoptimaltesterrorisfoundbytrading\noﬀthesequantities.Neuralnetworkstypicallyperformbestwhenthetraining\nerrorisverylow(andthus,whencapacityishigh)andthetesterrorisprimarily\ndrivenbythegapbetweentrainandtesterror. Yourgoalistoreducethisgap\nwithoutincreasingtrainingerrorfasterthanthegapdecreases.Toreducethegap,\nchangeregularizationhyperparameters toreduceeﬀectivemodelcapacity,suchas\nbyaddingdropoutorweightdecay.Usuallythebestperformancecomesfroma\nlargemodelthatisregularizedwell,forexamplebyusingdropout.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1069, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 607}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1070_6435182b", "text": "Mosthyperparameters canbesetbyreasoningaboutwhethertheyincreaseor\ndecreasemodelcapacity.SomeexamplesareincludedinTable.11.1\nWhilemanuallytuninghyperparameters,donotlosesightofyourendgoal:\ngoodperformanceonthetestset.Addingregularizationisonlyonewaytoachieve\nthisgoal.Aslongasyouhavelowtrainingerror,youcanalwaysreducegeneral-\nizationerrorbycollectingmoretrainingdata.Thebruteforcewaytopractically\nguaranteesuccessistocontinuallyincreasemodelcapacityandtrainingsetsize\nuntilthetaskissolved.Thisapproachdoesofcourseincreasethecomputational\ncostoftrainingandinference,soitisonlyfeasiblegivenappropriateresources.In\n4 3 0\nCHAPTER11.PRACTICALMETHODOLOGY\nHyperparameterIncreases\ncapacity\nwhen...Reason Caveats\nNumberofhid-\ndenunitsincreasedIncreasingthenumberof\nhiddenunitsincreasesthe\nrepresentationalcapacity\nofthemodel.Increasingthenumber\nofhiddenunits increases\nboththetimeandmemory\ncostofessentiallyeveryop-\nerationonthemodel.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1070, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 925}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1071_48fb6bae", "text": "Learningratetunedop-\ntimallyAnimproperlearningrate,\nwhether toohigh ortoo\nlow,resultsinamodel\nwithloweﬀectivecapacity\nduetooptimizationfailure\nConvolutionker-\nnelwidthincreasedIncreasingthekernelwidth\nincreasesthenumberofpa-\nrametersinthemodelAwiderkernelresultsin\nanarroweroutputdimen-\nsion,reducingmodelca-\npacityunlessyouuseim-\nplicitzeropaddingtore-\nducethiseﬀect.Wider\nkernelsrequiremoremem-\noryforparameterstorage\nandincreaseruntime,but\nanarroweroutputreduces\nmemorycost. Implicitzero\npaddingincreasedAddingimplicitzerosbe-\nforeconvolutionkeepsthe\nrepresentationsizelargeIncreasedtimeandmem-\norycostofmostopera-\ntions. Weightdecayco-\neﬃcientdecreasedDecreasingtheweightde-\ncaycoeﬃcientfreesthe\nmodelparameterstobe-\ncomelarger\nDropoutratedecreasedDroppingunitslessoften\ngivestheunitsmoreoppor-\ntunitiesto“conspire”with\neachothertoﬁtthetrain-\ningset\nTable11.1:Theeﬀectofvarioushyperparametersonmodelcapacity.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1071, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 912}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1072_cd5693f3", "text": "4 3 1\nCHAPTER11.PRACTICALMETHODOLOGY\nprinciple,thisapproachcouldfailduetooptimization diﬃculties,butformany\nproblemsoptimization doesnotseemtobeasigniﬁcantbarrier,providedthatthe\nmodelischosenappropriately . 1 1 . 4 .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1072, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 217}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1073_d31378c6", "text": "1 1 . 4 . 2 A u t o m a t i c Hyp erp a ra m et er O p t i m i za t i o n A l g o ri t h m s\nTheideallearningalgorithmjusttakesadatasetandoutputsafunction,without\nrequiringhand-tuning ofhyperparameters .Thepopularityofseverallearning\nalgorithmssuchaslogisticregressionandSVMsstemsinpartfromtheirabilityto\nperformwellwithonlyoneortwotunedhyperparameters .Neuralnetworkscan\nsometimesperformwellwithonlyasmallnumberoftunedhyperparameters ,but\noftenbeneﬁtsigniﬁcantlyfromtuningoffortyormorehyperparameters .Manual\nhyperparametertuningcanworkverywellwhentheuserhasagoodstartingpoint,\nsuchasonedeterminedbyothershavingworkedonthesametypeofapplication\nandarchitecture, orwhentheuserhasmonthsoryearsofexperienceinexploring\nhyperparametervaluesforneuralnetworksappliedtosimilartasks.However,\nformanyapplications,thesestartingpointsarenotavailable.Inthesecases,\nautomatedalgorithmscanﬁndusefulvaluesofthehyperparameters .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1073, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 911}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1074_5e2970e3", "text": "Ifwethinkaboutthewayinwhichtheuserofalearningalgorithmsearchesfor\ngoodvaluesofthehyperparameters ,werealizethatanoptimizationistakingplace:\nwearetryingtoﬁndavalueofthehyperparametersthatoptimizesanobjective\nfunction,suchasvalidationerror,sometimesunderconstraints(suchasabudget\nfortrainingtime,memoryorrecognitiontime).Itisthereforepossible,inprinciple,\nto develop h y p e r par am e t e r   o p t i m i z a t i o nalgorithms thatwrap a learnin g\nalgorithmandchooseitshyperparameters ,thushidingthehyperparameters ofthe\nlearningalgorithmfromtheuser.Unfortunately,hyperparameter optimization\nalgorithmsoftenhavetheirownhyperparameters,suchastherangeofvaluesthat\nshouldbeexploredforeachofthelearningalgorithm’shyperparameters .However,\nthesesecondaryhyperparameters areusuallyeasiertochoose,inthesensethat\nacceptableperformancemaybeachievedonawiderangeoftasksusingthesame\nsecondaryhyperparameters foralltasks. 1 1 . 4 .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1074, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 917}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1075_acabde83", "text": "1 1 . 4 . 3 G ri d S ea rch\nWhentherearethreeorfewerhyperparameters ,thecommonpracticeistoperform\ng r i d se ar c h.Foreachhyperparameter, the userselectsasmallﬁnitesetof\nvaluestoexplore.Thegridsearchalgorithmthentrainsamodelforeveryjoint\nspeciﬁcationofhyperparametervaluesintheCartesianproductofthesetofvalues\nforeachindividualhyperparameter.Theexperimentthatyieldsthebestvalidation\n4 3 2\nCHAPTER11.PRACTICALMETHODOLOGY\nGrid Random\nFigure11.2:Comparisonofgridsearchandrandomsearch.Forillustrationpurposeswe\ndisplaytwohyperparametersbutwearetypicallyinterestedinhavingmanymore.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1075, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 577}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1076_43f2938c", "text": "( L e f t )To\nperformgridsearch,weprovideasetofvaluesforeachhyperparameter.Thesearch\nalgorithmrunstrainingforeveryjointhyperparametersettinginthecrossproductofthese\nsets.Toperformrandomsearch,weprovideaprobabilitydistributionoverjoint ( R i g h t )\nhyperparameterconﬁgurations.Usuallymostofthesehyperparametersareindependent\nfromeachother.Commonchoicesforthedistributionoverasinglehyperparameterinclude\nuniformandlog-uniform(tosamplefromalog-uniformdistribution,taketheexpofa\nsamplefromauniformdistribution).Thesearchalgorithmthenrandomlysamplesjoint\nhyperparameterconﬁgurationsandrunstrainingwitheachofthem.Bothgridsearch\nandrandomsearchevaluatethevalidationseterrorandreturnthebestconﬁguration.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1076, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 696}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1077_b1bd9593", "text": "Theﬁgureillustratesthetypicalcasewhereonlysomehyperparametershaveasigniﬁcant\ninﬂuenceontheresult.Inthisillustration,onlythehyperparameteronthehorizontalaxis\nhasasigniﬁcanteﬀect.Gridsearchwastesanamountofcomputationthatisexponential\ninthenumberofnon-inﬂuentialhyperparameters,whilerandomsearchtestsaunique\nvalueofeveryinﬂuentialhyperparameteronnearlyeverytrial.Figurereproducedwith\npermissionfrom (). BergstraandBengio2012\n4 3 3\nCHAPTER11.PRACTICALMETHODOLOGY\nseterroristhenchosenashavingfoundthebesthyperparameters .Seetheleftof\nﬁgureforanillustrationofagridofhyperparameter values.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1077, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 582}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1078_f6da858d", "text": "BergstraandBengio2012\n4 3 3\nCHAPTER11.PRACTICALMETHODOLOGY\nseterroristhenchosenashavingfoundthebesthyperparameters .Seetheleftof\nﬁgureforanillustrationofagridofhyperparameter values. 11.2\nHowshouldthelistsofvaluestosearchoverbechosen?Inthecaseofnumerical\n(ordered)hyperparameters ,thesmallestandlargestelementofeachlistischosen\nconservatively,basedonpriorexperiencewithsimilarexperiments,tomakesure\nthattheoptimalvalueisverylikelytobeintheselectedrange.Typically,agrid\nsearchinvolvespickingvaluesapproximately onalogarithmicscale,e.g.,alearning\nratetakenwithintheset{ .1 , .01 ,10−3,10−4,10−5},oranumberofhiddenunits\ntakenwiththeset . { } 501002005001000 2000 , , , , ,\nGridsearchusuallyperformsbestwhenitisperformedrepeatedly.Forexample,\nsupposethatweranagridsearchoverahyperparameter αusingvaluesof{−1 ,0 ,1}.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1078, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 811}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1079_fba8a230", "text": "{ } 501002005001000 2000 , , , , ,\nGridsearchusuallyperformsbestwhenitisperformedrepeatedly.Forexample,\nsupposethatweranagridsearchoverahyperparameter αusingvaluesof{−1 ,0 ,1}. Ifthebestvaluefoundis,thenweunderestimatedtherangeinwhichthebest 1 α\nliesandweshouldshiftthegridandrunanothersearchwith αin,forexample,\n{1 ,2 ,3}.Ifweﬁndthatthebestvalueof αis,thenwemaywishtoreﬁneour 0\nestimatebyzoominginandrunningagridsearchover. {− } . , , .101\nTheobviousproblemwithgridsearchisthatitscomputational costgrows\nexponentiallywiththenumberofhyperparameters .Ifthereare mhyperparameters,\neachtakingatmost nvalues,thenthenumberoftrainingandevaluationtrials\nrequiredgrowsas O( nm).Thetrialsmayberuninparallelandexploitloose\nparallelism(withalmostnoneedforcommunication betweendiﬀerentmachines\ncarryingoutthesearch)Unfortunately,duetotheexponentialcostofgridsearch,\nevenparallelization maynotprovideasatisfactorysizeofsearch. 1 1 . 4 .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1079, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 923}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1080_9dad0986", "text": "1 1 . 4 . 4 Ra n d o m S ea rch\nFortunately,thereisanalternativetogridsearchthatisassimpletoprogram,more\nconvenienttouse,andconvergesmuchfastertogoodvaluesofthehyperparameters :\nrandomsearch( ,). BergstraandBengio2012\nArandomsearchproceedsasfollows.Firstwedeﬁneamarginaldistribution\nforeachhyperparameter, e.g.,aBernoulliormultinoulliforbinaryordiscrete\nhyperparameters,orauniformdistributiononalog-scaleforpositivereal-valued\nhyperparameters.Forexample,\nl o g l e a r n i n g r a t e __ ∼−− u(1 ,5) (11.2)\nl e a r n i n g r a t e_ = 10loglearningrate _ _. (11.3)\nwhere u( a , b)indicatesasampleoftheuniformdistributionintheinterval( a , b). Similarlythe l o g n u m b e r o f h i d d e n u n i t s ____maybesampledfrom u(log(50) ,\nlog(2000) ). 4 3 4\nCHAPTER11.PRACTICALMETHODOLOGY\nUnlikeinthecaseofagridsearch,oneshouldnotdiscretizeorbinthevalues\nofthehyperparameters.Thisallowsonetoexplorealargersetofvalues,anddoes\nnotincuradditionalcomputational cost.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1080, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 955}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1081_e8b7ab43", "text": "Infact,asillustratedinﬁgure,a11.2\nrandomsearchcanbeexponentiallymoreeﬃcientthanagridsearch,whenthere\nareseveralhyperparametersthatdonotstronglyaﬀecttheperformancemeasure. Thisisstudiedatlengthin (),whofoundthatrandom BergstraandBengio2012\nsearchreducesthevalidationseterrormuchfasterthangridsearch,intermsof\nthenumberoftrialsrunbyeachmethod. Aswithgridsearch,onemayoftenwanttorunrepeatedversionsofrandom\nsearch,toreﬁnethesearchbasedontheresultsoftheﬁrstrun.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1081, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 457}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1082_e778edd0", "text": "Aswithgridsearch,onemayoftenwanttorunrepeatedversionsofrandom\nsearch,toreﬁnethesearchbasedontheresultsoftheﬁrstrun. Themainreasonwhyrandomsearchﬁndsgoodsolutionsfasterthangridsearch\nisthattherearenowastedexperimentalruns,unlikeinthecaseofgridsearch,\nwhentwovaluesofahyperparameter(givenvaluesoftheotherhyperparameters )\nwouldgivethesameresult.Inthecaseofgridsearch,theotherhyperparameters\nwouldhavethesamevaluesforthesetworuns,whereaswithrandomsearch,they\nwouldusuallyhavediﬀerentvalues.Henceifthechangebetweenthesetwovalues\ndoesnotmarginallymakemuchdiﬀerenceintermsofvalidationseterror,grid\nsearchwillunnecessarilyrepeattwoequivalentexperimentswhilerandomsearch\nwillstillgivetwoindependentexplorationsoftheotherhyperparameters . 1 1 . 4 . 5 Mo d el - B a s ed Hyp erp a ra m et er O p t i m i za t i o n\nThesearchforgoodhyperparameters canbecastasanoptimization problem.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1082, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 871}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1083_5d21e25d", "text": "1 1 . 4 . 5 Mo d el - B a s ed Hyp erp a ra m et er O p t i m i za t i o n\nThesearchforgoodhyperparameters canbecastasanoptimization problem. Thedecisionvariablesarethehyperparameters.Thecosttobeoptimizedisthe\nvalidationseterrorthatresultsfromtrainingusingthesehyperparameters .In\nsimpliﬁedsettingswhereitisfeasibletocomputethegradientofsomediﬀerentiable\nerrormeasureonthevalidationsetwithrespecttothehyperparameters ,wecan\nsimplyfollowthisgradient( ,;,; , Bengioetal.1999Bengio2000Maclaurin etal. 2015).Unfortunately,inmostpracticalsettings,thisgradientisunavailable,either\nduetoitshighcomputationandmemorycost,orduetohyperparametershaving\nintrinsicallynon-diﬀerentiable interactionswiththevalidationseterror,asinthe\ncaseofdiscrete-valuedhyperparameters .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1083, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 756}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1084_c3a44c8f", "text": "Tocompensateforthislackofagradient,wecanbuildamodelofthevalidation\nseterror,thenproposenewhyperparameterguessesbyperformingoptimization\nwithinthismodel.Mostmodel-basedalgorithmsforhyperparameter searchusea\nBayesianregressionmodeltoestimateboththeexpectedvalueofthevalidationset\nerrorforeachhyperparameterandtheuncertaintyaroundthisexpectation.Opti-\nmizationthusinvolvesatradeoﬀbetweenexploration(proposinghyperparameters\n4 3 5\nCHAPTER11.PRACTICALMETHODOLOGY\nforwhichthereishighuncertainty,whichmayleadtoalargeimprovementbutmay\nalsoperformpoorly)andexploitation(proposinghyperparameters whichthemodel\nisconﬁdentwillperformaswellasanyhyperparameters ithasseensofar—usually\nhyperparametersthatareverysimilartoonesithasseenbefore).Contemporary\napproachestohyperparameter optimizationincludeSpearmint(,), Snoeketal.2012\nTPE( ,)andSMAC( ,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1084, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 834}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1085_2602e95f", "text": "Bergstraetal.2011 Hutteretal.2011\nCurrently,wecannotunambiguously recommendBayesianhyperparameter\noptimization asanestablishedtoolforachievingbetterdeeplearningresultsor\nforobtainingthoseresultswithlesseﬀort.Bayesianhyperparameteroptimization\nsometimesperformscomparablytohumanexperts,sometimesbetter,butfails\ncatastrophicallyonotherproblems.Itmaybeworthtryingtoseeifitworkson\naparticularproblembutisnotyetsuﬃcientlymatureorreliable.Thatbeing\nsaid,hyperparameter optimization isanimportantﬁeldofresearchthat,while\noftendrivenprimarilybytheneedsofdeeplearning,holdsthepotentialtobeneﬁt\nnotonlytheentireﬁeldofmachinelearningbutthedisciplineofengineeringin\ngeneral.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1085, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 662}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1086_5aeeb436", "text": "Onedrawbackcommontomosthyperparameter optimization algorithmswith\nmoresophisticationthanrandomsearchisthattheyrequireforatrainingex-\nperimenttoruntocompletionbeforetheyareabletoextractanyinformation\nfromtheexperiment.Thisismuchlesseﬃcient,inthesenseofhowmuchinfor-\nmationcanbegleanedearlyinanexperiment,thanmanualsearchbyahuman\npractitioner,sinceonecanusuallytellearlyonifsomesetofhyperparameters is\ncompletelypathological. ()haveintroducedanearlyversion Swerskyetal.2014\nofanalgorithmthatmaintainsasetofmultipleexperiments.Atvarioustime\npoints,thehyperparameter optimization algorithmcanchoosetobeginanew\nexperiment,to“freeze”arunningexperimentthatisnotpromising,orto“thaw”\nandresumeanexperimentthatwasearlierfrozenbutnowappearspromisinggiven\nmoreinformation. 11.5DebuggingStrategies\nWhenamachinelearningsystemperformspoorly,itisusuallydiﬃculttotell\nwhetherthepoorperformanceisintrinsictothealgorithmitselforwhetherthere\nisabugintheimplementation ofthealgorithm.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1086, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 963}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1087_967c8abd", "text": "Machine learningsystemsare\ndiﬃculttodebugforavarietyofreasons. Inmostcases,wedonotknowaprioriwhattheintendedbehaviorofthe\nalgorithmis.Infact,theentirepointofusingmachinelearningisthatitwill\ndiscoverusefulbehaviorthatwewerenotabletospecifyourselves.Ifwetraina\n4 3 6\nCHAPTER11.PRACTICALMETHODOLOGY\nneuralnetworkonaclassiﬁcationtaskanditachieves5%testerror,wehave new\nnostraightforwardwayofknowingifthisistheexpectedbehaviororsub-optimal\nbehavior.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1087, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 444}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1088_659a6143", "text": "Afurtherdiﬃcultyisthatmostmachinelearningmodelshavemultipleparts\nthatareeachadaptive.Ifonepartisbroken,theotherpartscanadaptandstill\nachieveroughlyacceptableperformance.Forexample,supposethatwearetraining\naneuralnetwithseverallayersparametrized byweights Wandbiases b.Suppose\nfurtherthatwehavemanuallyimplemented thegradientdescentruleforeach\nparameterseparately,andwemadeanerrorintheupdateforthebiases:\nb b←− α (11.4)\nwhere αisthelearningrate.Thiserroneousupdatedoesnotusethegradientat\nall.Itcausesthebiasestoconstantlybecomenegativethroughoutlearning,which\nisclearlynotacorrectimplementation ofanyreasonablelearningalgorithm.The\nbugmaynotbeapparentjustfromexaminingtheoutputofthemodelthough. Dependingonthedistributionoftheinput,theweightsmaybeabletoadaptto\ncompensateforthenegativebiases.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1088, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 791}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1089_735cd28a", "text": "Dependingonthedistributionoftheinput,theweightsmaybeabletoadaptto\ncompensateforthenegativebiases. Mostdebuggingstrategiesforneuralnetsaredesignedtogetaroundoneor\nbothofthesetwodiﬃculties.Eitherwedesignacasethatissosimplethatthe\ncorrectbehavioractuallycanbepredicted,orwedesignatestthatexercisesone\npartoftheneuralnetimplementationinisolation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1089, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 342}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1090_bb03d4da", "text": "Someimportantdebuggingtestsinclude:\nVisualizethemodelinaction:Whentrainingamodeltodetectobjectsin\nimages,viewsomeimageswiththedetectionsproposedbythemodeldisplayed\nsuperimposedontheimage.Whentrainingagenerativemodelofspeech,listento\nsomeofthespeechsamplesitproduces.Thismayseemobvious,butitiseasyto\nfallintothepracticeofonlylookingatquantitativeperformancemeasurements\nlikeaccuracyorlog-likelihood.Directlyobservingthemachinelearningmodel\nperformingitstaskwillhelptodeterminewhetherthequantitativeperformance\nnumbersitachievesseemreasonable.Evaluationbugscanbesomeofthemost\ndevastatingbugsbecausetheycanmisleadyouintobelievingyoursystemis\nperformingwellwhenitisnot.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1090, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 665}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1091_4b439f3f", "text": "Visualizetheworstmistakes: Mostmodelsareabletooutputsomesortof\nconﬁdencemeasureforthetasktheyperform.Forexample,classiﬁersbasedona\nsoftmaxoutputlayerassignaprobabilitytoeachclass.Theprobabilityassigned\ntothemostlikelyclassthusgivesanestimateoftheconﬁdencethemodelhasin\nitsclassiﬁcationdecision.Typically,maximumlikelihoodtrainingresultsinthese\nvaluesbeingoverestimatesratherthanaccurateprobabilitiesofcorrectprediction,\n4 3 7\nCHAPTER11.PRACTICALMETHODOLOGY\nbuttheyaresomewhatusefulinthesensethatexamplesthatareactuallyless\nlikelytobecorrectlylabeledreceivesmallerprobabilities underthemodel.By\nviewingthetrainingsetexamplesthatarethehardesttomodelcorrectly,onecan\noftendiscoverproblemswiththewaythedatahasbeenpreprocessedorlabeled.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1091, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 731}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1092_5b8d95cf", "text": "Forexample,theStreetViewtranscriptionsystemoriginallyhadaproblemwhere\ntheaddressnumberdetectionsystemwouldcroptheimagetootightlyandomit\nsomeofthedigits.Thetranscriptionnetworkthenassignedverylowprobability\ntothecorrectanswerontheseimages.Sortingtheimagestoidentifythemost\nconﬁdentmistakesshowedthattherewasasystematicproblemwiththecropping. Modifyingthedetectionsystemtocropmuchwiderimagesresultedinmuchbetter\nperformanceoftheoverallsystem,eventhoughthetranscriptionnetworkneeded\ntobeabletoprocessgreatervariationinthepositionandscaleoftheaddress\nnumbers. Reasoningaboutsoftwareusingtrainandtesterror:Itisoftendiﬃcultto\ndeterminewhethertheunderlyingsoftwareiscorrectlyimplemented.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1092, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 680}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1093_261f966a", "text": "Reasoningaboutsoftwareusingtrainandtesterror:Itisoftendiﬃcultto\ndeterminewhethertheunderlyingsoftwareiscorrectlyimplemented. Someclues\ncanbeobtainedfromthetrainandtesterror.Iftrainingerrorislowbuttesterror\nishigh,thenitislikelythatthatthetrainingprocedureworkscorrectly,andthe\nmodelisoverﬁttingforfundamentalalgorithmicreasons.Analternativepossibility\nisthatthetesterrorismeasuredincorrectlyduetoaproblemwithsavingthe\nmodelaftertrainingthenreloadingitfortestsetevaluation,orifthetestdata\nwasprepareddiﬀerentlyfromthetrainingdata.Ifbothtrainandtesterrorare\nhigh,thenitisdiﬃculttodeterminewhetherthereisasoftwaredefectorwhether\nthemodelisunderﬁttingduetofundamentalalgorithmicreasons.Thisscenario\nrequiresfurthertests,describednext.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1093, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 730}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1094_74aeef55", "text": "Fitatinydataset:Ifyouhavehigherroronthetrainingset,determinewhether\nitisduetogenuineunderﬁttingorduetoasoftwaredefect.Usuallyevensmall\nmodelscanbeguaranteedtobeableﬁtasuﬃcientlysmalldataset.Forexample,\naclassiﬁcationdatasetwithonlyoneexamplecanbeﬁtjustbysettingthebiases\noftheoutputlayercorrectly.Usuallyifyoucannottrainaclassiﬁertocorrectly\nlabelasingleexample,anautoencodertosuccessfullyreproduceasingleexample\nwithhighﬁdelity,oragenerativemodeltoconsistentlyemitsamplesresemblinga\nsingleexample,thereisasoftwaredefectpreventingsuccessfuloptimization onthe\ntrainingset.Thistestcanbeextendedtoasmalldatasetwithfewexamples.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1094, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 623}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1095_0946c6d0", "text": "Compareback-propagatedderivativestonumericalderivatives:Ifyouareusing\nasoftwareframeworkthatrequiresyoutoimplementyourowngradientcom-\nputations,orifyouareaddinganewoperationtoadiﬀerentiation libraryand\nmustdeﬁneitsbpropmethod,thenacommonsourceoferrorisimplementingthis\ngradientexpressionincorrectly.Onewaytoverifythatthesederivativesarecorrect\n4 3 8\nCHAPTER11.PRACTICALMETHODOLOGY\nistocomparethederivativescomputedbyyourimplementation ofautomatic\ndiﬀerentiationtothederivativescomputedbya .Because ﬁni t e di ﬀ e r e nc e s\nf() =lim x\n →0f x  f x (+)−()\n, (11.5)\nwecanapproximate thederivativebyusingasmall,ﬁnite: \nf() x≈f x  f x (+)−()\n. (11.6)\nWecanimprovetheaccuracyoftheapproximation byusingthe c e n t e r e d di ﬀ e r -\ne nc e:\nf() x≈f x(+1\n2 f x )−(−1\n2 )\n. (11.7)\nTheperturbationsize mustchosentobelargeenoughtoensurethatthepertur-\nbationisnotroundeddowntoomuchbyﬁnite-precisionnumericalcomputations.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1095, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 921}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1096_f8b1057e", "text": "(11.7)\nTheperturbationsize mustchosentobelargeenoughtoensurethatthepertur-\nbationisnotroundeddowntoomuchbyﬁnite-precisionnumericalcomputations. Usually,wewillwanttotestthegradientorJacobianofavector-valuedfunction\ng: Rm→ Rn.Unfortunately,ﬁnitediﬀerencingonlyallowsustotakeasingle\nderivativeatatime.Wecaneitherrunﬁnitediﬀerencing m ntimestoevaluateall\nofthepartialderivativesof g,orwecanapplythetesttoanewfunctionthatuses\nrandomprojectionsatboththeinputandoutputof g.Forexample,wecanapply\nourtestoftheimplementationofthederivativesto f( x)where f( x) = uTg( v x),\nwhere uand varerandomlychosenvectors.Computing f( x)correctlyrequires\nbeingabletoback-propagatethrough gcorrectly,yetiseﬃcienttodowithﬁnite\ndiﬀerencesbecause fhasonlyasingleinputandasingleoutput.Itisusually\nagoodideatorepeatthistestformorethanonevalueof uand vtoreduce\nthechancethatthetestoverlooksmistakesthatareorthogonaltotherandom\nprojection.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1096, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 911}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1097_73e44c99", "text": "Ifonehasaccesstonumericalcomputationoncomplexnumbers,thenthereis\naveryeﬃcientwaytonumericallyestimatethegradientbyusingcomplexnumbers\nasinputtothefunction(SquireandTrapp1998,).Themethodisbasedonthe\nobservationthat\nf x i  f x i  f (+) = ()+()+( x O 2) (11.8)\nreal((+)) = ()+( f x i  f x O 2)imag( ,f x i  (+)\n) = f()+( x O 2) ,(11.9)\nwhere i=√\n−1.Unlikeinthereal-valuedcaseabove,thereisnocancellationeﬀect\nduetotakingthediﬀerencebetweenthevalueof fatdiﬀerentpoints.Thisallows\ntheuseoftinyvaluesof like = 10−150,whichmakethe O( 2)errorinsigniﬁcant\nforallpracticalpurposes.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1097, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 583}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1098_b3aec1b2", "text": "4 3 9\nCHAPTER11.PRACTICALMETHODOLOGY\nMonitorhistogramsofactivationsandgradient:Itisoftenusefultovisualize\nstatisticsofneuralnetworkactivationsandgradients,collectedoveralargeamount\noftrainingiterations(maybeoneepoch).Thepre-activationvalueofhiddenunits\ncantellusiftheunitssaturate,orhowoftentheydo.Forexample,forrectiﬁers,\nhowoftenaretheyoﬀ?Arethereunitsthatarealwaysoﬀ?Fortanhunits,\ntheaverageoftheabsolutevalueofthepre-activationstellsushowsaturated\ntheunitis.Inadeepnetworkwherethepropagatedgradientsquicklygrowor\nquicklyvanish,optimization maybehampered.Finally,itisusefultocomparethe\nmagnitudeofparametergradientstothemagnitudeoftheparametersthemselves.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1098, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 658}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1099_5ea5ae8e", "text": "Assuggestedby(),wewouldlikethemagnitudeofparameterupdates Bottou2015\noveraminibatchtorepresentsomethinglike1%ofthemagnitudeoftheparameter,\nnot50%or0.001%(whichwouldmaketheparametersmovetooslowly).Itmay\nbethatsomegroupsofparametersaremovingatagoodpacewhileothersare\nstalled.Whenthedataissparse(likeinnaturallanguage),someparametersmay\nbeveryrarelyupdated,andthisshouldbekeptinmindwhenmonitoringtheir\nevolution. Finally,manydeeplearningalgorithmsprovidesomesortofguaranteeabout\ntheresultsproducedateachstep.Forexample,inpart,wewillseesomeapprox- III\nimateinferencealgorithmsthatworkbyusingalgebraicsolutionstooptimization\nproblems. Typicallythesecanbedebuggedbytestingeachoftheirguarantees. Someguaranteesthatsomeoptimizationalgorithmsoﬀerincludethattheobjective\nfunctionwillneverincreaseafteronestepofthealgorithm,thatthegradientwith\nrespecttosomesubsetofvariableswillbezeroaftereachstepofthealgorithm,\nandthatthegradientwithrespecttoallvariableswillbezeroatconvergence.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1099, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 969}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1100_40f0992c", "text": "Usuallyduetoroundingerror,theseconditionswillnotholdexactlyinadigital\ncomputer,sothedebuggingtestshouldincludesometoleranceparameter. 11.6Example:Multi-DigitNumberRecognition\nToprovideanend-to-enddescriptionofhowtoapplyourdesignmethodology\ninpractice,wepresentabriefaccountoftheStreetViewtranscriptionsystem,\nfromthepointofviewofdesigningthedeeplearningcomponents.Obviously,\nmanyothercomponentsofthecompletesystem,suchastheStreetViewcars,the\ndatabaseinfrastructure,andsoon,wereofparamountimportance. Fromthepointofviewofthemachinelearningtask,theprocessbeganwith\ndatacollection. The carscollectedtherawdataandhumanoperatorsprovided\nlabels.Thetranscriptiontaskwasprecededbyasigniﬁcantamountofdataset\ncuration,includingusingothermachinelearningtechniquestodetectthehouse\n4 4 0\nCHAPTER11.PRACTICALMETHODOLOGY\nnumberspriortotranscribingthem. Thetranscriptionprojectbeganwithachoiceofperformancemetricsand\ndesiredvaluesforthesemetrics.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1100, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 930}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1101_fa592c7c", "text": "Thetranscriptionprojectbeganwithachoiceofperformancemetricsand\ndesiredvaluesforthesemetrics. Animportantgeneralprincipleistotailorthe\nchoiceofmetrictothebusinessgoalsfortheproject.Becausemapsareonlyuseful\niftheyhavehighaccuracy,itwasimportanttosetahighaccuracyrequirement\nforthisproject. Speciﬁcally,thegoalwastoobtainhuman-level,98%accuracy. Thislevelofaccuracymaynotalwaysbefeasibletoobtain.Inordertoreach\nthislevelofaccuracy,theStreetViewtranscriptionsystemsacriﬁcescoverage. Coveragethusbecamethemainperformancemetricoptimizedduringtheproject,\nwithaccuracyheldat98%.Astheconvolutionalnetworkimproved,itbecame\npossibletoreducetheconﬁdencethresholdbelowwhichthenetworkrefusesto\ntranscribetheinput,eventuallyexceedingthegoalof95%coverage.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1101, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 739}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1102_baf1c39e", "text": "Afterchoosingquantitativegoals,thenextstepinourrecommendedmethodol-\nogyistorapidlyestablishasensiblebaselinesystem.Forvisiontasks,thismeansa\nconvolutionalnetworkwithrectiﬁedlinearunits.Thetranscriptionprojectbegan\nwithsuchamodel.Atthetime,itwasnotcommonforaconvolutionalnetwork\ntooutputasequenceofpredictions.Inordertobeginwiththesimplestpossible\nbaseline,theﬁrstimplementation oftheoutputlayerofthemodelconsistedof n\ndiﬀerentsoftmaxunitstopredictasequenceof ncharacters.Thesesoftmaxunits\nweretrainedexactlythesameasifthetaskwereclassiﬁcation,witheachsoftmax\nunittrainedindependently.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1102, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 584}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1103_8f8b8eff", "text": "Ourrecommendedmethodologyistoiterativelyreﬁnethebaselineandtest\nwhethereachchangemakesanimprovement.TheﬁrstchangetotheStreetView\ntranscriptionsystemwasmotivatedbyatheoreticalunderstandingofthecoverage\nmetricandthestructureofthedata.Speciﬁcally,thenetworkrefusestoclassify\naninput xwhenevertheprobabilityoftheoutputsequence p( y x|) < tfor\nsomethreshold t.Initially,thedeﬁnitionof p( y x|)wasad-hoc,basedonsimply\nmultiplyingallofthesoftmaxoutputstogether.Thismotivatedthedevelopment\nofaspecializedoutputlayerandcostfunctionthatactuallycomputedaprincipled\nlog-likelihood.Thisapproachallowedtheexamplerejectionmechanismtofunction\nmuchmoreeﬀectively.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1103, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 646}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1104_2eba39f0", "text": "Atthispoint,coveragewasstillbelow90%,yettherewerenoobvioustheoretical\nproblemswiththeapproach.Ourmethodologythereforesuggeststoinstrument\nthetrainandtestsetperformanceinordertodeterminewhethertheproblem\nisunderﬁttingoroverﬁtting.Inthiscase,trainandtestseterrorwerenearly\nidentical.Indeed,themainreasonthisprojectproceededsosmoothlywasthe\navailabilityofadatasetwithtensofmillionsoflabeledexamples.Becausetrain\nandtestseterrorweresosimilar,thissuggestedthattheproblemwaseitherdue\n4 4 1\nCHAPTER11.PRACTICALMETHODOLOGY\ntounderﬁttingorduetoaproblemwiththetrainingdata.Oneofthedebugging\nstrategieswerecommendistovisualizethemodel’sworsterrors.Inthiscase,that\nmeantvisualizingtheincorrecttrainingsettranscriptionsthatthemodelgavethe\nhighestconﬁdence.Theseprovedtomostlyconsistofexampleswheretheinput\nimagehadbeencroppedtootightly,withsomeofthedigitsoftheaddressbeing\nremovedbythecroppingoperation.Forexample,aphotoofanaddress“1849”\nmightbecroppedtootightly,withonlythe“849”remainingvisible.Thisproblem\ncouldhavebeenresolvedbyspendingweeksimprovingtheaccuracyoftheaddress\nnumberdetectionsystemresponsiblefordeterminingthecroppingregions.Instead,\ntheteamtookamuchmorepracticaldecision,tosimplyexpandthewidthofthe\ncropregiontobesystematicallywiderthantheaddressnumberdetectionsystem\npredicted.Thissinglechangeaddedtenpercentagepointstothetranscription\nsystem’scoverage.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1104, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1359}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1105_61e3afe1", "text": "Finally,thelastfewpercentagepointsofperformancecamefromadjusting\nhyperparameters.Thismostlyconsistedofmakingthemodellargerwhilemain-\ntainingsomerestrictionsonitscomputational cost.Becausetrainandtesterror\nremainedroughlyequal,itwasalwaysclearthatanyperformancedeﬁcitsweredue\ntounderﬁtting, aswellasduetoafewremainingproblemswiththedatasetitself. Overall,thetranscriptionprojectwasagreatsuccess,andallowedhundredsof\nmillionsofaddressestobetranscribedbothfasterandatlowercostthanwould\nhavebeenpossibleviahumaneﬀort. Wehopethatthedesignprinciplesdescribedinthischapterwillleadtomany\nothersimilarsuccesses. 4 4 2\nC h a p t e r 1 2\nA p p l i cat i on s\nInthischapter,wedescribehowtousedeeplearningtosolveapplicationsincom-\nputervision,speechrecognition,naturallanguageprocessing,andotherapplication\nareasofcommercialinterest.Webeginbydiscussingthelargescaleneuralnetwork\nimplementationsrequiredformostseriousAIapplications.Next,wereviewseveral\nspeciﬁcapplicationareasthatdeeplearninghasbeenusedtosolve.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1105, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 997}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1106_7d495311", "text": "Whileone\ngoalofdeeplearningistodesignalgorithmsthatarecapableofsolvingabroad\nvarietyoftasks,sofarsomedegreeofspecializationisneeded.Forexample,vision\ntasksrequireprocessingalargenumberofinputfeatures(pixels)perexample. Languagetasksrequiremodelingalargenumberofpossiblevalues(wordsinthe\nvocabulary)perinputfeature. 12.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1106, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 318}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1107_5f265aef", "text": "Languagetasksrequiremodelingalargenumberofpossiblevalues(wordsinthe\nvocabulary)perinputfeature. 12. 1 L arge- S c a l e D eep L earni n g\nDeeplearningisbasedonthephilosophyofconnectionism: whileanindividual\nbiologicalneuronoranindividualfeatureinamachinelearningmodelisnot\nintelligent,alargepopulationoftheseneuronsorfeaturesactingtogethercan\nexhibitintelligentbehavior.Ittrulyisimportanttoemphasizethefactthatthe\nnumberofneuronsmustbe l a r g e.Oneofthekeyfactorsresponsibleforthe\nimprovementinneuralnetwork’saccuracyandtheimprovementofthecomplexity\noftaskstheycansolvebetweenthe1980sandtodayisthedramaticincreasein\nthesizeofthenetworksweuse.Aswesawinsection,networksizeshave 1.2.3\ngrownexponentiallyforthepastthreedecades,yetartiﬁcialneuralnetworksare\nonlyaslargeasthenervoussystemsofinsects. Becausethesizeofneuralnetworksisofparamountimportance,deeplearning\n443\nCHAPTER12.APPLICATIONS\nrequireshighperformancehardwareandsoftwareinfrastructure.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1107, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 946}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1108_aacf4faa", "text": "Becausethesizeofneuralnetworksisofparamountimportance,deeplearning\n443\nCHAPTER12.APPLICATIONS\nrequireshighperformancehardwareandsoftwareinfrastructure. 12.1.1FastCPUImplementations\nTraditionally,neuralnetworksweretrainedusingtheCPUofasinglemachine. Today,thisapproachisgenerallyconsideredinsuﬃcient.WenowmostlyuseGPU\ncomputingortheCPUsofmanymachinesnetworkedtogether.Beforemovingto\ntheseexpensivesetups,researchersworkedhardtodemonstratethatCPUscould\nnotmanagethehighcomputational workloadrequiredbyneuralnetworks.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1108, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 514}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1109_a6603560", "text": "AdescriptionofhowtoimplementeﬃcientnumericalCPUcodeisbeyond\nthescopeofthisbook,butweemphasizeherethatcarefulimplementation for\nspeciﬁcCPUfamiliescanyieldlargeimprovements.Forexample,in2011,thebest\nCPUsavailablecouldrunneuralnetworkworkloadsfasterwhenusingﬁxed-point\narithmeticratherthanﬂoating-pointarithmetic.Bycreatingacarefullytunedﬁxed-\npointimplementation,Vanhoucke2011 e t a l .()obtainedathreefoldspeedupover\nastrongﬂoating-pointsystem.EachnewmodelofCPUhasdiﬀerentperformance\ncharacteristics,sosometimesﬂoating-pointimplementations canbefastertoo. Theimportantprincipleisthatcarefulspecializationofnumericalcomputation\nroutinescanyieldalargepayoﬀ.Otherstrategies,besideschoosingwhethertouse\nﬁxedorﬂoatingpoint,includeoptimizingdatastructurestoavoidcachemisses\nandusingvectorinstructions.Manymachinelearningresearchersneglectthese\nimplementationdetails,butwhentheperformanceofanimplementation restricts\nthesizeofthemodel,theaccuracyofthemodelsuﬀers.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1109, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 955}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1110_a5912791", "text": "12.1.2GPUImplementations\nMostmodernneuralnetworkimplementationsarebasedongraphicsprocessing\nunits.Graphicsprocessingunits(GPUs)arespecializedhardwarecomponents\nthatwereoriginallydevelopedforgraphicsapplications.Theconsumermarketfor\nvideogamingsystemsspurreddevelopmentofgraphicsprocessinghardware.The\nperformancecharacteristicsneededforgoodvideogamingsystemsturnouttobe\nbeneﬁcialforneuralnetworksaswell. Videogamerenderingrequiresperformingmanyoperationsinparallelquickly. Modelsof characters and environments arespeciﬁed intermsof listsof 3-D\ncoordinatesofvertices.Graphicscardsmustperformmatrixmultiplication and\ndivisiononmanyverticesinparalleltoconvertthese3-Dcoordinatesinto2-D\non-screencoordinates.Thegraphicscardmustthenperformmanycomputations\nateachpixelinparalleltodeterminethecolorofeachpixel.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1110, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 803}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1111_5611c46f", "text": "Inbothcases,the\n4 4 4\nCHAPTER12.APPLICATIONS\ncomputations arefairlysimpleanddonotinvolvemuchbranchingcomparedto\nthecomputational workloadthataCPUusuallyencounters.Forexample,each\nvertexinthesamerigidobjectwillbemultipliedbythesamematrix;thereisno\nneedtoevaluateanifstatementper-vertextodeterminewhichmatrixtomultiply\nby.Thecomputations arealsoentirelyindependentofeachother,andthusmay\nbeparallelizedeasily.Thecomputations alsoinvolveprocessingmassivebuﬀersof\nmemory,containingbitmapsdescribingthetexture(colorpattern)ofeachobject\ntoberendered.Together,thisresultsingraphicscardshavingbeendesignedto\nhaveahighdegreeofparallelismandhighmemorybandwidth,atthecostof\nhavingalowerclockspeedandlessbranchingcapabilityrelativetotraditional\nCPUs.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1111, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 737}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1112_694ae328", "text": "Neuralnetworkalgorithmsrequirethesameperformancecharacteristicsasthe\nreal-timegraphicsalgorithmsdescribedabove.Neuralnetworksusuallyinvolve\nlargeandnumerousbuﬀersofparameters,activationvalues,andgradientvalues,\neachofwhichmustbecompletelyupdatedduringeverystepoftraining.These\nbuﬀersarelargeenoughtofalloutsidethecacheofatraditionaldesktopcomputer\nsothememorybandwidthofthesystemoftenbecomestheratelimitingfactor. GPUsoﬀeracompellingadvantageoverCPUsduetotheirhighmemorybandwidth. Neuralnetworktrainingalgorithmstypicallydonotinvolvemuchbranchingor\nsophisticatedcontrol,sotheyareappropriateforGPUhardware.Sinceneural\nnetworkscanbedividedintomultipleindividual“neurons”thatcanbeprocessed\nindependentlyfromtheotherneuronsinthesamelayer,neuralnetworkseasily\nbeneﬁtfromtheparallelismofGPUcomputing.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1112, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 794}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1113_1d8f2f8c", "text": "GPUhardwarewasoriginallysospecializedthatitcouldonlybeusedfor\ngraphicstasks.Overtime,GPUhardwarebecamemoreﬂexible,allowingcustom\nsubroutinestobeusedtotransformthecoordinatesofverticesorassigncolorsto\npixels.Inprinciple,therewasnorequirementthatthesepixelvaluesactuallybe\nbasedonarenderingtask.TheseGPUscouldbeusedforscientiﬁccomputingby\nwritingtheoutputofacomputationtoabuﬀerofpixelvalues.Steinkrau e t a l . ()implemented atwo-layerfullyconnectedneuralnetworkonaGPUand 2005\nreportedathreefoldspeedupovertheirCPU-basedbaseline.Shortlythereafter,\nChellapilla 2006 e t a l .()demonstratedthatthesametechniquecouldbeusedto\nacceleratesupervisedconvolutionalnetworks. Thepopularityofgraphicscardsforneuralnetworktrainingexplodedafter\ntheadventofgeneralpurposeGPUs.TheseGP-GPUscouldexecutearbitrary\ncode,notjustrenderingsubroutines.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1113, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 826}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1114_f6927946", "text": "Thepopularityofgraphicscardsforneuralnetworktrainingexplodedafter\ntheadventofgeneralpurposeGPUs.TheseGP-GPUscouldexecutearbitrary\ncode,notjustrenderingsubroutines. NVIDIA’sCUDAprogramming language\nprovidedawaytowritethisarbitrarycodeinaC-likelanguage.Withtheir\nrelativelyconvenientprogramming model,massiveparallelism,andhighmemory\n4 4 5\nCHAPTER12.APPLICATIONS\nbandwidth,GP-GPUsnowoﬀeranidealplatformforneuralnetworkprogramming. Thisplatformwasrapidlyadoptedbydeeplearningresearcherssoonafteritbecame\navailable(,; ,). Raina e t a l .2009Ciresan e t a l .2010\nWritingeﬃcientcodeforGP-GPUsremainsadiﬃculttaskbestlefttospe-\ncialists. ThetechniquesrequiredtoobtaingoodperformanceonGPUarevery\ndiﬀerentfromthoseusedonCPU.Forexample,goodCPU-basedcodeisusually\ndesignedtoreadinformationfromthecacheasmuchaspossible.OnGPU,most\nwritablememorylocationsarenotcached,soitcanactuallybefastertocompute\nthesamevaluetwice,ratherthancomputeitonceandreaditbackfrommemory.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1114, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 952}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1115_191aa76f", "text": "GPUcodeisalsoinherentlymulti-threaded andthediﬀerentthreadsmustbe\ncoordinatedwitheachothercarefully.Forexample,memoryoperationsarefasterif\ntheycanbecoalesced.Coalescedreadsorwritesoccurwhenseveralthreadscan\neachreadorwriteavaluethattheyneedsimultaneously,aspartofasinglememory\ntransaction.DiﬀerentmodelsofGPUsareabletocoalescediﬀerentkindsofread\norwritepatterns.Typically,memoryoperationsareeasiertocoalesceifamong n\nthreads,thread iaccessesbyte i+ jofmemory,and jisamultipleofsomepower\nof2. TheexactspeciﬁcationsdiﬀerbetweenmodelsofGPU.Anothercommon\nconsiderationforGPUsismakingsurethateachthreadinagroupexecutesthe\nsameinstructionsimultaneously.Thismeansthatbranchingcanbediﬃculton\nGPU.Threadsaredividedintosmallgroupscalledwarps.Eachthreadinawarp\nexecutesthesameinstructionduringeachcycle,soifdiﬀerentthreadswithinthe\nsamewarpneedtoexecutediﬀerentcodepaths,thesediﬀerentcodepathsmust\nbetraversedsequentiallyratherthaninparallel.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1115, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 931}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1116_2e6648cb", "text": "DuetothediﬃcultyofwritinghighperformanceGPUcode,researchersshould\nstructuretheirworkﬂowtoavoidneedingtowritenewGPUcodeinordertotest\nnewmodelsoralgorithms.Typically,onecandothisbybuildingasoftwarelibrary\nofhighperformanceoperationslikeconvolutionandmatrixmultiplication, then\nspecifyingmodelsintermsofcallstothislibraryofoperations.Forexample,the\nmachinelearninglibraryPylearn2(Goodfellow2013c e t a l .,)speciﬁesallofits\nmachinelearningalgorithmsintermsofcallstoTheano( ,; Bergstra e t a l .2010\nBastien2012 e t a l .,)andcuda-convnet(,),whichprovidethese Krizhevsky2010\nhigh-performanceoperations.Thisfactoredapproachcanalsoeasesupportfor\nmultiplekindsofhardware.Forexample,thesameTheanoprogramcanrunon\neitherCPUorGPU,withoutneedingtochangeanyofthecallstoTheanoitself. OtherlibrarieslikeTensorFlow(,)andTorch( , Abadi e t a l .2015 Collobert e t a l . 2011b)providesimilarfeatures.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1116, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 882}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1117_c83272fe", "text": "OtherlibrarieslikeTensorFlow(,)andTorch( , Abadi e t a l .2015 Collobert e t a l . 2011b)providesimilarfeatures. 4 4 6\nCHAPTER12.APPLICATIONS\n12.1.3Large-ScaleDistributedImplementations\nInmanycases,thecomputational resourcesavailableonasinglemachineare\ninsuﬃcient.Wethereforewanttodistributetheworkloadoftrainingandinference\nacrossmanymachines. Distributinginferenceissimple,becauseeachinputexamplewewanttoprocess\ncanberunbyaseparatemachine.Thisisknownas .dataparallelism\nItisalsopossibletogetmodelparallelism,wheremultiplemachineswork\ntogetheronasingledatapoint,witheachmachinerunningadiﬀerentpartofthe\nmodel.Thisisfeasibleforbothinferenceandtraining.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1117, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 652}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1118_f744e38c", "text": "Dataparallelismduringtrainingissomewhatharder.Wecanincreasethesize\noftheminibatchusedforasingleSGDstep,butusuallywegetlessthanlinear\nreturnsintermsofoptimization performance.Itwouldbebettertoallowmultiple\nmachinestocomputemultiplegradientdescentstepsinparallel.Unfortunately,\nthestandarddeﬁnitionofgradientdescentisasacompletelysequentialalgorithm:\nthegradientatstepisafunctionoftheparametersproducedbystep. t t−1\nThiscanbesolvedusingasynchronousstochasticgradientdescent(Ben-\ngio2001Recht2011 e t a l .,; e t a l .,).Inthisapproach,severalprocessorcoresshare\nthememoryrepresentingtheparameters.Eachcorereadsparameterswithouta\nlock,thencomputesagradient,thenincrementstheparameterswithoutalock.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1118, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 694}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1119_358fee53", "text": "Thisreducestheaverageamountofimprovementthateachgradientdescentstep\nyields,becausesomeofthecoresoverwriteeachother’sprogress,buttheincreased\nrateofproductionofstepscausesthelearningprocesstobefasteroverall.Dean\ne t a l .()pioneeredthemulti-machineimplementationofthislock-freeapproach 2012\ntogradientdescent,wheretheparametersaremanagedbyaparameterserver\nratherthanstoredinsharedmemory.Distributedasynchronousgradientdescent\nremainstheprimarystrategyfortraininglargedeepnetworksandisusedby\nmostmajordeeplearninggroupsinindustry( ,; Chilimbi e t a l .2014Wu e t a l .,\n2015).Academicdeeplearningresearcherstypicallycannotaﬀordthesamescale\nofdistributedlearningsystemsbutsomeresearchhasfocusedonhowtobuild\ndistributednetworkswithrelativelylow-costhardwareavailableintheuniversity\nsetting( ,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1119, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 790}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1120_ed75e72b", "text": "Coates e t a l .2013\n12.1.4ModelCompression\nInmanycommercialapplications,itismuchmoreimportantthatthetimeand\nmemorycostofrunninginferenceinamachinelearningmodelbelowthanthat\nthetimeandmemorycostoftrainingbelow.Forapplicationsthatdonotrequire\n4 4 7\nCHAPTER12.APPLICATIONS\npersonalization,itispossibletotrainamodelonce,thendeployittobeusedby\nbillionsofusers.Inmanycases,theenduserismoreresource-constrainedthan\nthedeveloper.Forexample,onemighttrainaspeechrecognitionnetworkwitha\npowerfulcomputercluster,thendeployitonmobilephones. Akeystrategyforreducingthecostofinferenceismodelcompression(Bu-\nciluˇa2006 e t a l .,).Thebasicideaofmodelcompressionistoreplacetheoriginal,\nexpensivemodelwithasmallermodelthatrequireslessmemoryandruntimeto\nstoreandevaluate. Modelcompressionisapplicablewhenthesizeoftheoriginalmodelisdriven\nprimarilybyaneedtopreventoverﬁtting.Inmostcases,themodelwiththe\nlowestgeneralization errorisanensembleofseveralindependentlytrainedmodels.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1120, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 958}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1121_7164daba", "text": "Evaluatingall nensemblemembersisexpensive.Sometimes,evenasinglemodel\ngeneralizesbetterifitislarge(forexample,ifitisregularizedwithdropout). Theselargemodelslearnsomefunction f(x),butdosousingmanymore\nparametersthanarenecessaryforthetask.Theirsizeisnecessaryonlydueto\nthelimitednumberoftrainingexamples.Assoonaswehaveﬁtthisfunction\nf(x),wecangenerateatrainingsetcontaininginﬁnitelymanyexamples,simply\nbyapplying ftorandomlysampledpointsx.Wethentrainthenew,smaller,\nmodeltomatch f(x)onthesepoints.Inordertomosteﬃcientlyusethecapacity\nofthenew,smallmodel,itisbesttosamplethenewxpointsfromadistribution\nresemblingtheactualtestinputsthatwillbesuppliedtothemodellater.Thiscan\nbedonebycorruptingtrainingexamplesorbydrawingpointsfromagenerative\nmodeltrainedontheoriginaltrainingset. Alternatively,onecantrainthesmallermodelonlyontheoriginaltraining\npoints,buttrainittocopyotherfeaturesofthemodel,suchasitsposterior\ndistributionovertheincorrectclasses(Hinton20142015 e t a l .,,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1121, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 971}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1122_9602bfc8", "text": "Alternatively,onecantrainthesmallermodelonlyontheoriginaltraining\npoints,buttrainittocopyotherfeaturesofthemodel,suchasitsposterior\ndistributionovertheincorrectclasses(Hinton20142015 e t a l .,,). 12.1.5DynamicStructure\nOnestrategyforacceleratingdataprocessingsystemsingeneralistobuildsystems\nthathavedynamicstructureinthegraphdescribingthecomputationneeded\ntoprocessaninput.Dataprocessingsystemscandynamicallydeterminewhich\nsubsetofmanyneuralnetworksshouldberunonagiveninput.Individualneural\nnetworkscanalsoexhibitdynamicstructureinternallybydeterminingwhichsubset\noffeatures(hiddenunits)tocomputegiveninformationfromtheinput.This\nformofdynamicstructureinsideneuralnetworksissometimescalledconditional\ncomputation(,; ,). Sincemanycomponentsof Bengio2013Bengio e t a l .2013b\nthearchitecturemayberelevantonlyforasmallamountofpossibleinputs,the\n4 4 8\nCHAPTER12.APPLICATIONS\nsystemcanrunfasterbycomputingthesefeaturesonlywhentheyareneeded.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1122, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 937}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1123_542355bb", "text": "Dynamicstructureofcomputationsisabasiccomputerscienceprincipleapplied\ngenerallythroughoutthesoftwareengineeringdiscipline. Thesimplestversions\nofdynamicstructureappliedtoneuralnetworksarebasedondeterminingwhich\nsubsetofsomegroupofneuralnetworks(orothermachinelearningmodels)should\nbeappliedtoaparticularinput. Avenerablestrategyforacceleratinginferenceinaclassiﬁeristouseacascade\nofclassiﬁers.Thecascadestrategymaybeappliedwhenthegoalistodetectthe\npresenceofarareobject(orevent).Toknowforsurethattheobjectispresent,\nwemustuseasophisticatedclassiﬁerwithhighcapacity,thatisexpensivetorun.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1123, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 586}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1124_9d1778b9", "text": "However,becausetheobjectisrare,wecanusuallyusemuchlesscomputation\ntorejectinputsasnotcontainingtheobject.Inthesesituations,wecantrain\nasequenceofclassiﬁers.Theﬁrstclassiﬁersinthesequencehavelowcapacity,\nandaretrainedtohavehighrecall.Inotherwords,theyaretrainedtomakesure\nwedonotwronglyrejectaninputwhentheobjectispresent.Theﬁnalclassiﬁer\nistrainedtohavehighprecision.Attesttime,weruninferencebyrunningthe\nclassiﬁersinasequence,abandoninganyexampleassoonasanyoneelementin\nthecascaderejectsit.Overall,thisallowsustoverifythepresenceofobjectswith\nhighconﬁdence,usingahighcapacitymodel,butdoesnotforceustopaythecost\noffullinferenceforeveryexample.Therearetwodiﬀerentwaysthatthecascade\ncanachievehighcapacity.Onewayistomakethelatermembersofthecascade\nindividuallyhavehighcapacity.Inthiscase,thesystemasawholeobviouslyhas\nhighcapacity,becausesomeofitsindividualmembersdo.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1124, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 865}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1125_9ac21050", "text": "Itisalsopossibleto\nmakeacascadeinwhicheveryindividualmodelhaslowcapacitybutthesystem\nasawholehashighcapacityduetothecombinationofmanysmallmodels.Viola\nandJones2001()usedacascadeofboosteddecisiontreestoimplementafastand\nrobustfacedetectorsuitableforuseinhandhelddigitalcameras.Theirclassiﬁer\nlocalizesafaceusingessentiallyaslidingwindowapproachinwhichmanywindows\nareexaminedandrejectediftheydonotcontainfaces.Anotherversionofcascades\nusestheearliermodelstoimplementasortofhardattentionmechanism:the\nearlymembersofthecascadelocalizeanobjectandlatermembersofthecascade\nperformfurtherprocessinggiventhelocationoftheobject.Forexample,Google\ntranscribesaddressnumbersfromStreetViewimageryusingatwo-stepcascade\nthatﬁrstlocatestheaddressnumberwithonemachinelearningmodelandthen\ntranscribesitwithanother(Goodfellow2014d e t a l .,). Decisiontreesthemselvesareanexampleofdynamicstructure,becauseeach\nnodeinthetreedetermineswhichofitssubtreesshouldbeevaluatedforeachinput.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1125, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 961}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1126_9928c67d", "text": "Decisiontreesthemselvesareanexampleofdynamicstructure,becauseeach\nnodeinthetreedetermineswhichofitssubtreesshouldbeevaluatedforeachinput. Asimplewaytoaccomplishtheunionofdeeplearninganddynamicstructure\n4 4 9\nCHAPTER12.APPLICATIONS\nistotrainadecisiontreeinwhicheachnodeusesaneuralnetworktomakethe\nsplittingdecision( ,),thoughthishastypicallynotbeen GuoandGelfand1992\ndonewiththeprimarygoalofacceleratinginferencecomputations.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1126, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 424}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1127_42cb3308", "text": "Inthesamespirit,onecanuseaneuralnetwork,calledthegatertoselect\nwhichoneoutofseveralexpertnetworkswillbeusedtocomputetheoutput,\ngiventhecurrentinput.Theﬁrstversionofthisideaiscalledthemixtureof\nexperts(Nowlan1990Jacobs 1991 ,; e t a l .,),inwhichthegateroutputsaset\nofprobabilities orweights(obtainedviaasoftmaxnonlinearity), oneperexpert,\nandtheﬁnaloutputisobtainedbytheweightedcombinationoftheoutputof\ntheexperts.Inthatcase, theuseofthegaterdoesnotoﬀerareductionin\ncomputational cost,butifasingleexpertischosenbythegaterforeachexample,\nweobtainthehardmixtureofexperts( ,,),which Collobert e t a l .20012002\ncanconsiderablyacceleratetrainingandinferencetime.Thisstrategyworkswell\nwhenthenumberofgatingdecisionsissmallbecauseitisnotcombinatorial. But\nwhenwewanttoselectdiﬀerentsubsetsofunitsorparameters,itisnotpossible\ntousea“softswitch”becauseitrequiresenumerating(andcomputingoutputsfor)\nallthegaterconﬁgurations. Todealwiththisproblem,severalapproacheshave\nbeenexploredtotraincombinatorialgaters.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1127, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 999}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1128_865608b7", "text": "Todealwiththisproblem,severalapproacheshave\nbeenexploredtotraincombinatorialgaters. ()experimentwith Bengio e t a l .2013b\nseveralestimatorsofthegradientonthegatingprobabilities, whileBacon e t a l . ()and ()usereinforcementlearningtechniques(policy 2015Bengio e t a l .2015a\ngradient)tolearnaformofconditionaldropoutonblocksofhiddenunitsandget\nanactualreductionincomputational costwithoutimpactingnegativelyonthe\nqualityoftheapproximation. Another kindof dynamicstructure isa switch, where ahidden unit can\nreceiveinputfromdiﬀerentunitsdependingonthecontext.Thisdynamicrouting\napproachcanbeinterpretedasanattentionmechanism( ,). Olshausen e t a l .1993\nSofar,theuseofahardswitchhasnotproveneﬀectiveonlarge-scaleapplications.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1128, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 725}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1129_289fc840", "text": "Olshausen e t a l .1993\nSofar,theuseofahardswitchhasnotproveneﬀectiveonlarge-scaleapplications. Contemporaryapproachesinsteaduseaweightedaverageovermanypossibleinputs,\nandthusdonotachieveallofthepossiblecomputational beneﬁtsofdynamic\nstructure.Contemporaryattentionmechanismsaredescribedinsection.12.4.5.1\nOnemajorobstacletousingdynamicallystructuredsystemsisthedecreased\ndegreeofparallelismthatresultsfromthesystemfollowingdiﬀerentcodebranches\nfordiﬀerentinputs.Thismeansthatfewoperationsinthenetworkcanbedescribed\nasmatrixmultiplication orbatchconvolutiononaminibatchofexamples.We\ncanwritemorespecializedsub-routinesthatconvolveeachexamplewithdiﬀerent\nkernelsormultiplyeachrowofadesignmatrixbyadiﬀerentsetofcolumns\nofweights.Unfortunately, thesemorespecializedsubroutinesarediﬃcultto\nimplementeﬃciently.CPUimplementations willbeslowduetothelackofcache\n4 5 0\nCHAPTER12.APPLICATIONS\ncoherenceandGPUimplementations willbeslowduetothelackofcoalesced\nmemorytransactionsandtheneedtoserializewarpswhenmembersofawarptake\ndiﬀerentbranches.Insomecases,theseissuescanbemitigatedbypartitioningthe\nexamplesintogroupsthatalltakethesamebranch,andprocessingthesegroups\nofexamplessimultaneously.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1129, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1180}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1130_59ef2bec", "text": "Thiscanbeanacceptablestrategyforminimizing\nthetimerequiredtoprocessaﬁxedamountofexamplesinanoﬄinesetting.In\nareal-timesettingwhereexamplesmustbeprocessedcontinuously,partitioning\ntheworkloadcanresultinload-balancing issues.Forexample,ifweassignone\nmachinetoprocesstheﬁrststepinacascadeandanothermachinetoprocess\nthelaststepinacascade,thentheﬁrstwilltendtobeoverloadedandthelast\nwilltendtobeunderloaded. Similarissuesariseifeachmachineisassignedto\nimplementdiﬀerentnodesofaneuraldecisiontree. 12.1.6SpecializedHardwareImplementationsofDeepNetworks\nSincetheearlydaysofneuralnetworksresearch,hardwaredesignershaveworked\nonspecializedhardwareimplementations thatcouldspeeduptrainingand/or\ninferenceofneuralnetworkalgorithms.Seeearlyandmorerecentreviewsof\nspecializedhardwarefordeepnetworks( ,;, LindseyandLindblad1994Beiu e t a l . 2003MisraandSaha2010 ; ,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1130, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 854}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1131_4364e4bc", "text": "2003MisraandSaha2010 ; ,). Diﬀerentformsofspecializedhardware(GrafandJackel1989Meadand ,;\nIsmail2012Kim2009Pham2012Chen 2014ab ,; e t a l .,; e t a l .,; e t a l .,,)have\nbeendevelopedoverthelastdecades,eitherwithASICs(application-speciﬁcinte-\ngratedcircuit),eitherwithdigital(basedonbinaryrepresentationsofnumbers),\nanalog(GrafandJackel1989MeadandIsmail2012 ,; ,)(basedonphysicalimple-\nmentationsofcontinuousvaluesasvoltagesorcurrents)orhybridimplementations\n(combiningdigitalandanalogcomponents).InrecentyearsmoreﬂexibleFPGA\n(ﬁeldprogrammable gatedarray)implementations(wheretheparticularsofthe\ncircuitcanbewrittenonthechipafterithasbeenbuilt)havebeendeveloped.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1131, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 663}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1132_daaab18c", "text": "Thoughsoftwareimplementationsongeneral-purposeprocessingunits(CPUs\nandGPUs)typicallyuse32or64bitsofprecisiontorepresentﬂoatingpoint\nnumbers,ithaslongbeenknownthatitwaspossibletouselessprecision,at\nleastatinferencetime(HoltandBaker1991HoliandHwang1993Presley ,; ,;\nandHaggard1994SimardandGraf1994Wawrzynek 1996Savich ,; ,; e t a l .,; e t a l .,\n2007).Thishasbecomeamorepressingissueinrecentyearsasdeeplearning\nhasgainedinpopularityinindustrialproducts,andasthegreatimpactoffaster\nhardwarewasdemonstratedwithGPUs.Anotherfactorthatmotivatescurrent\nresearchonspecializedhardwarefordeepnetworksisthattherateofprogressof\nasingleCPUorGPUcorehassloweddown,andmostrecentimprovementsin\n4 5 1\nCHAPTER12.APPLICATIONS\ncomputingspeedhavecomefromparallelization acrosscores(eitherinCPUsor\nGPUs).Thisisverydiﬀerentfromthesituationofthe1990s(thepreviousneural\nnetworkera)wherethehardwareimplementations ofneuralnetworks(whichmight\ntaketwoyearsfrominceptiontoavailabilityofachip)couldnotkeepupwith\ntherapidprogressandlowpricesofgeneral-purposeCPUs.Buildingspecialized\nhardwareisthusawaytopushtheenvelopefurther,atatimewhennewhardware\ndesignsarebeingdevelopedforlow-powerdevicessuchasphones,aimingfor\ngeneral-public applicationsofdeeplearning(e.g.,withspeech,computervisionor\nnaturallanguage).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1132, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1275}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1133_5268a26c", "text": "Recentworkonlow-precisionimplementationsofbackprop-based neuralnets\n(Vanhoucke2011Courbariaux 2015Gupta2015 e t a l .,; e t a l .,; e t a l .,)suggests\nthatbetween8and16bitsofprecisioncansuﬃceforusingortrainingdeep\nneuralnetworkswithback-propagation. Whatisclearisthatmoreprecisionis\nrequiredduringtrainingthanatinferencetime,andthatsomeformsofdynamic\nﬁxedpointrepresentationofnumberscanbeusedtoreducehowmanybitsare\nrequiredpernumber.Traditionalﬁxedpointnumbersarerestrictedtoaﬁxed\nrange(whichcorrespondstoagivenexponentinaﬂoatingpointrepresentation). Dynamicﬁxedpointrepresentationssharethatrangeamongasetofnumbers\n(suchasalltheweightsinonelayer).Usingﬁxedpointratherthanﬂoatingpoint\nrepresentationsandusinglessbitspernumberreducesthehardwaresurfacearea,\npowerrequirementsandcomputingtimeneededforperformingmultiplications,\nandmultiplications arethemostdemandingoftheoperationsneededtouseor\ntrainamoderndeepnetworkwithbackprop. 12.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1133, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 932}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1134_5545e43a", "text": "12. 2 C om p u t er V i s i on\nComputervisionhastraditionallybeenoneofthemostactiveresearchareasfor\ndeeplearningapplications,becausevisionisataskthatiseﬀortlessforhumans\nandmanyanimalsbutchallengingforcomputers( ,).Manyof Ballard e t a l .1983\nthemostpopularstandardbenchmarktasksfordeeplearningalgorithmsareforms\nofobjectrecognitionoropticalcharacterrecognition. Computervisionisaverybroadﬁeldencompassingawidevarietyofways\nofprocessingimages,andanamazingdiversityofapplications. Applicationsof\ncomputervisionrangefromreproducinghumanvisualabilities,suchasrecognizing\nfaces,tocreatingentirelynewcategoriesofvisualabilities.Asanexampleof\nthelattercategory,onerecentcomputervisionapplicationistorecognizesound\nwavesfromthevibrationstheyinduceinobjectsvisibleinavideo(,Davis e t a l .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1134, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 782}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1135_b2f7ecea", "text": "2014).Mostdeeplearningresearchoncomputervisionhasnotfocusedonsuch\n4 5 2\nCHAPTER12.APPLICATIONS\nexoticapplicationsthatexpandtherealmofwhatispossiblewithimagerybut\nratherasmallcoreofAIgoalsaimedatreplicatinghumanabilities.Mostdeep\nlearningforcomputervisionisusedforobjectrecognitionordetectionofsome\nform,whetherthismeansreportingwhichobjectispresentinanimage,annotating\nanimagewithboundingboxesaroundeachobject,transcribingasequenceof\nsymbolsfromanimage,orlabelingeachpixelinanimagewiththeidentityofthe\nobjectitbelongsto.Becausegenerativemodelinghasbeenaguidingprinciple\nofdeeplearningresearch,thereisalsoalargebodyofworkonimagesynthesis\nusingdeepmodels.Whileimagesynthesisisusuallynotconsidereda e x nihil o\ncomputervisionendeavor,modelscapableofimagesynthesisareusuallyusefulfor\nimagerestoration,acomputervisiontaskinvolvingrepairingdefectsinimagesor\nremovingobjectsfromimages.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1135, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 878}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1136_77bfcbf6", "text": "12.2.1Preprocessing\nManyapplicationareasrequiresophisticatedpreprocessingbecausetheoriginal\ninputcomesinaformthatisdiﬃcultformanydeeplearningarchitecturesto\nrepresent.Computervisionusuallyrequiresrelativelylittleofthiskindofpre-\nprocessing.Theimagesshouldbestandardizedsothattheirpixelsalllieinthe\nsame,reasonablerange,like[0,1]or[-1,1]. Mixingimagesthatliein[0,1]with\nimagesthatliein[0,255]willusuallyresultinfailure.Formattingimagestohave\nthesamescaleistheonlykindofpreprocessingthatisstrictlynecessary.Many\ncomputervisionarchitectures requireimagesofastandardsize,soimagesmustbe\ncroppedorscaledtoﬁtthatsize.Eventhisrescalingisnotalwaysstrictlynecessary. Someconvolutionalmodelsacceptvariably-sizedinputsanddynamicallyadjust\nthesizeoftheirpoolingregionstokeeptheoutputsizeconstant(Waibel e t a l .,\n1989).Otherconvolutionalmodelshavevariable-sizedoutputthatautomatically\nscalesinsizewiththeinput,suchasmodelsthatdenoiseorlabeleachpixelinan\nimage( ,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1136, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 952}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1137_41a0f431", "text": "Hadsell e t a l .2007\nDatasetaugmentation maybeseenasawayofpreprocessingthetrainingset\nonly.Datasetaugmentationisanexcellentwaytoreducethegeneralization error\nofmostcomputervisionmodels.Arelatedideaapplicableattesttimeistoshow\nthemodelmanydiﬀerentversionsofthesameinput(forexample,thesameimage\ncroppedatslightlydiﬀerentlocations)andhavethediﬀerentinstantiationsofthe\nmodelvotetodeterminetheoutput.Thislatterideacanbeinterpretedasan\nensembleapproach,andhelpstoreducegeneralization error.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1137, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 486}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1138_56b18641", "text": "Otherkindsofpreprocessingareappliedtoboththetrainandthetestsetwith\nthegoalofputtingeachexampleintoamorecanonicalforminordertoreducethe\namountofvariationthatthemodelneedstoaccountfor.Reducingtheamountof\n4 5 3\nCHAPTER12.APPLICATIONS\nvariationinthedatacanbothreducegeneralization errorandreducethesizeof\nthemodelneededtoﬁtthetrainingset.Simplertasksmaybesolvedbysmaller\nmodels,andsimplersolutionsaremorelikelytogeneralizewell.Preprocessing\nofthiskindisusuallydesignedtoremovesomekindofvariabilityintheinput\ndatathatiseasyforahumandesignertodescribeandthatthehumandesigner\nisconﬁdenthasnorelevancetothetask.Whentrainingwithlargedatasetsand\nlargemodels,thiskindofpreprocessingisoftenunnecessary,anditisbesttojust\nletthemodellearnwhichkindsofvariabilityitshouldbecomeinvariantto.For\nexample,theAlexNetsystemforclassifyingImageNetonlyhasonepreprocessing\nstep:subtractingthemeanacrosstrainingexamplesofeachpixel(Krizhevsky\ne t a l .,).2012\n12.2.1.1ContrastNormalization\nOneofthemostobvioussourcesofvariationthatcanbesafelyremoved for\nmanytasksistheamountofcontrastintheimage.Contrastsimplyreferstothe\nmagnitudeofthediﬀerencebetweenthebrightandthedarkpixelsinanimage.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1138, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1158}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1139_92f5f317", "text": "Therearemanywaysofquantifyingthecontrastofanimage.Inthecontextof\ndeeplearning,contrastusuallyreferstothestandarddeviationofthepixelsinan\nimageorregionofanimage.Supposewehaveanimagerepresentedbyatensor\nX∈ Rr c××3,with X i , j ,1beingtheredintensityatrow iandcolumn j, X i , j ,2giving\nthegreenintensityand X i , j ,3givingtheblueintensity.Thenthecontrastofthe\nentireimageisgivenby\n1\n3 r cr \ni=1c \nj=13 \nk=1\nX i , j , k−¯ X2(12.1)\nwhere ¯ Xisthemeanintensityoftheentireimage:\n¯ X=1\n3 r cr \ni=1c \nj=13 \nk=1X i , j , k .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1139, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 528}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1140_f2b51970", "text": "(12.2)\nGlobalcontrastnormalization(GCN)aimstopreventimagesfromhaving\nvaryingamountsofcontrastbysubtractingthemeanfromeachimage, then\nrescalingitsothatthe standarddeviation across its pixelsis equaltosome\nconstant s.Thisapproachiscomplicatedbythefactthatnoscalingfactorcan\nchangethecontrastofazero-contrastimage(onewhosepixelsallhaveequal\nintensity).Imageswithverylowbutnon-zerocontrastoftenhavelittleinformation\ncontent.Dividingbythetruestandarddeviationusuallyaccomplishesnothing\n4 5 4\nCHAPTER12.APPLICATIONS\nmorethanamplifyingsensornoiseorcompressionartifactsinsuchcases.This\nmotivatesintroducingasmall,positiveregularizationparameter λtobiasthe\nestimateofthestandarddeviation.Alternately,onecanconstrainthedenominator\ntobeatleast .Givenaninputimage X,GCNproducesanoutputimage X,\ndeﬁnedsuchthat\nX\ni , j , k= sX i , j , k−¯ X\nmax\n ,\nλ+1\n3 r cr\ni=1c\nj=13\nk=1\nX i , j , k−¯ X2 .(12.3)\nDatasetsconsistingoflargeimagescroppedtointerestingobjectsareunlikely\ntocontainanyimageswithnearlyconstantintensity.Inthesecases,itissafe\ntopracticallyignorethesmalldenominator problembysetting λ= 0andavoid\ndivisionby0inextremelyrarecasesbysetting toanextremelylowvaluelike\n10−8.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1140, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1176}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1141_53810c15", "text": "Thisistheapproachusedby ()ontheCIFAR-10 Goodfellow e t a l .2013a\ndataset.Smallimagescroppedrandomlyaremorelikelytohavenearlyconstant\nintensity,makingaggressiveregularizationmoreuseful. ()used Coates e t a l .2011\n λ = 0and = 10onsmall,randomlyselectedpatchesdrawnfromCIFAR-10. Thescaleparameter scanusuallybesetto,asdoneby (), 1 Coates e t a l .2011\norchosentomakeeachindividualpixelhavestandarddeviationacrossexamples\ncloseto1,asdoneby ().", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1141, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 442}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1142_adac123f", "text": "Thescaleparameter scanusuallybesetto,asdoneby (), 1 Coates e t a l .2011\norchosentomakeeachindividualpixelhavestandarddeviationacrossexamples\ncloseto1,asdoneby (). Goodfellow e t a l .2013a\nThestandarddeviationinequationisjustarescalingofthe 12.3 L2norm\noftheimage(assumingthemeanoftheimagehasalreadybeenremoved).Itis\npreferabletodeﬁneGCNintermsofstandarddeviationratherthan L2norm\nbecausethestandarddeviationincludesdivisionbythenumberofpixels,soGCN\nbasedonstandarddeviationallowsthesame stobeusedregardlessofimage\nsize.However,theobservationthatthe L2normisproportionaltothestandard\ndeviationcanhelpbuildausefulintuition.OnecanunderstandGCNasmapping\nexamplestoasphericalshell.Seeﬁgureforanillustration.Thiscanbea 12.1\nusefulpropertybecauseneuralnetworksareoftenbetteratrespondingtodirections\ninspaceratherthanexactlocations.Respondingtomultipledistancesinthe\nsamedirectionrequireshiddenunitswithcollinearweightvectorsbutdiﬀerent\nbiases.Suchcoordinationcanbediﬃcultforthelearningalgorithmtodiscover.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1142, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1000}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1143_bd7c96d5", "text": "Additionally,manyshallowgraphicalmodelshaveproblemswithrepresenting\nmultipleseparatedmodesalongthesameline.GCNavoidstheseproblemsby\nreducingeachexampletoadirectionratherthanadirectionandadistance. Counterintuitively,thereisapreprocessingoperationknownasspheringand\nitisnotthesameoperationasGCN.Spheringdoesnotrefertomakingthedata\nlieonasphericalshell,butrathertorescalingtheprincipalcomponentstohave\n4 5 5\nCHAPTER12.APPLICATIONS\n− 1 5 0 0 1 5 . . . x 0− 1 5 .0 0 .1 5 .x 1Rawinput\n− 1 5 0 0 1 5 . . . x 0GCN, = 10 λ− 2\n− 1 5 0 0 1 5 . . . x 0GCN, = 0 λ\nFigure12.1:GCNmapsexamplesontoasphere. ( L e f t )Rawinputdatamayhaveanynorm. ( C e n t e r )GCNwith λ= 0mapsallnon-zeroexamplesperfectlyontoasphere.Hereweuse\ns= 1and = 10− 8.BecauseweuseGCNbasedonnormalizingthestandarddeviation\nratherthanthe L2norm,theresultingsphereisnottheunitsphere. ( R i g h t )Regularized\nGCN,with λ >0,drawsexamplestowardthespherebutdoesnotcompletelydiscardthe\nvariationintheirnorm.Weleaveandthesameasbefore.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1143, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 987}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1144_6835852f", "text": "( R i g h t )Regularized\nGCN,with λ >0,drawsexamplestowardthespherebutdoesnotcompletelydiscardthe\nvariationintheirnorm.Weleaveandthesameasbefore. s \nequalvariance,sothatthemultivariatenormaldistributionusedbyPCAhas\nsphericalcontours.Spheringismorecommonlyknownas .whitening\nGlobalcontrastnormalization willoftenfailtohighlightimagefeatureswe\nwouldliketostandout,suchasedgesandcorners.Ifwehaveascenewithalarge\ndarkareaandalargebrightarea(suchasacitysquarewithhalftheimagein\ntheshadowofabuilding)thenglobalcontrastnormalization willensurethereisa\nlargediﬀerencebetweenthebrightnessofthedarkareaandthebrightnessofthe\nlightarea.Itwillnot,however,ensurethatedgeswithinthedarkregionstandout. Thismotivateslocalcontrastnormalization.Localcontrastnormalization\nensuresthatthecontrastisnormalizedacrosseachsmallwindow,ratherthanover\ntheimageasawhole.Seeﬁgureforacomparisonofglobalandlocalcontrast 12.2\nnormalization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1144, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 908}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1145_99680114", "text": "Variousdeﬁnitionsoflocalcontrastnormalization arepossible.Inallcases,\nonemodiﬁeseachpixelbysubtractingameanofnearbypixelsanddividingby\nastandarddeviationofnearbypixels.Insomecases,thisisliterallythemean\nandstandarddeviationofallpixelsinarectangularwindowcenteredonthe\npixeltobemodiﬁed(,).Inothercases,thisisaweightedmean Pinto e t a l .2008\nandweightedstandarddeviationusingGaussianweightscenteredonthepixelto\nbemodiﬁed.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1145, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 420}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1146_8df8716f", "text": "Inthecaseofcolorimages,somestrategiesprocessdiﬀerentcolor\n4 5 6\nCHAPTER12.APPLICATIONS\nInputimage GCN LCN\nFigure12.2:Acomparisonofglobalandlocalcontrastnormalization.Visually,theeﬀects\nofglobalcontrastnormalizationaresubtle.Itplacesallimagesonroughlythesame\nscale,whichreducestheburdenonthelearningalgorithmtohandlemultiplescales.Local\ncontrastnormalizationmodiﬁestheimagemuchmore,discardingallregionsofconstant\nintensity.Thisallowsthemodeltofocusonjusttheedges.Regionsofﬁnetexture,\nsuchasthehousesinthesecondrow,maylosesomedetailduetothebandwidthofthe\nnormalizationkernelbeingtoohigh. channelsseparatelywhileotherscombineinformationfromdiﬀerentchannelsto\nnormalizeeachpixel( ,). Sermanet e t a l .2012\nLocalcontrastnormalization canusuallybeimplemented eﬃcientlybyusing\nseparableconvolution(seesection)tocomputefeaturemapsoflocalmeansand 9.8\nlocalstandarddeviations,thenusingelement-wisesubtractionandelement-wise\ndivisionondiﬀerentfeaturemaps.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1146, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 945}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1147_c05dd6ba", "text": "Localcontrastnormalization isadiﬀerentiable operationandcanalsobeusedas\nanonlinearityappliedtothehiddenlayersofanetwork,aswellasapreprocessing\noperationappliedtotheinput. Aswithglobalcontrastnormalization, wetypicallyneedtoregularizelocal\ncontrastnormalization toavoiddivisionbyzero.Infact,becauselocalcontrast\nnormalization typicallyactsonsmallerwindows,itisevenmoreimportantto\nregularize.Smallerwindowsaremorelikelytocontainvaluesthatareallnearly\nthesameaseachother,andthusmorelikelytohavezerostandarddeviation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1147, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 513}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1148_5f8ab2d8", "text": "4 5 7\nCHAPTER12.APPLICATIONS\n12.2.1.2DatasetAugmentation\nAsdescribedinsection,itiseasytoimprovethegeneralization ofaclassiﬁer 7.4\nbyincreasingthesizeofthetrainingsetbyaddingextracopiesofthetraining\nexamplesthathavebeenmodiﬁedwithtransformationsthatdonotchangethe\nclass.Objectrecognitionisaclassiﬁcationtaskthatisespeciallyamenableto\nthisform ofdataset augmentationbecause theclass isinvariant toso many\ntransformationsandtheinputcanbeeasilytransformedwithmanygeometric\noperations.Asdescribedbefore,classiﬁerscanbeneﬁtfromrandomtranslations,\nrotations,andinsomecases,ﬂipsoftheinputtoaugmentthedataset.Inspecialized\ncomputervisionapplications,moreadvancedtransformationsarecommonlyused\nfordatasetaugmentation. Theseschemesincluderandomperturbationofthe\ncolorsinanimage( ,)andnonlineargeometricdistortionsof Krizhevsky e t a l .2012\ntheinput( ,). LeCun e t a l .1998b\n12.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1148, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 868}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1149_845d08bb", "text": "Theseschemesincluderandomperturbationofthe\ncolorsinanimage( ,)andnonlineargeometricdistortionsof Krizhevsky e t a l .2012\ntheinput( ,). LeCun e t a l .1998b\n12. 3 S p eec h R ec ogn i t i o n\nThetaskofspeechrecognitionistomapanacousticsignalcontainingaspoken\nnaturallanguageutteranceintothecorrespondingsequenceofwordsintendedby\nthespeaker.LetX= (x(1),x(2), . . . ,x() T)denotethesequenceofacousticinput\nvectors(traditionallyproducedbysplittingtheaudiointo20msframes).Most\nspeechrecognitionsystemspreprocesstheinputusingspecializedhand-designed\nfeatures,butsome( ,)deeplearningsystemslearnfeatures JaitlyandHinton2011\nfromrawinput.Lety= ( y1 , y2 , . . . , y N)denotethetargetoutputsequence(usually\nasequenceofwordsorcharacters).Theautomaticspeechrecognition(ASR)\ntaskconsistsofcreatingafunction f∗\nASRthatcomputesthemostprobablelinguistic\nsequencegiventheacousticsequence: y X\nf∗\nASR() = argmaxX\nyP∗( = ) y X|X (12.4)\nwhere P∗isthetrueconditionaldistributionrelatingtheinputsXtothetargets\ny.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1149, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1150_b6e86b29", "text": "Sincethe1980sanduntilabout2009–2012,state-of-theartspeechrecognition\nsystemsprimarilycombinedhiddenMarkovmodels(HMMs)andGaussianmixture\nmodels(GMMs).GMMsmodeledtheassociationbetweenacousticfeaturesand\nphonemes(,),whileHMMsmodeledthesequenceofphonemes.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1150, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 251}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1151_9f6547ca", "text": "Bahl e t a l .1987\nTheGMM-HMM modelfamilytreats acousticwaveformsasbeinggenerated\nbythefollowingprocess: ﬁrstanHMMgeneratesasequenceofphonemesand\ndiscretesub-phonemicstates(suchasthebeginning,middle,andendofeach\n4 5 8\nCHAPTER12.APPLICATIONS\nphoneme),thenaGMMtransformseachdiscretesymbolintoabriefsegmentof\naudiowaveform.AlthoughGMM-HMMsystemsdominatedASRuntilrecently,\nspeechrecognitionwasactuallyoneoftheﬁrstareaswhereneuralnetworkswere\napplied,andnumerousASRsystemsfromthelate1980sandearly1990sused\nneuralnets(BourlardandWellekens1989Waibel1989Robinsonand ,; e t a l .,;\nFallside1991Bengio19911992Konig 1996 ,; e t a l .,,; e t a l .,).Atthetime,the\nperformanceofASRbasedonneuralnetsapproximately matchedtheperformance\nofGMM-HMMsystems.Forexample,RobinsonandFallside1991()achieved\n26%phonemeerrorrateontheTIMIT( ,)corpus(with39 Garofolo e t a l .1993\nphonemestodiscriminatebetween), whichwasbetterthanorcomparableto\nHMM-basedsystems.Sincethen,TIMIThasbeenabenchmarkforphoneme\nrecognition,playingarolesimilartotheroleMNISTplaysforobjectrecognition.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1151, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1049}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1152_3923abbe", "text": "However,becauseofthecomplexengineeringinvolvedinsoftwaresystemsfor\nspeechrecognitionandtheeﬀortthathadbeeninvestedinbuildingthesesystems\nonthebasisofGMM-HMMs,theindustrydidnotseeacompellingargument\nforswitchingtoneuralnetworks.Asaconsequence,untilthelate2000s,both\nacademicandindustrialresearchinusingneuralnetsforspeechrecognitionmostly\nfocusedonusingneuralnetstolearnextrafeaturesforGMM-HMMsystems. Later,with m u c h l a r g e r a nd d e e p e r m o d e l sandmuchlargerdatasets,recognition\naccuracywasdramatically improvedbyusingneuralnetworkstoreplaceGMMs\nforthetaskofassociatingacousticfeaturestophonemes(orsub-phonemicstates).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1152, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 633}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1153_3bfe9639", "text": "Startingin2009,speechresearchersappliedaformofdeeplearningbasedon\nunsupervisedlearningtospeechrecognition.Thisapproachtodeeplearningwas\nbasedontrainingundirectedprobabilisticmodelscalledrestrictedBoltzmann\nmachines(RBMs)tomodeltheinputdata.RBMswillbedescribedinpart.III\nTosolvespeechrecognitiontasks,unsupervisedpretrainingwasusedtobuild\ndeepfeedforwardnetworkswhoselayerswereeachinitializedbytraininganRBM.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1153, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 407}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1154_75a6a86e", "text": "Thesenetworkstakespectralacousticrepresentationsinaﬁxed-sizeinputwindow\n(aroundacenterframe)andpredicttheconditionalprobabilities ofHMMstates\nforthatcenterframe.Trainingsuchdeepnetworkshelpedtosigniﬁcantlyimprove\ntherecognitionrateonTIMIT( ,,),bringingdownthe Mohamed e t a l .20092012a\nphonemeerrorratefromabout26%to20.7%.See ()foran Mohamed e t a l .2012b\nanalysisofreasonsforthesuccessofthesemodels.Extensionstothebasicphone\nrecognitionpipelineincludedtheadditionofspeaker-adaptivefeatures(Mohamed\ne t a l .,)thatfurtherreducedtheerrorrate.Thiswasquicklyfollowedup 2011\nbyworktoexpandthearchitecturefromphonemerecognition(whichiswhat\nTIMITisfocusedon)tolarge-vocabulary speechrecognition(,), Dahl e t a l .2012\nwhichinvolvesnotjustrecognizingphonemesbutalsorecognizingsequencesof\nwordsfromalargevocabulary.Deepnetworksforspeechrecognitioneventually\n4 5 9\nCHAPTER12.APPLICATIONS\nshiftedfrombeingbasedonpretrainingandBoltzmannmachinestobeingbased\nontechniquessuchasrectiﬁedlinearunitsanddropout(,; Zeiler e t a l .2013Dahl\ne t a l .,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1154, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1036}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1155_3b8595dd", "text": "Bythattime,severalofthemajorspeechgroupsinindustryhad 2013\nstartedexploringdeeplearningincollaborationwithacademicresearchers.Hinton\ne t a l .()describethebreakthroughs achievedbythesecollaborators,which 2012a\narenowdeployedinproductssuchasmobilephones. Later,asthesegroupsexploredlargerandlargerlabeleddatasetsandincorpo-\nratedsomeofthemethodsforinitializing,training,andsettingupthearchitecture\nofdeepnets,theyrealizedthattheunsupervisedpretrainingphasewaseither\nunnecessaryordidnotbringanysigniﬁcantimprovement.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1155, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 514}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1156_aa61ff4b", "text": "Thesebreakthroughs inrecognitionperformanceforworderrorrateinspeech\nrecognitionwereunprecedented (around30%improvement)andwerefollowinga\nlongperiodofabouttenyearsduringwhicherrorratesdidnotimprovemuchwith\nthetraditionalGMM-HMMtechnology,inspiteofthecontinuouslygrowingsizeof\ntrainingsets(seeﬁgure2.4ofDengandYu2014()).Thiscreatedarapidshiftin\nthespeechrecognitioncommunitytowardsdeeplearning.Inamatterofroughly\ntwoyears,mostoftheindustrialproductsforspeechrecognitionincorporateddeep\nneuralnetworksandthissuccessspurredanewwaveofresearchintodeeplearning\nalgorithmsandarchitectures forASR,whichisstillongoingtoday. Oneoftheseinnovationswastheuseofconvolutionalnetworks( , Sainath e t a l . 2013)thatreplicateweightsacrosstimeandfrequency,improvingovertheearlier\ntime-delayneuralnetworksthatreplicatedweightsonlyacrosstime.Thenew\ntwo-dimensionalconvolutionalmodelsregardtheinputspectrogramnotasone\nlongvectorbutasanimage,withoneaxiscorrespondingtotimeandtheotherto\nfrequencyofspectralcomponents.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1156, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 993}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1157_f0572426", "text": "Anotherimportantpush, stillongoing,hasbeentowardsend-to-enddeep\nlearningspeechrecognitionsystemsthatcompletelyremovetheHMM.Theﬁrst\nmajorbreakthrough inthisdirectioncamefromGraves2013 e t a l .()whotrained\nadeepLSTMRNN(seesection),usingMAPinferenceovertheframe-to- 10.10\nphonemealignment,asin ()andintheCTCframework( LeCun e t a l .1998b Graves\ne t a l .,;2006Graves2012 Graves2013 ,).AdeepRNN( e t a l .,)hasstatevariables\nfromseverallayersateachtimestep,givingtheunfoldedgraphtwokindsofdepth:\nordinarydepthduetoastackoflayers,anddepthduetotimeunfolding. This\nworkbroughtthephonemeerrorrateonTIMITtoarecordlowof17.7%.See\nPascanu2014aChung2014 e t a l .()and e t a l .()forothervariantsofdeepRNNs,\nappliedinothersettings. Anothercontemporarysteptowardend-to-enddeeplearningASRistoletthe\nsystemlearnhowto“align”theacoustic-levelinformationwiththephonetic-level\n4 6 0\nCHAPTER12.APPLICATIONS\ninformation( ,;,). Chorowski e t a l .2014Lu e t a l .2015\n12.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1157, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 950}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1158_94422569", "text": "Chorowski e t a l .2014Lu e t a l .2015\n12. 4 Nat u ra l L an gu a g e Pro c es s i n g\nNaturallanguageprocessing(NLP)istheuseofhumanlanguages,suchas\nEnglishorFrench,byacomputer.Computerprogramstypicallyreadandemit\nspecializedlanguagesdesignedtoalloweﬃcientandunambiguousparsingbysimple\nprograms.Morenaturallyoccurringlanguagesareoftenambiguousanddefyformal\ndescription. Naturallanguageprocessingincludesapplicationssuchasmachine\ntranslation,inwhichthelearnermustreadasentenceinonehumanlanguageand\nemitanequivalentsentenceinanotherhumanlanguage.ManyNLPapplications\narebasedonlanguagemodelsthatdeﬁneaprobabilitydistributionoversequences\nofwords,charactersorbytesinanaturallanguage. Aswiththeotherapplicationsdiscussedinthischapter,verygenericneural\nnetworktechniquescanbesuccessfullyappliedtonaturallanguageprocessing.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1158, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 817}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1159_1d3f40d2", "text": "Aswiththeotherapplicationsdiscussedinthischapter,verygenericneural\nnetworktechniquescanbesuccessfullyappliedtonaturallanguageprocessing. However,toachieveexcellentperformanceandtoscalewelltolargeapplications,\nsomedomain-speciﬁcstrategiesbecomeimportant.Tobuildaneﬃcientmodelof\nnaturallanguage,wemustusuallyusetechniquesthatarespecializedforprocessing\nsequentialdata.Inmanycases,wechoosetoregardnaturallanguageasasequence\nofwords,ratherthanasequenceofindividualcharactersorbytes.Becausethetotal\nnumberofpossiblewordsissolarge,word-basedlanguagemodelsmustoperateon\nanextremelyhigh-dimensionalandsparsediscretespace.Severalstrategieshave\nbeendevelopedtomakemodelsofsuchaspaceeﬃcient,bothinacomputational\nandinastatisticalsense.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1159, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 724}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1160_61e222bf", "text": "12.4.1-grams n\nAlanguagemodeldeﬁnesaprobabilitydistributionoversequencesoftokens\ninanaturallanguage.Dependingonhowthemodelisdesigned,atokenmay\nbeaword,acharacter,orevenabyte.Tokensarealwaysdiscreteentities.The\nearliestsuccessfullanguagemodelswerebasedonmodelsofﬁxed-lengthsequences\noftokenscalled-grams.An-gramisasequenceoftokens. n n n\nModelsbasedon n-gramsdeﬁnetheconditionalprobabilityofthe n-thtoken\ngiventhepreceding n−1tokens.Themodelusesproductsoftheseconditional\ndistributionstodeﬁnetheprobabilitydistributionoverlongersequences:\nP x(1 , . . . , x τ) = ( P x1 , . . . , x n−1)τ\nt n=P x( t| x t n−+1 , . . . , x t−1) .(12.5)\n4 6 1\nCHAPTER12.APPLICATIONS\nThisdecompositionisjustiﬁedbythechainruleofprobability.Theprobability\ndistributionovertheinitialsequence P( x1 , . . . , x n−1)maybemodeledbyadiﬀerent\nmodelwithasmallervalueof.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1160, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 838}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1161_e1bf0430", "text": ". . , x n−1)maybemodeledbyadiﬀerent\nmodelwithasmallervalueof. n\nTraining n-grammodelsisstraightforwardbecausethemaximumlikelihood\nestimatecanbecomputedsimplybycountinghowmanytimeseachpossible n\ngramoccursinthetrainingset.Modelsbasedon n-gramshavebeenthecore\nbuildingblockofstatisticallanguagemodelingformanydecades(Jelinekand\nMercer1980Katz1987ChenandGoodman1999 ,;,; ,). Forsmallvaluesof n,modelshaveparticularnames:unigramfor n=1,bigram\nfor n=2,andtrigramfor n=3. ThesenamesderivefromtheLatinpreﬁxesfor\nthecorrespondingnumbersandtheGreeksuﬃx“-gram”denotingsomethingthat\niswritten. Usuallywetrainbothan n-grammodelandan n−1 grammodelsimultaneously. Thismakesiteasytocompute\nP x( t| x t n−+1 , . . . , x t−1) =P n( x t n−+1 , . . . , x t)\nP n−1( x t n−+1 , . . . , x t−1)(12.6)\nsimplybylookinguptwostoredprobabilities. Forthistoexactlyreproduce\ninferencein P n,wemustomittheﬁnalcharacterfromeachsequencewhenwe\ntrain P n−1.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1161, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 922}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1162_dfaac5fd", "text": ". . , x t)\nP n−1( x t n−+1 , . . . , x t−1)(12.6)\nsimplybylookinguptwostoredprobabilities. Forthistoexactlyreproduce\ninferencein P n,wemustomittheﬁnalcharacterfromeachsequencewhenwe\ntrain P n−1. Asanexample,wedemonstratehowatrigrammodelcomputestheprobability\nofthesentence“THEDOGRANAWAY.”Theﬁrstwordsofthesentencecannotbe\nhandledbythedefaultformulabasedonconditionalprobabilitybecausethereisno\ncontextatthebeginningofthesentence.Instead,wemustusethemarginalprob-\nabilityoverwordsatthestartofthesentence.Wethusevaluate P3( T H E D O G R A N). Finally,thelastwordmaybepredictedusingthetypicalcase,ofusingthecondi-\ntionaldistribution P( A W A Y D O G R A N | ).Puttingthistogetherwithequation,12.6\nweobtain:\nP P ( ) = T H E D O G R A N A W A Y3( ) T H E D O G R A N P3( ) D O G R A N A W A Y /P2( ) D O G R A N . (12.7)\nAfundamentallimitationofmaximumlikelihoodfor n-grammodelsisthat P n\nasestimatedfromtrainingsetcountsisverylikelytobezeroinmanycases,even\nthoughthetuple ( x t n−+1 , . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1162, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 987}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1163_038e8666", "text": "(12.7)\nAfundamentallimitationofmaximumlikelihoodfor n-grammodelsisthat P n\nasestimatedfromtrainingsetcountsisverylikelytobezeroinmanycases,even\nthoughthetuple ( x t n−+1 , . . . , x t)mayappearinthetestset.Thiscancausetwo\ndiﬀerentkindsofcatastrophicoutcomes.When P n−1iszero,theratioisundeﬁned,\nsothemodeldoesnotevenproduceasensibleoutput.When P n−1isnon-zerobut\nP niszero,thetestlog-likelihoodis−∞. Toavoidsuchcatastrophicoutcomes,\nmost n-grammodelsemploysomeformofsmoothing.Smoothingtechniques\n4 6 2\nCHAPTER12.APPLICATIONS\nshiftprobabilitymassfromtheobservedtuplestounobservedonesthataresimilar.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1163, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 597}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1164_a778f6e9", "text": "Toavoidsuchcatastrophicoutcomes,\nmost n-grammodelsemploysomeformofsmoothing.Smoothingtechniques\n4 6 2\nCHAPTER12.APPLICATIONS\nshiftprobabilitymassfromtheobservedtuplestounobservedonesthataresimilar. See ()forareviewandempiricalcomparisons.Onebasic ChenandGoodman1999\ntechniqueconsistsofaddingnon-zeroprobabilitymasstoallofthepossiblenext\nsymbolvalues.ThismethodcanbejustiﬁedasBayesianinferencewithauniform\norDirichletprioroverthecountparameters.Anotherverypopularideaistoform\namixturemodelcontaininghigher-orderandlower-order n-grammodels,withthe\nhigher-order modelsprovidingmorecapacityandthelower-ordermodelsbeing\nmorelikelytoavoidcountsofzero.Back-oﬀmethodslook-upthelower-order\nn-gramsifthefrequencyofthecontext x t−1 , . . . , x t n−+1istoosmalltousethe\nhigher-ordermodel.Moreformally,theyestimatethedistributionover x tbyusing\ncontexts x t n k −+ , . . . , x t−1,forincreasing k,untilasuﬃcientlyreliableestimateis\nfound.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1164, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 925}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1165_8238760c", "text": ". . , x t n−+1istoosmalltousethe\nhigher-ordermodel.Moreformally,theyestimatethedistributionover x tbyusing\ncontexts x t n k −+ , . . . , x t−1,forincreasing k,untilasuﬃcientlyreliableestimateis\nfound. Classical n-grammodelsareparticularlyvulnerabletothecurseofdimension-\nality.Thereare|| Vnpossible n-gramsand|| Visoftenverylarge.Evenwitha\nmassivetrainingsetandmodest n,most n-gramswillnotoccurinthetrainingset. Onewaytoviewaclassical n-grammodelisthatitisperformingnearest-neighbor\nlookup.Inotherwords,itcanbeviewedasalocalnon-parametric predictor,\nsimilarto k-nearestneighbors.Thestatisticalproblemsfacingtheseextremely\nlocalpredictorsaredescribedinsection.Theproblemforalanguagemodel 5.11.2\nisevenmoreseverethanusual,becauseanytwodiﬀerentwordshavethesamedis-\ntancefromeachotherinone-hotvectorspace.Itisthusdiﬃculttoleveragemuch\ninformationfromany“neighbors”—onlytrainingexamplesthatrepeatliterallythe\nsamecontextareusefulforlocalgeneralization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1165, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 947}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1166_677f2466", "text": "T oovercometheseproblems,a\nlanguagemodelmustbeabletoshareknowledgebetweenonewordandother\nsemanticallysimilarwords. Toimprovethestatisticaleﬃciencyof n-grammodels,class-basedlanguage\nmodels(Brown1992NeyandKneser1993Niesler1998 e t a l .,; ,; e t a l .,)introduce\nthenotionofwordcategoriesandthensharestatisticalstrengthbetweenwordsthat\nareinthesamecategory.Theideaistouseaclusteringalgorithmtopartitionthe\nsetofwordsintoclustersorclasses,basedontheirco-occurrencefrequencieswith\notherwords.ThemodelcanthenusewordclassIDsratherthanindividualword\nIDstorepresentthecontextontherightsideoftheconditioningbar.Composite\nmodelscombiningword-basedandclass-basedmodelsviamixingorback-oﬀare\nalsopossible.Althoughwordclassesprovideawaytogeneralizebetweensequences\ninwhichsomewordisreplacedbyanotherofthesameclass,muchinformationis\nlostinthisrepresentation.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1166, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 844}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1167_81b61301", "text": "4 6 3\nCHAPTER12.APPLICATIONS\n12.4.2NeuralLanguageModels\nNeurallanguagemodelsorNLMsare aclassoflanguagemodeldesigned\ntoovercomethecurseofdimensionalityproblemformodelingnaturallanguage\nsequencesbyusingadistributedrepresentationofwords( ,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1167, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 238}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1168_ee43c4ea", "text": "Bengio e t a l .2001\nUnlikeclass-based n-grammodels,neurallanguagemodelsareabletorecognize\nthattwowordsaresimilarwithoutlosingtheabilitytoencodeeachwordasdistinct\nfromtheother.Neurallanguagemodelssharestatisticalstrengthbetweenone\nword(anditscontext)andothersimilarwordsandcontexts.Thedistributed\nrepresentationthemodellearnsforeachwordenablesthissharingbyallowingthe\nmodeltotreatwordsthathavefeaturesincommonsimilarly.Forexample,ifthe\nworddogandthewordcatmaptorepresentationsthatsharemanyattributes,then\nsentencesthatcontainthewordcatcaninformthepredictionsthatwillbemadeby\nthemodelforsentencesthatcontaintheworddog,andvice-versa.Becausethere\naremanysuchattributes,therearemanywaysinwhichgeneralization canhappen,\ntransferringinformationfromeachtrainingsentencetoanexponentiallylarge\nnumberofsemanticallyrelatedsentences.Thecurseofdimensionalityrequiresthe\nmodeltogeneralizetoanumberofsentencesthatisexponentialinthesentence\nlength.Themodelcountersthiscursebyrelatingeachtrainingsentencetoan\nexponentialnumberofsimilarsentences.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1168, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1029}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1169_16fd724b", "text": "Wesometimescallthesewordrepresentationswordembeddings.Inthis\ninterpretation,weviewtherawsymbolsaspointsinaspaceofdimensionequal\ntothevocabularysize.Thewordrepresentationsembedthosepointsinafeature\nspaceoflowerdimension.Intheoriginalspace,everywordisrepresentedby\naone-hotvector,soeverypairofwordsisatEuclideandistance√\n2fromeach\nother.Intheembeddingspace,wordsthatfrequentlyappearinsimilarcontexts\n(oranypairofwordssharingsome“features”learnedbythemodel)arecloseto\neachother.Thisoftenresultsinwordswithsimilarmeaningsbeingneighbors. Figurezoomsinonspeciﬁcareasofalearnedwordembeddingspacetoshow 12.3\nhowsemanticallysimilarwordsmaptorepresentationsthatareclosetoeachother.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1169, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 671}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1170_0e46e484", "text": "Figurezoomsinonspeciﬁcareasofalearnedwordembeddingspacetoshow 12.3\nhowsemanticallysimilarwordsmaptorepresentationsthatareclosetoeachother. Neuralnetworksinotherdomainsalsodeﬁneembeddings.Forexample,a\nhiddenlayerofaconvolutionalnetworkprovidesan“imageembedding.”Usually\nNLPpractitioners aremuchmoreinterestedinthisideaofembeddingsbecause\nnaturallanguagedoesnotoriginallylieinareal-valuedvectorspace.Thehidden\nlayerhasprovidedamorequalitativelydramaticchangeinthewaythedatais\nrepresented. Thebasicideaofusingdistributedrepresentationstoimprovemodelsfor\nnaturallanguageprocessingisnotrestrictedtoneuralnetworks.Itmayalsobe\nusedwithgraphicalmodelsthathavedistributedrepresentationsintheformof\n4 6 4\nCHAPTER12.APPLICATIONS\nmultiplelatentvariables(MnihandHinton2007,). − − − − − 3432302826−14−13−12−11−10−9−8−7−6\nCanadaEuropeOntario\nNorthEnglish\nCanadianUnionAfricanAfrica\nBritishFrance\nRussianChina\nGermanyFrench\nAssemblyEU JapanIraq\nSouthEuropean\n350355360365370375380 . . . . . .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1170, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 976}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1171_05331049", "text": ". . . . . .171819202122\n1995199619971998199920002001\n200220032004\n20052006200720082009\nFigure12.3:Two-dimensionalvisualizationsofwordembeddingsobtainedfromaneural\nmachinetranslationmodel( ,),zoominginonspeciﬁcareaswhere Bahdanau e t a l .2015\nsemanticallyrelatedwordshaveembeddingvectorsthatareclosetoeachother.Countries\nappearontheleftandnumbersontheright.Keepinmindthattheseembeddingsare2-D\nforthepurposeofvisualization.Inrealapplications,embeddingstypicallyhavehigher\ndimensionalityandcansimultaneouslycapturemanykindsofsimilaritybetweenwords.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1171, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 546}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1172_62f8b9ec", "text": "12.4.3High-DimensionalOutputs\nInmanynaturallanguageapplications,weoftenwantourmodelstoproduce\nwords(ratherthancharacters)asthefundamentalunitoftheoutput.Forlarge\nvocabularies,itcanbeverycomputationally expensivetorepresentanoutput\ndistributionoverthechoiceofaword,becausethevocabularysizeislarge.Inmany\napplications, Vcontainshundredsofthousandsofwords.Thenaiveapproachto\nrepresentingsuchadistributionistoapplyanaﬃnetransformationfromahidden\nrepresentationtotheoutputspace,thenapplythesoftmaxfunction.Suppose\nwehaveavocabulary Vwithsize|| V.Theweightmatrixdescribingthelinear\ncomponentofthisaﬃnetransformationisverylarge,becauseitsoutputdimension\nis|| V.Thisimposesahighmemorycosttorepresentthematrix,andahigh\ncomputational costtomultiplybyit.Becausethesoftmaxisnormalizedacrossall\n|| Voutputs,itisnecessarytoperformthefullmatrixmultiplicationattraining\ntimeaswellastesttime—wecannotcalculateonlythedotproductwiththeweight\nvectorforthecorrectoutput.Thehighcomputational costsoftheoutputlayer\nthusarisebothattrainingtime(tocomputethelikelihoodanditsgradient)and\nattesttime(tocomputeprobabilities forallorselectedwords).Forspecialized\n4 6 5\nCHAPTER12.APPLICATIONS\nlossfunctions,thegradientcanbecomputedeﬃciently( ,),but Vincent e t a l .2015\nthestandardcross-entropylossappliedtoatraditionalsoftmaxoutputlayerposes\nmanydiﬃculties.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1172, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1328}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1173_a1c7197b", "text": "Supposethathisthetophiddenlayerusedtopredicttheoutputprobabilities\nˆy.IfweparametrizethetransformationfromhtoˆywithlearnedweightsW\nandlearnedbiasesb,thentheaﬃne-softmaxoutputlayerperformsthefollowing\ncomputations:\na i= b i+\njW i j h j∀∈{ ||} i1 , . . . , V , (12.8)\nˆ y i=ea i\n|| V\ni=1 eai . (12.9)\nIfhcontains n helementsthentheaboveoperationis O(|| V n h).With n hinthe\nthousandsand|| Vinthehundredsofthousands,thisoperationdominatesthe\ncomputationofmostneurallanguagemodels. 12.4.3.1UseofaShortList\nTheﬁrstneurallanguagemodels( ,,)dealtwiththehighcost Bengio e t a l .20012003\nofusingasoftmaxoveralargenumberofoutputwordsbylimitingthevocabulary\nsizeto10,000or20,000words.SchwenkandGauvain2002Schwenk2007 ()and ()\nbuiltuponthisapproachbysplittingthevocabulary Vintoashortlist Lofmost\nfrequentwords(handledbytheneuralnet)andatail T= V L\\ofmorerarewords\n(handledbyan n-grammodel).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1173, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 884}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1174_0d39568d", "text": "Tobeabletocombinethetwopredictions,the\nneuralnetalsohastopredicttheprobabilitythatawordappearingaftercontext\nCbelongstothetaillist.Thismaybeachievedbyaddinganextrasigmoidoutput\nunittoprovideanestimateof P( i C ∈| T ).Theextraoutputcanthenbeusedto\nachieveanestimateoftheprobabilitydistributionoverallwordsinasfollows: V\nP y i C (= |) =1 i∈ L P y i C, i P i C (= | ∈ − L)(1 (∈| T ))\n+1 i∈ T P y i C, i P i C (= | ∈ T)(∈| T )(12.10)\nwhere P( y= i C, i| ∈ L)isprovidedbytheneurallanguagemodeland P( y= i|\nC, i∈ T) isprovidedbythe n-grammodel.Withslightmodiﬁcation,thisapproach\ncanalsoworkusinganextraoutputvalueintheneurallanguagemodel’ssoftmax\nlayer,ratherthanaseparatesigmoidunit. Anobviousdisadvantageoftheshortlistapproachisthatthepotentialgener-\nalizationadvantageoftheneurallanguagemodelsislimitedtothemostfrequent\n4 6 6\nCHAPTER12.APPLICATIONS\nwords,where,arguably,itistheleastuseful. Thisdisadvantagehasstimulated\ntheexplorationofalternativemethodstodealwithhigh-dimensionaloutputs,\ndescribedbelow.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1174, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1001}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1175_804571d9", "text": "Thisdisadvantagehasstimulated\ntheexplorationofalternativemethodstodealwithhigh-dimensionaloutputs,\ndescribedbelow. 12.4.3.2HierarchicalSoftmax\nAclassicalapproach(,)toreducingthecomputational burden Goodman2001\nofhigh-dimensionaloutputlayersoverlargevocabularysets Vistodecompose\nprobabilities hierarchically .Insteadofnecessitatinganumberofcomputations\nproportionalto|| V(andalsoproportionaltothenumberofhiddenunits, n h),\nthe|| Vfactorcanbereducedtoaslowaslog|| V.()and Bengio2002Morinand\nBengio2005()introducedthisfactorizedapproachtothecontextofneurallanguage\nmodels. Onecanthinkofthishierarchyasbuildingcategoriesofwords,thencategories\nofcategoriesofwords,thencategoriesofcategoriesofcategoriesofwords,etc. Thesenestedcategoriesformatree,withwordsattheleaves.Inabalancedtree,\nthetreehasdepth O(log|| V). Theprobabilityofachoosingawordisgivenby\ntheproductoftheprobabilities ofchoosingthebranchleadingtothatwordat\neverynodeonapathfromtherootofthetreetotheleafcontainingtheword.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1175, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 979}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1176_9789577c", "text": "Theprobabilityofachoosingawordisgivenby\ntheproductoftheprobabilities ofchoosingthebranchleadingtothatwordat\neverynodeonapathfromtherootofthetreetotheleafcontainingtheword. Figureillustratesasimpleexample. ()alsodescribe 12.4 MnihandHinton2009\nhowtousemultiplepathstoidentifyasinglewordinordertobettermodelwords\nthathavemultiplemeanings.Computingtheprobabilityofawordtheninvolves\nsummationoverallofthepathsthatleadtothatword. Topredicttheconditionalprobabilities requiredateachnodeofthetree,we\ntypicallyusealogisticregressionmodelateachnodeofthetree,andprovidethe\nsamecontext Casinputtoallofthesemodels.Becausethecorrectoutputis\nencodedinthetrainingset,wecanusesupervisedlearningtotrainthelogistic\nregressionmodels.Thisistypicallydoneusingastandardcross-entropyloss,\ncorrespondingtomaximizingthelog-likelihoodofthecorrectsequenceofdecisions.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1176, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 840}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1177_b07f938f", "text": "Becausetheoutputlog-likelihoodcanbecomputedeﬃciently(aslowaslog|| V\nratherthan|| V),itsgradientsmayalsobecomputedeﬃciently.Thisincludesnot\nonlythegradientwithrespecttotheoutputparametersbutalsothegradients\nwithrespecttothehiddenlayeractivations. Itispossiblebutusuallynotpracticaltooptimizethetreestructuretominimize\ntheexpectednumberofcomputations. Toolsfrominformationtheoryspecifyhow\ntochoosetheoptimalbinarycodegiventherelativefrequenciesofthewords.To\ndoso,wecouldstructurethetreesothatthenumberofbitsassociatedwithaword\nisapproximatelyequaltothelogarithmofthefrequencyofthatword.However,in\n4 6 7\nCHAPTER12.APPLICATIONS\n( 1) ( 0)\n( 0, 0, 0) ( 0, 0, 1) ( 0, 1, 0) ( 0, 1, 1) ( 1, 0, 0) ( 1, 0, 1) ( 1, 1, 0) ( 1, 1, 1)( 1, 1) ( 1, 0) ( 0, 1) ( 0, 0)\nw 0 w 0 w 1 w 1 w 2 w 2 w 3 w 3 w 4 w 4 w 5 w 5 w 6 w 6 w 7 w 7\nFigure12.4:Illustrationofasimplehierarchyofwordcategories,with8words w 0 , . . . , w 7\norganizedintoathreelevelhierarchy.Theleavesofthetreerepresentactualspeciﬁcwords.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1177, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 984}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1178_a15c36e8", "text": ". . , w 7\norganizedintoathreelevelhierarchy.Theleavesofthetreerepresentactualspeciﬁcwords. Internalnodesrepresentgroupsofwords.Anynodecanbeindexedbythesequence\nofbinarydecisions(0=left,1=right)toreachthenodefromtheroot.Super-class(0)\ncontainstheclasses(0 ,0) (0and ,1),whichrespectivelycontainthesetsofwords{ w 0 , w 1}\nand{ w 2 , w 3},andsimilarlysuper-classcontainstheclasses (1) (1 ,0) (1and ,1),which\nrespectivelycontainthewords( w 4 , w 5) (and w 6 , w 7).Ifthetreeissuﬃcientlybalanced,\nthemaximumdepth(numberofbinarydecisions)isontheorderofthelogarithmof\nthenumberofwords|| V: thechoiceofoneoutof|| Vwordscanbeobtainedbydoing\nO(log|| V)operations(oneforeachofthenodesonthepathfromtheroot).Inthisexample,\ncomputingtheprobabilityofaword ycanbedonebymultiplyingthreeprobabilities,\nassociatedwiththebinarydecisionstomoveleftorrightateachnodeonthepathfrom\ntheroottoanode y.Let bi( y)bethe i-thbinarydecisionwhentraversingthetree\ntowardsthevalue y.Theprobabilityofsamplinganoutputydecomposesintoaproduct\nofconditionalprobabilities,usingthechainruleforconditionalprobabilities,witheach\nnodeindexedbythepreﬁxofthesebits.Forexample,node(1 ,0)correspondstothe\npreﬁx( b 0( w4) = 1 , b1( w4) = 0),andtheprobabilityof w 4canbedecomposedasfollows:\nP w (= y 4) = ( Pb 0= 1 ,b 1= 0 ,b 2= 0) (12.11)\n= ( Pb 0= 1) ( Pb 1= 0 |b 0= 1) ( Pb 2= 0 |b 0= 1 ,b 1= 0) .(12.12)\n4 6 8\nCHAPTER12.APPLICATIONS\npractice,thecomputational savingsaretypicallynotworththeeﬀortbecausethe\ncomputationoftheoutputprobabilitiesisonlyonepartofthetotalcomputation\nintheneurallanguagemodel.Forexample,supposethereare lfullyconnected\nhiddenlayersofwidth n h.Let n bbetheweightedaverageofthenumberofbits\nrequiredtoidentifyaword,withtheweightinggivenbythefrequencyofthese\nwords.Inthisexample,thenumberofoperationsneededtocomputethehidden\nactivationsgrowsasas O( l n2\nh)whiletheoutputcomputations growas O( n h n b).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1178, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1875}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1179_f6af032e", "text": "Aslongas n b≤ l n h,wecanreducecomputationmorebyshrinking n hthanby\nshrinking n b.Indeed, n bisoftensmall.Becausethesizeofthevocabularyrarely\nexceedsamillionwordsandlog2(106)≈20,itispossibletoreduce n btoabout,20\nbut n hisoftenmuchlarger,around 103ormore.Ratherthancarefullyoptimizing\natreewithabranchingfactorof,onecaninsteaddeﬁneatreewithdepthtwo 2\nandabranchingfactorof\n|| V.Suchatreecorrespondstosimplydeﬁningaset\nofmutuallyexclusivewordclasses.Thesimpleapproachbasedonatreeofdepth\ntwocapturesmostofthecomputational beneﬁtofthehierarchicalstrategy. Onequestionthatremainssomewhatopenishowtobestdeﬁnetheseword\nclasses,orhowtodeﬁnethewordhierarchyingeneral.Earlyworkusedexisting\nhierarchies( ,)butthehierarchycanalsobelearned,ideally MorinandBengio2005\njointlywiththeneurallanguagemodel.Learningthehierarchyisdiﬃcult.Anexact\noptimization ofthelog-likelihoodappearsintractablebecausethechoiceofaword\nhierarchyisadiscreteone,notamenabletogradient-basedoptimization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1179, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 966}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1180_65821b7a", "text": "However,\nonecouldusediscreteoptimization toapproximately optimizethepartitionof\nwordsintowordclasses. Animportantadvantageofthehierarchicalsoftmaxisthatitbringscomputa-\ntionalbeneﬁtsbothattrainingtimeandattesttime,ifattesttimewewantto\ncomputetheprobabilityofspeciﬁcwords. Ofcourse,computingtheprobabilityofall|| Vwordswillremainexpensive\nevenwiththehierarchicalsoftmax.Anotherimportantoperationisselectingthe\nmostlikelywordinagivencontext.Unfortunatelythetreestructuredoesnot\nprovideaneﬃcientandexactsolutiontothisproblem. Adisadvantageisthatinpracticethehierarchicalsoftmaxtendstogiveworse\ntestresultsthansampling-basedmethodswewilldescribenext.Thismaybedue\ntoapoorchoiceofwordclasses.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1180, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 686}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1181_1a5afd3e", "text": "Adisadvantageisthatinpracticethehierarchicalsoftmaxtendstogiveworse\ntestresultsthansampling-basedmethodswewilldescribenext.Thismaybedue\ntoapoorchoiceofwordclasses. 12.4.3.3ImportanceSampling\nOnewaytospeedupthetrainingofneurallanguagemodelsistoavoidexplicitly\ncomputingthecontributionofthegradientfromallofthewordsthatdonotappear\n4 6 9\nCHAPTER12.APPLICATIONS\ninthenextposition.Everyincorrectwordshouldhavelowprobabilityunderthe\nmodel.Itcanbecomputationally costlytoenumerateallofthesewords.Instead,\nitispossibletosampleonlyasubsetofthewords.Usingthenotationintroduced\ninequation,thegradientcanbewrittenasfollows: 12.8\n∂ P y C log(|)\n∂ θ=∂logsoftmax y()a\n∂ θ(12.13)\n=∂\n∂ θlogea y\n\ni ea i(12.14)\n=∂\n∂ θ( a y−log\niea i) (12.15)\n=∂ a y\n∂ θ−\niP y i C (= |)∂ a i\n∂ θ(12.16)\nwhereaisthevectorofpre-softmaxactivations(orscores),withoneelement\nperword.Theﬁrsttermisthepositivephaseterm(pushing a yup)whilethe\nsecondtermisthenegativephaseterm(pushing a idownforall i,withweight\nP( i C|).Sincethenegativephasetermisanexpectation,wecanestimateitwith\naMonteCarlosample.However,thatwouldrequiresamplingfromthemodelitself.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1181, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1109}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1182_97ced2af", "text": "Samplingfromthemodelrequirescomputing P( i C|)forall iinthevocabulary,\nwhichispreciselywhatwearetryingtoavoid. Insteadofsamplingfromthemodel,onecansamplefromanotherdistribution,\ncalledtheproposaldistribution(denoted q),anduseappropriateweightstocorrect\nforthebiasintroducedbysamplingfromthewrongdistribution(Bengioand\nSénécal2003BengioandSénécal2008 ,; ,).Thisisanapplicationofamoregeneral\ntechniquecalledimportancesampling,whichwillbedescribedinmoredetail\ninsection.Unfortunately,evenexactimportancesamplingisnoteﬃcient 17.2\nbecauseitrequirescomputingweights p i /q i,where p i= P( i C|),whichcan\nonlybecomputedifallthescores a iarecomputed.Thesolutionadoptedfor\nthisapplicationiscalledbiasedimportancesampling,wheretheimportance\nweightsarenormalizedtosumto1.Whennegativeword n iissampled,the\nassociatedgradientisweightedby\nw i=p n i /q n iN\nj=1 p n j /q n j.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1182, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 861}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1183_c13b7a85", "text": "(12.17)\nTheseweightsareusedtogivetheappropriateimportancetothe mnegative\nsamplesfrom qusedtoformtheestimatednegativephasecontributiontothe\n4 7 0\nCHAPTER12.APPLICATIONS\ngradient:\n|| V\ni=1P i C(|)∂ a i\n∂ θ≈1\nmm \ni=1w i∂ a n i\n∂ θ. (12.18)\nAunigramorabigramdistributionworkswellastheproposaldistribution q.Itis\neasytoestimatetheparametersofsuchadistributionfromdata.Afterestimating\ntheparameters,itisalsopossibletosamplefromsuchadistributionveryeﬃciently. Importancesamplingisnotonlyusefulforspeedingupmodelswithlarge\nsoftmaxoutputs.Moregenerally,itisusefulforacceleratingtrainingwithlarge\nsparseoutputlayers,wheretheoutputisasparsevectorratherthana-of-1 n\nchoice.Anexampleisabagofwords.Abagofwordsisasparsevectorv\nwhere v iindicatesthepresenceorabsenceofword ifromthevocabularyinthe\ndocument.Alternately, v icanindicatethenumberoftimesthatword iappears.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1183, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 853}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1184_3049e6c2", "text": "Machinelearningmodelsthatemitsuchsparsevectorscanbeexpensivetotrain\nforavarietyofreasons.Earlyinlearning,themodelmaynotactuallychooseto\nmaketheoutputtrulysparse.Moreover,thelossfunctionweusefortrainingmight\nmostnaturallybedescribedintermsofcomparingeveryelementoftheoutputto\neveryelementofthetarget.Thismeansthatitisnotalwaysclearthatthereisa\ncomputational beneﬁttousingsparseoutputs,becausethemodelmaychooseto\nmakethemajorityoftheoutputnon-zeroandallofthesenon-zerovaluesneedto\nbecomparedtothecorrespondingtrainingtarget,evenifthetrainingtargetiszero. Dauphin 2011 e t a l .()demonstratedthatsuchmodelscanbeacceleratedusing\nimportancesampling.Theeﬃcientalgorithmminimizesthelossreconstructionfor\nthe“positivewords”(thosethatarenon-zerointhetarget)andanequalnumber\nof“negativewords.”Thenegativewordsarechosenrandomly,usingaheuristicto\nsamplewordsthataremorelikelytobemistaken. Thebiasintroducedbythis\nheuristicoversamplingcanthenbecorrectedusingimportanceweights.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1184, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 963}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1185_078e5674", "text": "Thebiasintroducedbythis\nheuristicoversamplingcanthenbecorrectedusingimportanceweights. Inallofthesecases,thecomputational complexityofgradientestimationfor\ntheoutputlayerisreducedtobeproportionaltothenumberofnegativesamples\nratherthanproportionaltothesizeoftheoutputvector. 12.4.3.4Noise-ContrastiveEstimationandRankingLoss\nOtherapproachesbasedonsamplinghavebeenproposedtoreducethecomputa-\ntionalcostoftrainingneurallanguagemodelswithlargevocabularies.Anearly\nexampleistherankinglossproposedbyCollobertandWeston2008a(),which\nviewstheoutputoftheneurallanguagemodelforeachwordasascoreandtriesto\nmakethescoreofthecorrectword a yberankedhighincomparisontotheother\n4 7 1\nCHAPTER12.APPLICATIONS\nscores a i.Therankinglossproposedthenis\nL=\nimax(01 ,− a y+ a i) .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1185, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 755}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1186_09dbeee3", "text": "(12.19)\nThegradientiszeroforthe i-thtermifthescoreoftheobservedword, a y,is\ngreaterthanthescoreofthenegativeword a ibyamarginof1.Oneissuewith\nthiscriterionisthatitdoesnotprovideestimatedconditionalprobabilities, which\nareusefulinsomeapplications,includingspeechrecognitionandtextgeneration\n(includingconditionaltextgenerationtaskssuchastranslation). Amorerecentlyusedtrainingobjectiveforneurallanguagemodelisnoise-\ncontrastiveestimation,whichisintroducedinsection.Thisapproachhas 18.6\nbeensuccessfullyappliedtoneurallanguagemodels(MnihandTeh2012Mnih,;\nandKavukcuoglu2013,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1186, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 573}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1187_ef57f1f2", "text": "12.4.4CombiningNeuralLanguageModelswith-grams n\nAmajoradvantageof n-grammodelsoverneuralnetworksisthat n-grammodels\nachievehighmodelcapacity(bystoringthefrequenciesofverymanytuples)\nwhilerequiringverylittlecomputationtoprocessanexample(bylookingup\nonlyafewtuplesthatmatchthecurrentcontext).Ifweusehashtablesortrees\ntoaccessthecounts,thecomputationusedfor n-gramsisalmostindependent\nofcapacity.Incomparison,doublinganeuralnetwork’snumberofparameters\ntypicallyalsoroughlydoublesitscomputationtime.Exceptionsincludemodels\nthatavoidusingallparametersoneachpass.Embeddinglayersindexonlyasingle\nembeddingineachpass,sowecanincreasethevocabularysizewithoutincreasing\nthecomputationtimeperexample.Someothermodels,suchastiledconvolutional\nnetworks,canaddparameterswhilereducingthedegreeofparametersharing\ninordertomaintainthesameamountofcomputation. However,typicalneural\nnetworklayersbasedonmatrixmultiplication useanamountofcomputation\nproportionaltothenumberofparameters.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1187, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 964}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1188_8ef631d7", "text": "However,typicalneural\nnetworklayersbasedonmatrixmultiplication useanamountofcomputation\nproportionaltothenumberofparameters. Oneeasywaytoaddcapacityisthustocombinebothapproachesinanensemble\nconsistingofaneurallanguagemodelandan n-gramlanguagemodel(Bengio\ne t a l .,,).Aswithanyensemble,thistechniquecanreducetesterrorif 20012003\ntheensemblemembersmakeindependentmistakes.Theﬁeldofensemblelearning\nprovidesmanywaysofcombiningtheensemblemembers’predictions,including\nuniformweightingandweightschosenonavalidationset.Mikolov2011a e t a l .()\nextendedtheensembletoincludenotjusttwomodelsbutalargearrayofmodels.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1188, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 606}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1189_e60221fc", "text": "Itisalsopossibletopairaneuralnetworkwithamaximumentropymodeland\ntrainbothjointly(Mikolov2011b e t a l .,).Thisapproachcanbeviewedastraining\n4 7 2\nCHAPTER12.APPLICATIONS\naneuralnetworkwithanextrasetofinputsthatareconnecteddirectlytothe\noutput,andnotconnectedtoanyotherpartofthemodel.Theextrainputsare\nindicatorsforthepresenceofparticular n-gramsintheinputcontext,sothese\nvariablesareveryhigh-dimensionalandverysparse.Theincreaseinmodelcapacity\nishuge—thenewportionofthearchitecturecontainsupto|| s Vnparameters—but\ntheamountofaddedcomputationneededtoprocessaninputisminimalbecause\ntheextrainputsareverysparse. 12.4.5NeuralMachineTranslation\nMachinetranslationisthetaskofreadingasentenceinonenaturallanguageand\nemittingasentencewiththeequivalentmeaninginanotherlanguage.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1189, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 768}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1190_21d98309", "text": "12.4.5NeuralMachineTranslation\nMachinetranslationisthetaskofreadingasentenceinonenaturallanguageand\nemittingasentencewiththeequivalentmeaninginanotherlanguage. Mac hine\ntranslationsystemsofteninvolvemanycomponents.Atahighlevel,thereis\noftenonecomponentthatproposesmanycandidatetranslations.Manyofthese\ntranslationswillnotbegrammaticalduetodiﬀerencesbetweenthelanguages.For\nexample,manylanguagesputadjectivesafternouns,sowhentranslatedtoEnglish\ndirectlytheyyieldphrasessuchas“applered.”Theproposalmechanismsuggests\nmanyvariantsofthesuggestedtranslation,ideallyincluding“redapple.”Asecond\ncomponentofthetranslationsystem,alanguagemodel,evaluatestheproposed\ntranslations,andcanscore“redapple”asbetterthan“applered.”\nTheearliestuseofneuralnetworksformachinetranslationwastoupgradethe\nlanguagemodelofatranslationsystembyusinganeurallanguagemodel(Schwenk\ne t a l .,;2006Schwenk2010,).Previously,mostmachinetranslationsystemshad\nusedan n-grammodelforthiscomponent.The n-grambasedmodelsusedfor\nmachinetranslationincludenotjusttraditionalback-oﬀ n-grammodels(Jelinek\nandMercer1980Katz1987ChenandGoodman1999 ,;,; ,)butalsomaximum\nentropylanguagemodels(,),inwhichanaﬃne-softmaxlayer Berger e t a l .1996\npredictsthenextwordgiventhepresenceoffrequent-gramsinthecontext.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1190, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1257}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1191_337cca3a", "text": "n\nTraditionallanguagemodelssimplyreporttheprobabilityofanaturallanguage\nsentence.Becausemachinetranslationinvolvesproducinganoutputsentencegiven\naninputsentence,itmakessensetoextendthenaturallanguagemodeltobe\nconditional.Asdescribedinsection,itisstraightforwardtoextendamodel 6.2.1.1\nthatdeﬁnesamarginaldistributionoversomevariabletodeﬁneaconditional\ndistributionoverthatvariablegivenacontext C,where Cmightbeasinglevariable\noralistofvariables. ()beatthestate-of-the-art insomestatistical Devlin e t a l .2014\nmachinetranslationbenchmarksbyusinganMLPtoscoreaphraset1 ,t2 , . . . ,t k\ninthetargetlanguagegivenaphrases1 ,s2 , . . . ,s ninthesourcelanguage.The\nMLPestimates P(t1 ,t2 , . . . ,t k|s1 ,s2 , . . . ,s n).TheestimateformedbythisMLP\nreplacestheestimateprovidedbyconditional-grammodels.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1191, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 793}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1192_7671e2ae", "text": ". . ,s ninthesourcelanguage.The\nMLPestimates P(t1 ,t2 , . . . ,t k|s1 ,s2 , . . . ,s n).TheestimateformedbythisMLP\nreplacestheestimateprovidedbyconditional-grammodels. n\n4 7 3\nCHAPTER12.APPLICATIONS\nD e c ode rO ut put ob j e c t   ( E ngl i s h \ns e nt e nc e )\nI nt e r m e di at e ,   s e m a n t i c   r e pr e s e nt a t i o n\nSourc e   ob j e c t   ( F r e nc h  s e n t e nc e   or   i m a g e )E nc ode r\nFigure12.5:Theencoder-decoderarchitecturetomapbackandforthbetweenasurface\nrepresentation(suchasasequenceofwordsoranimage)andasemanticrepresentation. Byusingtheoutputofanencoderofdatafromonemodality(suchastheencodermapping\nfromFrenchsentencestohiddenrepresentationscapturingthemeaningofsentences)as\ntheinputtoadecoderforanothermodality(suchasthedecodermappingfromhidden\nrepresentationscapturingthemeaningofsentencestoEnglish),wecantrainsystemsthat\ntranslatefromonemodalitytoanother.Thisideahasbeenappliedsuccessfullynotjust\ntomachinetranslationbutalsotocaptiongenerationfromimages.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1192, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 993}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1193_7a6f4b3c", "text": "AdrawbackoftheMLP-basedapproachisthatitrequiresthesequencestobe\npreprocessedtobeofﬁxedlength.Tomakethetranslationmoreﬂexible,wewould\nliketouseamodelthatcanaccommodatevariablelengthinputsandvariable\nlengthoutputs.AnRNNprovidesthisability.Section describesseveralways 10.2.4\nofconstructinganRNNthatrepresentsaconditionaldistributionoverasequence\ngivensomeinput,andsectiondescribeshowtoaccomplishthisconditioning 10.4\nwhentheinputisasequence.Inallcases,onemodelﬁrstreadstheinputsequence\nandemitsadatastructurethatsummarizestheinputsequence.Wecallthis\nsummarythe“context” C.Thecontext Cmaybealistofvectors,oritmaybea\nvectorortensor.Themodelthatreadstheinputtoproduce CmaybeanRNN\n(,; Cho e t a l .2014aSutskever2014Jean2014 e t a l .,; e t a l .,)oraconvolutional\nnetwork(KalchbrennerandBlunsom2013,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1193, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 796}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1194_ce650eac", "text": "Asecondmodel,usuallyanRNN,\nthenreadsthecontext Candgeneratesasentenceinthetargetlanguage.This\ngeneralideaofanencoder-decoderframeworkformachinetranslationisillustrated\ninﬁgure.12.5\nInordertogenerateanentiresentenceconditionedonthesourcesentence,the\nmodelmusthaveawaytorepresenttheentiresourcesentence. Earliermodels\nwereonlyabletorepresentindividualwordsorphrases. Fromarepresentation\n4 7 4\nCHAPTER12.APPLICATIONS\nlearningpointofview,itcanbeusefultolearnarepresentationinwhichsentences\nthathavethesamemeaninghavesimilarrepresentationsregardlessofwhether\ntheywerewritteninthesourcelanguageorthetargetlanguage.Thisstrategywas\nexploredﬁrstusingacombinationofconvolutionsandRNNs(Kalchbrennerand\nBlunsom2013,).LaterworkintroducedtheuseofanRNNforscoringproposed\ntranslations(,)andforgeneratingtranslatedsentences( Cho e t a l .2014a Sutskever\ne t a l . e t a l . ,).2014Jean()scaledthesemodelstolargervocabularies.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1194, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 908}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1195_1e51ea65", "text": "e t a l . ,).2014Jean()scaledthesemodelstolargervocabularies. 2014\n12.4.5.1UsinganAttentionMechanismandAligningPiecesofData\nα( t − 1 )α( t − 1 )α( ) tα( ) tα( + 1 ) tα( + 1 ) t\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) tc c\n× × × × × ×+\nFigure12.6:Amodernattentionmechanism,asintroducedby (),is Bahdanau e t a l .2015\nessentiallyaweightedaverage.Acontextvectorcisformedbytakingaweightedaverage\noffeaturevectorsh( ) twithweights α( ) t.Insomeapplications,thefeaturevectorshare\nhiddenunitsofaneuralnetwork,buttheymayalsoberawinputtothemodel.The\nweights α( ) tareproducedbythemodelitself.Theyareusuallyvaluesintheinterval\n[0 ,1]andareintendedtoconcentratearoundjustoneh( ) tsothattheweightedaverage\napproximatesreadingthatonespeciﬁctimestepprecisely.Theweights α( ) tareusually\nproducedbyapplyingasoftmaxfunctiontorelevancescoresemittedbyanotherportion\nofthemodel.Theattentionmechanismismoreexpensivecomputationallythandirectly\nindexingthedesiredh( ) t,butdirectindexingcannotbetrainedwithgradientdescent.The\nattentionmechanismbasedonweightedaveragesisasmooth,diﬀerentiableapproximation\nthatcanbetrainedwithexistingoptimizationalgorithms.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1195, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1144}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1196_e05e335f", "text": "Usingaﬁxed-sizerepresentationtocaptureallthesemanticdetailsofavery\nlongsentenceofsay60wordsisverydiﬃcult. Itcanbeachievedbytraininga\nsuﬃcientlylargeRNNwellenoughandforlongenough,asdemonstratedbyCho\ne t a l .()and2014aSutskever2014 e t a l .().However,amoreeﬃcientapproachis\ntoreadthewholesentenceorparagraph(togetthecontextandthegistofwhat\n4 7 5\nCHAPTER12.APPLICATIONS\nisbeingexpressed),thenproducethetranslatedwordsoneatatime,eachtime\nfocusingonadiﬀerentpartoftheinputsentenceinordertogatherthesemantic\ndetailsthatarerequiredtoproducethenextoutputword. Thatisexactlythe\nideathat ()ﬁrstintroduced.Theattentionmechanismused Bahdanau e t a l .2015\ntofocusonspeciﬁcpartsoftheinputsequenceateachtimestepisillustratedin\nﬁgure.12.6\nWecanthinkofanattention-basedsystemashavingthreecomponents:\n1.Aprocessthat“ r e a d s”rawdata(suchassourcewordsinasourcesentence),\nandconvertsthemintodistributedrepresentations,withonefeaturevector\nassociatedwitheachwordposition.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1196, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 955}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1197_8255d8df", "text": "2.Alistoffeaturevectorsstoringtheoutputofthereader.Thiscanbe\nunderstoodasa“” containingasequenceoffacts,whichcanbe m e m o r y\nretrievedlater,notnecessarilyinthesameorder,withouthavingtovisitall\nofthem. 3.Aprocessthat“”thecontentofthememorytosequentiallyperform e x p l o i t s\natask,ateachtimestephavingtheabilityputattentiononthecontentof\nonememoryelement(orafew,withadiﬀerentweight). Thethirdcomponentgeneratesthetranslatedsentence. Whenwordsinasentencewritteninonelanguagearealignedwithcorrespond-\ningwordsinatranslatedsentenceinanotherlanguage,itbecomespossibletorelate\nthecorrespondingwordembeddings.Earlierworkshowedthatonecouldlearna\nkindoftranslationmatrixrelatingthewordembeddingsinonelanguagewiththe\nwordembeddingsinanother(Kočiský2014 e t a l .,),yieldingloweralignmenterror\nratesthantraditionalapproachesbasedonthefrequencycountsinthephrasetable.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1197, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 859}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1198_063f0af7", "text": "Thereisevenearlierworkonlearningcross-lingualwordvectors(Klementiev e t a l .,\n2012).Manyextensionstothisapproacharepossible.Forexample,moreeﬃcient\ncross-lingualalignment( ,)allowstrainingonlargerdatasets. Gouws e t a l .2014\n12.4.6HistoricalPerspective\nTheideaofdistributedrepresentationsforsymbolswasintroducedbyRumelhart\ne t a l .()inoneoftheﬁrstexplorationsofback-propagation, withsymbols 1986a\ncorrespondingtotheidentityoffamilymembersandtheneuralnetworkcapturing\ntherelationshipsbetweenfamilymembers,withtrainingexamplesformingtriplets\nsuchas(Colin,Mother,Victoria). The ﬁrstlayeroftheneuralnetworklearned\narepresentationofeachfamilymember.Forexample, thefeaturesforColin\n4 7 6\nCHAPTER12.APPLICATIONS\nmightrepresentwhichfamilytreeColinwasin,whatbranchofthattreehewas\nin,whatgenerationhewasfrom,etc.Onecanthinkoftheneuralnetworkas\ncomputinglearnedrulesrelatingtheseattributestogetherinordertoobtainthe\ndesiredpredictions.Themodelcanthenmakepredictionssuchasinferringwhois\nthemotherofColin.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1198, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 994}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1199_5a06a321", "text": "Theideaofforminganembeddingforasymbolwasextendedtotheideaofan\nembeddingforawordbyDeerwester1990 e t a l .().Theseembeddingswerelearned\nusingtheSVD.Later,embeddingswouldbelearnedbyneuralnetworks. Thehistoryofnaturallanguageprocessingismarkedbytransitionsinthe\npopularityofdiﬀerentwaysofrepresentingtheinputtothemodel.Following\nthisearlyworkonsymbolsorwords,someoftheearliestapplicationsofneural\nnetworkstoNLP( ,; Miikkulainen andDyer1991Schmidhuber1996,)represented\ntheinputasasequenceofcharacters.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1199, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 497}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1200_fdb08ad4", "text": "Bengio2001 e t a l .()returnedthefocustomodelingwordsandintroduced\nneurallanguagemodels,whichproduceinterpretable wordembeddings.These\nneuralmodelshavescaledupfromdeﬁningrepresentationsofasmallsetofsymbols\ninthe1980stomillionsofwords(includingpropernounsandmisspellings)in\nmodernapplications.Thiscomputational scalingeﬀortledtotheinventionofthe\ntechniquesdescribedaboveinsection.12.4.3\nInitially,theuseofwordsasthefundamentalunitsoflanguagemodelsyielded\nimprovedlanguage modeling performance( ,).Tothisday, Bengio e t a l .2001\nnewtechniquescontinuallypushbothcharacter-based models(Sutskever e t a l .,\n2011)andword-basedmodelsforward,withrecentwork( ,)even Gillick e t a l .2015\nmodelingindividualbytesofUnicodecharacters.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1200, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 724}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1201_58826a54", "text": "Theideasbehindneurallanguagemodelshavebeenextendedintoseveral\nnaturallanguageprocessingapplications,suchasparsing(,,; Henderson20032004\nCollobert2011,),part-of-speechtagging,semanticrolelabeling,chunking,etc,\nsometimesusingasinglemulti-tasklearningarchitecture(CollobertandWeston,\n2008aCollobert2011a ; e t a l .,)inwhichthewordembeddingsaresharedacross\ntasks. Two-dimensionalvisualizationsofembeddingsbecameapopulartoolforan-\nalyzinglanguagemodelsfollowingthedevelopmentofthet-SNEdimensionality\nreductionalgorithm(vanderMaatenandHinton2008,)anditshigh-proﬁleappli-\ncationtovisualizationwordembeddingsbyJosephTurianin2009. 4 7 7\nCHAPTER12.APPLICATIONS\n12. 5 O t h er A p p l i c a t i o n s\nInthissectionwecoverafewothertypesofapplicationsofdeeplearningthat\narediﬀerentfromthestandardobjectrecognition,speechrecognitionandnatural\nlanguageprocessingtasksdiscussedabove.Partofthisbookwillexpandthat III\nscopeevenfurthertotasksthatremainprimarilyresearchareas.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1201, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 957}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1202_4804ca45", "text": "12.5.1RecommenderSystems\nOneofthemajorfamiliesofapplicationsofmachinelearningintheinformation\ntechnologysectoristheabilitytomakerecommendations ofitemstopotential\nusersorcustomers.Twomajortypesofapplicationscanbedistinguished:online\nadvertisinganditemrecommendations (oftentheserecommendations arestillfor\nthepurposeofsellingaproduct).Bothrelyonpredictingtheassociationbetween\nauserandanitem,eithertopredicttheprobabilityofsomeaction(theuser\nbuyingtheproduct,orsomeproxyforthisaction)ortheexpectedgain(which\nmaydependonthevalueoftheproduct)ifanadisshownorarecommendation is\nmaderegardingthatproducttothatuser.Theinternetiscurrentlyﬁnancedin\ngreatpartbyvariousformsofonlineadvertising. Therearemajorpartsofthe\neconomythatrelyononlineshopping. CompaniesincludingAmazonandeBay\nusemachinelearning,includingdeeplearning,fortheirproductrecommendations .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1202, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 847}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1203_3973dc57", "text": "Therearemajorpartsofthe\neconomythatrelyononlineshopping. CompaniesincludingAmazonandeBay\nusemachinelearning,includingdeeplearning,fortheirproductrecommendations . Sometimes,theitemsarenotproductsthatareactuallyforsale.Examplesinclude\nselectingpoststodisplayonsocialnetworknewsfeeds,recommendingmoviesto\nwatch,recommendingjokes,recommendingadvicefromexperts,matchingplayers\nforvideogames,ormatchingpeopleindatingservices. Often,thisassociationproblemishandledlikeasupervisedlearningproblem:\ngivensomeinformationabouttheitemandabouttheuser,predicttheproxyof\ninterest(userclicksonad,userentersarating,userclicksona“like”button,user\nbuysproduct,userspendssomeamountofmoneyontheproduct,userspends\ntimevisitingapagefortheproduct,etc).Thisoftenendsupbeingeithera\nregressionproblem(predictingsomeconditionalexpectedvalue)oraprobabilistic\nclassiﬁcationproblem(predictingtheconditionalprobabilityofsomediscrete\nevent).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1203, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 908}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1204_b4da73ec", "text": "Theearlyworkonrecommendersystemsreliedonminimalinformationas\ninputsforthesepredictions:theuserIDandtheitemID.Inthiscontext,the\nonlywaytogeneralizeistorelyonthesimilaritybetweenthepatternsofvaluesof\nthetargetvariablefordiﬀerentusersorfordiﬀerentitems.Supposethatuser1\nanduser2bothlikeitemsA,BandC.Fromthis,wemayinferthatuser1and\n4 7 8\nCHAPTER12.APPLICATIONS\nuser2havesimilartastes.Ifuser1likesitemD,thenthisshouldbeastrong\ncuethatuser2willalsolikeD.Algorithmsbasedonthisprinciplecomeunder\nthenameofcollaborativeﬁltering.Bothnon-parametric approaches(suchas\nnearest-neighbormethodsbasedontheestimatedsimilaritybetweenpatternsof\npreferences)andparametricmethodsarepossible.Parametricmethodsoftenrely\nonlearningadistributedrepresentation(alsocalledanembedding)foreachuser\nandforeachitem.Bilinearpredictionofthetargetvariable(suchasarating)isa\nsimpleparametricmethodthatishighlysuccessfulandoftenfoundasacomponent\nofstate-of-the-art systems.Thepredictionisobtainedbythedotproductbetween\ntheuserembeddingandtheitemembedding(possiblycorrectedbyconstantsthat\ndependonlyoneithertheuserIDortheitemID).LetˆRbethematrixcontaining\nourpredictions,AamatrixwithuserembeddingsinitsrowsandBamatrixwith\nitemembeddingsinitscolumns.Letbandcbevectorsthatcontainrespectively\nakindofbiasforeachuser(representinghowgrumpyorpositivethatuseris\ningeneral)andforeachitem(representingitsgeneralpopularity).Thebilinear\npredictionisthusobtainedasfollows:\nˆ R u , i= b u+ c i+\njA u , j B j , i .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1204, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1463}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1205_3bd9c045", "text": "(12.20)\nTypicallyonewantstominimizethesquarederrorbetweenpredictedratings\nˆ R u , iandactualratings R u , i.Userembeddingsanditemembeddingscanthenbe\nconvenientlyvisualizedwhentheyareﬁrstreducedtoalowdimension(twoor\nthree),ortheycanbeusedtocompareusersoritemsagainsteachother,just\nlikewordembeddings. One waytoobtaintheseembeddingsisbyperforminga\nsingularvaluedecompositionofthematrixRofactualtargets(suchasratings). ThiscorrespondstofactorizingR=UDV(oranormalizedvariant)intothe\nproductoftwofactors,thelowerrankmatricesA=UDandB=V.One\nproblemwiththeSVDisthatittreatsthemissingentriesinanarbitraryway,\nasiftheycorrespondedtoatargetvalueof0.Insteadwewouldliketoavoid\npayinganycostforthepredictionsmadeonmissingentries.Fortunately,thesum\nofsquarederrorsontheobservedratingscanalsobeeasilyminimizedbygradient-\nbasedoptimization.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1205, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 825}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1206_7814d931", "text": "TheSVDandthebilinearpredictionofequation both12.20\nperformedverywellinthecompetitionfortheNetﬂixprize( , BennettandLanning\n2007),aimingatpredictingratingsforﬁlms,basedonlyonpreviousratingsby\nalargesetofanonymoususers. Manymachinelearningexpertsparticipatedin\nthiscompetition,whichtookplacebetween2006and2009.Itraisedthelevelof\nresearchinrecommendersystemsusingadvancedmachinelearningandyielded\nimprovementsinrecommendersystems.Eventhoughitdidnotwinbyitself,\nthesimplebilinearpredictionorSVDwasacomponentoftheensemblemodels\n4 7 9\nCHAPTER12.APPLICATIONS\npresentedbymostofthecompetitors,includingthewinners( ,; Töscher e t a l .2009\nKoren2009,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1206, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 642}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1207_b58c8593", "text": "Beyondthesebilinearmodelswithdistributedrepresentations,oneoftheﬁrst\nusesofneuralnetworksforcollaborativeﬁlteringisbasedontheRBMundirected\nprobabilisticmodel(Salakhutdinov2007 e t a l .,).RBMswereanimportantelement\noftheensembleofmethodsthatwontheNetﬂixcompetition(Töscher2009 e t a l .,;\nKoren2009,).Moreadvancedvariantsontheideaoffactorizingtheratingsmatrix\nhavealsobeenexploredintheneuralnetworkscommunity(Salakhutdinovand\nMnih2008,). However,thereisabasiclimitationofcollaborativeﬁlteringsystems:whena\nnewitemoranewuserisintroduced,itslackofratinghistorymeansthatthere\nisnowaytoevaluateitssimilaritywithotheritemsorusers(respectively),or\nthedegreeofassociationbetween,say,thatnewuserandexistingitems.This\niscalledtheproblemofcold-startrecommendations .Ageneralwayofsolving\nthecold-startrecommendation problemistointroduceextrainformationabout\ntheindividualusersanditems.Forexample,thisextrainformationcouldbeuser\nproﬁleinformationorfeaturesofeachitem.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1207, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 955}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1208_b93cc88c", "text": "Systems thatusesuchinformation\narecalledcontent-basedrecommendersystems.Themappingfromarich\nsetofuserfeaturesoritemfeaturestoanembeddingcanbelearnedthrougha\ndeeplearningarchitecture( ,; Huang e t a l .2013Elkahky2015 e t a l .,). Specializeddeeplearningarchitecturessuchasconvolutionalnetworkshavealso\nbeenappliedtolearntoextractfeaturesfromrichcontentsuchasfrommusical\naudiotracks,formusicrecommendation (vandenOörd2013 e t a l .,).Inthatwork,\ntheconvolutionalnettakesacousticfeaturesasinputandcomputesanembedding\nfortheassociatedsong.Thedotproductbetweenthissongembeddingandthe\nembeddingforauseristhenusedtopredictwhetherauserwilllistentothesong.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1208, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 648}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1209_58853d08", "text": "12.5.1.1ExplorationVersusExploitation\nWhenmakingrecommendations tousers,anissuearisesthatgoesbeyondordinary\nsupervisedlearningandintotherealmofreinforcementlearning.Manyrecom-\nmendationproblemsaremostaccuratelydescribedtheoreticallyascontextual\nbandits( ,;,).Theissueisthatwhenwe LangfordandZhang2008Lu e t a l .2010\nusetherecommendation systemtocollectdata,wegetabiasedandincomplete\nviewofthepreferencesofusers:weonlyseetheresponsesofuserstotheitems\ntheywererecommendedandnottotheotheritems.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1209, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 492}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1210_ad032f2d", "text": "Inaddition,insomecases\nwemaynotgetanyinformationonusersforwhomnorecommendation hasbeen\nmade(forexample,withadauctions,itmaybethatthepriceproposedforan\n4 8 0\nCHAPTER12.APPLICATIONS\nadwasbelowaminimumpricethreshold,ordoesnotwintheauction,sothe\nadisnotshownatall).Moreimportantly,wegetnoinformationaboutwhat\noutcomewouldhaveresultedfromrecommendinganyoftheotheritems.This\nwouldbeliketrainingaclassiﬁerbypickingoneclassˆ yforeachtrainingexample\nx(typicallytheclasswiththehighestprobabilityaccordingtothemodel)and\nthenonlygettingasfeedbackwhetherthiswasthecorrectclassornot.Clearly,\neachexampleconveyslessinformationthaninthesupervisedcasewherethetrue\nlabel yisdirectlyaccessible,somoreexamplesarenecessary.Worse,ifwearenot\ncareful,wecouldendupwithasystemthatcontinuespickingthewrongdecisions\nevenasmoreandmoredataiscollected,becausethecorrectdecisioninitiallyhada\nverylowprobability:untilthelearnerpicksthatcorrectdecision,itdoesnotlearn\naboutthecorrectdecision.Thisissimilartothesituationinreinforcementlearning\nwhereonlytherewardfortheselectedactionisobserved.Ingeneral,reinforcement\nlearningcaninvolveasequenceofmanyactionsandmanyrewards.Thebandits\nscenarioisaspecialcaseofreinforcementlearning,inwhichthelearnertakesonly\nasingleactionandreceivesasinglereward.Thebanditproblemiseasierinthe\nsensethatthelearnerknowswhichrewardisassociatedwithwhichaction.In\nthegeneralreinforcementlearningscenario,ahighrewardoralowrewardmight\nhavebeencausedbyarecentactionorbyanactioninthedistantpast.Theterm\ncontextualbanditsreferstothecasewheretheactionistakeninthecontextof\nsomeinputvariablethatcaninformthedecision.Forexample,weatleastknow\ntheuseridentity,andwewanttopickanitem.Themappingfromcontextto\nactionisalsocalledapolicy.Thefeedbackloopbetweenthelearnerandthedata\ndistribution(whichnowdependsontheactionsofthelearner)isacentralresearch\nissueinthereinforcementlearningandbanditsliterature.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1210, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1880}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1211_f1ddb39b", "text": "Reinforcementlearningrequireschoosingatradeoﬀbetweenexplorationand\nexploitation.Exploitationreferstotakingactionsthatcomefromthecurrent,\nbestversionofthelearnedpolicy—actionsthatweknowwillachieveahighreward. Explorationreferstotakingactionsspeciﬁcallyinordertoobtainmoretraining\ndata.Ifweknowthatgivencontextx,action agivesusarewardof1,wedonot\nknowwhetherthatisthebestpossiblereward.Wemaywanttoexploitourcurrent\npolicyandcontinuetakingaction ainordertoberelativelysureofobtaininga\nrewardof1.However,wemayalsowanttoexplorebytryingaction a.Wedonot\nknowwhatwillhappenifwetryaction a.Wehopetogetarewardof,butwe 2\nruntheriskofgettingarewardof.Eitherway,weatleastgainsomeknowledge. 0\nExplorationcanbeimplementedinmanyways,rangingfromoccasionally\ntakingrandomactionsintendedtocovertheentirespaceofpossibleactions,to\nmodel-basedapproachesthatcomputeachoiceofactionbasedonitsexpected\nrewardandthemodel’samountofuncertaintyaboutthatreward.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1211, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 931}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1212_99bee916", "text": "4 8 1\nCHAPTER12.APPLICATIONS\nManyfactorsdeterminetheextenttowhichwepreferexplorationorexploitation. Oneofthemostprominentfactorsisthetimescaleweareinterestedin. Ifthe\nagenthasonlyashortamountoftimetoaccruereward,thenweprefermore\nexploitation.Iftheagenthasalongtimetoaccruereward,thenwebeginwith\nmoreexplorationsothatfutureactionscanbeplannedmoreeﬀectivelywithmore\nknowledge.Astimeprogressesandourlearnedpolicyimproves,wemovetoward\nmoreexploitation. Supervised learninghas notradeoﬀ between explorationand exploitation\nbecausethesupervisionsignalalwaysspeciﬁeswhichoutputiscorrectforeach\ninput.Thereisnoneedtotryoutdiﬀerentoutputstodetermineifoneisbetter\nthanthemodel’scurrentoutput—wealwaysknowthatthelabelisthebestoutput.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1212, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 722}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1213_309ad8b4", "text": "Anotherdiﬃcultyarisinginthecontextofreinforcementlearning,besidesthe\nexploration-exploitationtrade-oﬀ,isthediﬃcultyofevaluatingandcomparing\ndiﬀerentpolicies.Reinforcementlearninginvolvesinteractionbetweenthelearner\nandtheenvironment.Thisfeedbackloopmeansthatitisnotstraightforwardto\nevaluatethelearner’sperformanceusingaﬁxedsetoftestsetinputvalues.The\npolicyitselfdetermineswhichinputswillbeseen. ()present Dudik e t a l .2011\ntechniquesforevaluatingcontextualbandits.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1213, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 468}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1214_c0d8ed39", "text": "()present Dudik e t a l .2011\ntechniquesforevaluatingcontextualbandits. 12.5.2KnowledgeRepresentation,ReasoningandQuestionAn-\nswering\nDeeplearningapproacheshavebeenverysuccessfulinlanguagemodeling,machine\ntranslationandnaturallanguageprocessingduetotheuseofembeddingsfor\nsymbols( ,)andwords( Rumelhart e t a l .1986a Deerwester1990Bengio e t a l .,; e t a l .,\n2001).Theseembeddingsrepresentsemanticknowledgeaboutindividualwords\nandconcepts.Aresearchfrontieristodevelopembeddingsforphrasesandfor\nrelationsbetweenwordsandfacts.Searchenginesalreadyusemachinelearningfor\nthispurposebutmuchmoreremainstobedonetoimprovethesemoreadvanced\nrepresentations. 12.5.2.1Knowledge,RelationsandQuestionAnswering\nOneinterestingresearchdirectionisdetermininghowdistributedrepresentations\ncanbetrainedtocapturetherelationsbetweentwoentities.Theserelations\nallowustoformalizefactsaboutobjectsandhowobjectsinteractwitheachother.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1214, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 908}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1215_015ea41f", "text": "Inmathematics,abinaryrelationisasetoforderedpairsofobjects.Pairs\nthatareinthesetaresaidtohavetherelationwhilethosewhoarenotintheset\n4 8 2\nCHAPTER12.APPLICATIONS\ndonot.Forexample,wecandeﬁnetherelation“islessthan”onthesetofentities\n{1 ,2 ,3}bydeﬁningthesetoforderedpairs S={(1 ,2) ,(1 ,3) ,(2 ,3)}.Oncethis\nrelationisdeﬁned,wecanuseitlikeaverb.Because(1 ,2)∈ S,wesaythat1is\nlessthan2.Because(2 ,1)∈ S,wecannotsaythat2islessthan1.Ofcourse,the\nentitiesthatarerelatedtooneanotherneednotbenumbers.Wecoulddeﬁnea\nrelation containingtupleslike(,). is_a_type_of dogmammal\nInthecontextofAI,wethinkofarelationasasentenceinasyntactically\nsimpleandhighlystructuredlanguage.Therelationplaystheroleofaverb,\nwhiletwoargumentstotherelationplaytheroleofitssubjectandobject.These\nsentencestaketheformofatripletoftokens\n(subjectverbobject) , , (12.21)\nwithvalues\n(entityi ,relation j ,entityk) . (12.22)\nWecanalsodeﬁneanattribute,aconceptanalogoustoarelation,buttaking\nonlyoneargument:\n(entity i ,attribute j) .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1215, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 991}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1216_218aaa14", "text": "(12.22)\nWecanalsodeﬁneanattribute,aconceptanalogoustoarelation,buttaking\nonlyoneargument:\n(entity i ,attribute j) . (12.23)\nForexample,wecoulddeﬁnethehas_furattribute,andapplyittoentitieslike\ndog. Manyapplicationsrequirerepresentingrelationsandreasoningaboutthem. Howshouldwebestdothiswithinthecontextofneuralnetworks? Machinelearningmodelsofcourserequiretrainingdata.Wecaninferrelations\nbetweenentitiesfromtrainingdatasetsconsistingofunstructurednaturallanguage.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1216, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 463}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1217_73ae2031", "text": "Howshouldwebestdothiswithinthecontextofneuralnetworks? Machinelearningmodelsofcourserequiretrainingdata.Wecaninferrelations\nbetweenentitiesfromtrainingdatasetsconsistingofunstructurednaturallanguage. Therearealsostructureddatabasesthatidentifyrelationsexplicitly.Acommon\nstructureforthesedatabasesistherelationaldatabase,whichstoresthissame\nkindofinformation, alb eit notformattedasthreetokensentences.Whena\ndatabaseisintendedtoconveycommonsense knowledgeabouteverydaylifeor\nexpertknowledgeaboutanapplicationareatoanartiﬁcialintelligencesystem,\nwecallthedatabaseaknowledgebase.Knowledgebasesrangefromgeneral\noneslikeFreebase,OpenCyc,WordNet,orWikibase,1etc.tomorespecialized\nknowledgebases,likeGeneOntology.2Representationsforentitiesandrelations\ncanbelearnedbyconsideringeachtripletinaknowledgebaseasatrainingexample\nandmaximizingatrainingobjectivethatcapturestheirjointdistribution(Bordes\ne t a l .,).2013a\n1R e s p e c t i v e l y a v a i l a b l e   f ro m t h e s e   w e b   s i t e s : f r e e b a s e .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1217, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 1010}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1218_be98ecd3", "text": "c o m , c y c . c o m / o p e n c y c , w o r d n e t . p r i n c e t o n . e d u w i k i b a . s e ,\n2g e n e o n t o l o g y . o r g\n4 8 3\nCHAPTER12.APPLICATIONS\nInadditiontotrainingdata,wealsoneedtodeﬁneamodelfamilytotrain.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1218, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 226}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1219_26cb7601", "text": "p r i n c e t o n . e d u w i k i b a . s e ,\n2g e n e o n t o l o g y . o r g\n4 8 3\nCHAPTER12.APPLICATIONS\nInadditiontotrainingdata,wealsoneedtodeﬁneamodelfamilytotrain. Acommonapproachistoextendneurallanguagemodelstomodelentitiesand\nrelations.Neurallanguagemodelslearnavectorthatprovidesadistributed\nrepresentationofeachword.Theyalsolearnaboutinteractionsbetweenwords,\nsuchaswhichwordislikelytocomeafterasequenceofwords,bylearningfunctions\nofthesevectors.Wecanextendthisapproachtoentitiesandrelationsbylearning\nanembeddingvectorforeachrelation.Infact,theparallelbetweenmodeling\nlanguageandmodelingknowledgeencodedasrelationsissoclosethatresearchers\nhavetrainedrepresentationsofsuchentitiesbyusing b o t h a nd knowledgebases\nnaturallanguagesentences( ,,; Bordes e t a l .20112012Wang2014a e t a l .,)or\ncombiningdatafrommultiplerelationaldatabases( ,).Many Bordes e t a l .2013b\npossibilitiesexistfortheparticularparametrization associatedwithsuchamodel.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1219, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 956}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1220_13ee1144", "text": "Earlyworkonlearningaboutrelationsbetweenentities( , PaccanaroandHinton\n2000)positedhighlyconstrainedparametricforms(“linearrelationalembeddings”),\noftenusingadiﬀerentformofrepresentationfortherelationthanfortheentities. Forexample,PaccanaroandHinton2000Bordes2011 ()and e t a l .()usedvectorsfor\nentitiesandmatricesforrelations,withtheideathatarelationactslikeanoperator\nonentities.Alternatively,relationscanbeconsideredasanyotherentity(Bordes\ne t a l .,),allowingustomakestatementsaboutrelations,butmoreﬂexibilityis 2012\nputinthemachinerythatcombinestheminordertomodeltheirjointdistribution. Apracticalshort-termapplicationofsuchmodelsislinkprediction:predict-\ningmissingarcsintheknowledgegraph.Thisisaformofgeneralization tonew\nfacts,basedonoldfacts.Mostoftheknowledgebasesthatcurrentlyexisthave\nbeenconstructedthroughmanuallabor,whichtendstoleavemanyandprobably\nthemajorityoftruerelationsabsentfromtheknowledgebase.SeeWang e t a l .", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1220, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 935}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1221_d95a4863", "text": "(),()and ()forexamplesofsuchan 2014bLin e t a l .2015Garcia-Duran e t a l .2015\napplication. Evaluatingtheperformanceofamodelonalinkpredictiontaskisdiﬃcult\nbecausewehaveonlyadatasetofpositiveexamples(factsthatareknownto\nbetrue). Ifthemodelproposesafactthatisnotinthedataset,weareunsure\nwhetherthemodelhasmadeamistakeordiscoveredanew,previouslyunknown\nfact.Themetricsarethussomewhatimpreciseandarebasedontestinghowthe\nmodelranksaheld-outofsetofknowntruepositivefactscomparedtootherfacts\nthatarelesslikelytobetrue.Acommonwaytoconstructinterestingexamples\nthatareprobablynegative(factsthatareprobablyfalse)istobeginwithatrue\nfactandcreatecorruptedversionsofthatfact,forexamplebyreplacingoneentity\nintherelationwithadiﬀerententityselectedatrandom.Thepopularprecisionat\n10%metriccountshowmanytimesthemodelranksa“correct”factamongthe\ntop10%ofallcorruptedversionsofthatfact.", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1221, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 867}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1222_d7b23c45", "text": "4 8 4\nCHAPTER12.APPLICATIONS\nAnotherapplicationofknowledgebasesanddistributedrepresentationsfor\nthemisword-sensedisambiguation(NavigliandVelardi2005Bordes,; e t a l .,\n2012),whichisthetaskofdecidingwhichofthesensesofawordistheappropriate\none,insomecontext. Eventually,knowledgeofrelationscombinedwithareasoningprocessand\nunderstandingofnaturallanguagecouldallowustobuildageneralquestion\nansweringsystem.Ageneralquestionansweringsystemmustbeabletoprocess\ninputinformationandrememberimportantfacts,organizedinawaythatenables\nittoretrieveandreasonaboutthemlater.Thisremainsadiﬃcultopenproblem\nwhichcanonlybesolvedinrestricted“toy”environments.Currently,thebest\napproachtorememberingandretrievingspeciﬁcdeclarativefactsistousean\nexplicitmemorymechanism,asdescribedinsection.Memorynetworkswere 10.12\nﬁrstproposedtosolveatoyquestionansweringtask(Weston2014Kumar e t a l .,).", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1222, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 868}}
{"id": "computer_science_goodfellow_deep_learning_chunk_1223_38ed8426", "text": "e t a l .()haveproposedanextensionthatusesGRUrecurrentnetstoread 2015\ntheinputintothememoryandtoproducetheanswergiventhecontentsofthe\nmemory. Deeplearninghasbeenappliedtomanyotherapplicationsbesidestheones\ndescribedhere,andwillsurelybeappliedtoevenmoreafterthiswriting.Itwould\nbeimpossibletodescribeanythingremotelyresemblingacomprehensivecoverage\nofsuchatopic.Thissurveyprovidesarepresentativesampleofwhatispossible\nasofthiswriting. Thisconcludespart,whichhasdescribedmodernpracticesinvolvingdeep II\nnetworks,comprisingallofthemostsuccessfulmethods.Generallyspeaking,these\nmethodsinvolveusingthegradientofacostfunctiontoﬁndtheparametersofa\nmodelthatapproximates somedesiredfunction.Withenoughtrainingdata,this\napproachisextremelypowerful.Wenowturntopart,inwhichwestepintothe III\nterritoryofresearch—methodsthataredesignedtoworkwithlesstrainingdata\nortoperformagreatervarietyoftasks,wherethechallengesaremorediﬃcult\nandnotasclosetobeingsolvedasthesituationswehavedescribedsofar. 4 8 5", "metadata": {"book_id": "computer_science_goodfellow_deep_learning", "book_title": "Goodfellow_Deep_Learning", "category": "computer_science", "chunk_index": 1223, "total_chunks": 1224, "source_file": "Goodfellow_Deep_Learning.pdf", "file_type": "pdf", "char_count": 984}}
