{"id": "computer_science_murphy_probabilistic_ml_chunk_0000_17eaa4ba", "text": "Probabilistic Machine Learning\nAdaptive Computation and Machine Learning\nThomas Dietterich, Editor\nChristopher Bishop, David Heckerman, Michael Jordan, and Michael Kearns, Associate Editors\nBioinformatics: The Machine Learning Approach , Pierre Baldi and Søren Brunak\nReinforcement Learning: An Introduction , Richard S. Sutton and Andrew G. Barto\nGraphical Models for Machine Learning and Digital Communication , Brendan J. Frey\nLearning in Graphical Models , Michael I. Jordan\nCausation, Prediction, and Search , second edition, Peter Spirtes, Clark Glymour, and Richard\nScheines\nPrinciples of Data Mining , David Hand, Heikki Mannila, and Padhraic Smyth\nBioinformatics: The Machine Learning Approach , second edition, Pierre Baldi and Søren Brunak\nLearning Kernel Classiﬁers: Theory and Algorithms , Ralf Herbrich\nLearning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond ,\nBernhard Schölkopf and Alexander J.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 0, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 943}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0001_13a924fc", "text": "Smola\nIntroduction to Machine Learning , Ethem Alpaydin\nGaussian Processes for Machine Learning , Carl Edward Rasmussen and Christopher K.I. Williams\nSemi-Supervised Learning , Olivier Chapelle, Bernhard Schölkopf, and Alexander Zien, Eds. The Minimum Description Length Principle , Peter D. Grünwald\nIntroduction to Statistical Relational Learning , Lise Getoor and Ben Taskar, Eds. Probabilistic Graphical Models: Principles and Techniques , Daphne Koller and Nir Friedman\nIntroduction to Machine Learning , second edition, Ethem Alpaydin\nBoosting: Foundations and Algorithms , Robert E. Schapire and Yoav Freund\nMachine Learning: A Probabilistic Perspective , Kevin P. Murphy\nFoundations of Machine Learning , Mehryar Mohri, Afshin Rostami, and Ameet Talwalker\nProbabilistic Machine Learning: An Introduction , Kevin P. Murphy\nProbabilistic Machine Learning\nAn Introduction\nKevin P.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 885}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0002_4265ad7b", "text": "Murphy\nProbabilistic Machine Learning\nAn Introduction\nKevin P. Murphy\nThe MIT Press\nCambridge, Massachusetts\nLondon, England\n©2022 Massachusetts Institute of Technology\nThis work is subject to a Creative Commons CC-BY-NC-ND license. Subject to such license, all rights are reserved. The MIT Press would like to thank the anonymous peer reviewers who provided comments on drafts of this\nbook. The generous work of academic experts is essential for establishing the authority and quality of our\npublications. We acknowledge with gratitude the contributions of these otherwise uncredited readers. Printed and bound in the United States of America. Library of Congress Cataloging-in-Publication Data\nNames: Murphy, Kevin P., author. Title: Probabilistic machine learning : an introduction / Kevin P. Murphy. Description: Cambridge, Massachusetts : The MIT Press, [2022]\nSeries: Adaptive computation and machine learning series\nIncludes bibliographical references and index.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 2, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 969}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0003_0713808c", "text": "Murphy. Description: Cambridge, Massachusetts : The MIT Press, [2022]\nSeries: Adaptive computation and machine learning series\nIncludes bibliographical references and index. Identiﬁers: LCCN 2021027430 | ISBN 9780262046824 (hardcover)\nSubjects: LCSH: Machine learning. | Probabilities. Classiﬁcation: LCC Q325.5 .M872 2022 | DDC 006.3/1–dc23\nLC record available at https://lccn.loc.gov/2021027430\n10 9 8 7 6 5 4 3 2 1\nThis book is dedicated to my mother, Brigid Murphy,\nwho introduced me to the joy of learning and teaching.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 3, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 524}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0004_af16d01c", "text": "Brief Contents\n1 Introduction 1\nI Foundations 29\n2 Probability: Univariate Models 31\n3 Probability: Multivariate Models 75\n4 Statistics 103\n5 Decision Theory 163\n6 Information Theory 199\n7 Linear Algebra 221\n8 Optimization 269\nII Linear Models 315\n9 Linear Discriminant Analysis 317\n10 Logistic Regression 333\n11 Linear Regression 365\n12 Generalized Linear Models * 409\nIII Deep Neural Networks 417\n13 Neural Networks for Structured Data 419\n14 Neural Networks for Images 461\n15 Neural Networks for Sequences 497\nIV Nonparametric Models 539\n16 Exemplar-based Methods 541\n17 Kernel Methods * 561\n18 Trees, Forests, Bagging, and Boosting 597\nviii BRIEF CONTENTS\nV Beyond Supervised Learning 619\n19 Learning with Fewer Labeled Examples 621\n20 Dimensionality Reduction 651\n21 Clustering 709\n22 Recommender Systems 735\n23 Graph Embeddings * 747\nA Notation 767\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\nContents\nPreface xxvii\n1 Introduction 1\n1.1 What is machine learning?", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 4, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1000}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0005_0e8df58f", "text": "August 27, 2021\nContents\nPreface xxvii\n1 Introduction 1\n1.1 What is machine learning? 1\n1.2 Supervised learning 1\n1.2.1 Classiﬁcation 2\n1.2.2 Regression 8\n1.2.3 Overﬁtting and generalization 12\n1.2.4 No free lunch theorem 13\n1.3 Unsupervised learning 14\n1.3.1 Clustering 14\n1.3.2 Discovering latent “factors of variation” 15\n1.3.3 Self-supervised learning 16\n1.3.4 Evaluating unsupervised learning 16\n1.4 Reinforcement learning 17\n1.5 Data 19\n1.5.1 Some common image datasets 19\n1.5.2 Some common text datasets 21\n1.5.3 Preprocessing discrete input data 23\n1.5.4 Preprocessing text data 24\n1.5.5 Handling missing data 26\n1.6 Discussion 27\n1.6.1 The relationship between ML and other ﬁelds 27\n1.6.2 Structure of the book 28\n1.6.3 Caveats 28\nI Foundations 29\n2 Probability: Univariate Models 31\n2.1 Introduction 31\n2.1.1 What is probability?", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 5, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 839}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0006_cb7c6051", "text": "31\nx CONTENTS\n2.1.2 Types of uncertainty 31\n2.1.3 Probability as an extension of logic 32\n2.2 Random variables 33\n2.2.1 Discrete random variables 33\n2.2.2 Continuous random variables 34\n2.2.3 Sets of related random variables 36\n2.2.4 Independence and conditional independence 37\n2.2.5 Moments of a distribution 38\n2.2.6 Limitations of summary statistics * 41\n2.3 Bayes’ rule 43\n2.3.1 Example: Testing for COVID-19 44\n2.3.2 Example: The Monty Hall problem 45\n2.3.3 Inverse problems * 47\n2.4 Bernoulli and binomial distributions 47\n2.4.1 Deﬁnition 47\n2.4.2 Sigmoid (logistic) function 48\n2.4.3 Binary logistic regression 50\n2.5 Categorical and multinomial distributions 51\n2.5.1 Deﬁnition 51\n2.5.2 Softmax function 52\n2.5.3 Multiclass logistic regression 53\n2.5.4 Log-sum-exp trick 54\n2.6 Univariate Gaussian (normal) distribution 55\n2.6.1 Cumulative distribution function 55\n2.6.2 Probability density function 56\n2.6.3 Regression 57\n2.6.4 Why is the Gaussian distribution so widely used?", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 6, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0007_26f18d6d", "text": "58\n2.6.5 Dirac delta function as a limiting case 58\n2.7 Some other common univariate distributions * 59\n2.7.1 Student tdistribution 59\n2.7.2 Cauchy distribution 60\n2.7.3 Laplace distribution 61\n2.7.4 Beta distribution 61\n2.7.5 Gamma distribution 62\n2.7.6 Empirical distribution 63\n2.8 Transformations of random variables * 64\n2.8.1 Discrete case 64\n2.8.2 Continuous case 64\n2.8.3 Invertible transformations (bijections) 65\n2.8.4 Moments of a linear transformation 67\n2.8.5 The convolution theorem 68\n2.8.6 Central limit theorem 69\n2.8.7 Monte Carlo approximation 70\n2.9 Exercises 71\nDraft of “Probabilistic Machine Learning: An Introduction”.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 7, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 642}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0008_6b17966b", "text": "August 27, 2021\nCONTENTS xi\n3 Probability: Multivariate Models 75\n3.1 Joint distributions for multiple random variables 75\n3.1.1 Covariance 75\n3.1.2 Correlation 76\n3.1.3 Uncorrelated does not imply independent 77\n3.1.4 Correlation does not imply causation 77\n3.1.5 Simpson’s paradox 78\n3.2 The multivariate Gaussian (normal) distribution 79\n3.2.1 Deﬁnition 79\n3.2.2 Mahalanobis distance 81\n3.2.3 Marginals and conditionals of an MVN * 82\n3.2.4 Example: conditioning a 2d Gaussian 83\n3.2.5 Example: Imputing missing values * 83\n3.3 Linear Gaussian systems * 84\n3.3.1 Bayes rule for Gaussians 85\n3.3.2 Derivation * 85\n3.3.3 Example: Inferring an unknown scalar 86\n3.3.4 Example: inferring an unknown vector 88\n3.3.5 Example: sensor fusion 89\n3.4 The exponential family * 90\n3.4.1 Deﬁnition 90\n3.4.2 Example 91\n3.4.3 Log partition function is cumulant generating function 92\n3.4.4 Maximum entropy derivation of the exponential family 92\n3.5 Mixture models 93\n3.5.1 Gaussian mixture models 94\n3.5.2 Bernoulli mixture models 95\n3.6 Probabilistic graphical models * 96\n3.6.1 Representation 97\n3.6.2 Inference 99\n3.6.3 Learning 100\n3.7 Exercises 100\n4 Statistics 103\n4.1 Introduction 103\n4.2 Maximum likelihood estimation (MLE) 103\n4.2.1 Deﬁnition 103\n4.2.2 Justiﬁcation for MLE 104\n4.2.3 Example: MLE for the Bernoulli distribution 106\n4.2.4 Example: MLE for the categorical distribution 107\n4.2.5 Example: MLE for the univariate Gaussian 107\n4.2.6 Example: MLE for the multivariate Gaussian 108\n4.2.7 Example: MLE for linear regression 110\n4.3 Empirical risk minimization (ERM) 111\n4.3.1 Example: minimizing the misclassiﬁcation rate 111\nAuthor: Kevin P.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 8, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1649}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0009_a2846a4a", "text": "Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 9, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 22}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0010_ca3d8a6a", "text": "CC-BY-NC-ND license\nxii CONTENTS\n4.3.2 Surrogate loss 112\n4.4 Other estimation methods * 112\n4.4.1 The method of moments 112\n4.4.2 Online (recursive) estimation 114\n4.5 Regularization 116\n4.5.1 Example: MAP estimation for the Bernoulli distribution 117\n4.5.2 Example: MAP estimation for the multivariate Gaussian * 118\n4.5.3 Example: weight decay 119\n4.5.4 Picking the regularizer using a validation set 120\n4.5.5 Cross-validation 121\n4.5.6 Early stopping 123\n4.5.7 Using more data 123\n4.6 Bayesian statistics * 124\n4.6.1 Conjugate priors 125\n4.6.2 The beta-binomial model 125\n4.6.3 The Dirichlet-multinomial model 133\n4.6.4 The Gaussian-Gaussian model 137\n4.6.5 Beyond conjugate priors 140\n4.6.6 Credible intervals 141\n4.6.7 Bayesian machine learning 143\n4.6.8 Computational issues 147\n4.7 Frequentist statistics * 150\n4.7.1 Sampling distributions 150\n4.7.2 Gaussian approximation of the sampling distribution of the MLE 151\n4.7.3 Bootstrap approximation of the sampling distribution of any estimator 151\n4.7.4 Conﬁdence intervals 153\n4.7.5 Caution: Conﬁdence intervals are not credible 154\n4.7.6 The bias-variance tradeoﬀ 155\n4.8 Exercises 160\n5 Decision Theory 163\n5.1 Bayesian decision theory 163\n5.1.1 Basics 163\n5.1.2 Classiﬁcation problems 165\n5.1.3 ROC curves 167\n5.1.4 Precision-recall curves 170\n5.1.5 Regression problems 172\n5.1.6 Probabilistic prediction problems 173\n5.2 Bayesian hypothesis testing 175\n5.2.1 Example: Testing if a coin is fair 176\n5.2.2 Bayesian model selection 177\n5.2.3 Occam’s razor 178\n5.2.4 Connection between cross validation and marginal likelihood 179\n5.2.5 Information criteria 180\n5.3 Frequentist decision theory 182\nDraft of “Probabilistic Machine Learning: An Introduction”.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 10, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1716}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0011_4aae07e2", "text": "August 27, 2021\nCONTENTS xiii\n5.3.1 Computing the risk of an estimator 182\n5.3.2 Consistent estimators 185\n5.3.3 Admissible estimators 185\n5.4 Empirical risk minimization 186\n5.4.1 Empirical risk 186\n5.4.2 Structural risk 188\n5.4.3 Cross-validation 189\n5.4.4 Statistical learning theory * 189\n5.5 Frequentist hypothesis testing * 191\n5.5.1 Likelihood ratio test 191\n5.5.2 Null hypothesis signiﬁcance testing (NHST) 192\n5.5.3 p-values 193\n5.5.4 p-values considered harmful 193\n5.5.5 Why isn’t everyone a Bayesian?", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 11, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 512}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0012_91257e63", "text": "195\n5.6 Exercises 197\n6 Information Theory 199\n6.1 Entropy 199\n6.1.1 Entropy for discrete random variables 199\n6.1.2 Cross entropy 201\n6.1.3 Joint entropy 201\n6.1.4 Conditional entropy 202\n6.1.5 Perplexity 203\n6.1.6 Diﬀerential entropy for continuous random variables * 204\n6.2 Relative entropy (KL divergence) * 205\n6.2.1 Deﬁnition 205\n6.2.2 Interpretation 206\n6.2.3 Example: KL divergence between two Gaussians 206\n6.2.4 Non-negativity of KL 206\n6.2.5 KL divergence and MLE 207\n6.2.6 Forward vs reverse KL 208\n6.3 Mutual information * 209\n6.3.1 Deﬁnition 209\n6.3.2 Interpretation 210\n6.3.3 Example 210\n6.3.4 Conditional mutual information 211\n6.3.5 MI as a “generalized correlation coeﬃcient” 212\n6.3.6 Normalized mutual information 213\n6.3.7 Maximal information coeﬃcient 213\n6.3.8 Data processing inequality 215\n6.3.9 Suﬃcient Statistics 216\n6.3.10 Fano’s inequality * 217\n6.4 Exercises 218\n7 Linear Algebra 221\nAuthor: Kevin P. Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 12, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 955}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0013_c3a3bee0", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\nxiv CONTENTS\n7.1 Introduction 221\n7.1.1 Notation 221\n7.1.2 Vector spaces 224\n7.1.3 Norms of a vector and matrix 226\n7.1.4 Properties of a matrix 228\n7.1.5 Special types of matrices 231\n7.2 Matrix multiplication 234\n7.2.1 Vector–vector products 234\n7.2.2 Matrix–vector products 235\n7.2.3 Matrix–matrix products 235\n7.2.4 Application: manipulating data matrices 237\n7.2.5 Kronecker products * 240\n7.2.6 Einstein summation * 240\n7.3 Matrix inversion 241\n7.3.1 The inverse of a square matrix 241\n7.3.2 Schur complements * 242\n7.3.3 The matrix inversion lemma * 243\n7.3.4 Matrix determinant lemma * 243\n7.3.5 Application: deriving the conditionals of an MVN * 244\n7.4 Eigenvalue decomposition (EVD) 245\n7.4.1 Basics 245\n7.4.2 Diagonalization 246\n7.4.3 Eigenvalues and eigenvectors of symmetric matrices 247\n7.4.4 Geometry of quadratic forms 248\n7.4.5 Standardizing and whitening data 248\n7.4.6 Power method 250\n7.4.7 Deﬂation 251\n7.4.8 Eigenvectors optimize quadratic forms 251\n7.5 Singular value decomposition (SVD) 251\n7.5.1 Basics 251\n7.5.2 Connection between SVD and EVD 252\n7.5.3 Pseudo inverse 253\n7.5.4 SVD and the range and null space of a matrix * 254\n7.5.5 Truncated SVD 256\n7.6 Other matrix decompositions * 256\n7.6.1 LU factorization 256\n7.6.2 QR decomposition 257\n7.6.3 Cholesky decomposition 258\n7.7 Solving systems of linear equations * 258\n7.7.1 Solving square systems 259\n7.7.2 Solving underconstrained systems (least norm estimation) 259\n7.7.3 Solving overconstrained systems (least squares estimation) 261\n7.8 Matrix calculus 261\n7.8.1 Derivatives 262\n7.8.2 Gradients 262\nDraft of “Probabilistic Machine Learning: An Introduction”.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 13, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1688}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0014_9cce854d", "text": "August 27, 2021\nCONTENTS xv\n7.8.3 Directional derivative 263\n7.8.4 Total derivative * 263\n7.8.5 Jacobian 263\n7.8.6 Hessian 264\n7.8.7 Gradients of commonly used functions 265\n7.9 Exercises 266\n8 Optimization 269\n8.1 Introduction 269\n8.1.1 Local vs global optimization 269\n8.1.2 Constrained vs unconstrained optimization 271\n8.1.3 Convex vs nonconvex optimization 271\n8.1.4 Smooth vs nonsmooth optimization 275\n8.2 First-order methods 276\n8.2.1 Descent direction 278\n8.2.2 Step size (learning rate) 278\n8.2.3 Convergence rates 280\n8.2.4 Momentum methods 281\n8.3 Second-order methods 283\n8.3.1 Newton’s method 283\n8.3.2 BFGS and other quasi-Newton methods 284\n8.3.3 Trust region methods 285\n8.4 Stochastic gradient descent 286\n8.4.1 Application to ﬁnite sum problems 287\n8.4.2 Example: SGD for ﬁtting linear regression 287\n8.4.3 Choosing the step size (learning rate) 288\n8.4.4 Iterate averaging 291\n8.4.5 Variance reduction * 291\n8.4.6 Preconditioned SGD 292\n8.5 Constrained optimization 295\n8.5.1 Lagrange multipliers 296\n8.5.2 The KKT conditions 297\n8.5.3 Linear programming 299\n8.5.4 Quadratic programming 300\n8.5.5 Mixed integer linear programming * 301\n8.6 Proximal gradient method * 301\n8.6.1 Projected gradient descent 302\n8.6.2 Proximal operator for `1-norm regularizer 303\n8.6.3 Proximal operator for quantization 304\n8.6.4 Incremental (online) proximal methods 305\n8.7 Bound optimization * 306\n8.7.1 The general algorithm 306\n8.7.2 The EM algorithm 306\n8.7.3 Example: EM for a GMM 309\n8.8 Blackbox and derivative free optimization 313\nAuthor: Kevin P.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 14, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1559}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0015_a2846a4a", "text": "Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 15, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 22}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0016_07c9a4d6", "text": "CC-BY-NC-ND license\nxvi CONTENTS\n8.9 Exercises 314\nII Linear Models 315\n9 Linear Discriminant Analysis 317\n9.1 Introduction 317\n9.2 Gaussian discriminant analysis 317\n9.2.1 Quadratic decision boundaries 318\n9.2.2 Linear decision boundaries 319\n9.2.3 The connection between LDA and logistic regression 319\n9.2.4 Model ﬁtting 320\n9.2.5 Nearest centroid classiﬁer 322\n9.2.6 Fisher’s linear discriminant analysis * 322\n9.3 Naive Bayes classiﬁers 326\n9.3.1 Example models 326\n9.3.2 Model ﬁtting 327\n9.3.3 Bayesian naive Bayes 328\n9.3.4 The connection between naive Bayes and logistic regression 329\n9.4 Generative vs discriminative classiﬁers 330\n9.4.1 Advantages of discriminative classiﬁers 330\n9.4.2 Advantages of generative classiﬁers 331\n9.4.3 Handling missing features 331\n9.5 Exercises 332\n10 Logistic Regression 333\n10.1 Introduction 333\n10.2 Binary logistic regression 333\n10.2.1 Linear classiﬁers 333\n10.2.2 Nonlinear classiﬁers 334\n10.2.3 Maximum likelihood estimation 336\n10.2.4 Stochastic gradient descent 339\n10.2.5 Perceptron algorithm 340\n10.2.6 Iteratively reweighted least squares 340\n10.2.7 MAP estimation 342\n10.2.8 Standardization 343\n10.3 Multinomial logistic regression 344\n10.3.1 Linear and nonlinear classiﬁers 345\n10.3.2 Maximum likelihood estimation 345\n10.3.3 Gradient-based optimization 347\n10.3.4 Bound optimization 347\n10.3.5 MAP estimation 349\n10.3.6 Maximum entropy classiﬁers 350\n10.3.7 Hierarchical classiﬁcation 351\n10.3.8 Handling large numbers of classes 352\nDraft of “Probabilistic Machine Learning: An Introduction”.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 16, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1551}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0017_cd608c23", "text": "August 27, 2021\nCONTENTS xvii\n10.4 Robust logistic regression * 353\n10.4.1 Mixture model for the likelihood 353\n10.4.2 Bi-tempered loss 354\n10.5 Bayesian logistic regression * 357\n10.5.1 Laplace approximation 357\n10.5.2 Approximating the posterior predictive 358\n10.6 Exercises 361\n11 Linear Regression 365\n11.1 Introduction 365\n11.2 Least squares linear regression 365\n11.2.1 Terminology 365\n11.2.2 Least squares estimation 366\n11.2.3 Other approaches to computing the MLE 370\n11.2.4 Measuring goodness of ﬁt 374\n11.3 Ridge regression 375\n11.3.1 Computing the MAP estimate 376\n11.3.2 Connection between ridge regression and PCA 377\n11.3.3 Choosing the strength of the regularizer 378\n11.4 Lasso regression 379\n11.4.1 MAP estimation with a Laplace prior ( `1regularization) 379\n11.4.2 Why does `1regularization yield sparse solutions?", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 17, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 834}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0018_431546f2", "text": "380\n11.4.3 Hard vs soft thresholding 381\n11.4.4 Regularization path 383\n11.4.5 Comparison of least squares, lasso, ridge and subset selection 384\n11.4.6 Variable selection consistency 386\n11.4.7 Group lasso 387\n11.4.8 Elastic net (ridge and lasso combined) 390\n11.4.9 Optimization algorithms 391\n11.5 Regression splines * 393\n11.5.1 B-spline basis functions 393\n11.5.2 Fitting a linear model using a spline basis 395\n11.5.3 Smoothing splines 395\n11.5.4 Generalized additive models 395\n11.6 Robust linear regression * 396\n11.6.1 Laplace likelihood 396\n11.6.2 Student- tlikelihood 398\n11.6.3 Huber loss 398\n11.6.4 RANSAC 398\n11.7 Bayesian linear regression * 399\n11.7.1 Priors 399\n11.7.2 Posteriors 399\n11.7.3 Example 400\n11.7.4 Computing the posterior predictive 400\n11.7.5 The advantage of centering 402\nAuthor: Kevin P. Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 18, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 843}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0019_1d25768d", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\nxviii CONTENTS\n11.7.6 Dealing with multicollinearity 403\n11.7.7 Automatic relevancy determination (ARD) * 404\n11.8 Exercises 405\n12 Generalized Linear Models * 409\n12.1 Introduction 409\n12.2 Examples 409\n12.2.1 Linear regression 410\n12.2.2 Binomial regression 410\n12.2.3 Poisson regression 411\n12.3 GLMs with non-canonical link functions 411\n12.4 Maximum likelihood estimation 412\n12.5 Worked example: predicting insurance claims 413\nIII Deep Neural Networks 417\n13 Neural Networks for Structured Data 419\n13.1 Introduction 419\n13.2 Multilayer perceptrons (MLPs) 420\n13.2.1 The XOR problem 421\n13.2.2 Diﬀerentiable MLPs 422\n13.2.3 Activation functions 422\n13.2.4 Example models 423\n13.2.5 The importance of depth 428\n13.2.6 The “deep learning revolution” 429\n13.2.7 Connections with biology 429\n13.3 Backpropagation 432\n13.3.1 Forward vs reverse mode diﬀerentiation 432\n13.3.2 Reverse mode diﬀerentiation for multilayer perceptrons 434\n13.3.3 Vector-Jacobian product for common layers 436\n13.3.4 Computation graphs 438\n13.4 Training neural networks 440\n13.4.1 Tuning the learning rate 441\n13.4.2 Vanishing and exploding gradients 441\n13.4.3 Non-saturating activation functions 442\n13.4.4 Residual connections 445\n13.4.5 Parameter initialization 446\n13.4.6 Parallel training 447\n13.5 Regularization 448\n13.5.1 Early stopping 448\n13.5.2 Weight decay 449\n13.5.3 Sparse DNNs 449\n13.5.4 Dropout 449\n13.5.5 Bayesian neural networks 451\nDraft of “Probabilistic Machine Learning: An Introduction”.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 19, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1532}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0020_a9c722b7", "text": "August 27, 2021\nCONTENTS xix\n13.5.6 Regularization eﬀects of (stochastic) gradient descent * 451\n13.6 Other kinds of feedforward networks * 453\n13.6.1 Radial basis function networks 453\n13.6.2 Mixtures of experts 454\n13.7 Exercises 457\n14 Neural Networks for Images 461\n14.1 Introduction 461\n14.2 Common layers 462\n14.2.1 Convolutional layers 462\n14.2.2 Pooling layers 469\n14.2.3 Putting it all together 470\n14.2.4 Normalization layers 470\n14.3 Common architectures for image classiﬁcation 473\n14.3.1 LeNet 473\n14.3.2 AlexNet 475\n14.3.3 GoogLeNet (Inception) 476\n14.3.4 ResNet 477\n14.3.5 DenseNet 478\n14.3.6 Neural architecture search 479\n14.4 Other forms of convolution * 479\n14.4.1 Dilated convolution 479\n14.4.2 Transposed convolution 481\n14.4.3 Depthwise separable convolution 482\n14.5 Solving other discriminative vision tasks with CNNs * 482\n14.5.1 Image tagging 483\n14.5.2 Object detection 483\n14.5.3 Instance segmentation 484\n14.5.4 Semantic segmentation 484\n14.5.5 Human pose estimation 486\n14.6 Generating images by inverting CNNs * 487\n14.6.1 Converting a trained classiﬁer into a generative model 487\n14.6.2 Image priors 488\n14.6.3 Visualizing the features learned by a CNN 490\n14.6.4 Deep Dream 490\n14.6.5 Neural style transfer 491\n15 Neural Networks for Sequences 497\n15.1 Introduction 497\n15.2 Recurrent neural networks (RNNs) 497\n15.2.1 Vec2Seq (sequence generation) 497\n15.2.2 Seq2Vec (sequence classiﬁcation) 500\n15.2.3 Seq2Seq (sequence translation) 501\n15.2.4 Teacher forcing 503\n15.2.5 Backpropagation through time 504\nAuthor: Kevin P.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 20, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1556}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0021_a2846a4a", "text": "Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 21, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 22}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0022_e418d7a2", "text": "CC-BY-NC-ND license\nxx CONTENTS\n15.2.6 Vanishing and exploding gradients 505\n15.2.7 Gating and long term memory 506\n15.2.8 Beam search 509\n15.3 1d CNNs 510\n15.3.1 1d CNNs for sequence classiﬁcation 510\n15.3.2 Causal 1d CNNs for sequence generation 511\n15.4 Attention 512\n15.4.1 Attention as soft dictionary lookup 513\n15.4.2 Kernel regression as non-parametric attention 514\n15.4.3 Parametric attention 514\n15.4.4 Seq2Seq with attention 515\n15.4.5 Seq2vec with attention (text classiﬁcation) 518\n15.4.6 Seq+Seq2Vec with attention (text pair classiﬁcation) 518\n15.4.7 Soft vs hard attention 519\n15.5 Transformers 520\n15.5.1 Self-attention 520\n15.5.2 Multi-headed attention 521\n15.5.3 Positional encoding 522\n15.5.4 Putting it all together 523\n15.5.5 Comparing transformers, CNNs and RNNs 525\n15.5.6 Transformers for images * 526\n15.5.7 Other transformer variants * 526\n15.6 Eﬃcient transformers * 527\n15.6.1 Fixed non-learnable localized attention patterns 527\n15.6.2 Learnable sparse attention patterns 528\n15.6.3 Memory and recurrence methods 529\n15.6.4 Low-rank and kernel methods 529\n15.7 Language models and unsupervised representation learning 531\n15.7.1 ELMo 531\n15.7.2 BERT 532\n15.7.3 GPT 536\n15.7.4 T5 536\n15.7.5 Discussion 537\nIV Nonparametric Models 539\n16 Exemplar-based Methods 541\n16.1 K nearest neighbor (KNN) classiﬁcation 541\n16.1.1 Example 542\n16.1.2 The curse of dimensionality 542\n16.1.3 Reducing the speed and memory requirements 544\n16.1.4 Open set recognition 544\n16.2 Learning distance metrics 545\n16.2.1 Linear and convex methods 546\nDraft of “Probabilistic Machine Learning: An Introduction”.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 22, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1617}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0023_d8ae20a6", "text": "August 27, 2021\nCONTENTS xxi\n16.2.2 Deep metric learning 548\n16.2.3 Classiﬁcation losses 548\n16.2.4 Ranking losses 549\n16.2.5 Speeding up ranking loss optimization 550\n16.2.6 Other training tricks for DML 553\n16.3 Kernel density estimation (KDE) 554\n16.3.1 Density kernels 554\n16.3.2 Parzen window density estimator 555\n16.3.3 How to choose the bandwidth parameter 556\n16.3.4 From KDE to KNN classiﬁcation 557\n16.3.5 Kernel regression 557\n17 Kernel Methods * 561\n17.1 Mercer kernels 561\n17.1.1 Mercer’s theorem 562\n17.1.2 Some popular Mercer kernels 563\n17.2 Gaussian processes 568\n17.2.1 Noise-free observations 568\n17.2.2 Noisy observations 569\n17.2.3 Comparison to kernel regression 570\n17.2.4 Weight space vs function space 571\n17.2.5 Numerical issues 571\n17.2.6 Estimating the kernel 572\n17.2.7 GPs for classiﬁcation 575\n17.2.8 Connections with deep learning 576\n17.2.9 Scaling GPs to large datasets 577\n17.3 Support vector machines (SVMs) 579\n17.3.1 Large margin classiﬁers 579\n17.3.2 The dual problem 581\n17.3.3 Soft margin classiﬁers 583\n17.3.4 The kernel trick 584\n17.3.5 Converting SVM outputs into probabilities 585\n17.3.6 Connection with logistic regression 585\n17.3.7 Multi-class classiﬁcation with SVMs 586\n17.3.8 How to choose the regularizer C587\n17.3.9 Kernel ridge regression 588\n17.3.10 SVMs for regression 589\n17.4 Sparse vector machines 591\n17.4.1 Relevance vector machines (RVMs) 592\n17.4.2 Comparison of sparse and dense kernel methods 592\n17.5 Exercises 595\n18 Trees, Forests, Bagging, and Boosting 597\n18.1 Classiﬁcation and regression trees (CART) 597\n18.1.1 Model deﬁnition 597\nAuthor: Kevin P.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 23, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1621}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0024_a2846a4a", "text": "Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 24, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 22}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0025_57f5459d", "text": "CC-BY-NC-ND license\nxxii CONTENTS\n18.1.2 Model ﬁtting 599\n18.1.3 Regularization 600\n18.1.4 Handling missing input features 600\n18.1.5 Pros and cons 600\n18.2 Ensemble learning 602\n18.2.1 Stacking 602\n18.2.2 Ensembling is not Bayes model averaging 603\n18.3 Bagging 603\n18.4 Random forests 604\n18.5 Boosting 605\n18.5.1 Forward stagewise additive modeling 606\n18.5.2 Quadratic loss and least squares boosting 606\n18.5.3 Exponential loss and AdaBoost 607\n18.5.4 LogitBoost 610\n18.5.5 Gradient boosting 610\n18.6 Interpreting tree ensembles 614\n18.6.1 Feature importance 615\n18.6.2 Partial dependency plots 617\nV Beyond Supervised Learning 619\n19 Learning with Fewer Labeled Examples 621\n19.1 Data augmentation 621\n19.1.1 Examples 621\n19.1.2 Theoretical justiﬁcation 622\n19.2 Transfer learning 622\n19.2.1 Fine-tuning 623\n19.2.2 Adapters 624\n19.2.3 Supervised pre-training 625\n19.2.4 Unsupervised pre-training (self-supervised learning) 626\n19.2.5 Domain adaptation 631\n19.3 Semi-supervised learning 632\n19.3.1 Self-training and pseudo-labeling 632\n19.3.2 Entropy minimization 633\n19.3.3 Co-training 636\n19.3.4 Label propagation on graphs 637\n19.3.5 Consistency regularization 638\n19.3.6 Deep generative models * 640\n19.3.7 Combining self-supervised and semi-supervised learning 643\n19.4 Active learning 644\n19.4.1 Decision-theoretic approach 644\n19.4.2 Information-theoretic approach 644\n19.4.3 Batch active learning 645\n19.5 Meta-learning 645\nDraft of “Probabilistic Machine Learning: An Introduction”.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 25, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1496}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0026_44cec2ec", "text": "August 27, 2021\nCONTENTS xxiii\n19.5.1 Model-agnostic meta-learning (MAML) 646\n19.6 Few-shot learning 647\n19.6.1 Matching networks 648\n19.7 Weakly supervised learning 649\n19.8 Exercises 649\n20 Dimensionality Reduction 651\n20.1 Principal components analysis (PCA) 651\n20.1.1 Examples 651\n20.1.2 Derivation of the algorithm 653\n20.1.3 Computational issues 656\n20.1.4 Choosing the number of latent dimensions 658\n20.2 Factor analysis * 660\n20.2.1 Generative model 661\n20.2.2 Probabilistic PCA 662\n20.2.3 EM algorithm for FA/PPCA 663\n20.2.4 Unidentiﬁability of the parameters 665\n20.2.5 Nonlinear factor analysis 667\n20.2.6 Mixtures of factor analysers 668\n20.2.7 Exponential family factor analysis 669\n20.2.8 Factor analysis models for paired data 670\n20.3 Autoencoders 673\n20.3.1 Bottleneck autoencoders 674\n20.3.2 Denoising autoencoders 676\n20.3.3 Contractive autoencoders 676\n20.3.4 Sparse autoencoders 677\n20.3.5 Variational autoencoders 677\n20.4 Manifold learning * 682\n20.4.1 What are manifolds?", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 26, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 997}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0027_e4a4a077", "text": "683\n20.4.2 The manifold hypothesis 683\n20.4.3 Approaches to manifold learning 684\n20.4.4 Multi-dimensional scaling (MDS) 685\n20.4.5 Isomap 688\n20.4.6 Kernel PCA 689\n20.4.7 Maximum variance unfolding (MVU) 691\n20.4.8 Local linear embedding (LLE) 691\n20.4.9 Laplacian eigenmaps 692\n20.4.10 t-SNE 695\n20.5 Word embeddings 699\n20.5.1 Latent semantic analysis / indexing 699\n20.5.2 Word2vec 701\n20.5.3 GloVE 703\n20.5.4 Word analogies 704\n20.5.5 RAND-WALK model of word embeddings 705\n20.5.6 Contextual word embeddings 705\nAuthor: Kevin P. Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 27, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 556}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0028_14ffca5e", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\nxxiv CONTENTS\n20.6 Exercises 706\n21 Clustering 709\n21.1 Introduction 709\n21.1.1 Evaluating the output of clustering methods 709\n21.2 Hierarchical agglomerative clustering 711\n21.2.1 The algorithm 712\n21.2.2 Example 714\n21.2.3 Extensions 715\n21.3 K means clustering 716\n21.3.1 The algorithm 716\n21.3.2 Examples 716\n21.3.3 Vector quantization 718\n21.3.4 The K-means++ algorithm 719\n21.3.5 The K-medoids algorithm 719\n21.3.6 Speedup tricks 720\n21.3.7 Choosing the number of clusters K720\n21.4 Clustering using mixture models 723\n21.4.1 Mixtures of Gaussians 724\n21.4.2 Mixtures of Bernoullis 727\n21.5 Spectral clustering * 728\n21.5.1 Normalized cuts 728\n21.5.2 Eigenvectors of the graph Laplacian encode the clustering 729\n21.5.3 Example 730\n21.5.4 Connection with other methods 731\n21.6 Biclustering * 731\n21.6.1 Basic biclustering 732\n21.6.2 Nested partition models (Crosscat) 732\n22 Recommender Systems 735\n22.1 Explicit feedback 735\n22.1.1 Datasets 735\n22.1.2 Collaborative ﬁltering 736\n22.1.3 Matrix factorization 737\n22.1.4 Autoencoders 739\n22.2 Implicit feedback 741\n22.2.1 Bayesian personalized ranking 741\n22.2.2 Factorization machines 742\n22.2.3 Neural matrix factorization 743\n22.3 Leveraging side information 743\n22.4 Exploration-exploitation tradeoﬀ 744\n23 Graph Embeddings * 747\n23.1 Introduction 747\n23.2 Graph Embedding as an Encoder/Decoder Problem 748\nDraft of “Probabilistic Machine Learning: An Introduction”.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 28, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1469}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0029_c2e16394", "text": "August 27, 2021\nCONTENTS xxv\n23.3 Shallow graph embeddings 750\n23.3.1 Unsupervised embeddings 751\n23.3.2 Distance-based: Euclidean methods 751\n23.3.3 Distance-based: non-Euclidean methods 752\n23.3.4 Outer product-based: Matrix factorization methods 752\n23.3.5 Outer product-based: Skip-gram methods 753\n23.3.6 Supervised embeddings 755\n23.4 Graph Neural Networks 756\n23.4.1 Message passing GNNs 756\n23.4.2 Spectral Graph Convolutions 757\n23.4.3 Spatial Graph Convolutions 757\n23.4.4 Non-Euclidean Graph Convolutions 759\n23.5 Deep graph embeddings 759\n23.5.1 Unsupervised embeddings 760\n23.5.2 Semi-supervised embeddings 762\n23.6 Applications 763\n23.6.1 Unsupervised applications 763\n23.6.2 Supervised applications 765\nA Notation 767\nA.1 Introduction 767\nA.2 Common mathematical symbols 767\nA.3 Functions 768\nA.3.1 Common functions of one argument 768\nA.3.2 Common functions of two arguments 768\nA.3.3 Common functions of >2arguments 768\nA.4 Linear algebra 769\nA.4.1 General notation 769\nA.4.2 Vectors 769\nA.4.3 Matrices 769\nA.4.4 Matrix calculus 770\nA.5 Optimization 770\nA.6 Probability 771\nA.7 Information theory 771\nA.8 Statistics and machine learning 772\nA.8.1 Supervised learning 772\nA.8.2 Unsupervised learning and generative models 772\nA.8.3 Bayesian inference 772\nA.9 Abbreviations 773\nIndex 775\nBibliography 792\nAuthor: Kevin P.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 29, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1336}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0030_81653683", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n\nPreface\nIn 2012, I published a 1200-page book called Machine Learning: A Probabilistic Perspective , which\nprovided a fairly comprehensive coverage of the ﬁeld of machine learning (ML) at that time, under\nthe unifying lens of probabilistic modeling. The book was well received, and won the De Groot prize\nin 2013. The year 2012 is also generally considered the start of the “deep learning revolution”. The term\n“deep learning” refers to a branch of ML that is based on neural networks with many layers (hence\nthe term “deep”). Although this basic technology had been around for many years, it was in 2012\nwhen [KSH12] used deep neural networks (DNNs) to win the ImageNet image classiﬁcation challenge\nby such a large margin that it caught the attention of the wider community. Related advances on\nother hard problems, such as speech recognition, appeared around the same time (see e.g., [Cir+10;\nCir+11; Hin+12]).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 30, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 957}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0031_c840b585", "text": "Related advances on\nother hard problems, such as speech recognition, appeared around the same time (see e.g., [Cir+10;\nCir+11; Hin+12]). These breakthroughs were enabled by advances in hardware technology (in\nparticular, the repurposing of fast graphics processing units (GPUs) from video games to ML), data\ncollection technology (in particular, the use of crowd sourcing tools, such as Amazon’s Mechanical\nTurk platform, to collect large labeled datasets, such as ImageNet), as well as various new algorithmic\nideas. some of which we cover in this book. Since 2012, the ﬁeld of deep learning has exploded, with new advances coming at an increasing\npace. Interest in the ﬁeld has also exploded, fueled by the commercial success of the technology,\nand the breadth of applications to which it can be applied. Therefore, in 2018, I decided to write a\nsecond edition of my book, to attempt to summarize some of this progress.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 31, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 921}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0032_07dd4508", "text": "Therefore, in 2018, I decided to write a\nsecond edition of my book, to attempt to summarize some of this progress. By March 2020, my draft of the second edition had swollen to about 1600 pages, and I still had\nmany topics left to cover. As a result, MIT Press told me I would need to split the book into two\nvolumes. Then the COVID-19 pandemic struck. I decided to pivot away from book writing, and to\nhelp develop the risk score algorithm for Google’s exposure notiﬁcation app [MKS21] as well as to\nassist with various forecasting projects [Wah+21]. However, by the Fall of 2020, I decided to return\nto working on the book. To make up for lost time, I asked several colleagues to help me ﬁnish by writing various sections (see\nacknowledgements below). The result of all this is two new books, “Probabilistic Machine Learning:\nAn Introduction”, which you are currently reading, and “Probabilistic Machine Learning: Advanced\nTopics”, which is the sequel to this book [Mur22].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 32, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 974}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0033_1a1917f8", "text": "Together these two books attempt to present a\nfairly broad coverage of the ﬁeld of ML c. 2021, using the same unifying lens of probabilistic modeling\nand Bayesian decision theory that I used in the 2012 book. Nearly all of the content from the 2012 book has been retained, but it is now split fairly evenly\nxxviii Preface\nbetween the two new books. In addition, each new book has lots of fresh material, covering topics from\ndeep learning, as well as advances in other parts of the ﬁeld, such as generative models, variational\ninference and reinforcement learning. To make this introductory book more self-contained and useful for students, I have added some\nbackground material, on topics such as optimization and linear algebra, that was omitted from the\n2012 book due to lack of space. Advanced material, that can be skipped during an introductory\nlevel course, is denoted by an asterisk * in the section or chapter title. Exercises can be found at\nthe end of some chapters.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 33, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 977}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0034_9185326e", "text": "Advanced material, that can be skipped during an introductory\nlevel course, is denoted by an asterisk * in the section or chapter title. Exercises can be found at\nthe end of some chapters. Solutions to exercises marked with an asterisk * are available to qualiﬁed\ninstructors by contacting MIT Press; solutions to all other exercises can be found online. See the\nbook web site at probml.ai for additional teaching material (e.g., ﬁgures and slides). Another major change is that all of the software now uses Python instead of Matlab. (In the\nfuture, we may create a Julia version of the code.) The new code leverages standard Python libraries,\nsuch as NumPy, Scikit-learn, JAX, PyTorch, TensorFlow, PyMC3, etc. Details on how to use the\ncode can be found at probml.ai. Acknowledgements\nI would like to thank the following people for helping me with the book:\n•Zico Kolter, who helped write parts of Chapter 7 (Linear Algebra).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 34, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 926}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0035_f3640b41", "text": "Acknowledgements\nI would like to thank the following people for helping me with the book:\n•Zico Kolter, who helped write parts of Chapter 7 (Linear Algebra). •Frederik Kunstner, Si Yi Meng, Aaron Mishkin, Sharan Vaswani, and Mark Schmidt who helped\nwrite parts of Chapter 8 (Optimization). •Mathieu Blondel, who helped write Section 13.3 (Backpropagation). •Krzysztof Choromanski, who helped write Section 15.6 (Eﬃcient transformers *). •Colin Raﬀel, who helped write Section 19.2 (Transfer learning) and Section 19.3 (Semi-supervised\nlearning). •Bryan Perozzi, Sami Abu-El-Haija and Ines Chami, who helped write Chapter 23 (Graph Embed-\ndings *). •John Fearns and Peter Cerno for carefully proofreading the book. •Many members of the github community for ﬁnding typos, etc (see https://github.com/probml/\npml-book/issues?q=is:issue for a list of issues). •The 4 anonymous reviewers solicited by MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 35, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 906}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0036_4defa561", "text": "•Many members of the github community for ﬁnding typos, etc (see https://github.com/probml/\npml-book/issues?q=is:issue for a list of issues). •The 4 anonymous reviewers solicited by MIT Press. •Mahmoud Soliman for writing all the magic plumbing code that connects latex, colab, github, etc,\nand for teaching me about GCP and TPUs. •The 2021 cohort of Google Summer of Code students who worked on code for the book: Aleyna\nKara, Srikar Jilugu, Drishti Patel, Ming Liang Ang, Gerardo Durán-Martín. (See https://\nprobml.github.io/pml-book/gsoc2021.html for a summary of their contributions.)\n•Many members of the github community for their code contributions (see https://github.com/\nprobml/pyprobml#acknowledgements ). •The authors of [Zha+20], [Gér17] and [Mar18] for letting me reuse or modify some of their open\nsource code from their own excellent books. •My manager at Google, Doug Eck, for letting me spend company time on this book. •My wife Margaret for letting me spend family time on this book.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 36, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1002}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0037_a0176371", "text": "•My manager at Google, Doug Eck, for letting me spend company time on this book. •My wife Margaret for letting me spend family time on this book. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\nPreface xxix\nAbout the cover\nThe cover illustrates a neural network (Chapter 13) being used to classify a hand-written digit xinto\none of 10 class labels y2f0;1;:::; 9g. The histogram on the right is the output of the model, and\ncorresponds to the conditional probability distribution p(yjx). Kevin Patrick Murphy\nPalo Alto, California\nAugust 2021. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n\n1Introduction\n1.1 What is machine learning? A popular deﬁnition of machine learning orML, due to Tom Mitchell [Mit97], is as follows:\nA computer program is said to learn from experience E with respect to some class of tasks T,\nand performance measure P, if its performance at tasks in T, as measured by P, improves with\nexperience E.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 37, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 965}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0038_710505af", "text": "Thus there are many diﬀerent kinds of machine learning, depending on the nature of the task Twe\nwish the system to learn, the nature of the performance measure Pwe use to evaluate the system,\nand the nature of the training signal or experience Ewe give it. In this book, we will cover the most common types of ML, but from a probabilistic perspective . Roughly speaking, this means that we treat all unknown quantities (e.g., predictions about the\nfuture value of some quantity of interest, such as tomorrow’s temperature, or the parameters of some\nmodel) as random variables , that are endowed with probability distributions which describe a\nweighted set of possible values the variable may have. (See Chapter 2 for a quick refresher on the\nbasics of probability, if necessary.)\nThere are two main reasons we adopt a probabilistic approach. First, it is the optimal approach to\ndecision making under uncertainty , as we explain in Section 5.1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 38, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 944}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0039_b906739e", "text": "First, it is the optimal approach to\ndecision making under uncertainty , as we explain in Section 5.1. Second, probabilistic modeling\nis the language used by most other areas of science and engineering, and thus provides a unifying\nframework between these ﬁelds. As Shakir Mohamed, a researcher at DeepMind, put it:1\nAlmost all of machine learning can be viewed in probabilistic terms, making probabilistic\nthinking fundamental. It is, of course, not the only view. But it is through this view that we\ncan connect what we do in machine learning to every other computational science, whether that\nbe in stochastic optimisation, control theory, operations research, econometrics, information\ntheory, statistical physics or bio-statistics. For this reason alone, mastery of probabilistic\nthinking is essential. 1.2 Supervised learning\nThe most common form of ML is supervised learning . In this problem, the task Tis to learn\na mapping ffrom inputsx2Xto outputsy2Y. The inputs xare also called the features ,\n1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 39, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1008}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0040_79faec9d", "text": "1.2 Supervised learning\nThe most common form of ML is supervised learning . In this problem, the task Tis to learn\na mapping ffrom inputsx2Xto outputsy2Y. The inputs xare also called the features ,\n1. Source: Slide 2 of https://bit.ly/3pyHyPn\n2 Chapter 1. Introduction\n(a)\n (b)\n (c)\nFigure 1.1: Three types of Iris ﬂowers: Setosa, Versicolor and Virginica. Used with kind permission of Dennis\nKramb and SIGNA. index sl sw pl pw label\n0 5.1 3.5 1.4 0.2 Setosa\n1 4.9 3.0 1.4 0.2 Setosa\n\u0001\u0001\u0001\n50 7.0 3.2 4.7 1.4 Versicolor\n\u0001\u0001\u0001\n149 5.9 3.0 5.1 1.8 Virginica\nTable 1.1: A subset of the Iris design matrix. The features are: sepal length, sepal width, petal length, petal\nwidth. There are 50 examples of each class. covariates , orpredictors ; this is often a ﬁxed-dimensional vector of numbers, such as the height\nand weight of a person, or the pixels in an image. In this case, X=RD, whereDis the dimensionality\nof the vector (i.e., the number of input features).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 40, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 957}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0041_42cd90be", "text": "In this case, X=RD, whereDis the dimensionality\nof the vector (i.e., the number of input features). The output yis also known as the label,target, or\nresponse .2The experience Eis given in the form of a set of Ninput-output pairs D=f(xn;yn)gN\nn=1,\nknown as the training set . (Nis called the sample size .) The performance measure Pdepends\non the type of output we are predicting, as we discuss below. 1.2.1 Classiﬁcation\nInclassiﬁcation problems, the output space is a set of Cunordered and mutually exclusive labels\nknown as classes,Y=f1;2;:::;Cg. The problem of predicting the class label given an input is\nalso called pattern recognition . (If there are just two classes, often denoted by y2f0;1gor\ny2f\u0000 1;+1g, it is called binary classiﬁcation .)\n1.2.1.1 Example: classifying Iris ﬂowers\nAs an example, consider the problem of classifying Iris ﬂowers into their 3 subspecies, Setosa,\nVersicolor and Virginica. Figure 1.1 shows one example of each of these classes. 2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 41, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 972}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0042_4cb5721c", "text": "Figure 1.1 shows one example of each of these classes. 2. Sometimes (e.g., in the statsmodels Python package) xare called the exogenous variables andyare called the\nendogenous variables . Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n1.2. Supervised learning 3\nFigure 1.2: Illustration of the image classiﬁcation problem. From https: // cs231n. github. io/ . Used with\nkind permission of Andrej Karpathy. Inimage classiﬁcation , the input space Xis the set of images, which is a very high-dimensional\nspace: for a color image with C= 3channels (e.g., RGB) and D1\u0002D2pixels, we haveX=RD,\nwhereD=C\u0002D1\u0002D2. (In practice we represent each pixel intensity with an integer, typically from\nthe rangef0;1;:::; 255g, but we assume real valued inputs for notational simplicity.) Learning a\nmappingf:X!Yfrom images to labels is quite challenging, as illustrated in Figure 1.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 42, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 893}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0043_21db8315", "text": "However,\nit can be tackled using certain kinds of functions, such as a convolutional neural network or\nCNN, which we discuss in Section 14.1. Fortunately for us, some botanists have already identiﬁed 4 simple, but highly informative, numeric\nfeatures — sepal length, sepal width, petal length, petal width — which can be used to distinguish\nthe three kinds of Iris ﬂowers. In this section, we will use this much lower-dimensional input space,\nX=R4, for simplicity. The Iris dataset is a collection of 150 labeled examples of Iris ﬂowers, 50 of\neach type, described by these 4 features. It is widely used as an example, because it is small and\nsimple to understand. (We will discuss larger and more complex datasets later in the book.)\nWhen we have small datasets of features, it is common to store them in an N\u0002Dmatrix, in which\neach row represents an example, and each column represents a feature.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 43, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 898}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0044_8d860914", "text": "This is known as a design\nmatrix; see Table 1.1 for an example.3\nThe Iris dataset is an example of tabular data . When the inputs are of variable size (e.g.,\nsequences of words, or social networks), rather than ﬁxed-length vectors, the data is usually stored\n3. This particular design matrix has N= 150rows andD= 4columns, and hence has a tall and skinny shape, since\nN\u001dD. By contrast, some datasets (e.g., genomics) have more features than examples, D\u001dN; their design matrices\nareshort and fat . The term “ big data ” usually means that Nis large, whereas the term “ wide data ” means that\nDis large (relative to N). Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n4 Chapter 1. Introduction\n68sepal length (cm)\n234sepal width (cm)\n2.55.0petal length (cm)\n5.0 7.5\nsepal length (cm)012petal width (cm)\n2 4\nsepal width (cm)\n2.55.07.5\npetal length (cm)0 2\npetal width (cm)label\nsetosa\nversicolor\nvirginica\nFigure 1.3: Visualization of the Iris data as a pairwise scatter plot.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 44, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 985}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0045_0e4590c4", "text": "On the diagonal we plot the marginal\ndistribution of each feature for each class. The oﬀ-diagonals contain scatterplots of all possible pairs of\nfeatures. Generated by code at ﬁgures.probml.ai/book1/1.3\nin some other format rather than in a design matrix. However, such data is often converted to a\nﬁxed-sized feature representation (a process known as featurization ), thus implicitly creating a\ndesign matrix for further processing. We give an example of this in Section 1.5.4.1, where we discuss\nthe “bag of words” representation for sequence data. 1.2.1.2 Exploratory data analysis\nBefore tackling a problem with ML, it is usually a good idea to perform exploratory data analysis ,\nto see if there are any obvious patterns (which might give hints on what method to choose), or any\nobvious problems with the data (e.g., label noise or outliers).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 45, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 848}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0046_e74c46cb", "text": "For tabular data with a small number of features, it is common to make a pair plot , in which\npanel (i;j)shows a scatter plot of variables iandj, and the diagonal entries (i;i)show the marginal\ndensity of variable i; all plots are optionally color coded by class label — see Figure 1.3 for an\nexample. For higher-dimensional data, it is common to ﬁrst perform dimensionality reduction , and then\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n1.2. Supervised learning 5\n(a)\n0 1 2 3 4 5 6 7\npetal length (cm)0.5\n0.00.51.01.52.02.53.0petal width (cm)setosa\nversicolor\nvirginica (b)\nFigure 1.4: Example of a decision tree of depth 2 applied to the Iris data, using just the petal length and\npetal width features. Leaf nodes are color coded according to the predicted class. The number of training\nsamples that pass from the root to a node is shown inside each box; we show how many values of each class\nfall into this node.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 46, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 949}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0047_46789e80", "text": "The number of training\nsamples that pass from the root to a node is shown inside each box; we show how many values of each class\nfall into this node. This vector of counts can be normalized to get a distribution over class labels for each\nnode. We can then pick the majority class. Adapted from Figures 6.1 and 6.2 of [Gér19]. Generated by code\nat ﬁgures.probml.ai/book1/1.4. to visualize the data in 2d or 3d. We discuss methods for dimensionality reduction in Chapter 20. 1.2.1.3 Learning a classiﬁer\nFrom Figure 1.3, we can see that the Setosa class is easy to distinguish from the other two classes. For example, suppose we create the following decision rule :\nf(x;\u0012) =(\nSetosa if petal length <2:45\nVersicolor or Virginica otherwise(1.1)\nThis is a very simple example of a classiﬁer, in which we have partitioned the input space into two\nregions, deﬁned by the one-dimensional (1d) decision boundary atxpetal length = 2:45.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 47, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 928}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0048_00873183", "text": "Points\nlying to the left of this boundary are classiﬁed as Setosa; points to the right are either Versicolor or\nVirginica. We see that this rule perfectly classiﬁes the Setosa examples, but not the Virginica and Versicolor\nones. To improve performance, we can recursively partition the space, by splitting regions in which\nthe classiﬁer makes errors. For example, we can add another decision rule, to be applied to inputs\nthat fail the ﬁrst test, to check if the petal width is below 1.75cm (in which case we predict Versicolor)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n6 Chapter 1. Introduction\nEstimate\nSetosa Versicolor Virginica\nTruthSetosa 0 1 1\nVersicolor 1 0 1\nVirginica 10 10 0\nTable 1.2: Hypothetical asymmetric loss matrix for Iris classiﬁcation. or above (in which case we predict Virginica). We can arrange these nested rules into a tree structure,\ncalled adecision tree , as shown in Figure 1.4a This induces the 2d decision surface shown in\nFigure 1.4b.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 48, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0049_b6d1a6db", "text": "We can arrange these nested rules into a tree structure,\ncalled adecision tree , as shown in Figure 1.4a This induces the 2d decision surface shown in\nFigure 1.4b. We can represent the tree by storing, for each internal node, the feature index that is used, as well\nas the corresponding threshold value. We denote all these parameters by\u0012. We discuss how to\nlearn these parameters in Section 18.1. 1.2.1.4 Empirical risk minimization\nThe goal of supervised learning is to automatically come up with classiﬁcation models such as the\none shown in Figure 1.4a, so as to reliably predict the labels for any given input. A common way to\nmeasure performance on this task is in terms of the misclassiﬁcation rate on the training set:\nL(\u0012),1\nNNX\nn=1I(yn6=f(xn;\u0012)) (1.2)\nwhereI(e)is the binary indicator function , which returns 1 iﬀ (if and only if) the condition eis\ntrue, and returns 0 otherwise, i.e.,\nI(e) =\u001a1ifeis true\n0ifeis false(1.3)\nThis assumes all errors are equal.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 49, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 968}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0050_b734ce2a", "text": "However it may be the case that some errors are more costly\nthan others. For example, suppose we are foraging in the wilderness and we ﬁnd some Iris ﬂowers. Furthermore, suppose that Setosa and Versicolor are tasty, but Virginica is poisonous. In this case,\nwe might use the asymmetric loss function `(y;^y)shown in Table 1.2. We can then deﬁne empirical risk to be the average loss of the predictor on the training set:\nL(\u0012),1\nNNX\nn=1`(yn;f(xn;\u0012)) (1.4)\nWe see that the misclassiﬁcation rate Equation (1.2) is equal to the empirical risk when we use\nzero-one loss for comparing the true label with the prediction:\n`01(y;^y) =I(y6= ^y) (1.5)\nSee Section 5.1 for more details. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n1.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 50, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 756}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0051_dad08cd3", "text": "Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n1.2. Supervised learning 7\nOne way to deﬁne the problem of model ﬁtting ortraining is to ﬁnd a setting of the parameters\nthat minimizes the empirical risk on the training set:\n^\u0012= argmin\n\u0012L(\u0012) = argmin\n\u00121\nNNX\nn=1`(yn;f(xn;\u0012)) (1.6)\nThis is called empirical risk minimization . However, our true goal is to minimize the expected loss on future data that we have not yet\nseen. That is, we want to generalize , rather than just do well on the training set. We discuss this\nimportant point in Section 1.2.3. 1.2.1.5 Uncertainty\n[We must avoid] false conﬁdence bred from an ignorance of the probabilistic nature of the\nworld, from a desire to see black and white where we should rightly see gray. — Immanuel\nKant, as paraphrased by Maria Konnikova [Kon20].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 51, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 827}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0052_e7948756", "text": "— Immanuel\nKant, as paraphrased by Maria Konnikova [Kon20]. In many cases, we will not be able to perfectly predict the exact output given the input, due to\nlack of knowledge of the input-output mapping (this is called epistemic uncertainty ormodel\nuncertainty ), and/or due to intrinsic (irreducible) stochasticity in the mapping (this is called\naleatoric uncertainty ordata uncertainty ). Representing uncertainty in our prediction can be important for various applications. For example,\nlet us return to our poisonous ﬂower example, whose loss matrix is shown in Table 1.2. If we predict\nthe ﬂower is Virginica with high probability, then we should not eat the ﬂower. Alternatively, we\nmay be able to perform an information gathering action , such as performing a diagnostic test, to\nreduce our uncertainty. For more information about how to make optimal decisions in the presence\nof uncertainty, see Section 5.1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 52, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 916}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0053_60be412b", "text": "For more information about how to make optimal decisions in the presence\nof uncertainty, see Section 5.1. We can capture our uncertainty using the following conditional probability distribution :\np(y=cjx;\u0012) =fc(x;\u0012) (1.7)\nwheref:X! [0;1]Cmaps inputs to a probability distribution over the Cpossible output labels. Sincefc(x;\u0012)returnstheprobabilityofclasslabel c, werequire 0\u0014fc\u00141foreachc, andPC\nc=1fc= 1. To avoid this restriction, it is common to instead require the model to return unnormalized log-\nprobabilities. We can then convert these to probabilities using the softmax function , which is\ndeﬁned as follows\nS(a),\"\nea1\nPC\nc0=1eac0;:::;eaC\nPC\nc0=1eac0#\n(1.8)\nThis maps RCto[0;1]C, and satisﬁes the constraints that 0\u0014S(a)c\u00141andPC\nc=1S(a)c= 1. The\ninputs to the softmax, a=f(x;\u0012), are called logits. See Section 2.5.2 for details. We thus deﬁne\nthe overall model as follows:\np(y=cjx;\u0012) =Sc(f(x;\u0012)) (1.9)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n8 Chapter 1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 53, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 982}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0054_00907d22", "text": "See Section 2.5.2 for details. We thus deﬁne\nthe overall model as follows:\np(y=cjx;\u0012) =Sc(f(x;\u0012)) (1.9)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n8 Chapter 1. Introduction\nA common special case of this arises when fis anaﬃne function of the form\nf(x;\u0012) =b+wTx=b+w1x1+w2x2+\u0001\u0001\u0001+wDxD (1.10)\nwhere\u0012= (b;w)are the parameters of the model. This model is called logistic regression , and\nwill be discussed in more detail in Chapter 10. In statistics, the wparameters are usually called regression coeﬃcients (and are typically\ndenoted by\f) andbis called the intercept . In ML, the parameters ware called the weights andb\nis called the bias. This terminology arises from electrical engineering, where we view the function f\nas a circuit which takes in xand returns f(x). Each input is fed to the circuit on “wires”, which\nhave weights w. The circuit computes the weighted sum of its inputs, and adds a constant bias or\noﬀset term b.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 54, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 942}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0055_e9617c2b", "text": "Each input is fed to the circuit on “wires”, which\nhave weights w. The circuit computes the weighted sum of its inputs, and adds a constant bias or\noﬀset term b. (This use of the term “bias” should not be confused with the statistical concept of bias\ndiscussed in Section 4.7.6.1.)\nTo reduce notational clutter, it is common to absorb the bias term binto the weights wby deﬁning\n~w= [b;w1;:::;wD]and deﬁning ~x= [1;x1;:::;xD], so that\n~wT~x=b+wTx (1.11)\nThis converts the aﬃne function into a linear function .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 55, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 510}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0056_26752de1", "text": "We will usually assume that this has been\ndone, so we can just write the prediction function as follows:\nf(x;w) =wTx (1.12)\n1.2.1.6 Maximum likelihood estimation\nWhen ﬁtting probabilistic models, it is common to use the negative log probability as our loss\nfunction:\n`(y;f(x;\u0012)) =\u0000logp(yjf(x;\u0012)) (1.13)\nThe reasons for this are explained in Section 5.1.6.1, but the intuition is that a good model (with low\nloss) is one that assigns a high probability to the true output yfor each corresponding input x. The\naverage negative log probability of the training set is given by\nNLL(\u0012) =\u00001\nNNX\nn=1logp(ynjf(xn;\u0012)) (1.14)\nThis is called the negative log likelihood . If we minimize this, we can compute the maximum\nlikelihood estimate orMLE:\n^\u0012mle= argmin\n\u0012NLL(\u0012) (1.15)\nThis is a very common way to ﬁt models to data, as we will see. 1.2.2 Regression\nNow suppose that we want to predict a real-valued quantity y2Rinstead of a class label y2\nf1;:::;Cg; this is known as regression .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 56, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 975}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0057_08fc4eac", "text": "1.2.2 Regression\nNow suppose that we want to predict a real-valued quantity y2Rinstead of a class label y2\nf1;:::;Cg; this is known as regression . For example, in the case of Iris ﬂowers, ymight be the\ndegree of toxicity if the ﬂower is eaten, or the average height of the plant. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n1.2. Supervised learning 9\nRegression is very similar to classiﬁcation. However, since the output is real-valued, we need to\nuse a diﬀerent loss function. For regression, the most common choice is to use quadratic loss , or`2\nloss:\n`2(y;^y) = (y\u0000^y)2(1.16)\nThis penalizes large residualsy\u0000^ymore than small ones.4The empirical risk when using quadratic\nloss is equal to the mean squared error orMSE:\nMSE(\u0012) =1\nNNX\nn=1(yn\u0000f(xn;\u0012))2(1.17)\nBased on the discussion in Section 1.2.1.5, we should also model the uncertainty in our prediction. In regression problems, it is common to assume the output distribution is a Gaussian ornormal.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 57, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 988}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0058_9d932369", "text": "In regression problems, it is common to assume the output distribution is a Gaussian ornormal. As we explain in Section 2.6, this distribution is deﬁned by\nN(yj\u0016;\u001b2),1p\n2\u0019\u001b2e\u00001\n2\u001b2(y\u0000\u0016)2(1.18)\nwhere\u0016is the mean, \u001b2is the variance, andp\n2\u0019\u001b2is the normalization constant needed to ensure\nthe density integrates to 1. In the context of regression, we can make the mean depend on the inputs\nby deﬁning \u0016=f(xn;\u0012). We therefore get the following conditional probability distribution:\np(yjx;\u0012) =N(yjf(x;\u0012);\u001b2) (1.19)\nIf we assume that the variance \u001b2is ﬁxed (for simplicity), the corresponding negative log likelihood\nbecomes\nNLL(\u0012) =\u0000NX\nn=1log\"\u00121\n2\u0019\u001b2\u00131\n2\nexp\u0012\n\u00001\n2\u001b2(yn\u0000f(xn;\u0012))2\u0013#\n(1.20)\n=N\n2\u001b2MSE(\u0012) + const (1.21)\nWe see that the NLL is proportional to the MSE. Hence computing the maximum likelihood estimate\nof the parameters will result in minimizing the squared error, which seems like a sensible approach to\nmodel ﬁtting.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 58, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 925}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0059_82e8e55d", "text": "Hence computing the maximum likelihood estimate\nof the parameters will result in minimizing the squared error, which seems like a sensible approach to\nmodel ﬁtting. 1.2.2.1 Linear regression\nAs an example of a regression model, consider the 1d data in Figure 1.5a. We can ﬁt this data using\nasimple linear regression model of the form\nf(x;\u0012) =b+wx (1.22)\n4. If the data has outliers, the quadratic penalty can be too severe. In such cases, it can be better to use `1loss\ninstead, which is more robust. See Section 11.6 for details. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n10 Chapter 1. Introduction\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.07.5\n5.0\n2.5\n0.02.55.07.510.0\n(a)\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.07.5\n5.0\n2.5\n0.02.55.07.510.0\n (b)\nFigure 1.5: (a) Linear regression on some 1d data. (b) The vertical lines denote the residuals between\nthe observed output value for each input (blue circle) and its predicted value (red cross).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 59, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 960}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0060_3c4f4a23", "text": "(b) The vertical lines denote the residuals between\nthe observed output value for each input (blue circle) and its predicted value (red cross). The goal of least\nsquares regression is to pick a line that minimizes the sum of squared residuals. Generated by code at\nﬁgures.probml.ai/book1/1.5. wherewis theslope,bis theoﬀset, and\u0012= (w;b)are all the parameters of the model. By adjusting\n\u0012, we can minimize the sum of squared errors, shown by the vertical lines in Figure 1.5b. until we\nﬁnd theleast squares solution\n^\u0012= argmin\n\u0012MSE(\u0012) (1.23)\nSee Section 11.2.2.1 for details. If we have multiple input features, we can write\nf(x;\u0012) =b+w1x1+\u0001\u0001\u0001+wDxD=b+wTx (1.24)\nwhere\u0012= (w;b). This is called multiple linear regression . For example, consider the task of predicting temperature as a function of 2d location in a room.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 60, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 816}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0061_62164a6e", "text": "This is called multiple linear regression . For example, consider the task of predicting temperature as a function of 2d location in a room. Figure 1.6(a) plots the results of a linear model of the following form:\nf(x;\u0012) =b+w1x1+w2x2 (1.25)\nWe can extend this model to use D> 2input features (such as time of day), but then it becomes\nharder to visualize. 1.2.2.2 Polynomial regression\nThe linear model in Figure 1.5a is obviously not a very good ﬁt to the data. We can improve the\nﬁt by using a polynomial regression model of degree D. This has the form f(x;w) =wT\u001e(x),\nwhere\u001e(x)is a feature vector derived from the input, which has the following form:\n\u001e(x) = [1;x;x2;:::;xD] (1.26)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n1.2. Supervised learning 11\n0510152025303540 05101520253015.015.516.016.517.017.518.018.519.0\n(a)\n0510152025303540 05101520253015.015.516.016.517.017.518.018.519.0 (b)\nFigure 1.6: Linear and polynomial regression applied to 2d data.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 61, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0062_17f8c3a6", "text": "Vertical axis is temperature, horizontal\naxes are location within a room. Data was collected by some remote sensing motes at Intel’s lab in Berkeley,\nCA (data courtesy of Romain Thibaux). (a) The ﬁtted plane has the form ^f(x) =w0+w1x1+w2x2. (b)\nTemperature data is ﬁtted with a quadratic of the form ^f(x) =w0+w1x1+w2x2+w3x2\n1+w4x2\n2. Generated\nby code at ﬁgures.probml.ai/book1/1.6. This is a simple example of feature preprocessing , also called feature engineering . In Figure 1.7a, we see that using D= 2results in a much better ﬁt. We can keep increasing D, and\nhence the number of parameters in the model, until D=N\u00001; in this case, we have one parameter\nper data point, so we can perfectly interpolate the data. The resulting model will have 0 MSE, as\nshown in Figure 1.7c. However, intuitively the resulting function will not be a good predictor for\nfuture inputs, since it is too “wiggly”. We discuss this in more detail in Section 1.2.3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 62, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 948}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0063_82670123", "text": "However, intuitively the resulting function will not be a good predictor for\nfuture inputs, since it is too “wiggly”. We discuss this in more detail in Section 1.2.3. We can also apply polynomial regression to multi-dimensional inputs. For example, Figure 1.6(b)\nplots the predictions for the temperature model after performing a quadratic expansion of the inputs\nf(x;w) =w0+w1x1+w2x2+w3x2\n1+w4x2\n2 (1.27)\nThe quadratic shape is a better ﬁt to the data than the linear model in Figure 1.6(a), since it captures\nthe fact that the middle of the room is hotter. We can also add cross terms, such as x1x2, to capture\ninteraction eﬀects. See Section 1.5.3.2 for details. Note that the above models still use a prediction function that is a linear function of the parameters\nw, even though it is a nonlinear function of the original input x. The reason this is important is\nthat a linear model induces an MSE loss function MSE (\u0012)that has a unique global optimum, as we\nexplain in Section 11.2.2.1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 63, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0064_ebfd3cca", "text": "The reason this is important is\nthat a linear model induces an MSE loss function MSE (\u0012)that has a unique global optimum, as we\nexplain in Section 11.2.2.1. 1.2.2.3 Deep neural networks\nIn Section 1.2.2.2, we manually speciﬁed the transformation of the input features, namely polynomial\nexpansion,\u001e(x) = [1;x1;x2;x2\n1;x2\n2;:::]. We can create much more powerful models by learning to\ndo such nonlinear feature extraction automatically. If we let \u001e(x)have its own set of parameters,\nsayV, then the overall model has the form\nf(x;w;V) =wT\u001e(x;V) (1.28)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n12 Chapter 1. Introduction\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.010\n5\n051015degree 2\n(a)\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.010\n5\n051015degree 14 (b)\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.010\n5\n051015degree 20\n(c)\n2 4 6 8 10 12 14\ndegree0246810121416mse\ntest\ntrain (d)\nFigure 1.7: (a-c) Polynomials of degrees 2, 14 and 20 ﬁt to 21 datapoints (the same data as in Figure 1.5).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 64, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 993}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0065_7bffe406", "text": "(d) MSE vs degree. Generated by code at ﬁgures.probml.ai/book1/1.7. We can recursively decompose the feature extractor \u001e(x;V)into a composition of simpler functions. The resulting model then becomes a stack of Lnested functions:\nf(x;\u0012) =fL(fL\u00001(\u0001\u0001\u0001(f1(x))\u0001\u0001\u0001)) (1.29)\nwheref`(x) =f(x;\u0012`)is the function at layer `. The ﬁnal layer is linear and has the form\nfL(x) =wTf1:L\u00001(x), wheref1:L\u00001(x)is the learned feature extractor. This is the key idea behind\ndeep neural networks orDNNs, which includes common variants such as convolutional neural\nnetworks (CNNs) for images, and recurrent neural networks (RNNs) for sequences. See Part III\nfor details. 1.2.3 Overﬁtting and generalization\nWe can rewrite the empirical risk in Equation (1.4) in the following equivalent way:\nL(\u0012;Dtrain) =1\njDtrainjX\n(x;y)2Dtrain`(y;f(x;\u0012)) (1.30)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n1.2. Supervised learning 13\nwherejDtrainjis the size of the training set Dtrain.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 65, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 981}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0066_13a97faf", "text": "August 27, 2021\n1.2. Supervised learning 13\nwherejDtrainjis the size of the training set Dtrain. This formulation is useful because it makes explicit\nwhich dataset the loss is being evaluated on. With a suitably ﬂexible model, we can drive the training loss to zero (assuming no label noise), by\nsimply memorizing the correct output for each input. For example, Figure 1.7(c) perfectly interpolates\nthe training data (modulo the last point on the right). But what we care about is prediction accuracy\non new data, which may not be part of the training set. A model that perfectly ﬁts the training\ndata, but which is too complex, is said to suﬀer from overﬁtting . To detect if a model is overﬁtting, let us assume (for now) that we have access to the true (but\nunknown) distribution p\u0003(x;y)used to generate the training set.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 66, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 824}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0067_9a43257c", "text": "To detect if a model is overﬁtting, let us assume (for now) that we have access to the true (but\nunknown) distribution p\u0003(x;y)used to generate the training set. Then, instead of computing the\nempirical risk we compute the theoretical expected loss or population risk\nL(\u0012;p\u0003),Ep\u0003(x;y)[`(y;f(x;\u0012))] (1.31)\nThe diﬀerenceL(\u0012;p\u0003)\u0000L(\u0012;Dtrain)is called the generalization gap . If a model has a large\ngeneralization gap (i.e., low empirical risk but high population risk), it is a sign that it is overﬁtting. In practice we don’t know p\u0003. However, we can partition the data we do have into two subsets,\nknown as the training set and the test set. Then we can approximate the population risk using the\ntest risk :\nL(\u0012;Dtest),1\njDtestjX\n(x;y)2Dtest`(y;f(x;\u0012)) (1.32)\nAs an example, in Figure 1.7d, we plot the training error and test error for polynomial regression\nas a function of degree D. We see that the training error goes to 0 as the model becomes more\ncomplex.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 67, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 959}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0068_51e74b96", "text": "We see that the training error goes to 0 as the model becomes more\ncomplex. However, the test error has a characteristic U-shaped curve : on the left, where D= 1,\nthe model is underﬁtting ; on the right, where D\u001d1, the model is overﬁtting ; and when D= 2,\nthe model complexity is “just right”. How can we pick a model of the right complexity? If we use the training set to evaluate diﬀerent\nmodels, we will always pick the most complex model, since that will have the most degrees of\nfreedom , and hence will have minimum loss. So instead we should pick the model with minimum\ntest loss. In practice, we need to partition the data into three sets, namely the training set, the test set and\navalidation set ; the latter is used for model selection, and we just use the test set to estimate\nfuture performance (the population risk), i.e., the test set is not used for model ﬁtting or model\nselection. See Section 4.5.4 for further details.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 68, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 937}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0069_49d15ed0", "text": "See Section 4.5.4 for further details. 1.2.4 No free lunch theorem\nAll models are wrong, but some models are useful. — George Box [BD87, p424].5\nGiven the large variety of models in the literature, it is natural to wonder which one is best. Unfortunately, there is no single best model that works optimally for all kinds of problems — this\nis sometimes called the no free lunch theorem [Wol96]. The reason is that a set of assumptions\n(also called inductive bias ) that works well in one domain may work poorly in another. The best\n5. George Box is a retired statistics professor at the University of Wisconsin. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n14 Chapter 1. Introduction\nway to pick a suitable model is based on domain knowledge, and/or trial and error (i.e., using model\nselection techniques such as cross validation (Section 4.5.4) or Bayesian methods (Section 5.2.2)). For this reason, it is important to have many models and algorithmic techniques in one’s toolbox to\nchoose from.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 69, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1012}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0070_c071b239", "text": "For this reason, it is important to have many models and algorithmic techniques in one’s toolbox to\nchoose from. 1.3 Unsupervised learning\nIn supervised learning, we assume that each input example xin the training set has an associated\nset of output targets y, and our goal is to learn the input-output mapping. Although this is useful,\nand can be diﬃcult, supervised learning is essentially just “gloriﬁed curve ﬁtting” [Pea18]. An arguably much more interesting task is to try to “make sense of” data, as opposed to just\nlearning a mapping. That is, we just get observed “inputs” D=fxn:n= 1 :Ngwithout any\ncorresponding “outputs” yn. This is called unsupervised learning .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 70, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 674}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0071_14bef99c", "text": "That is, we just get observed “inputs” D=fxn:n= 1 :Ngwithout any\ncorresponding “outputs” yn. This is called unsupervised learning . From a probabilistic perspective, we can view the task of unsupervised learning as ﬁtting an\nunconditional model of the form p(x), which can generate new data x, whereas supervised learning\ninvolves ﬁtting a conditional model, p(yjx), which speciﬁes (a distribution over) outputs given\ninputs.6\nUnsupervised learning avoids the need to collect large labeled datasets for training, which can\noften be time consuming and expensive (think of asking doctors to label medical images). Unsupervised learning also avoids the need to learn how to partition the world into often arbitrary\ncategories. For example, consider the task of labeling when an action, such as “drinking” or “sipping”,\noccurs in a video. Is it when the person picks up the glass, or when the glass ﬁrst touches the mouth,\nor when the liquid pours out?", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 71, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 948}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0072_442f8b32", "text": "Is it when the person picks up the glass, or when the glass ﬁrst touches the mouth,\nor when the liquid pours out? What if they pour out some liquid, then pause, then pour again — is\nthat two actions or one? Humans will often disagree on such issues [Idr+17], which means the task is\nnot well deﬁned. It is therefore not reasonable to expect machines to learn such mappings.7\nFinally, unsupervised learning forces the model to “explain” the high-dimensional inputs, rather\nthan just the low-dimensional outputs. This allows us to learn richer models of “how the world works”. As Geoﬀ Hinton, who is a famous professor of ML at the University of Toronto, has said:\nWhen we’re learning to see, nobody’s telling us what the right answers are — we just look. Every so often, your mother says “that’s a dog”, but that’s very little information. You’d be\nlucky if you got a few bits of information — even one bit per second — that way. The brain’s\nvisual system has 1014neural connections.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 72, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 982}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0073_5a495d5a", "text": "You’d be\nlucky if you got a few bits of information — even one bit per second — that way. The brain’s\nvisual system has 1014neural connections. And you only live for 109seconds. So it’s no use\nlearning one bit per second. You need more like 105bits per second. And there’s only one place\nyou can get that much information: from the input itself. — Geoﬀrey Hinton, 1996 (quoted in\n[Gor06]). 1.3.1 Clustering\nA simple example of unsupervised learning is the problem of ﬁnding clusters in data. The goal is to\npartition the input into regions that contain “similar” points. As an example, consider a 2d version\n6. In the statistics community, it is common to use xto denote exogenous variables that are not modeled, but are\nsimply given as inputs. Therefore an unconditional model would be denoted p(y)rather than p(x). 7. A more reasonable approach is to try to capture the probability distribution over labels produced by a “crowd” of\nannotators (see e.g., [Dum+18; Aro+19]).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 73, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 974}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0074_29f408bd", "text": "7. A more reasonable approach is to try to capture the probability distribution over labels produced by a “crowd” of\nannotators (see e.g., [Dum+18; Aro+19]). This embraces the fact that there can be multiple “correct” labels for a\ngiven input due to the ambiguity of the task itself. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n1.3. Unsupervised learning 15\n1 2 3 4 5 6 7\npetal length (cm)0.00.51.01.52.02.5petal width (cm)\n(a)\n1 2 3 4 5 6 7\npetal length (cm)0.00.51.01.52.02.5petal width (cm)\nCluster 0\nCluster 1\nCluster 2 (b)\nFigure 1.8: (a) A scatterplot of the petal features from the iris dataset. (b) The result of unsupervised\nclustering using K= 3. Generated by code at ﬁgures.probml.ai/book1/1.8. sepal length4.55.05.56.06.57.07.58.0sepal width2.02.53.03.54.04.5petal length\n1234567\n(a)\nsepal length45678sepal width 2.02.53.03.54.04.5petal length\n1234567\n (b)\nFigure 1.9: (a) Scatterplot of iris data (ﬁrst 3 features). Points are color coded by class.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 74, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 993}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0075_1022ec6a", "text": "Points are color coded by class. (b) We ﬁt a 2d\nlinear subspace to the 3d data using PCA. The class labels are ignored. Red dots are the original data, black\ndots are points generated from the model using ^x=Wz+\u0016, wherezare latent points on the underlying\ninferred 2d linear manifold. Generated by code at ﬁgures.probml.ai/book1/1.9. of the Iris dataset. In Figure 1.8a, we show the points without any class labels. Intuitively there\nare at least two clusters in the data, one in the bottom left and one in the top right. Furthermore,\nif we assume that a “good” set of clusters should be fairly compact, then we might want to split\nthe top right into (at least) two subclusters. The resulting partition into three clusters is shown\nin Figure 1.8b. (Note that there is no correct number of clusters; instead, we need to consider the\ntradeoﬀ between model complexity and ﬁt to the data.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 75, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 884}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0076_57843f9b", "text": "(Note that there is no correct number of clusters; instead, we need to consider the\ntradeoﬀ between model complexity and ﬁt to the data. We discuss ways to make this tradeoﬀ in\nSection 21.3.7.)\n1.3.2 Discovering latent “factors of variation”\nWhen dealing with high-dimensional data, it is often useful to reduce the dimensionality by projecting\nit to a lower dimensional subspace which captures the “essence” of the data. One approach to this\nproblem is to assume that each observed high-dimensional output xn2RDwas generated by a set\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n16 Chapter 1. Introduction\nof hidden or unobserved low-dimensional latent factors zn2RK. We can represent the model\ndiagrammatically as follows: zn!xn, where the arrow represents causation. Since we don’t know\nthe latent factors zn, we often assume a simple prior probability model for p(zn)such as a Gaussian,\nwhich says that each factor is a random K-dimensional vector.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 76, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 967}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0077_89f72b39", "text": "Since we don’t know\nthe latent factors zn, we often assume a simple prior probability model for p(zn)such as a Gaussian,\nwhich says that each factor is a random K-dimensional vector. If the data is real-valued, we can use\na Gaussian likelihood as well. The simplest example is when we use a linear model, p(xnjzn;\u0012) =N(xnjWzn+\u0016;\u0006). The\nresulting model is called factor analysis (FA). It is similar to linear regression, except we only\nobserve the outputs xn, and not the inputs zn. In the special case that \u0006=\u001b2I, this reduces to\na model called probabilistic principal components analysis (PCA), which we will explain in\nSection 20.1. In Figure 1.9, we give an illustration of how this method can ﬁnd a 2d linear subspace\nwhen applied to some simple 3d data. Of course, assuming a linear mapping from zntoxnis very restrictive. However, we can create\nnonlinear extensions by deﬁning p(xnjzn;\u0012) =N(xnjf(zn;\u0012);\u001b2I), wheref(z;\u0012)is a nonlinear\nmodel, such as a deep neural network.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 77, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 977}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0078_4db288b4", "text": "However, we can create\nnonlinear extensions by deﬁning p(xnjzn;\u0012) =N(xnjf(zn;\u0012);\u001b2I), wheref(z;\u0012)is a nonlinear\nmodel, such as a deep neural network. It becomes much harder to ﬁt such a model (i.e., to estimate the\nparameters\u0012), because the inputs to the neural net have to be inferred, as well as the parameters of\nthe model. However, there are various approximate methods, such as the variational autoencoder\nwhich can be applied (see Section 20.3.5). 1.3.3 Self-supervised learning\nA recently popular approach to unsupervised learning is known as self-supervised learning . In this\napproach, we create proxy supervised tasks from unlabeled data. For example, we might try to learn\nto predict a color image from a grayscale image, or to mask out words in a sentence and then try to\npredict them given the surrounding context.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 78, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 827}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0079_dfa2eef0", "text": "For example, we might try to learn\nto predict a color image from a grayscale image, or to mask out words in a sentence and then try to\npredict them given the surrounding context. The hope is that the resulting predictor ^x1=f(x2;\u0012),\nwherex2is the observed input and ^x1is the predicted output, will learn useful features from the\ndata, that can then be used in standard, downstream supervised tasks. This avoids the hard problem\nof trying to infer the “true latent factors” zbehind the observed data, and instead relies on standard\nsupervised learning methods. We discuss this approach in more detail in Section 19.2. 1.3.4 Evaluating unsupervised learning\nAlthough unsupervised learning is appealing, it is very hard to evaluate the quality of the output of\nan unsupervised learning method, because there is no ground truth to compare to [TOB16]. A common method for evaluating unsupervised models is to measure the probability assigned by\nthe model to unseen test examples.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 79, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 975}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0080_ce56af6a", "text": "A common method for evaluating unsupervised models is to measure the probability assigned by\nthe model to unseen test examples. We can do this by computing the (unconditional) negative log\nlikelihood of the data:\nL(\u0012;D) =\u00001\njDjX\nx2Dlogp(xj\u0012) (1.33)\nThis treats the problem of unsupervised learning as one of density estimation . The idea is that a\ngood model will not be “surprised” by actual data samples (i.e., will assign them high probability). Furthermore, since probabilities must sum to 1.0, if the model assigns high probability to regions of\ndata space where the data samples come from, it implicitly assigns low probability to the regions\nwhere the data does not come from. Thus the model has learned to capture the typical patterns\nin the data. This can be used inside of a data compression algorithm. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n1.4. Reinforcement learning 17\n(a)\n (b)\nFigure 1.10: Examples of some control problems. (a) Space Invaders Atari game.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 80, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1007}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0081_a0607940", "text": "Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n1.4. Reinforcement learning 17\n(a)\n (b)\nFigure 1.10: Examples of some control problems. (a) Space Invaders Atari game. From https: // gym. openai. com/ envs/ SpaceInvaders-v0/ . (b) Controlling a humanoid robot in the MuJuCo simulator so it\nwalks as fast as possible without falling over. From https: // gym. openai. com/ envs/ Humanoid-v2/ . Unfortunately, density estimation is diﬃcult, especially in high dimensions. Furthermore, a model\nthat assigns high probability to the data may not have learned useful high-level patterns (after all,\nthe model could just memorize all the training examples). An alternative evaluation metric is to use the learned unsupervised representation as features or\ninput to a downstream supervised learning method.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 81, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 824}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0082_613b2de9", "text": "An alternative evaluation metric is to use the learned unsupervised representation as features or\ninput to a downstream supervised learning method. If the unsupervised method has discovered useful\npatterns, then it should be possible to use these patterns to perform supervised learning using much\nless labeled data than when working with the original features. For example, in Section 1.2.1.1, we\nsaw how the 4 manually deﬁned features of iris ﬂowers contained most of the information needed\nto perform classiﬁcation. We were thus able to train a classiﬁer with nearly perfect performance\nusing just 150 examples. If the input was raw pixels, we would need many more examples to achieve\ncomparable performance (see Section 14.1). That is, we can increase the sample eﬃciency of\nlearning (i.e., reduce the number of labeled examples needed to get good performance) by ﬁrst\nlearning a good representation.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 82, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 904}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0083_39283b96", "text": "That is, we can increase the sample eﬃciency of\nlearning (i.e., reduce the number of labeled examples needed to get good performance) by ﬁrst\nlearning a good representation. Increased sample eﬃciency is a useful evaluation metric, but in many applications, especially in\nscience, the goal of unsupervised learning is to gain understanding , not to improve performance on\nsome prediction task. This requires the use of models that are interpretable , but which can also\ngenerate or “explain” most of the observed patterns in the data. To paraphrase Plato, the goal is\nto discover how to “carve nature at its joints”. Of course, evaluating whether we have succesfully\ndiscovered the true underlying structure behind some dataset often requires performing experiments\nand thus interacting with the world. We discuss this topic further in Section 1.4. 1.4 Reinforcement learning\nIn addition to supervised and unsupervised learning, there is a third kind of ML known as reinforce-\nment learning (RL).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 83, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 995}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0084_357e5a91", "text": "We discuss this topic further in Section 1.4. 1.4 Reinforcement learning\nIn addition to supervised and unsupervised learning, there is a third kind of ML known as reinforce-\nment learning (RL). In this class of problems, the system or agenthas to learn how to interact\nwith its environment. This can be encoded by means of a policya=\u0019(x), which speciﬁes which\naction to take in response to each possible input x(derived from the environment state). For example, consider an agent that learns to play a video game, such as Atari Space Invaders (see\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n18 Chapter 1. Introduction\nFigure 1.11: The three types of machine learning visualized as layers of a chocolate cake. This ﬁgure (originally\nfrom https: // bit. ly/ 2m65Vs1 ) was used in a talk by Yann LeCun at NIPS’16, and is used with his kind\npermission. Figure 1.10a).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 84, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 880}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0085_7654cd69", "text": "This ﬁgure (originally\nfrom https: // bit. ly/ 2m65Vs1 ) was used in a talk by Yann LeCun at NIPS’16, and is used with his kind\npermission. Figure 1.10a). In this case, the input xis the image (or sequence of past images), and the output a\nis the direction to move in (left or right) and whether to ﬁre a missile or not. As a more complex\nexample, consider the problem of a robot learning to walk (see Figure 1.10b). In this case, the input\nxis the set of joint positions and angles for all the limbs, and the output ais a set of actuation or\nmotor control signals. The diﬀerence from supervised learning (SL) is that the system is not told which action is the\nbest one to take (i.e., which output to produce for a given input). Instead, the system just receives\nan occasional reward (or punishment) signal in response to the actions that it takes.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 85, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 848}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0086_51272e2a", "text": "Instead, the system just receives\nan occasional reward (or punishment) signal in response to the actions that it takes. This is like\nlearning with a critic , who gives an occasional thumbs up or thumbs down, as opposed to learning\nwith a teacher , who tells you what to do at each step. RL has grown in popularity recently, due to its broad applicability (since the reward signal that\nthe agent is trying to optimize can be any metric of interest). However, it can be harder to make RL\nwork than it is for supervised or unsupervised learning, for a variety of reasons. A key diﬃculty is\nthat the reward signal may only be given occasionally (e.g., if the agent eventually reaches a desired\nstate), and even then it may be unclear to the agent which of its many actions were responsible for\ngetting the reward.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 86, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 809}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0087_c49839be", "text": "(Think of playing a game like chess, where there is a single win or lose signal at\nthe end of the game.)\nTo compensate for the minimal amount of information coming from the reward signal, it is common\nto use other information sources, such as expert demonstrations, which can be used in a supervised\nway, or unlabeled data, which can be used by an unsupervised learning system to discover the\nunderlying structure of the environment. This can make it feasible to learn from a limited number of\ntrials (interactions with the environment). As Yann LeCun put it, in an invited talk at the NIPS8\nconference in 2016: “If intelligence was a cake, unsupervised learning would be the chocolate sponge,\nsupervised learning would be the icing, and reinforcement learning would be the cherry.” This is\nillustrated in Figure 1.11. 8. NIPS stands for “Neural Information Processing Systems”. It is one of the premier ML conferences. It has recently\nbeen renamed to NeurIPS.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 87, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 960}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0088_494f9add", "text": "8. NIPS stands for “Neural Information Processing Systems”. It is one of the premier ML conferences. It has recently\nbeen renamed to NeurIPS. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n1.5. Data 19\n(a)\na\n 6\n 3\n M\n c\n5\n 9\n l\n 4\n 7\nu\n T\n e\n t\n 6\nt\n v\n h\n 6\n 3\n7\n W\n 3\n e\n 9 (b)\nFigure 1.12: (a) Visualization of the MNIST dataset. Each image is 28\u000228. There are 60k training\nexamples and 10k test examples. We show the ﬁrst 25 images from the training set. Generated by code at\nﬁgures.probml.ai/book1/1.12. (b) Visualization of the EMNIST dataset. There are 697,932 training examples,\nand 116,323 test examples, each of size 28\u000228. There are 62 classes (a-z, A-Z, 0-9). We show the ﬁrst 25\nimages from the training set. Generated by code at ﬁgures.probml.ai/book1/1.12. More information on RL can be found in the sequel to this book, [Mur22]. 1.5 Data\nMachine learning is concerned with ﬁtting models to data using various algorithms.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 88, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 965}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0089_3ba679d7", "text": "More information on RL can be found in the sequel to this book, [Mur22]. 1.5 Data\nMachine learning is concerned with ﬁtting models to data using various algorithms. Although we\nfocus on the modeling and algorithm aspects, it is important to mention that the nature and quality\nof the training data also plays a vital role in the success of any learned model. In this section, we brieﬂy describe some common image and text datasets that we will use in this\nbook. We also brieﬂy discuss the topic of data preprocessing. 1.5.1 Some common image datasets\nIn this section, we brieﬂy discuss some image datasets that we will use in this book. 1.5.1.1 Small image datasets\nOne of the simplest and most widely used is known as MNIST [LeC+98; YB19].9This is a dataset\nof 60k training images and 10k test images, each of size 28\u000228(grayscale), illustrating handwritten\ndigits from 10 categories. Each pixel is an integer in the range f0;1;:::; 255g; these are usually\nrescaled to [0;1], to represent pixel intensity.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 89, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1006}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0090_2eb5347a", "text": "Each pixel is an integer in the range f0;1;:::; 255g; these are usually\nrescaled to [0;1], to represent pixel intensity. We can optionally convert this to a binary image by\nthresholding. See Figure 1.12a for an illustration. 9. The term “MNIST” stands for “Modiﬁed National Institute of Standards”; The term “modiﬁed” is used because the\nimages have been preprocessed to ensure the digits are mostly in the center of the image. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n20 Chapter 1. Introduction\n(a)\n (b)\nFigure 1.13: (a) Visualization of the Fashion-MNIST dataset [XRV17]. The dataset has the same size as\nMNIST, but is harder to classify. There are 10 classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal,\nShirt, Sneaker, Bag, Ankle-boot. We show the ﬁrst 25 images from the training set. Generated by code at\nﬁgures.probml.ai/book1/1.13. (b) Some images from the CIFAR-10 dataset [KH09]. Each image is 32\u000232\u00023,\nwhere the ﬁnal dimension of size 3 refers to RGB.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 90, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 990}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0091_b5647415", "text": "Generated by code at\nﬁgures.probml.ai/book1/1.13. (b) Some images from the CIFAR-10 dataset [KH09]. Each image is 32\u000232\u00023,\nwhere the ﬁnal dimension of size 3 refers to RGB. There are 50k training examples and 10k test examples. There are 10 classes: plane, car, bird, cat, deer, dog, frog, horse, ship, and truck. We show the ﬁrst 25 images\nfrom the training set. Generated by code at ﬁgures.probml.ai/book1/1.13. MNIST is so widely used in the ML community that Geoﬀ Hinton, a famous ML researcher, has\ncalled it the “drosophila of machine learning”, since if we cannot make a method work well on MNIST,\nit will likely not work well on harder datasets. However, nowadays MNIST classiﬁcation is considered\n“too easy”, since it is possible to distinguish most pairs of digits by looking at just a single pixel. Various extensions have been proposed. In [Coh+17], they proposed EMNIST (extended MNIST), that also includes lower and upper\ncase letters. See Figure 1.12b for a visualization.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 91, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 987}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0092_3756e846", "text": "Various extensions have been proposed. In [Coh+17], they proposed EMNIST (extended MNIST), that also includes lower and upper\ncase letters. See Figure 1.12b for a visualization. This dataset is much harder than MNIST, since\nthere are 62 classes, several of which are quite ambiguous (e.g., the digit 1 vs the lower case letter l). In [XRV17], they proposed Fashion-MNIST , which has exactly the same size and shape as\nMNIST, but where each image is the picture of a piece of clothing instead of a handwritten digit. See Figure 1.13a for a visualization. For small color images, the most common dataset is CIFAR [KH09].10This is a dataset of 60k\nimages, each of size 32\u000232\u00023, representing everyday objects from 10 or 100 classes; see Figure 1.13b\nfor an illustration. 1.5.1.2 ImageNet\nSmall datasets are useful for prototyping ideas, but it is also important to test methods on larger\ndatasets, both in terms of image size and number of labeled examples. The most widely used dataset\n10.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 92, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0093_fa65b1c4", "text": "The most widely used dataset\n10. CIFAR stands for “Canadian Institute For Advanced Research”. This is the agency that funded labeling of\nthe dataset, which was derived from the TinyImages dataset at http://groups.csail.mit.edu/vision/TinyImages/\ncreated by Antonio Torralba. See [KH09] for details. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n1.5. Data 21\n(a)\n (b)\nFigure 1.14: (a) Sample images from the ImageNet dataset [Rus+15]. This subset consists of 1.3M color\ntraining images, each of which is 256\u0002256pixels in size. There are 1000 possible labels, one per image, and\nthe task is to minimize the top-5 error rate, i.e., to ensure the correct label is within the 5 most probable\npredictions. Below each image we show the true label, and a distribution over the top 5 predicted labels. If the\ntrue label is in the top 5, its probability bar is colored red. Predictions are generated by a convolutional neural\nnetwork (CNN) called “AlexNet” (Section 14.3.2).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 93, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 994}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0094_bd01b2ed", "text": "If the\ntrue label is in the top 5, its probability bar is colored red. Predictions are generated by a convolutional neural\nnetwork (CNN) called “AlexNet” (Section 14.3.2). From Figure 4 of [KSH12]. Used with kind permission of\nAlex Krizhevsky. (b) Misclassiﬁcation rate (top 5) on the ImageNet competition over time. Used with kind\npermission of Andrej Karpathy. of this type is called ImageNet [Rus+15]. This is a dataset of \u001814Mimages of size 256\u0002256\u00023\nillustrating various objects from 20,000 classes; see Figure 1.14a for some examples. The ImageNet dataset was used as the basis of the ImageNet Large Scale Visual Recognition\nChallenge ( ILSVRC ), which ran from 2010 to 2018. This used a subset of 1.3M images from 1000\nclasses. During the course of the competition, signiﬁcant progress was made by the community, as\nshown in Figure 1.14b.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 94, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 845}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0095_958ce4ff", "text": "This used a subset of 1.3M images from 1000\nclasses. During the course of the competition, signiﬁcant progress was made by the community, as\nshown in Figure 1.14b. In particular, 2015 marked the ﬁrst year in which CNNs could outperform\nhumans (or at least one human, namely Andrej Karpathy) at the task of classifying images from\nImageNet. Note that this does not mean that CNNs are better at vision than humans (see e.g.,\n[YL21] for some common failure modes). Instead, it mostly likely reﬂects the fact that the dataset\nmakes many ﬁne-grained classiﬁcation distinctions — such as between a “tiger” and a “tiger cat”\n— that humans ﬁnd diﬃcult to understand; by contrast, suﬃciently ﬂexible CNNs can learn arbitrary\npatterns, including random labels [Zha+17a]. Although ImageNet is much harder than MNIST and CIFAR as a classiﬁcation benchmark, it too\nis almost “saturated” [Bey+20].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 95, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 883}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0096_7b6099d1", "text": "Although ImageNet is much harder than MNIST and CIFAR as a classiﬁcation benchmark, it too\nis almost “saturated” [Bey+20]. Nevertheless, relative performance of methods on ImageNet is often\na surprisingly good predictor of performance on other, unrelated image classiﬁcation tasks (see e.g.,\n[Rec+19]), so it remains very widely used. 1.5.2 Some common text datasets\nMachine learning is often applied to text to solve a variety of tasks. This is known as natural\nlanguage processing orNLP(see e.g., [JM20] for details). Below we brieﬂy mention a few text\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n22 Chapter 1. Introduction\n1. this ﬁlm was just brilliant casting location scenery story direction everyone’s really suited the part they played robert\n<UNK> is an amazing actor ... 2. big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy\nhorror movies and i’ve seen hundreds...", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 96, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 962}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0097_e9299711", "text": "2. big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy\nhorror movies and i’ve seen hundreds... Table 1.3: We show snippets of the ﬁrst two sentences from the IMDB movie review dataset. The ﬁrst example\nis labeled positive and the second negative. ( <UNK>refers to an unknown token.)\ndatasets that we will use in this book. 1.5.2.1 Text classiﬁcation\nA simple NLP task is text classiﬁcation, which can be used for email spam classiﬁcation ,senti-\nment analysis (e.g., is a movie or product review positive or negative), etc. A common dataset for\nevaluating such methods is the IMDB movie review dataset from [Maa+11]. (IMDB stands for\n“Internet Movie Database”.) This contains 25k labeled examples for training, and 25k for testing. Each example has a binary label, representing a positive or negative rating. See Table 1.3 for some\nexample sentences.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 97, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 921}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0098_5bd9c33b", "text": "Each example has a binary label, representing a positive or negative rating. See Table 1.3 for some\nexample sentences. 1.5.2.2 Machine translation\nA more diﬃcult NLP task is to learn to map a sentence xin one language to a “semantically equivalent”\nsentenceyin another language; this is called machine translation . Training such models requires\naligned (x;y)pairs. Fortunately, several such datasets exist, e.g., from the Canadian parliament\n(English-French pairs), and the European Union (Europarl). A subset of the latter, known as the\nWMT dataset (Workshop on Machine Translation), consists of English-German pairs, and is widely\nused as a benchmark dataset. 1.5.2.3 Other seq2seq tasks\nA generalization of machine translation is to learn a mapping from one sequence xto any other\nsequencey. This is called a seq2seq model , and can be viewed as a form of high-dimensional\nclassiﬁcation (see Section 15.2.3 for details).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 98, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 924}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0099_ae5ec25d", "text": "This is called a seq2seq model , and can be viewed as a form of high-dimensional\nclassiﬁcation (see Section 15.2.3 for details). This framing of the problem is very general, and\nincludes many tasks, such as document summarization ,question answering , etc. For example,\nTable 1.4 shows how to formulate question answering as a seq2seq problem: the input is the text T\nand question Q, and the output is the answer A, which is a set of words, possibly extracted from the\ninput. 1.5.2.4 Language modeling\nThe rather grandiose term “ language modeling ” refers to the task of creating unconditional\ngenerative models of text sequences, p(x1;:::;xT). This only requires input sentences x, without\nany corresponding “labels” y. We can therefore think of this as a form of unsupervised learning,\nwhich we discuss in Section 1.3. If the language model generates output in response to an input, as\nin seq2seq, we can regard it as a conditional generative model.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 99, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 952}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0100_a703a049", "text": "If the language model generates output in response to an input, as\nin seq2seq, we can regard it as a conditional generative model. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n1.5. Data 23\nT: In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under gravity. The\nmain forms of precipitation include drizzle, rain, sleet, snow, graupeland hail... Precipitation forms as smaller droplets\ncoalesce via collision with other rain drops or ice crystals within a cloud . Short, intense periods of rain in scattered\nlocations are called “showers”. Q1: What causes precipitation to fall? A1: gravity\nQ2: What is another main form of precipitation besides drizzle, rain, snow, sleet and hail? A2: graupel\nQ3: Where do water droplets collide with ice crystals to form precipitation? A3: within a cloud\nTable 1.4: Question-answer pairs for a sample passage in the SQuAD dataset. Each of the answers is a\nsegment of text from the passage.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 100, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1007}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0101_078439bf", "text": "A3: within a cloud\nTable 1.4: Question-answer pairs for a sample passage in the SQuAD dataset. Each of the answers is a\nsegment of text from the passage. This can be solved using sentence pair tagging. The input is the paragraph\ntext T and the question Q. The output is a tagging of the relevant words in T that answer the question in Q. From Figure 1 of [Raj+16]. Used with kind permission of Percy Liang. 1.5.3 Preprocessing discrete input data\nMany ML models assume that the data consists of real-valued feature vectors, x2RD. However,\nsometimes the input may have discrete input features, such as categorical variables like race and\ngender, or words from some vocabulary. In the sections below, we discuss some ways to preprocess\nsuch data to convert it to vector form. This is a common operation that is used for many diﬀerent\nkinds of models.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 101, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 848}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0102_b370af6f", "text": "In the sections below, we discuss some ways to preprocess\nsuch data to convert it to vector form. This is a common operation that is used for many diﬀerent\nkinds of models. 1.5.3.1 One-hot encoding\nWhen we have categorical features, we need to convert them to a numerical scale, so that computing\nweighted combinations of the inputs makes sense. The standard way to preprocess such categorical\nvariables is to use a one-hot encoding , also called a dummy encoding . If a variable xhasK\nvalues, we will denote its dummy encoding as follows: one-hot(x) = [I(x= 1);:::;I(x=K)]. For\nexample, if there are 3 colors (say red, green and blue), the corresponding one-hot vectors will be\none-hot(red) = [1 ;0;0],one-hot(green) = [0 ;1;0], and one-hot(blue) = [0 ;0;1]. 1.5.3.2 Feature crosses\nA linear model using a dummy encoding for each categorical variable can capture the main eﬀects\nof each variable, but cannot capture interaction eﬀects between them.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 102, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 949}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0103_6bee46b0", "text": "1.5.3.2 Feature crosses\nA linear model using a dummy encoding for each categorical variable can capture the main eﬀects\nof each variable, but cannot capture interaction eﬀects between them. For example, suppose we\nwant to predict the fuel eﬃciency of a vehicle given two categorical input variables: the type (say\nSUV, Truck, or Family car), and the country of origin (say USA or Japan). If we concatenate the\none-hot encodings for the ternary and binary features, we get the following input encoding:\n\u001e(x) = [1;I(x1=S);I(x1=T);I(x1=F);I(x2=U);I(x2=J)] (1.34)\nwherex1is the type and x2is the country of origin. This model cannot capture dependencies between the features. For example, we expect trucks to\nbe less fuel eﬃcient, but perhaps trucks from the USA are even less eﬃcient than trucks from Japan. This cannot be captured using the linear model in Equation (1.34) since the contribution from the\ncountry of origin is independent of the car type. Author: Kevin P. Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 103, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0104_ad8954e4", "text": "This cannot be captured using the linear model in Equation (1.34) since the contribution from the\ncountry of origin is independent of the car type. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n24 Chapter 1. Introduction\nWe can ﬁx this by computing explicit featurecrosses . For example, we can deﬁne a new composite\nfeature with 3\u00022possible values, to capture the interaction of type and country of origin. The new\nmodel becomes\nf(x;w) =wT\u001e(x) (1.35)\n=w0+w1I(x1=S) +w2I(x1=T) +w3I(x1=F)\n+w4I(x2=U) +w5I(x2=J)\n+w6I(x1=S;x2=U) +w7I(x1=T;x2=U) +w8I(x1=F;x2=U)\n+w9I(x1=S;x2=J) +w10I(x1=T;x2=J) +w11I(x1=F;x2=J) (1.36)\nWe can see that the use of feature crosses converts the original dataset into a wide format , with\nmany more columns. 1.5.4 Preprocessing text data\nIn Section 1.5.2, we brieﬂy discussed text classiﬁcation and other NLP tasks. To feed text data into\na classiﬁer, we need to tackle various issues.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 104, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 924}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0105_3efa84b4", "text": "1.5.4 Preprocessing text data\nIn Section 1.5.2, we brieﬂy discussed text classiﬁcation and other NLP tasks. To feed text data into\na classiﬁer, we need to tackle various issues. First, documents have a variable length, and are thus\nnot ﬁxed-length feature vectors, as assumed by many kinds of models. Second, words are categorical\nvariables with many possible values (equal to the size of the vocabulary), so the corresponding\none-hot encodings will be very high-dimensional, with no natural notion of similarity. Third, we may\nencounter words at test time that have not been seen during training (so-called out-of-vocabulary\norOOVwords). We discuss some solutions to these problems below. More details can be found in\ne.g., [BKL10; MRS08; JM20]. 1.5.4.1 Bag of words model\nA simple approach to dealing with variable-length text documents is to interpret them as a bag of\nwords, in which we ignore word order.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 105, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 909}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0106_d899cef3", "text": "1.5.4.1 Bag of words model\nA simple approach to dealing with variable-length text documents is to interpret them as a bag of\nwords, in which we ignore word order. To convert this to a vector from a ﬁxed input space, we ﬁrst\nmap each word to a tokenfrom some vocabulary. To reduce the number of tokens, we often use various pre-processing techniques such as the following:\ndropping punctuation, converting all words to lower case; dropping common but uninformative words,\nsuch as “and” and “the” (this is called stop word removal ); replacing words with their base form,\nsuch as replacing “running” and “runs” with “run” (this is called word stemming ); etc. For details,\nsee e.g., [BL12], and for some sample code, see code.probml.ai/book1/text_preproc_torch. Letxntbe the token at location tin then’th document.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 106, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 812}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0107_754b39e5", "text": "For details,\nsee e.g., [BL12], and for some sample code, see code.probml.ai/book1/text_preproc_torch. Letxntbe the token at location tin then’th document. If there are Dunique tokens in the\nvocabulary, then we can represent the n’th document as a D-dimensional vector ~xn, where ~xnvis\nthe number of times that word voccurs in document n:\n~xnv=TX\nt=1I(xnt=v) (1.37)\nwhereTis the length of document n. We can now interpret documents as vectors in RD. This is\ncalled the vector space model of text [SWY75; TP10]. We traditionally store input data in an N\u0002Ddesign matrix denoted by X, whereDis the number\nof features. In the context of vector space models, it is more common to represent the input data\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n1.5. Data 25\nFigure 1.15: Example of a term-document matrix, where raw counts have been replaced by their TF-IDF\nvalues (see Section 1.5.4.2). Darker cells are larger values. From https: // bit. ly/ 2kByLQI .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 107, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 984}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0108_b5b0890d", "text": "Data 25\nFigure 1.15: Example of a term-document matrix, where raw counts have been replaced by their TF-IDF\nvalues (see Section 1.5.4.2). Darker cells are larger values. From https: // bit. ly/ 2kByLQI . Used with\nkind permission of Christoph Carl Kling. as aD\u0002Nterm frequency matrix , where TFijis the frequency of term iin document j. See\nFigure 1.15 for an illustration. 1.5.4.2 TF-IDF\nOne problem with representing documents as word count vectors is that frequent words may have\nundue inﬂuence, just because the magnitude of their word count is higher, even if they do not carry\nmuch semantic content. A common solution to this is to transform the counts by taking logs, which\nreduces the impact of words that occur many times within a single document. To reduce the impact of words that occur many times in general (across all documents), we compute\na quantity called the inverse document frequency , deﬁned as follows: IDFi,logN\n1+DFi, where\nDFiis the number of documents with term i.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 108, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 990}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0109_bd5f4165", "text": "We can combine these transformations to compute the\nTF-IDF matrix as follows:\nTFIDFij= log(TF ij+ 1)\u0002IDFi (1.38)\n(We often normalize each row as well.) This provides a more meaningful representation of documents,\nand can be used as input to many ML algorithms. See code.probml.ai/book1/tﬁdf_demo for an\nexample. 1.5.4.3 Word embeddings\nAlthough the TF-IDF transformation improves the vector representation of words by placing more\nweighton“informative” wordsandlesson“uninformative” words, itdoesnotovercomethefundamental\nissue that semantically similar words, such as “man” and “woman”, may be further apart (in vector\nspace) than semantically dissimilar words, such as “man” and “banana”. Thus the assumption that\npoints that are close in input space should have similar outputs, which is implicitly made by logistic\nregression models, is invalid. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n26 Chapter 1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 109, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 923}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0110_a483aefd", "text": "Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n26 Chapter 1. Introduction\nThe standard way to solve this problem is to use word embeddings , in which we map each sparse\none-hot vector, xnt2f0;1gV, to a lower-dimensional dense vector, ent2RKusingent=Exnt,\nwhere Eis learned such that semantically similar words are placed close by. There are many ways to\nlearn such embeddings, as we discuss in Section 20.5. Once we have an embedding matrix, we can represent a variable-length text document as a bag of\nword embeddings . We can then convert this to a ﬁxed length vector by summing (or averaging)\nthe embeddings:\nen=TX\nt=1ent=E~xn (1.39)\nwhere ~xnis the bag of words representation from Equation (1.37). We can then use this inside of a\nlogistic regression classiﬁer, which we brieﬂy introduced in Section 1.2.1.5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 110, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 826}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0111_206a2b2a", "text": "We can then use this inside of a\nlogistic regression classiﬁer, which we brieﬂy introduced in Section 1.2.1.5. The overall model has the\nform\np(y=cjxn;\u0012) =Sc(WE~xn) (1.40)\nWe often use a pre-trained word embedding matrix E, in which case the model is linear in W,\nwhich simpliﬁes parameter estimation (see Chapter 10). See also Section 15.7 for a discussion of\ncontextual word embeddings. 1.5.4.4 Dealing with novel words\nAt test time, the model may encounter a completely novel word that it has not seen before. This is\nknown as the out of vocabulary orOOVproblem. Such novel words are bound to occur, because\nthe set of words is an open class . For example, the set of proper nouns (names of people and places)\nis unbounded. A standard heuristic to solve this problem is to replace all novel words with the special symbol\nUNK, which stands for “unknown”. However, this loses information.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 111, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 889}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0112_d0338d6f", "text": "A standard heuristic to solve this problem is to replace all novel words with the special symbol\nUNK, which stands for “unknown”. However, this loses information. For example, if we encounter\nthe word “athazagoraphobia”, we may guess it means “fear of something”, since phobia is a common\nsuﬃx in English (derived from Greek) to mean “fear of”. (It turns out that athazagoraphobia means\n“fear of being forgotten about or ignored”.)\nWe could work at the character level, but this would require the model to learn how to group\ncommon letter combinations together into words. It is better to leverage the fact that words have\nsubstructure, and then to take as input subword units orwordpieces [SHB16; Wu+16]; these\nare often created using a method called byte-pair encoding [Gag94], which is a form of data\ncompression that creates new symbols to represent common substrings. 1.5.5 Handling missing data\nSometimes we may have missing data , in which parts of the input xor outputymay be unknown.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 112, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0113_aec65deb", "text": "1.5.5 Handling missing data\nSometimes we may have missing data , in which parts of the input xor outputymay be unknown. If the output is unknown during training, the example is unlabeled; we consider such semi-supervised\nlearning scenarios in Section 19.3. We therefore focus on the case where some of the input features\nmay be missing, either at training or testing time, or both. To model this, let Mbe anN\u0002Dmatrix of binary variables, where Mnd= 1if featuredin\nexamplenis missing, and Mnd= 0otherwise. Let Xvbe the visible parts of the input feature matrix,\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n1.6. Discussion 27\ncorresponding to Mnd= 0, and Xhbe the missing parts, corresponding to Mnd= 1. Let Ybe the\noutput label matrix, which we assume is fully observed. If we assume p(MjXv;Xh;Y) =p(M), we\nsay the data is missing completely at random orMCAR, since the missingness does not depend\non the hidden or observed features.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 113, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 964}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0114_0a9ace66", "text": "If we assume p(MjXv;Xh;Y) =p(M), we\nsay the data is missing completely at random orMCAR, since the missingness does not depend\non the hidden or observed features. If we assume p(MjXv;Xh;Y) =p(MjXv;Y), we say the data is\nmissing at random orMAR, since the missingness does not depend on the hidden features, but\nmay depend on the visible features. If neither of these assumptions hold, we say the data is not\nmissing at random orNMAR. In the MCAR and MAR cases, we can ignore the missingness mechanism, since it tells us nothing\nabout the hidden features. However, in the NMAR case, we need to model the missing data\nmechanism , since the lack of information may be informative. For example, the fact that someone\ndid not ﬁll out an answer to a sensitive question on a survey (e.g., “Do you have COVID?”) could be\ninformative about the underlying value. See e.g., [LR87; Mar08] for more information on missing\ndata models. In this book, we will always make the MAR assumption.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 114, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 975}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0115_72fcbb98", "text": "See e.g., [LR87; Mar08] for more information on missing\ndata models. In this book, we will always make the MAR assumption. However, even with this assumption, we\ncannot directly use a discriminative model, such as a DNN, when we have missing input features, since\nthe inputxwill have some unknown values. A common heuristic is called mean value imputation ,\nin which missing values are replaced by their empirical mean. More generally, we can ﬁt a generative\nmodel to the input, and use that to ﬁll inthe missing values. We brieﬂy discuss some suitable\ngenerative models for this task in Chapter 20, and in more detail in the sequel to this book, [Mur22]. 1.6 Discussion\nIn this section, we situate ML and this book into a larger context. 1.6.1 The relationship between ML and other ﬁelds\nThere are several subcommunities that work on ML-related topics, each of which have diﬀerent names.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 115, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 888}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0116_098c4fcf", "text": "1.6.1 The relationship between ML and other ﬁelds\nThere are several subcommunities that work on ML-related topics, each of which have diﬀerent names. The ﬁeld of predictive analytics is similar to supervised learning (in particular, classiﬁcation\nand regression), but focuses more on business applications. Data mining covers both supervised\nand unsupervised machine learning, but focuses more on structured data, usually stored in large\ncommercial databases. Data science uses techniques from machine learning and statistics, but\nalso emphasizes other topics, such as data integration, data visualization, and working with domain\nexperts, often in an iterative feedback loop (see e.g., [BS17]). The diﬀerence between these areas is\noften just one of terminology.11\nML is also very closely related to the ﬁeld of statistics .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 116, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 825}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0117_d9fdcb6c", "text": "The diﬀerence between these areas is\noften just one of terminology.11\nML is also very closely related to the ﬁeld of statistics . Indeed, Jerry Friedman, a famous statistics\nprofessor at Stanford, said12\n[If the statistics ﬁeld had] incorporated computing methodology from its inception as a\nfundamental tool, as opposed to simply a convenient way to apply our existing tools, many of\nthe other data related ﬁelds [such as ML] would not have needed to exist — they would have\nbeen part of statistics. — Jerry Friedman [Fri97b]\nMachine learning is also related to artiﬁcial intelligence (AI). Historically, the ﬁeld of AI\nassumed that we could program “intelligence” by hand (see e.g., [RN10; PM17]), but this approach\n11. See https://developers.google.com/machine-learning/glossary/ for a useful “ML glossary”. 12. Quoted in https://brenocon.com/blog/2008/12/statistics-vs-machine-learning-fight/\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n28 Chapter 1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 117, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 970}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0118_0b701c1d", "text": "12. Quoted in https://brenocon.com/blog/2008/12/statistics-vs-machine-learning-fight/\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n28 Chapter 1. Introduction\nhas largely failed to live up to expectations, mostly because it proved to be too hard to explicitly\nencode all the knowledge such systems need. Consequently, there is renewed interest in using ML to\nhelp an AI system acquire its own knowledge. (Indeed the connections are so close that sometimes\nthe terms “ML” and “AI” are used interchangeably, although this is arguably misleading [Pre21].)\n1.6.2 Structure of the book\nWe have seen that ML is closely related to many other subjects in mathematics, statistics, computer\nscience, etc. It can be hard to know where to start. In this book, we take one particular path through this interconnected landscape, using probability\ntheory as our unifying lens. We cover statistical foundations in Part I, supervised learning in\nPart II–Part IV, and unsupervised learning in Part V.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 118, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 996}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0119_b8e0c426", "text": "We cover statistical foundations in Part I, supervised learning in\nPart II–Part IV, and unsupervised learning in Part V. For more information on these (and other)\ntopics, please see the sequel to this book, [Mur22],\nIn addition to the book, you may ﬁnd the online Python notebooks that accompany this book\nhelpful. See http://probml.ai for details. 1.6.3 Caveats\nIn this book, we will see how machine learning can be used to create systems that can (attempt\nto) predict outputs given inputs, and these predictions can then be used to choose actions so as to\nminimize expected loss. When designing such systems, it can be hard to design a loss function that\ncorrectly speciﬁes all of our preferences; this can result in “ reward hacking ” in which the machine\noptimizes the reward function we give it, but then we realize that the function did not capture various\nconstraints or preferences that we forgot to specify [Wei76; Amo+16; D’A+20].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 119, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 940}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0120_19d3199a", "text": "(This is particularly\nimportant when tradeoﬀs need to be made between multiple objectives.)\nReward hacking has raised various concerns in the context of AI ethics andAI safety (see e.g.,\n[KR19; Lia20]). Russell [Rus19] proposes to solve this problem by not specifying the reward function,\nbut instead forcing the machine to infer the reward by observing human behavior, an approach\nknown as inverse reinforcement learning . However, emulating current or past human behavior\ntoo closely may be undesirable, and can be biased by the data that is available for training (see e.g.,\n[Pau+20]). Another approach is to view ML as a tool for building adaptive components that form part of\na larger system. Such a system should be designed and regulated in the same way that we design\nand regulate other complex, semi-autonomous human artefacts, such as aeroplanes, online trading\nplatforms or medical diagnostic systems (c.f. [Jor19]).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 120, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 927}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0121_5d3bba61", "text": "[Jor19]). ML plays a key role in such systems, but\nadditional checks and balances should be put in place to encode our prior knowledge and preferences,\nand to ensure the system acts in the way that we intended. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\nPart I\nFoundations\n\n2Probability: Univariate Models\n2.1 Introduction\nIn this chapter, we give a brief introduction to the basics of probability theory. There are many good\nbooks that go into more detail, e.g., [GS97; BT08]. 2.1.1 What is probability? Probability theory is nothing but common sense reduced to calculation. — Pierre Laplace,\n1812\nWe are all comfortable saying that the probability that a (fair) coin will land heads is 50%. But\nwhat does this mean? There are actually two diﬀerent interpretations of probability. One is called\nthefrequentist interpretation. In this view, probabilities represent long run frequencies of events\nthat can happen multiple times.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 121, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 960}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0122_36149305", "text": "One is called\nthefrequentist interpretation. In this view, probabilities represent long run frequencies of events\nthat can happen multiple times. For example, the above statement means that, if we ﬂip the coin\nmany times, we expect it to land heads about half the time.1The other interpretation is called the\nBayesian interpretation of probability. In this view, probability is used to quantify our uncertainty\nor ignorance about something; hence it is fundamentally related to information rather than repeated\ntrials [Jay03; Lin06]. In the Bayesian view, the above statement means we believe the coin is equally\nlikely to land heads or tails on the next toss. One big advantage of the Bayesian interpretation is that it can be used to model our uncertainty\nabout one-oﬀ events that do not have long term frequencies. For example, we might want to compute\nthe probability that the polar ice cap will melt by 2030 CE. This event will happen zero or one times,\nbut cannot happen repeatedly.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 122, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 988}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0123_1d9f3677", "text": "For example, we might want to compute\nthe probability that the polar ice cap will melt by 2030 CE. This event will happen zero or one times,\nbut cannot happen repeatedly. Nevertheless, we ought to be able to quantify our uncertainty about\nthis event; based on how probable we think this event is, we can decide how to take the optimal\naction, as discussed in Chapter 5. We shall therefore adopt the Bayesian interpretation in this book. Fortunately, the basic rules of probability theory are the same, no matter which interpretation is\nadopted. 2.1.2 Types of uncertainty\nThe uncertainty in our predictions can arise for two fundamentally diﬀerent reasons. The ﬁrst is\ndue to our ignorance of the underlying hidden causes or mechanism generating our data. This is\n1. Actually, the Stanford statistician (and former professional magician) Persi Diaconis has shown that a coin is about\n51% likely to land facing the same way up as it started, due to the physics of the problem [DHM07]. 32 Chapter 2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 123, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 997}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0124_aff32aec", "text": "32 Chapter 2. Probability: Univariate Models\ncalledepistemic uncertainty , since epistemology is the philosophical term used to describe the\nstudy of knowledge. However, a simpler term for this is model uncertainty . The second kind of\nuncertainty arises from intrinsic variability, which cannot be reduced even if we collect more data. This is sometimes called aleatoric uncertainty [Hac75; KD09], derived from the Latin word for\n“dice”, although a simpler term would be data uncertainty . As a concrete example, consider tossing\na fair coin. We might know for sure that the probability of heads is p= 0:5, so there is no epistemic\nuncertainty, but we still cannot perfectly predict the outcome. This distinction can be important for applications such as active learning. A typical strategy is to\nquery examples for which H(p(yjx;D))is large (where H(p)is the entropy, discussed in Section 6.1).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 124, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 896}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0125_28d63405", "text": "This distinction can be important for applications such as active learning. A typical strategy is to\nquery examples for which H(p(yjx;D))is large (where H(p)is the entropy, discussed in Section 6.1). However, this could be due to uncertainty about the parameters, i.e., large H(p(\u0012jD)), or just due to\ninherent label noise or variability, corresponding to large entropy of p(yjx;\u0012). See [Osb16] for further\ndiscussion. 2.1.3 Probability as an extension of logic\nIn this section, we review the basic rules of probability, following the presentation of [Jay03], in which\nwe view probability as an extension of Boolean logic . 2.1.3.1 Probability of an event\nWe deﬁne an event, denoted by the binary variable A, as some state of the world that either holds\nor does not hold. For example, Amight be event “it will rain tomorrow”, or “it rained yesterday”, or\n“the label is y= 1”, or “the parameter \u0012is between 1:5and2:0”, etc.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 125, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 922}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0126_773b5f76", "text": "For example, Amight be event “it will rain tomorrow”, or “it rained yesterday”, or\n“the label is y= 1”, or “the parameter \u0012is between 1:5and2:0”, etc. The expression Pr(A)denotes\nthe probability with which you believe event Ais true (or the long run fraction of times that Awill\noccur). We require that 0\u0014Pr(A)\u00141, where Pr(A) = 0means the event deﬁnitely will not happen,\nandPr(A) = 1means the event deﬁnitely will happen. We write Pr(A)to denote the probability of\neventAnot happening; this is deﬁned to Pr(A) = 1\u0000Pr(A). 2.1.3.2 Probability of a conjunction of two events\nWe denote the joint probability of eventsAandBboth happening as follows:\nPr(A^B) = Pr(A;B) (2.1)\nIfAandBare independent events, we have\nPr(A;B) = Pr(A) Pr(B) (2.2)\nFor example, suppose XandYare chosen uniformly at random from the set X=f1;2;3;4g. Let\nAbe the event that X2f1;2g, andBbe the event that Y2f3g. Then we have Pr(A;B) =\nPr(A) Pr(B) =1\n2\u00011\n4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 126, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 925}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0127_32b181ee", "text": "Let\nAbe the event that X2f1;2g, andBbe the event that Y2f3g. Then we have Pr(A;B) =\nPr(A) Pr(B) =1\n2\u00011\n4. 2.1.3.3 Probability of a union of two events\nThe probability of event AorBhappening is given by\nPr(A_B) = Pr(A) + Pr(B)\u0000Pr(A^B) (2.3)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.2. Random variables 33\nIf the events are mutually exclusive (so they cannot happen at the same time), we get\nPr(A_B) = Pr(A) + Pr(B) (2.4)\nFor example, suppose Xis chosen uniformly at random from the set X=f1;2;3;4g. LetAbe the\nevent thatX2f1;2gandBbe the event that X2f3g. Then we have Pr(A_B) =2\n4+1\n4. 2.1.3.4 Conditional probability of one event given another\nWe deﬁne the conditional probability of eventBhappening given that Ahas occurred as follows:\nPr(BjA),Pr(A;B)\nPr(A)(2.5)\nThis is not deﬁned if Pr(A) = 0, since we cannot condition on an impossible event.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 127, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 884}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0128_59dc2fa7", "text": "2.1.3.5 Independence of events\nWe say that event Ais conditionally independent of event Bif\nPr(A;B) = Pr(A) Pr(B) (2.6)\n2.1.3.6 Conditional independence of events\nWe say that events AandBareconditionally independent given event Cif\nPr(A;BjC) = Pr(AjC) Pr(BjC) (2.7)\nThis is written as A?BjC. Events are often dependent on each other, but may be rendered\nindependent if we condition on the relevant intermediate variables, as we discuss in more detail later\nin this chapter. 2.2 Random variables\nSupposeXrepresents some unknown quantity of interest, such as which way a dice will land when\nwe roll it, or the temperature outside your house at the current time. If the value of Xis unknown\nand/or could change, we call it a random variable orrv. The set of possible values, denoted X, is\nknown as the sample space orstate space . Aneventis a set of outcomes from a given sample\nspace.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 128, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 882}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0129_5de891f1", "text": "The set of possible values, denoted X, is\nknown as the sample space orstate space . Aneventis a set of outcomes from a given sample\nspace. For example, if Xrepresents the face of a dice that is rolled, so X=f1;2;:::; 6g, the event\nof “seeing a 1” is denoted X= 1, the event of “seeing an odd number” is denoted X2f1;3;5g, the\nevent of “seeing a number between 1 and 3” is denoted 1\u0014X\u00143, etc. 2.2.1 Discrete random variables\nIf the sample space Xis ﬁnite or countably inﬁnite, then Xis called a discrete random variable . In this case, we denote the probability of the event that Xhas valuexbyPr(X=x). We deﬁne the\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n34 Chapter 2. Probability: Univariate Models\n1 2 3 40.000.250.500.751.00\n(a)\n1 2 3 40.000.250.500.751.00 (b)\nFigure 2.1: Some discrete distributions on the state space X=f1;2;3;4g. (a) A uniform distribution with\np(x=k) = 1=4. (b) A degenerate distribution (delta function) that puts all its mass on x= 1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 129, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 979}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0130_894a9df4", "text": "(a) A uniform distribution with\np(x=k) = 1=4. (b) A degenerate distribution (delta function) that puts all its mass on x= 1. Generated by\ncode at ﬁgures.probml.ai/book1/2.1. probability mass function orpmfas a function which computes the probability of events which\ncorrespond to setting the rv to each possible value:\np(x),Pr(X=x) (2.8)\nThe pmf satisﬁes the properties 0\u0014p(x)\u00141andP\nx2Xp(x) = 1. IfXhas a ﬁnite number of values, say K, the pmf can be represented as a list of Knumbers, which\nwe can plot as a histogram. For example, Figure 2.1 shows two pmf’s deﬁned on X=f1;2;3;4g. On the left we have a uniform distribution, p(x) = 1=4, and on the right, we have a degenerate\ndistribution, p(x) =I(x= 1), where I()is the binary indicator function. Thus the distribution in\nFigure 2.1(b) represents the fact that Xis always equal to the value 1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 130, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 846}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0131_afb852d3", "text": "Thus the distribution in\nFigure 2.1(b) represents the fact that Xis always equal to the value 1. (Thus we see that random\nvariables can also be constant.)\n2.2.2 Continuous random variables\nIfX2Ris a real-valued quantity, it is called a continuous random variable . In this case, we\ncan no longer create a ﬁnite (or countable) set of distinct possible values it can take on. However,\nthere are a countable number of intervals which we can partition the real line into. If we associate\nevents with Xbeing in each one of these intervals, we can use the methods discussed above for\ndiscrete random variables. By allowing the size of the intervals to shrink to zero, we can represent\nthe probability of Xtaking on a speciﬁc real value, as we show below. 2.2.2.1 Cumulative distribution function (cdf)\nDeﬁne the events A= (X\u0014a),B= (X\u0014b)andC= (a < X\u0014b), wherea < b.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 131, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 858}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0132_8204939d", "text": "2.2.2.1 Cumulative distribution function (cdf)\nDeﬁne the events A= (X\u0014a),B= (X\u0014b)andC= (a < X\u0014b), wherea < b. We have that\nB=A_C, and since AandCare mutually exclusive, the sum rules gives\nPr(B) = Pr(A) + Pr(C) (2.9)\nand hence the probability of being in interval Cis given by\nPr(C) = Pr(B)\u0000Pr(A) (2.10)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.2. Random variables 35\n3\n 2\n 1\n 0 1 2 30.00.20.40.60.81.0Gaussian cdf\n(a)\n)-1( ,/2)0)-1(1- ,/2),/2 ,/2 (b)\nFigure 2.2: (a) Plot of the cdf for the standard normal, N(0;1). Generated by code at ﬁg-\nures.probml.ai/book1/2.2. (b) Corresponding pdf. The shaded regions each contain \u000b=2of the probability mass. Therefore the nonshaded region contains 1\u0000\u000bof the probability mass. The leftmost cutoﬀ point is \b\u00001(\u000b=2),\nwhere \bis the cdf of the Gaussian. By symmetry, the rightmost cutoﬀ point is \b\u00001(1\u0000\u000b=2) =\u0000\b\u00001(\u000b=2). Generated by code at ﬁgures.probml.ai/book1/2.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 132, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 942}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0133_a3f5d47e", "text": "The leftmost cutoﬀ point is \b\u00001(\u000b=2),\nwhere \bis the cdf of the Gaussian. By symmetry, the rightmost cutoﬀ point is \b\u00001(1\u0000\u000b=2) =\u0000\b\u00001(\u000b=2). Generated by code at ﬁgures.probml.ai/book1/2.2. In general, we deﬁne the cumulative distribution function orcdfof the rvXas follows:\nP(x),Pr(X\u0014x) (2.11)\n(Note that we use a capital Pto represent the cdf.) Using this, we can compute the probability of\nbeing in any interval as follows:\nPr(a<X\u0014b) =P(b)\u0000P(a) (2.12)\nCdf’s are monotonically non-decreasing functions. See Figure 2.2a for an example, where we\nillustrate the cdf of a standard normal distribution, N(xj0;1); see Section 2.6 for details. 2.2.2.2 Probability density function (pdf)\nWe deﬁne the probability density function orpdfas the derivative of the cdf:\np(x),d\ndxP(x) (2.13)\n(Note that this derivative does not always exist, in which case the pdf is not deﬁned.) See Figure 2.2b\nfor an example, where we illustrate the pdf of a univariate Gaussian (see Section 2.6 for details).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 133, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 980}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0134_0eec69c9", "text": "Given a pdf, we can compute the probability of a continuous variable being in a ﬁnite interval as\nfollows:\nPr(a<X\u0014b) =Zb\nap(x)dx=P(b)\u0000P(a) (2.14)\nAs the size of the interval gets smaller, we can write\nPr(x\u0014X\u0014x+dx)\u0019p(x)dx (2.15)\nIntuitively, this says the probability of Xbeing in a small interval around xis the density at xtimes\nthe width of the interval. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n36 Chapter 2. Probability: Univariate Models\n2.2.2.3 Quantiles\nIf the cdfPis strictly monotonically increasing, it has an inverse, called the inverse cdf , orpercent\npoint function (ppf), orquantile function . IfPis the cdf of X, thenP\u00001(q)is the value xqsuch that Pr(X\u0014xq) =q; this is called the q’th\nquantile ofP. The value P\u00001(0:5)is themedian of the distribution, with half of the probability\nmass on the left, and half on the right. The values P\u00001(0:25)andP\u00001(0:75)are the lower and upper\nquartiles .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 134, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 922}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0135_29945177", "text": "The value P\u00001(0:5)is themedian of the distribution, with half of the probability\nmass on the left, and half on the right. The values P\u00001(0:25)andP\u00001(0:75)are the lower and upper\nquartiles . For example, let \bbe the cdf of the Gaussian distribution N(0;1), and \b\u00001be the inverse cdf. Then points to the left of \b\u00001(\u000b=2)contain\u000b=2of the probability mass, as illustrated in Figure 2.2b. By symmetry, points to the right of \b\u00001(1\u0000\u000b=2)also contain \u000b=2of the mass. Hence the central\ninterval (\b\u00001(\u000b=2);\b\u00001(1\u0000\u000b=2))contains 1\u0000\u000bof the mass. If we set \u000b= 0:05, the central 95%\ninterval is covered by the range\n(\b\u00001(0:025);\b\u00001(0:975)) = (\u00001:96;1:96) (2.16)\nIf the distribution is N(\u0016;\u001b2), then the 95% interval becomes (\u0016\u00001:96\u001b;\u0016+ 1:96\u001b). This is often\napproximated by writing \u0016\u00062\u001b. 2.2.3 Sets of related random variables\nIn this section, we discuss distributions over sets of related random variables. Suppose, to start, that we have two random variables, XandY.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 135, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 952}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0136_56a65f73", "text": "2.2.3 Sets of related random variables\nIn this section, we discuss distributions over sets of related random variables. Suppose, to start, that we have two random variables, XandY. We can deﬁne the joint\ndistribution of two random variables using p(x;y) =p(X=x;Y =y)for all possible values of\nXandY. If both variables have ﬁnite cardinality, we can represent the joint distribution as a 2d\ntable, all of whose entries sum to one. For example, consider the following example with two binary\nvariables:\np(X;Y )Y= 0Y= 1\nX= 00.2 0.3\nX= 10.3 0.2\nIf two variables are independent, we can represent the joint as the product of the two marginals. If\nboth variables have ﬁnite cardinality, we can factorize the 2d joint table into a product of two 1d\nvectors, as shown in Figure 2.3. Given a joint distribution, we deﬁne the marginal distribution of an rvas follows:\np(X=x) =X\nyp(X=x;Y =y) (2.17)\nwhere we are summing over all possible states of Y.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 136, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 939}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0137_60e616d3", "text": "Given a joint distribution, we deﬁne the marginal distribution of an rvas follows:\np(X=x) =X\nyp(X=x;Y =y) (2.17)\nwhere we are summing over all possible states of Y. This is sometimes called the sum rule or the\nrule of total probability . We deﬁne p(Y=y)similarly. For example, from the above 2d table, we\nseep(X= 0) = 0:2 + 0:3 = 0:5andp(Y= 0) = 0:2 + 0:3 = 0:5. (The term “marginal” comes from\nthe accounting practice of writing the sums of rows and columns on the side, or margin, of a table.)\nWe deﬁne the conditional distribution of an rvusing\np(Y=yjX=x) =p(X=x;Y =y)\np(X=x)(2.18)\nWe can rearrange this equation to get\np(x;y) =p(x)p(yjx) (2.19)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.2. Random variables 37\nP(X, Y)P(Y)\nP(X)=\nFigure 2.3: Computing p(x;y) =p(x)p(y), whereX?Y. HereXandYare discrete random variables; X\nhas 6 possible states (values) and Yhas 5 possible states.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 137, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 918}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0138_5389917a", "text": "Random variables 37\nP(X, Y)P(Y)\nP(X)=\nFigure 2.3: Computing p(x;y) =p(x)p(y), whereX?Y. HereXandYare discrete random variables; X\nhas 6 possible states (values) and Yhas 5 possible states. A general joint distribution on two such variables\nwould require (6\u00025)\u00001 = 29parameters to deﬁne it (we subtract 1 because of the sum-to-one constraint). By assuming (unconditional) independence, we only need (6\u00001) + (5\u00001) = 9parameters to deﬁne p(x;y). This is called the product rule . By extending the product rule to Dvariables, we get the chain rule of probability :\np(x1:D) =p(x1)p(x2jx1)p(x3jx1;x2)p(x4jx1;x2;x3):::p(xDjx1:D\u00001) (2.20)\nThis provides a way to create a high dimensional joint distribution from a set of conditional\ndistributions. We discuss this in more detail in Section 3.6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 138, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 786}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0139_3752ef87", "text": "We discuss this in more detail in Section 3.6. 2.2.4 Independence and conditional independence\nWesayXandYareunconditionallyindependent ormarginallyindependent , denotedX?Y,\nif we can represent the joint as the product of the two marginals (see Figure 2.3), i.e.,\nX?Y()p(X;Y ) =p(X)p(Y) (2.21)\nIn general, we say a set of variables X1;:::;Xnisindependent if the joint can be written as a\nproduct of marginals, i.e.,\np(X1;:::;Xn) =nY\ni=1p(Xi) (2.22)\nUnfortunately, unconditional independence is rare, because most variables can inﬂuence most other\nvariables. However, usually this inﬂuence is mediated via other variables rather than being direct. We therefore say XandYareconditionally independent (CI) given Ziﬀ the conditional joint\ncan be written as a product of conditional marginals:\nX?YjZ()p(X;YjZ) =p(XjZ)p(YjZ) (2.23)\nWe can write this assumption as a graph X\u0000Z\u0000Y, which captures the intuition that all the\ndependencies between XandYare mediated via Z.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 139, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 959}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0140_c7b3fc70", "text": "By using larger graphs, we can deﬁne complex\njoint distributions; these are known as graphical models , and are discussed in Section 3.6. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n38 Chapter 2. Probability: Univariate Models\n2.2.5 Moments of a distribution\nIn this section, we describe various summary statistics that can be derived from a probability\ndistribution (either a pdf or pmf). 2.2.5.1 Mean of a distribution\nThe most familiar property of a distribution is its mean, orexpected value , often denoted by \u0016. For continuous rv’s, the mean is deﬁned as follows:\nE[X],Z\nXxp(x)dx (2.24)\nIf the integral is not ﬁnite, the mean is not deﬁned; we will see some examples of this later. For discrete rv’s, the mean is deﬁned as follows:\nE[X],X\nx2Xxp(x) (2.25)\nHowever, this is only meaningful if the values of xare ordered in some way (e.g., if they represent\ninteger counts). Since the mean is a linear operator, we have\nE[aX+b] =aE[X] +b (2.26)\nThis is called the linearity of expectation .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 140, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1009}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0141_53663fba", "text": "Since the mean is a linear operator, we have\nE[aX+b] =aE[X] +b (2.26)\nThis is called the linearity of expectation . For a set of nrandom variables, one can show that the expectation of their sum is as follows:\nE\"nX\ni=1Xi#\n=nX\ni=1E[Xi] (2.27)\nIf they are independent, the expectation of their product is given by\nE\"nY\ni=1Xi#\n=nY\ni=1E[Xi] (2.28)\n2.2.5.2 Variance of a distribution\nThevariance is a measure of the “spread” of a distribution, often denoted by \u001b2. This is deﬁned as\nfollows:\nV[X],E\u0002\n(X\u0000\u0016)2\u0003\n=Z\n(x\u0000\u0016)2p(x)dx (2.29)\n=Z\nx2p(x)dx+\u00162Z\np(x)dx\u00002\u0016Z\nxp(x)dx=E\u0002\nX2\u0003\n\u0000\u00162(2.30)\nfrom which we derive the useful result\nE\u0002\nX2\u0003\n=\u001b2+\u00162(2.31)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.2. Random variables 39\nThestandard deviation is deﬁned as\nstd [X],p\nV[X] =\u001b (2.32)\nThis is useful since it has the same units as Xitself.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 141, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 852}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0142_3a06797b", "text": "August 27, 2021\n2.2. Random variables 39\nThestandard deviation is deﬁned as\nstd [X],p\nV[X] =\u001b (2.32)\nThis is useful since it has the same units as Xitself. The variance of a shifted and scaled version of a random variable is given by\nV[aX+b] =a2V[X] (2.33)\nIf we have a set of nindependent random variables, the variance of their sum is given by the sum\nof their variances:\nV\"nX\ni=1Xi#\n=nX\ni=1V[Xi] (2.34)\nThe variance of their product can also be derived, as follows:\nV\"nY\ni=1Xi#\n=E\"\n(Y\niXi)2#\n\u0000(E\"Y\niXi#\n)2(2.35)\n=E\"Y\niX2\ni#\n\u0000(Y\niE[Xi])2(2.36)\n=Y\niE\u0002\nX2\ni\u0003\n\u0000Y\ni(E[Xi])2(2.37)\n=Y\ni(V[Xi] + (E[Xi])2)\u0000Y\ni(E[Xi])2(2.38)\n=Y\ni(\u001b2\ni+\u00162\ni)\u0000Y\ni\u00162\ni (2.39)\n2.2.5.3 Mode of a distribution\nThemodeof a distribution is the value with the highest probability mass or probability density:\nx\u0003= argmax\nxp(x) (2.40)\nIf the distribution is multimodal , this may not be unique, as illustrated in Figure 2.4. Furthermore,\neven if there is a unique mode, this point may not be a good summary of the distribution.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 142, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 993}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0143_04de31ec", "text": "Furthermore,\neven if there is a unique mode, this point may not be a good summary of the distribution. 2.2.5.4 Conditional moments\nWhen we have two or more dependent random variables, we can compute the moments of one given\nknowledge of the other. For example, the law of iterated expectations , also called the law of\ntotal expectation , tells us that\nE[X] =E[E[XjY]] (2.41)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n40 Chapter 2. Probability: Univariate Models\n2\n 1\n 0 1 2 3 40.000.050.100.150.200.250.300.350.40\nFigure 2.4: Illustration of a mixture of two 1d Gaussians, p(x) = 0:5N(xj0;0:5) + 0:5N(xj2;0:5). Generated\nby code at ﬁgures.probml.ai/book1/2.4. To prove this, let us suppose, for simplicity, that XandYare both discrete rv’s.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 143, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 759}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0144_d02b58bd", "text": "Generated\nby code at ﬁgures.probml.ai/book1/2.4. To prove this, let us suppose, for simplicity, that XandYare both discrete rv’s. Then we have\nE[E[XjY]] =E\"X\nxxp(X=xjY)#\n(2.42)\n=X\ny\"X\nxxp(X=xjY)#\np(Y=y) =X\nx;yxp(X=x;Y =y) =E[X] (2.43)\nTo give a more intuitive explanation, consider the following simple example.2LetXbe the\nlifetime duration of a lightbulb, and let Ybe the factory the lightbulb was produced in. Suppose\nE[XjY= 1] = 5000andE[XjY= 2] = 4000, indicating that factory 1 produces longer lasting bulbs. Suppose factory 1 supplies 60% of the lightbulbs, so p(Y= 1) = 0:6andp(Y= 2) = 0:4. Then the\nexpected duration of a random lightbulb is given by\nE[X] =E[XjY= 1]p(Y= 1) + E[XjY= 2]p(Y= 2) = 5000\u00020:6 + 4000\u00020:4 = 4600 (2.44)\nThere is a similar formula for the variance.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 144, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 781}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0145_5ab02dff", "text": "Then the\nexpected duration of a random lightbulb is given by\nE[X] =E[XjY= 1]p(Y= 1) + E[XjY= 2]p(Y= 2) = 5000\u00020:6 + 4000\u00020:4 = 4600 (2.44)\nThere is a similar formula for the variance. In particular, the law of total variance , also called\ntheconditional variance formula , tells us that\nV[X] =E[V[XjY]] +V[E[XjY]] (2.45)\nTo see this, let us deﬁne the conditional moments, \u0016XjY=E[XjY],sXjY=E\u0002\nX2jY\u0003\n, and\n\u001b2\nXjY=V[XjY]=sXjY\u0000\u00162\nXjY, which are functions of Y(and therefore are random quantities). Then we have\nV[X] =E\u0002\nX2\u0003\n\u0000(E[X])2=E\u0002\nsXjYjY\u0003\n\u0000\u0000\nE\u0002\n\u0016XjYjY\u0003\u00012(2.46)\n=Eh\n\u001b2\nXjYjYi\n+Eh\n\u00162\nXjYjYi\n\u0000\u0000\nE\u0002\n\u0016XjYjY\u0003\u00012(2.47)\n=EY[V[XjY]] +VY[\u0016XjY] (2.48)\nTo get some intuition for these formulas, consider a mixture of Kunivariate Gaussians. Let\nYbe the hidden indicator variable that speciﬁes which mixture component we are using, and let\n2. This example is from https://en.wikipedia.org/wiki/Law_of_total_expectation , but with modiﬁed notation. Draft of “Probabilistic Machine Learning: An Introduction”.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 145, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 993}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0146_bd658262", "text": "This example is from https://en.wikipedia.org/wiki/Law_of_total_expectation , but with modiﬁed notation. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.2. Random variables 41\n5 10 15 20\nx4681012y\ndataset = I\n5 10 15 20\nx\ndataset = II\n5 10 15 20\nx\ndataset = III\n5 10 15 20\nx\ndataset = IV\nFigure 2.5: Illustration of Anscombe’s quartet. All of these datasets have the same low order summary\nstatistics. Generated by code at ﬁgures.probml.ai/book1/2.5. X=PK\ny=1\u0019yN(Xj\u0016y;\u001by). In Figure 2.4, we have \u00191=\u00192= 0:5,\u00161= 0,\u00162= 2,\u001b1=\u001b2= 0:5. Thus\nE[V[XjY]] =\u00191\u001b2\n1+\u00192\u001b2\n2= 0:5 (2.49)\nV[E[XjY]] =\u00191(\u00161\u0000\u0016)2+\u00192(\u00162\u0000\u0016)2= 0:5(0\u00001)2+ 0:5(2\u00001)2= 0:5 + 0:5 = 1 (2.50)\nSo we get the intuitive result that the variance of Xis dominated by which centroid it is drawn from\n(i.e., diﬀerence in the means), rather than the local variance around each centroid.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 146, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 863}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0147_c7e0a365", "text": "2.2.6 Limitations of summary statistics *\nAlthough it is common to summarize a probability distribution (or points sampled from a distribution)\nusing simple statistics such as the mean and variance, this can lose a lot of information. A striking\nexample of this is known as Anscombe’s quartet [Ans73], which is illustrated in Figure 2.5. This\nshows 4 diﬀerent datasets of (x;y)pairs, all of which have identical low order statistics: E[x]= 9,\nV[x]= 11,E[y]= 7:50,V[y]= 4:125, and\u001a= 0:816. (The quantity \u001ais the correlation coeﬃcient,\ndeﬁned in Section 3.1.2.) However, the joint distributions p(x;y)from which these points were\nsampled are clearly very diﬀerent. Anscombe invented these datasets, each consisting of 10 data\npoints, to counter the impression among statisticians that numerical summaries are superior to data\nvisualization [Ans73]. An even more striking example of this phenomenon is shown in Figure 2.6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 147, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 919}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0148_75e136e3", "text": "An even more striking example of this phenomenon is shown in Figure 2.6. This consists of a\ndataset that looks like a dinosaur3, plus 11 other datasets, all of which have identical low order\nstatistics. This collection of datasets is called the Datasaurus Dozen [MF17]. The exact values of\nthe(x;y)points are available online.4They were computed using simulated annealing a derivative\nfree optimization method, which we discuss in the sequel to this book, [Mur22]. (The objective\nfunction being optimized measures deviation from the target summary statistics of the original\ndinasour, plus distance from a particular target shape.)\n3. This dataset was created by Alberto Cairo, and is available at http://www.thefunctionalart.com/2016/08/\ndownload-datasaurus-never-trust-summary.html\n4.https://www.autodesk.com/research/publications/same-stats-different-graphs . There are actually 13\ndatasets in total, including the dinasour. We omitted the “away” dataset for visual clarity. Author: Kevin P. Murphy.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 148, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1002}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0149_d68cd16f", "text": "There are actually 13\ndatasets in total, including the dinasour. We omitted the “away” dataset for visual clarity. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n42 Chapter 2. Probability: Univariate Models\n020406080100ydataset = dino\n dataset = h_lines\n dataset = v_lines\n dataset = x_shape\n020406080100ydataset = star\n dataset = high_lines\n dataset = dots\n dataset = circle\n20 40 60 80 100\nx020406080100ydataset = bullseye\n20 40 60 80 100\nxdataset = slant_up\n20 40 60 80 100\nxdataset = slant_down\n20 40 60 80 100\nxdataset = wide_lines\nFigure 2.6: Illustration of the Datasaurus Dozen. All of these datasets have the same low order summary\nstatistics. Adapted from Figure 1 of [MF17]. Generated by code at ﬁgures.probml.ai/book1/2.6. Figure 2.7: Illustration of 7 diﬀerent datasets (left), the corresponding box plots (middle) and\nviolin box plots (right). From Figure 8 of https: // www. autodesk. com/ research/ publications/\nsame-stats-different-graphs .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 149, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 971}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0150_e683af79", "text": "From Figure 8 of https: // www. autodesk. com/ research/ publications/\nsame-stats-different-graphs . Used with kind permission of Justin Matejka. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.3. Bayes’ rule 43\nThe same simulated annealing approach can be applied to 1d datasets, as shown in Figure 2.7. We\nsee that all the datasets are quite diﬀerent, but they all have the same median and inter-quartile\nrangeas shown by the central shaded part of the box plots in the middle. A better visualization\nis known as a violin plot , shown on the right. This shows (two copies of) the 1d kernel density\nestimate (Section 16.3) of the distribution on the vertical axis, in addition to the median and IQR\nmarkers. This visualization is better able to distinguish diﬀerences in the distributions. However, the\ntechnique is limited to 1d data. 2.3 Bayes’ rule\nBayes’s theorem is to the theory of probability what Pythagoras’s theorem is to geometry. — Sir Harold Jeﬀreys, 1973 [Jef73].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 150, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1008}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0151_19f98d31", "text": "However, the\ntechnique is limited to 1d data. 2.3 Bayes’ rule\nBayes’s theorem is to the theory of probability what Pythagoras’s theorem is to geometry. — Sir Harold Jeﬀreys, 1973 [Jef73]. In this section, we discuss the basics of Bayesian inference . According to the Merriam-Webster\ndictionary, the term “inference” means “the act of passing from sample data to generalizations, usually\nwith calculated degrees of certainty”. The term “Bayesian” is used to refer to inference methods\nthat represent “degrees of certainty” using probability theory, and which leverage Bayes’ rule5, to\nupdate the degree of certainty given data. Bayes’ rule itself is very simple: it is just a formula for computing the probability distribution over\npossible values of an unknown (or hidden) quantityHgiven some observed data Y=y:\np(H=hjY=y) =p(H=h)p(Y=yjH=h)\np(Y=y)(2.51)\nThis follows automatically from the identity\np(hjy)p(y) =p(h)p(yjh) =p(h;y) (2.52)\nwhich itself follows from the product rule of probability .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 151, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 997}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0152_1b6f119d", "text": "In Equation (2.51), the term p(H)represents what we know about possible values of Hbefore\nwe see any data; this is called the prior distribution . (IfHhasKpossible values, then p(H)is\na vector of Kprobabilities, that sum to 1.) The term p(YjH=h)represents the distribution over\nthe possible outcomes Ywe expect to see if H=h; this is called the observation distribution . When we evaluate this at a point corresponding to the actual observations, y, we get the function\np(Y=yjH=h), which is called the likelihood . (Note that this is a function of h, sinceyis\nﬁxed, but it is not a probability distribution, since it does not sum to one.) Multiplying the prior\ndistribution p(H=h)by the likelihood function p(Y=yjH=h)for eachhgives the unnormalized\njoint distribution p(H=h;Y =y).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 152, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 780}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0153_d079d763", "text": "We can convert this into a normalized distribution by dividing\nbyp(Y=y), which is known as the marginal likelihood , since it is computed by marginalizing\nover the unknown H:\np(Y=y) =X\nh02Hp(H=h0)p(Y=yjH=h0) =X\nh02Hp(H=h0;Y=y) (2.53)\n5. Thomas Bayes (1702–1761) was an English mathematician and Presbyterian minister. For a discussion of whether\nto spell this as Bayes’ rule or Bayes’s rule, see https://bit.ly/2kDtLuK . Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n44 Chapter 2. Probability: Univariate Models\nObservation\n0 1\nTruth0TNR=Speciﬁcity=0.975 FPR=1-TNR=0.025\n1FNR=1-TPR=0.125 TPR=Sensitivity=0.875\nTable 2.1: Likelihood function p(YjH)for a binary observation Ygiven two possible hidden states H. Each\nrow sums to one. Abbreviations: TNR is true negative rate, TPR is true positive rate, FNR is false negative\nrate, FPR is false positive rate.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 153, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 869}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0154_4efd52af", "text": "Each\nrow sums to one. Abbreviations: TNR is true negative rate, TPR is true positive rate, FNR is false negative\nrate, FPR is false positive rate. Normalizing the joint distribution by computing p(H=h;Y =y)=p(Y=y)for eachhgives the\nposterior distribution p(H=hjY=y); this represents our new belief state about the possible\nvalues ofH. We can summarize Bayes rule in words as follows:\nposterior/prior\u0002likelihood (2.54)\nHere we use the symbol /to denote “proportional to”, since we are ignoring the denominator, which is\njust a constant, independent of H. Using Bayes rule to update a distribution over unknown values of\nsome quantity of interest, given relevant observed data, is called Bayesian inference , orposterior\ninference . It can also just be called probabilistic inference . Below we give some simple examples of Bayesian inference in action. We will see many more\ninteresting examples later in this book.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 154, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 914}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0155_353a278d", "text": "It can also just be called probabilistic inference . Below we give some simple examples of Bayesian inference in action. We will see many more\ninteresting examples later in this book. 2.3.1 Example: Testing for COVID-19\nSuppose you think you may have contracted COVID-19 , which is an infectious disease caused by\ntheSARS-CoV-2 virus. You decide to take a diagnostic test, and you want to use its result to\ndetermine if you are infected or not. LetH= 1be the event that you are infected, and H= 0be the event you are not infected. Let\nY= 1if the test is positive, and Y= 0if the test is negative. We want to compute p(H=hjY=y),\nforh2f0;1g, whereyis the observed test outcome. (We will write the distribution of values,\n[p(H= 0jY=y);p(H= 1jY=y)]asp(Hjy), for brevity.) We can think of this as a form of binary\nclassiﬁcation , whereHis the unknown class label, and yis the feature vector. First we must specify the likelihood. This quantity obviously depends on how reliable the\ntest is.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 155, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 985}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0156_541e1189", "text": "First we must specify the likelihood. This quantity obviously depends on how reliable the\ntest is. There are two key parameters. The sensitivity (akatrue positive rate ) is deﬁned as\np(Y= 1jH= 1), i.e., the probability of a positive test given that the truth is positive. The false\nnegative rate is deﬁned as one minus the sensitivity. The speciﬁcity (akatrue negative rate )\nis deﬁned as p(Y= 0jH= 0), i.e., the probability of a negative test given that the truth is negative. Thefalse positive rate is deﬁned as one minus the speciﬁcity. We summarize all these quantities\nin Table 2.1. (See Section 5.1.3.1 for more details.) Following https://nyti.ms/31MTZgV , we set\nthe sensitivity to 87.5% and the speciﬁcity to 97.5%. Next we must specify the prior. The quantity p(H= 1)represents the prevalence of the\ndisease in the area in which you live. We set this to p(H= 1) = 0:1(i.e., 10%), which was the\nprevalence in New York City in Spring 2020.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 156, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 947}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0157_0482e7d9", "text": "The quantity p(H= 1)represents the prevalence of the\ndisease in the area in which you live. We set this to p(H= 1) = 0:1(i.e., 10%), which was the\nprevalence in New York City in Spring 2020. (This example was chosen to match the numbers in\nhttps://nyti.ms/31MTZgV .)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.3. Bayes’ rule 45\nNow suppose you test positive. We have\np(H= 1jY= 1) =p(Y= 1jH= 1)p(H= 1)\np(Y= 1jH= 1)p(H= 1) +p(Y= 1jH= 0)p(H= 0)(2.55)\n=TPR\u0002prior\nTPR\u0002prior +FPR\u0002(1\u0000prior )(2.56)\n=0:875\u00020:1\n0:875\u00020:1 + 0:025\u00020:9= 0:795 (2.57)\nSo there is a 79.5% chance you are infected. Now suppose you test negative. The probability you are infected is given by\np(H= 1jY= 0) =p(Y= 0jH= 1)p(H= 1)\np(Y= 0jH= 1)p(H= 1) +p(Y= 0jH= 0)p(H= 0)(2.58)\n=FNR\u0002prior\nFNR\u0002prior +TNR\u0002(1\u0000prior )(2.59)\n=0:125\u00020:1\n0:125\u00020:1 + 0:975\u00020:9= 0:014 (2.60)\nSo there is just a 1.4% chance you are infected. Nowadays COVID-19 prevalence is much lower.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 157, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 957}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0158_5a3cc8f9", "text": "Nowadays COVID-19 prevalence is much lower. Suppose we repeat these calculations using a base\nrate of 1%; now the posteriors reduce to 26% and 0.13% respectively. The fact that you only have a 26% chance of being infected with COVID-19, even after a positive\ntest, is very counter-intuitive. The reason is that a single positive test is more likely to be a false\npositive than due to the disease, since the disease is rare. To see this, suppose we have a population\nof 100,000 people, of whom 1000 are infected. Of those who are infected, 875 = 0:875\u00021000test\npositive, and of those who are uninfected, 2475 = 0:025\u000299;000test positive. Thus the total number\nof positives is 3350 = 875 + 2475 , so the posterior probability of being infected given a positive test\nis875=3350 = 0:26. Of course, the above calculations assume we know the sensitivity and speciﬁcity of the test. See\n[GC20] for how to apply Bayes rule for diagnostic testing when there is uncertainty about these\nparameters.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 158, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 987}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0159_5bc0ab8f", "text": "See\n[GC20] for how to apply Bayes rule for diagnostic testing when there is uncertainty about these\nparameters. 2.3.2 Example: The Monty Hall problem\nIn this section, we consider a more “frivolous” application of Bayes rule. In particular, we apply it to\nthe famous Monty Hall problem . Imagine a game show with the following rules: There are three doors, labeled 1, 2, 3. A single prize\n(e.g., a car) has been hidden behind one of them. You get to select one door. Then the gameshow\nhost opens one of the other two doors (not the one you picked), in such a way as to not reveal the\nprize location. At this point, you will be given a fresh choice of door: you can either stick with your\nﬁrst choice, or you can switch to the other closed door. All the doors will then be opened and you\nwill receive whatever is behind your ﬁnal choice of door. For example, suppose you choose door 1, and the gameshow host opens door 3, revealing nothing\nbehind the door, as promised.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 159, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 967}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0160_277a0d20", "text": "For example, suppose you choose door 1, and the gameshow host opens door 3, revealing nothing\nbehind the door, as promised. Should you (a) stick with door 1, or (b) switch to door 2, or (c) does\nit make no diﬀerence? Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n46 Chapter 2. Probability: Univariate Models\nDoor 1 Door 2 Door 3 Switch Stay\nCar - - Lose Win\n- Car - Win Lose\n- - Car Win Lose\nTable 2.2: 3 possible states for the Monty Hall game, showing that switching doors is two times better (on\naverage) than staying with your original choice. Adapted from Table 6.1 of [PM18]. Intuitively, it seems it should make no diﬀerence, since your initial choice of door cannot inﬂuence\nthe location of the prize. However, the fact that the host opened door 3 tells us something about the\nlocation of the prize, since he made his choice conditioned on the knowledge of the true location and\non your choice. As we show below, you are in fact twice as likely to win the prize if you switch to\ndoor 2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 160, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1008}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0161_d346532a", "text": "As we show below, you are in fact twice as likely to win the prize if you switch to\ndoor 2. To show this, we will use Bayes’ rule. Let Hidenote the hypothesis that the prize is behind door i. We make the following assumptions: the three hypotheses H1,H2andH3are equiprobable a priori,\ni.e.,\nP(H1) =P(H2) =P(H3) =1\n3: (2.61)\nThe datum we receive, after choosing door 1, is either Y= 3andY= 2(meaning door 3 or 2 is\nopened, respectively). We assume that these two possible outcomes have the following probabilities. If the prize is behind door 1, then the host selects at random between Y= 2andY= 3. Otherwise\nthe choice of the host is forced and the probabilities are 0 and 1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 161, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 675}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0162_cddcbdde", "text": "If the prize is behind door 1, then the host selects at random between Y= 2andY= 3. Otherwise\nthe choice of the host is forced and the probabilities are 0 and 1. P(Y= 2jH1) =1\n2P(Y= 2jH2) = 0P(Y= 2jH3) = 1\nP(Y= 3jH1) =1\n2P(Y= 3jH2) = 1P(Y= 3jH3) = 0(2.62)\nNow, using Bayes’ theorem, we evaluate the posterior probabilities of the hypotheses:\nP(HijY= 3) =P(Y= 3jHi)P(Hi)\nP(Y= 3)(2.63)\nP(H1jY= 3) =(1=2)(1=3)\nP(Y=3)P(H2jY= 3) =(1)(1=3)\nP(Y=3)P(H3jY= 3) =(0)(1=3)\nP(Y=3)(2.64)\nThe denominator P(Y= 3)isP(Y= 3) =1\n6+1\n3=1\n2. So\nP(H1jY= 3) =1\n3P(H2jY= 3) =2\n3P(H3jY= 3) = 0 : (2.65)\nSo the contestant should switch to door 2 in order to have the biggest chance of getting the prize. See Table 2.2 for a worked example. Many people ﬁnd this outcome surprising. One way to make it more intuitive is to perform a\nthought experiment in which the game is played with a million doors.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 162, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 873}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0163_903cf8fc", "text": "See Table 2.2 for a worked example. Many people ﬁnd this outcome surprising. One way to make it more intuitive is to perform a\nthought experiment in which the game is played with a million doors. The rules are now that the\ncontestant chooses one door, then the game show host opens 999,998 doors in such a way as not to\nreveal the prize, leaving the contestant’s selected door and one other door closed. The contestant\nmay now stick or switch. Imagine the contestant confronted by a million doors, of which doors 1 and\n234,598 have not been opened, door 1 having been the contestant’s initial guess. Where do you think\nthe prize is? Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.4. Bernoulli and binomial distributions 47\nFigure 2.8: Any planar line-drawing is geometrically consistent with inﬁnitely many 3-D structures. From\nFigure 11 of [SA93]. Used with kind permission of Pawan Sinha.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 163, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 921}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0164_00c988c4", "text": "From\nFigure 11 of [SA93]. Used with kind permission of Pawan Sinha. 2.3.3 Inverse problems *\nProbability theory is concerned with predicting a distribution over outcomes ygiven knowledge (or\nassumptions) about the state of the world, h. By contrast, inverse probability is concerned with\ninferring the state of the world from observations of outcomes. We can think of this as inverting the\nh!ymapping. For example, consider trying to infer a 3d shape hfrom a 2d image y, which is a classic problem\ninvisual scene understanding . Unfortunately, this is a fundamentally ill-posed problem, as\nillustrated in Figure 2.8, since there are multiple possible hidden h’s consistent with the same observed\ny(see e.g., [Piz01]). Similarly, we can view natural language understanding as an ill-posed\nproblem, in which the listener must infer the intention hfrom the (often ambiguous) words spoken\nby the speaker (see e.g., [Sab21]).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 164, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 920}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0165_206ed58c", "text": "To tackle such inverse problems , we can use Bayes’ rule to compute the posterior, p(hjy), which\ngives a distribution over possible states of the world. This requires specifying the forwards model ,\np(yjh), as well as a prior p(h), which can be used to rule out (or downweight) implausible world\nstates. We discuss this topic in more detail in the sequel to this book, [Mur22]. 2.4 Bernoulli and binomial distributions\nPerhaps the simplest probability distribution is the Bernoulli distribution , which can be used to\nmodel binary events, as we discuss below. 2.4.1 Deﬁnition\nConsider tossing a coin, where the probability of event that it lands heads is given by 0\u0014\u0012\u00141. LetY= 1denote this event, and let Y= 0denote the event that the coin lands tails. Thus we are\nassuming that p(Y= 1) =\u0012andp(Y= 0) = 1\u0000\u0012. This is called the Bernoulli distribution , and\ncan be written as follows\nY\u0018Ber(\u0012) (2.66)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n48 Chapter 2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 165, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 970}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0166_126b745f", "text": "This is called the Bernoulli distribution , and\ncan be written as follows\nY\u0018Ber(\u0012) (2.66)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n48 Chapter 2. Probability: Univariate Models\n0 1 2 3 4 5 6 7 8 9 1000.050.10.150.20.250.30.35θ=0.250\n(a)\n0 1 2 3 4 5 6 7 8 9 1000.050.10.150.20.250.30.350.4θ=0.900 (b)\nFigure 2.9: Illustration of the binomial distribution with N= 10and (a)\u0012= 0:25and (b)\u0012= 0:9. Generated\nby code at ﬁgures.probml.ai/book1/2.9. where the symbol \u0018means “is sampled from” or “is distributed as”, and Berrefers to Bernoulli. The\nprobability mass function (pmf) of this distribution is deﬁned as follows:\nBer(yj\u0012) =(\n1\u0000\u0012ify= 0\n\u0012ify= 1(2.67)\n(See Section 2.2.1 for details on pmf’s.) We can write this in a more concise manner as follows:\nBer(yj\u0012),\u0012y(1\u0000\u0012)1\u0000y(2.68)\nThe Bernoulli distribution is a special case of the binomial distribution . To explain this, suppose\nwe observe a set of NBernoulli trials, denoted yn\u0018Ber(\u0001j\u0012), forn= 1 :N. Concretely, think of\ntossing a coin Ntimes.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 166, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1008}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0167_b5654ef7", "text": "To explain this, suppose\nwe observe a set of NBernoulli trials, denoted yn\u0018Ber(\u0001j\u0012), forn= 1 :N. Concretely, think of\ntossing a coin Ntimes. Let us deﬁne sto be the total the number of heads, s,PN\nn=1I(yn= 1). The distribution of sis given by the binomial distribution:\nBin(sjN;\u0012),\u0012N\ns\u0013\n\u0012s(1\u0000\u0012)N\u0000s(2.69)\nwhere\u0012N\nk\u0013\n,N! (N\u0000k)!k!(2.70)\nis the number of ways to choose kitems from N(this is known as the binomial coeﬃcient , and is\npronounced “N choose k”). See Figure 2.9 for some examples of the binomial distribution. If N= 1,\nthe binomial distribution reduces to the Bernoulli distribution. 2.4.2 Sigmoid (logistic) function\nWhen we want to predict a binary variable y2f0;1ggiven some inputs x2X, we need to use a\nconditional probability distribution of the form\np(yjx;\u0012) = Ber(yjf(x;\u0012)) (2.71)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 167, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 876}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0168_15a4862d", "text": "August 27, 2021\n2.4. Bernoulli and binomial distributions 49\n4\n 3\n 2\n 1\n 0 1 2 3 40.00.20.40.60.81.0sigmoid function\n(a)\n−4−3−2−1 0 1 2 3 40.00.20.40.60.81.0Heaviside function (b)\nFigure 2.10: (a) The sigmoid (logistic) function \u001b(a) = (1 +e\u0000a)\u00001. (b) The Heaviside function I(a>0). Generated by code at ﬁgures.probml.ai/book1/2.10. \u001b(x),1\n1 +e\u0000x=ex\n1 +ex(2.72)\nd\ndx\u001b(x) =\u001b(x)(1\u0000\u001b(x)) (2.73)\n1\u0000\u001b(x) =\u001b(\u0000x) (2.74)\n\u001b\u00001(p) = log\u0012p\n1\u0000p\u0013\n,logit(p) (2.75)\n\u001b+(x),log(1 +ex),softplus (x) (2.76)\nd\ndx\u001b+(x) =\u001b(x) (2.77)\nTable 2.3: Some useful properties of the sigmoid (logistic) and related functions. Note that the logitfunction\nis the inverse of the sigmoid function, and has a domain of [0;1]. wheref(x;\u0012)is some function that predicts the mean parameter of the output distribution. We will\nconsider many diﬀerent kinds of function fin Part II–Part IV.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 168, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 846}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0169_936329e5", "text": "wheref(x;\u0012)is some function that predicts the mean parameter of the output distribution. We will\nconsider many diﬀerent kinds of function fin Part II–Part IV. To avoid the requirement that 0\u0014f(x;\u0012)\u00141, we can let fbe an unconstrained function, and\nuse the following model:\np(yjx;\u0012) = Ber(yj\u001b(f(x;\u0012))) (2.78)\nHere\u001b()is thesigmoid orlogistic function, deﬁned as follows:\n\u001b(a),1\n1 +e\u0000a(2.79)\nwherea=f(x;\u0012). The term “sigmoid” means S-shaped: see Figure 2.10a for a plot. We see that it\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n50 Chapter 2. Probability: Univariate Models\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nPetal width (cm)0.00.20.40.60.81.0Probability\nDecision boundaryIris-Virginica\nNot Iris-Virginica\nFigure 2.11: Logistic regression applied to a 1-dimensional, 2-class version of the Iris dataset. Generated by\ncode at ﬁgures.probml.ai/book1/2.11. Adapted from Figure 4.23 of [Gér19].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 169, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 896}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0170_ad738b8a", "text": "Generated by\ncode at ﬁgures.probml.ai/book1/2.11. Adapted from Figure 4.23 of [Gér19]. maps the whole real line to [0;1], which is necessary for the output to be interpreted as a probability\n(and hence a valid value for the Bernoulli parameter \u0012). The sigmoid function can be thought of as a\n“soft” version of the heaviside step function , deﬁned by\nH(a),I(a>0) (2.80)\nas shown in Figure 2.10b. Plugging the deﬁnition of the sigmoid function into Equation (2.78) we get\np(y= 1jx;\u0012) =1\n1 +e\u0000a=ea\n1 +ea=\u001b(a) (2.81)\np(y= 0jx;\u0012) = 1\u00001\n1 +e\u0000a=e\u0000a\n1 +e\u0000a=1\n1 +ea=\u001b(\u0000a) (2.82)\nThe quantity ais equal to the log odds ,log(p\n1\u0000p), wherep=p(y= 1jx;\u0012). To see this, note that\nlog\u0012p\n1\u0000p\u0013\n= log\u0012ea\n1 +ea1 +ea\n1\u0013\n= log(ea) =a (2.83)\nThelogistic function orsigmoid function maps the log-odds atop:\np=logistic (a) =\u001b(a),1\n1 +e\u0000a=ea\n1 +ea(2.84)\nThe inverse of this is called the logit function , and maps pto the log-odds a:\na=logit(p) =\u001b\u00001(p),log\u0012p\n1\u0000p\u0013\n(2.85)\nSee Table 2.3 for some useful properties of these functions.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 170, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1005}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0171_b558de77", "text": "2.4.3 Binary logistic regression\nIn this section, we use a conditional Bernoulli model, where we use a linear predictor of the form\nf(x;\u0012) =wTx. Thus the model has the form\np(yjx;\u0012) = Ber(yj\u001b(wTx+b)) (2.86)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.5. Categorical and multinomial distributions 51\nIn other words,\np(y= 1jx;\u0012) =\u001b(wTx+b) =1\n1 +e\u0000(wTx+b)(2.87)\nThis is called logistic regression . For example consider a 1-dimensional, 2-class version of the iris dataset, where the positive class is\n“Virginica” and the negative class is “not Virginica”, and the feature xwe use is the petal width. We\nﬁt a logistic regression model to this and show the results in Figure 2.11. The decision boundary\ncorresponds to the value x\u0003wherep(y= 1jx=x\u0003;\u0012) = 0:5. We see that, in this example, x\u0003\u00191:7. Asxmoves away from this boundary, the classiﬁer becomes more conﬁdent in its prediction about\nthe class label.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 171, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 935}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0172_11e15f22", "text": "We see that, in this example, x\u0003\u00191:7. Asxmoves away from this boundary, the classiﬁer becomes more conﬁdent in its prediction about\nthe class label. It should be clear from this example why it would be inappropriate to use linear regression for a\n(binary) classiﬁcation problem. In such a model, the probabilities would increase above 1 as we move\nfar enough to the right, and below 0 as we move far enough to the left. For more detail on logistic regression, see Chapter 10. 2.5 Categorical and multinomial distributions\nTo represent a distribution over a ﬁnite set of labels, y2f1;:::;Cg, we can use the categorical\ndistribution, which generalizes the Bernoulli to C > 2values. 2.5.1 Deﬁnition\nThe categorical distribution is a discrete probability distribution with one parameter per class:\nCat(yj\u0012),CY\nc=1\u0012I(y=c)\nc (2.88)\nIn other words, p(y=cj\u0012) =\u0012c. Note that the parameters are constrained so that 0\u0014\u0012c\u00141andPC\nc=1\u0012c= 1; thus there are only C\u00001independent parameters.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 172, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 973}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0173_276a5475", "text": "Note that the parameters are constrained so that 0\u0014\u0012c\u00141andPC\nc=1\u0012c= 1; thus there are only C\u00001independent parameters. We can write the categorical distribution in another way by converting the discrete variable yinto\naone-hot vector withCelements, all of which are 0 except for the entry corresponding to the class\nlabel. (The term “one-hot” arises from electrical engineering, where binary vectors are encoded as\nelectrical current on a set of wires, which can be active (“hot”) or not (“cold”).) For example, if C= 3,\nwe encode the classes 1, 2 and 3 as (1;0;0),(0;1;0), and (0;0;1). More generally, we can encode the\nclasses using unit vectors , whereecis all 0s except for dimension c. (This is also called a dummy\nencoding .) Using one-hot encodings, we can write the categorical distribution as follows:\nCat(yj\u0012),CY\nc=1\u0012yc\nc (2.89)\nThe categorical distribution is a special case of the multinomial distribution . To explain this,\nsuppose we observe Ncategorical trials, yn\u0018Cat(\u0001j\u0012), forn= 1 :N.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 173, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1000}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0174_2b46e9ea", "text": "To explain this,\nsuppose we observe Ncategorical trials, yn\u0018Cat(\u0001j\u0012), forn= 1 :N. Concretely, think of rolling a\nC-sided diceNtimes. Let us deﬁne sto be a vector that counts the number of times each face shows\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n52 Chapter 2. Probability: Univariate Models\n0.00.20.40.60.81.0T = 100\n0.00.20.40.60.81.0T = 2\n0.00.20.40.60.81.0T = 1\nFigure 2.12: Softmax distribution S(a=T), wherea= (3;0;1), at temperatures of T= 100,T= 2and\nT= 1. When the temperature is high (left), the distribution is uniform, whereas when the temperature is\nlow (right), the distribution is “spiky”, with most of its mass on the largest element. Generated by code at\nﬁgures.probml.ai/book1/2.12. up, i.e.,sc,PN\nn=1I(yn=c). (Equivalently, if we use one-hot encodings, we have s=P\nnyn.) The\ndistribution of sis given by the multinomial distribution :\nMu(sjN;\u0012),\u0012N\ns1:::sC\u0013CY\nc=1\u0012sc\nc (2.90)\nwhere\u0012cis the probability that side cshows up, and\n\u0012N\ns1:::sC\u0013\n,N!", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 174, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 983}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0175_87f847e7", "text": "s1!s2!\u0001\u0001\u0001sC!(2.91)\nis themultinomial coeﬃcient , which is the number of ways to divide a set of size N=PC\nc=1sc\ninto subsets with sizes s1up tosC. IfN= 1, the multinomial distribution becomes the categorical\ndistribution. 2.5.2 Softmax function\nIn the conditional case, we can deﬁne\np(yjx;\u0012) = Cat(yjf(x;\u0012)) (2.92)\nwhich we can also write as\np(yjx;\u0012) = Mu(yj1;f(x;\u0012)) (2.93)\nWe require that 0\u0014fc(x;\u0012)\u00141andPC\nc=1fc(x;\u0012) = 1. To avoid the requirement that fdirectly predict a probability vector, it is common to pass the\noutput from finto thesoftmax function [Bri90], also called the multinomial logit . This is deﬁned\nas follows:\nS(a),\"\nea1\nPC\nc0=1eac0;:::;eaC\nPC\nc0=1eac0#\n(2.94)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 175, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 760}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0176_66f4c83a", "text": "This is deﬁned\nas follows:\nS(a),\"\nea1\nPC\nc0=1eac0;:::;eaC\nPC\nc0=1eac0#\n(2.94)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.5. Categorical and multinomial distributions 53\n1 2 3 4 5 6 7\nPetal length1\n01234Petal width\n0.150\n0.3000.450\n0.600\n0.7500.900Iris-Virginica\nIris-Versicolor\nIris-Setosa\nFigure 2.13: Logistic regression on the 3-class, 2-feature version of the Iris dataset. Adapted from Figure of\n4.25 [Gér19]. Generated by code at ﬁgures.probml.ai/book1/2.13. This maps RCto[0;1]C, and satisﬁes the constraints that 0\u0014S(a)c\u00141andPC\nc=1S(a)c= 1. The\ninputs to the softmax, a=f(x;\u0012), are called logits, and are a generalization of the log odds. The softmax function is so-called since it acts a bit like the argmax function.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 176, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 761}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0177_caa09cae", "text": "The\ninputs to the softmax, a=f(x;\u0012), are called logits, and are a generalization of the log odds. The softmax function is so-called since it acts a bit like the argmax function. To see this, let us\ndivide each acby a constant Tcalled the temperature .6Then asT!0, we ﬁnd\nS(a=T)c=\u001a1:0ifc= argmaxc0ac0\n0:0otherwise(2.95)\nIn other words, at low temperatures, the distribution puts most of its probability mass in the most\nprobable state (this is called winner takes all ), whereas at high temperatures, it spreads the mass\nuniformly. See Figure 2.12 for an illustration. 2.5.3 Multiclass logistic regression\nIf we use a linear predictor of the form f(x;\u0012) =Wx+b, where Wis aC\u0002Dmatrix, andbis a\nC-dimensional bias vector, the ﬁnal model becomes\np(yjx;\u0012) = Cat(yjS(Wx+b)) (2.96)\nLeta=Wx+bbe theC-dimensional vector of logits. Then we can rewrite the above as follows:\np(y=cjx;\u0012) =eac\nPC\nc0=1eac0(2.97)\nThis is known as multinomial logistic regression .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 177, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 947}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0178_bdeecf5e", "text": "Then we can rewrite the above as follows:\np(y=cjx;\u0012) =eac\nPC\nc0=1eac0(2.97)\nThis is known as multinomial logistic regression . If we have just two classes, this reduces to binary logistic regression. To see this, note that\nS(a)0=ea0\nea0+ea1=1\n1 +ea1\u0000a0=\u001b(a0\u0000a1) (2.98)\nso we can just train the model to predict a=a1\u0000a0. This can be done with a single weight vector\nw; if we use the multi-class formulation, we will have two weight vectors, w0andw1. Such a model\nisover-parameterized , which can hurt interpretability, but the predictions will be the same. 6. This terminology comes from the area of statistical physics. The Boltzmann distribution is a distribution over\nstates which has the same form as the softmax function. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n54 Chapter 2. Probability: Univariate Models\nWe discuss this in more detail in Section 10.3. For now, we just give an example.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 178, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 912}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0179_4d690baf", "text": "Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n54 Chapter 2. Probability: Univariate Models\nWe discuss this in more detail in Section 10.3. For now, we just give an example. Figure 2.13\nshows what happens when we ﬁt this model to the 3-class iris dataset, using just 2 features. We see\nthat the decision boundaries between each class are linear. We can create nonlinear boundaries by\ntransforming the features (e.g., using polynomials), as we discuss in Section 10.3.1. 2.5.4 Log-sum-exp trick\nIn this section, we discuss one important practical detail to pay attention to when working with\nthe softmax distribution. Suppose we want to compute the normalized probability pc=p(y=cjx),\nwhich is given by\npc=eac\nZ(a)=eac\nPC\nc0=1eac0(2.99)\nwherea=f(x;\u0012)are the logits. We might encounter numerical problems when computing the\npartitionfunction Z. For example, suppose we have 3 classes, with logits a= (0;1;0). Then we ﬁnd\nZ=e0+e1+e0= 4:71.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 179, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 949}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0180_1abda0b2", "text": "We might encounter numerical problems when computing the\npartitionfunction Z. For example, suppose we have 3 classes, with logits a= (0;1;0). Then we ﬁnd\nZ=e0+e1+e0= 4:71. But now suppose a= (1000;1001;1000); we ﬁndZ=1, since on a computer,\neven using 64 bit precision, np.exp(1000)=inf . Similarly, suppose a= (\u00001000;\u0000999;\u00001000); now\nwe ﬁndZ= 0, since np.exp(-1000)=0 . To avoid numerical problems, we can use the following\nidentity:\nlogCX\nc=1exp(ac) =m+ logCX\nc=1exp(ac\u0000m) (2.100)\nThis holds for any m. It is common to use m=maxcacwhich ensures that the largest value you\nexponentiate will be zero, so you will deﬁnitely not overﬂow, and even if you underﬂow, the answer\nwill be sensible. This is known as the log-sum-exp trick . We use this trick when implementing the\nlsefunction:\nlse(a),logCX\nc=1exp(ac) (2.101)\nWe can use this to compute the probabilities from the logits:\np(y=cjx) = exp(ac\u0000lse(a)) (2.102)\nWe can then pass this to the cross-entropy loss, deﬁned in Equation (5.41).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 180, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 988}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0181_d24d788f", "text": "However, to save computational eﬀort, and for numerical stability, it is quite common to modify\nthe cross-entropy loss so that it takes the logits aas inputs, instead of the probability vector p. For\nexample, consider the binary case. The CE loss for one example is\nL=\u0000[I(y= 0) logp0+I(y= 1) logp1] (2.103)\nwhere\nlogp1= log\u00121\n1 + exp(\u0000a)\u0013\n= log(1)\u0000log(1 + exp(\u0000a)) = 0\u0000lse([0;\u0000a]) (2.104)\nlogp0= 0\u0000lse([0;+a]) (2.105)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.6. Univariate Gaussian (normal) distribution 55\n2.6 Univariate Gaussian (normal) distribution\nThe most widely used distribution of real-valued random variables y2Ris theGaussian distribu-\ntion, also called the normal distribution (see Section 2.6.4 for a discussion of these names).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 181, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 778}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0182_60e4a626", "text": "2.6.1 Cumulative distribution function\nWe deﬁne the cumulative distribution function orcdfof a continuous random variable Yas\nfollows:\nP(y),Pr(Y\u0014y) (2.106)\n(Note that we use a capital Pto represent the cdf.) Using this, we can compute the probability of\nbeing in any interval as follows:\nPr(a<Y\u0014b) =P(b)\u0000P(a) (2.107)\nCdf’s are monotonically non-decreasing functions. The cdf of the Gaussian is deﬁned by\n\b(y;\u0016;\u001b2),Zy\n\u00001N(zj\u0016;\u001b2)dz=1\n2[1 +erf(z=p\n2)] (2.108)\nwherez= (y\u0000\u0016)=\u001band erf (u)is theerror function , deﬁned as\nerf(u),2p\u0019Zu\n0e\u0000t2dt (2.109)\nThis latter function is built in to most software packages. See Figure 2.2a for a plot. The parameter \u0016encodes the mean of the distribution, which is the same as the mode, since the\ndistribution is unimodal. The parameter \u001b2encodes the variance. (Sometimes we talk about the\nprecision of a Gaussian, which is the inverse variance, denoted \u0015= 1=\u001b2.) When\u0016= 0and\u001b= 1,\nthe Gaussian is called the standard normal distribution.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 182, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 968}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0183_9b658977", "text": "(Sometimes we talk about the\nprecision of a Gaussian, which is the inverse variance, denoted \u0015= 1=\u001b2.) When\u0016= 0and\u001b= 1,\nthe Gaussian is called the standard normal distribution. IfPis the cdf of Y, thenP\u00001(q)is the value yqsuch thatp(Y\u0014yq) =q; this is called the q’th\nquantile ofP. The value P\u00001(0:5)is themedian of the distribution, with half of the probability\nmass on the left, and half on the right. The values P\u00001(0:25)andP\u00001(0:75)are the lower and upper\nquartiles . For example, let \bbe the cdf of the Gaussian distribution N(0;1), and \b\u00001be the inverse cdf (also\nknown as the probit function ). Then points to the left of \b\u00001(\u000b=2)contain\u000b=2of the probability\nmass, as illustrated in Figure 2.2b. By symmetry, points to the right of \b\u00001(1\u0000\u000b=2)also contain\n\u000b=2of the mass. Hence the central interval (\b\u00001(\u000b=2);\b\u00001(1\u0000\u000b=2))contains 1\u0000\u000bof the mass.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 183, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 849}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0184_958f6caf", "text": "By symmetry, points to the right of \b\u00001(1\u0000\u000b=2)also contain\n\u000b=2of the mass. Hence the central interval (\b\u00001(\u000b=2);\b\u00001(1\u0000\u000b=2))contains 1\u0000\u000bof the mass. If\nwe set\u000b= 0:05, the central 95% interval is covered by the range\n(\b\u00001(0:025);\b\u00001(0:975)) = (\u00001:96;1:96) (2.110)\nIf the distribution is N(\u0016;\u001b2), then the 95% interval becomes (\u0016\u00001:96\u001b;\u0016+ 1:96\u001b). This is often\napproximated by writing \u0016\u00062\u001b. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n56 Chapter 2. Probability: Univariate Models\n2.6.2 Probability density function\nWe deﬁne the probability density function orpdfas the derivative of the cdf:\np(y),d\ndyP(y) (2.111)\nThe pdf of the Gaussian is given by\nN(yj\u0016;\u001b2),1p\n2\u0019\u001b2e\u00001\n2\u001b2(y\u0000\u0016)2(2.112)\nwherep\n2\u0019\u001b2is the normalization constant needed to ensure the density integrates to 1 (see\nExercise 2.12). See Figure 2.2b for a plot.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 184, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 835}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0185_cb43f804", "text": "See Figure 2.2b for a plot. Given a pdf, we can compute the probability of a continuous variable being in a ﬁnite interval as\nfollows:\nPr(a<Y\u0014b) =Zb\nap(y)dy=P(b)\u0000P(a) (2.113)\nAs the size of the interval gets smaller, we can write\nPr(y\u0014Y\u0014y+dy)\u0019p(y)dy (2.114)\nIntuitively, this says the probability of Ybeing in a small interval around yis the density at ytimes\nthe width of the interval. One important consequence of the above result is that the pdf at a point\ncan be larger than 1. For example, N(0j0;0:1) = 3:99. We can use the pdf to compute the mean, orexpected value , of the distribution:\nE[Y],Z\nYyp(y)dy (2.115)\nFor a Gaussian, we have the familiar result that E\u0002\nN(\u0001j\u0016;\u001b2)\u0003\n=\u0016. (Note, however, that for some\ndistributions, this integral is not ﬁnite, so the mean is not deﬁned.)\nWe can also use the pdf to compute the variance of a distribution. This is a measure of the\n“spread”, and is often denoted by \u001b2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 185, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 915}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0186_710a0b3e", "text": "This is a measure of the\n“spread”, and is often denoted by \u001b2. The variance is deﬁned as follows:\nV[Y],E\u0002\n(Y\u0000\u0016)2\u0003\n=Z\n(y\u0000\u0016)2p(y)dy (2.116)\n=Z\ny2p(y)dy+\u00162Z\np(y)dy\u00002\u0016Z\nyp(y)dy=E\u0002\nY2\u0003\n\u0000\u00162(2.117)\nfrom which we derive the useful result\nE\u0002\nY2\u0003\n=\u001b2+\u00162(2.118)\nThestandard deviation is deﬁned as\nstd [Y],p\nV[Y] =\u001b (2.119)\n(The standard deviation can be more intepretable than the variance since it has the same units as Y\nitself.) For a Gaussian, we have the familiar result that std\u0002\nN(\u0001j\u0016;\u001b2)\u0003\n=\u001b. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.6. Univariate Gaussian (normal) distribution 57\n20\n 10\n 0 10 20 30 40 50 6005101520\n(a)\n20\n 10\n 0 10 20 30 40 50 6005101520\n (b)\nFigure 2.14: Linear regression using Gaussian output with mean \u0016(x) =b+wxand (a) ﬁxed variance\n\u001b2(homoskedastic) or (b) input-dependent variance \u001b(x)2(heteroscedastic). Generated by code at ﬁg-\nures.probml.ai/book1/2.14.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 186, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 917}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0187_9bc25b05", "text": "Generated by code at ﬁg-\nures.probml.ai/book1/2.14. 2.6.3 Regression\nSo far we have been considering the unconditional Gaussian distribution. In some cases, it is helpful\nto make the parameters of the Gaussian be functions of some input variables, i.e., we want to create\na conditional density model of the form\np(yjx;\u0012) =N(yjf\u0016(x;\u0012);f\u001b(x;\u0012)2) (2.120)\nwheref\u0016(x;\u0012)2Rpredicts the mean, and f\u001b(x;\u0012)2R+predicts the variance. It is common to assume that the variance is ﬁxed, and is independent of the input. This is called\nhomoscedastic regression . Furthermore it is common to assume the mean is a linear function of\nthe input. The resulting model is called linear regression :\np(yjx;\u0012) =N(yjwTx+b;\u001b2) (2.121)\nwhere\u0012= (w;b;\u001b2). See Figure 2.14(a) for an illustration of this model in 1d. and Section 11.2 for\nmore details on this model. However, we can also make the variance depend on the input; this is called heteroskedastic\nregression .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 187, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 938}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0188_2d5f9049", "text": "and Section 11.2 for\nmore details on this model. However, we can also make the variance depend on the input; this is called heteroskedastic\nregression . In the linear regression setting, we have\np(yjx;\u0012) =N(yjwT\n\u0016x+b;\u001b+(wT\n\u001bx)) (2.122)\nwhere\u0012= (w\u0016;w\u001b)are the two forms of regression weights, and\n\u001b+(a) = log(1 + ea) (2.123)\nis thesoftplus function, that maps from RtoR+, to ensure the predicted standard deviation is\nnon-negative. See Figure 2.14(b) for an illustration of this model in 1d. Note that Figure 2.14 plots the 95% predictive interval, [\u0016(x)\u00002\u001b(x);\u0016(x) + 2\u001b(x)]. This is the\nuncertainty in the predicted observation ygivenx, and captures the variability in the blue dots. By contrast, the uncertainty in the underlying (noise-free) function is represented byp\nV[f\u0016(x;\u0012)],\nwhich does not involve the \u001bterm; now the uncertainty is over the parameters \u0012, rather than the\noutputy. See Section 11.7 for details on how to model parameter uncertainty. Author: Kevin P. Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 188, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 996}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0189_ab67dfb6", "text": "See Section 11.7 for details on how to model parameter uncertainty. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n58 Chapter 2. Probability: Univariate Models\n2.6.4 Why is the Gaussian distribution so widely used? The Gaussian distribution is the most widely used distribution in statistics and machine learning. There are several reasons for this. First, it has two parameters which are easy to interpret, and which\ncapture some of the most basic properties of a distribution, namely its mean and variance. Second,\nthe central limit theorem (Section 2.8.6) tells us that sums of independent random variables have an\napproximately Gaussian distribution, making it a good choice for modeling residual errors or “noise”. Third, the Gaussian distribution makes the least number of assumptions (has maximum entropy),\nsubject to the constraint of having a speciﬁed mean and variance, as we show in Section 3.4.4; this\nmakes it a good default choice in many cases.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 189, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 972}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0190_68400225", "text": "Finally, it has a simple mathematical form, which\nresults in easy to implement, but often highly eﬀective, methods, as we will see in Section 3.2. From a historical perspective, it’s worth remarking that the term “Gaussian distribution” is a bit\nmisleading, since, as Jaynes [Jay03, p241] notes: “The fundamental nature of this distribution and\nits main properties were noted by Laplace when Gauss was six years old; and the distribution itself\nhad been found by de Moivre before Laplace was born”. However, Gauss popularized the use of the\ndistribution in the 1800s, and the term “Gaussian” is now widely used in science and engineering. The name “normal distribution” seems to have arisen in connection with the normal equations\nin linear regression (see Section 11.2.2.2).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 190, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 775}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0191_61bdf0ef", "text": "The name “normal distribution” seems to have arisen in connection with the normal equations\nin linear regression (see Section 11.2.2.2). However, we prefer to avoid the term “normal”, since it\nsuggests other distributions are “abnormal”, whereas, as Jaynes [Jay03] points out, it is the Gaussian\nthat is abnormal in the sense that it has many special properties that are untypical of general\ndistributions. 2.6.5 Dirac delta function as a limiting case\nAs the variance of a Gaussian goes to 0, the distribution approaches an inﬁnitely narrow, but inﬁnitely\ntall, “spike” at the mean. We can write this as follows:\nlim\n\u001b!0N(yj\u0016;\u001b2)!\u000e(y\u0000\u0016) (2.124)\nwhere\u000eis theDirac delta function , deﬁned by\n\u000e(x) =(\n+1ifx= 0\n0ifx6= 0(2.125)\nwhere\nZ1\n\u00001\u000e(x)dx= 1 (2.126)\nA slight variant of this is to deﬁne\n\u000ey(x) =(\n+1ifx=y\n0ifx6=y(2.127)\nNote that we have\n\u000ey(x) =\u000e(x\u0000y) (2.128)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.7.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 191, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 942}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0192_89adef18", "text": "August 27, 2021\n2.7. Some other common univariate distributions * 59\n4\n 3\n 2\n 1\n 0 1 2 3 40.00.10.20.30.40.50.60.7pdfGauss\nStudent(dof 1)\nStudent (dof 2)\nLaplace\n(a)\n4\n 3\n 2\n 1\n 0 1 2 3 48\n6\n4\n2\n0log pdf\nGauss\nStudent(dof 1)\nStudent (dof 2)\nLaplace (b)\nFigure 2.15: (a) The pdf’s for a N(0;1),T(\u0016= 0;\u001b= 1;\u0017= 1),T(\u0016= 0;\u001b= 1;\u0017= 2), and Lap(0;1=p\n2). The mean is 0 and the variance is 1 for both the Gaussian and Laplace. When \u0017= 1, the Student is the same\nas the Cauchy, which does not have a well-deﬁned mean and variance. (b) Log of these pdf’s. Note that the\nStudent distribution is not log-concave for any parameter value, unlike the Laplace distribution. Nevertheless,\nboth are unimodal. Generated by code at ﬁgures.probml.ai/book1/2.15.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 192, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 740}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0193_13d61e2c", "text": "Note that the\nStudent distribution is not log-concave for any parameter value, unlike the Laplace distribution. Nevertheless,\nboth are unimodal. Generated by code at ﬁgures.probml.ai/book1/2.15. The delta function distribution satisﬁes the following sifting property , which we will use later on:\nZ1\n\u00001f(y)\u000e(x\u0000y)dy=f(x) (2.129)\n2.7 Some other common univariate distributions *\nIn this section, we brieﬂy introduce some other univariate distributions that we will use in this book. 2.7.1 Student tdistribution\nThe Gaussian distribution is quite sensitive to outliers. Arobustalternative to the Gaussian is\ntheStudentt-distribution , which we shall call the Student distribution for short.7Its pdf is\nas follows:\nT(yj\u0016;\u001b2;\u0017)/\"\n1 +1\n\u0017\u0012y\u0000\u0016\n\u001b\u00132#\u0000(\u0017+1\n2)\n(2.130)\nwhere\u0016is the mean, \u001b>0is the scale parameter (not the standard deviation), and \u0017 >0is called\nthedegrees of freedom (although a better term would be the degree of normality [Kru13], since\nlarge values of \u0017make the distribution act like a Gaussian).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 193, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1004}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0194_9754bdc5", "text": "7. This distribution has a colorful etymology. It was ﬁrst published in 1908 by William Sealy Gosset, who worked at\nthe Guinness brewery in Dublin, Ireland. Since his employer would not allow him to use his own name, he called it the\n“Student” distribution. The origin of the term tseems to have arisen in the context of tables of the Student distribution,\nused by Fisher when developing the basis of classical statistical inference. See http://jeff560.tripod.com/s.html for\nmore historical details. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n60 Chapter 2. Probability: Univariate Models\n-5 0 5 1000.10.20.30.40.5\ngaussian\nstudent T\nlaplace\n(a)\n-5 0 5 1000.10.20.30.40.5\ngaussian\nstudent T\nlaplace (b)\nFigure 2.16: Illustration of the eﬀect of outliers on ﬁtting Gaussian, Student and Laplace distributions. (a)\nNo outliers (the Gaussian and Student curves are on top of each other). (b) With outliers.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 194, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 919}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0195_cd42d850", "text": "(a)\nNo outliers (the Gaussian and Student curves are on top of each other). (b) With outliers. We see that the\nGaussian is more aﬀected by outliers than the Student and Laplace distributions. Adapted from Figure 2.16\nof [Bis06]. Generated by code at ﬁgures.probml.ai/book1/2.16. We see that the probability density decays as a polynomial function of the squared distance from\nthe center, as opposed to an exponential function, so there is more probability mass in the tail than\nwith a Gaussian distribution, as shown in Figure 2.15. We say that the Student distribution has\nheavy tails , which makes it robust to outliers. To illustrate the robustness of the Student distribution, consider Figure 2.16. On the left, we show\na Gaussian and a Student distribution ﬁt to some data with no outliers. On the right, we add some\noutliers. We see that the Gaussian is aﬀected a lot, whereas the Student hardly changes. We discuss\nhow to use the Student distribution for robust linear regression in Section 11.6.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 195, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1005}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0196_b9de73e8", "text": "We see that the Gaussian is aﬀected a lot, whereas the Student hardly changes. We discuss\nhow to use the Student distribution for robust linear regression in Section 11.6.2. For later reference, we note that the Student distribution has the following properties:\nmean =\u0016;mode =\u0016;var=\u0017\u001b2\n(\u0017\u00002)(2.131)\nThe mean is only deﬁned if \u0017 >1. The variance is only deﬁned if \u0017 >2. For\u0017\u001d5, the Student\ndistribution rapidly approaches a Gaussian distribution and loses its robustness properties. It is\ncommon to use \u0017= 4, which gives good performance in a range of problems [LLT89]. 2.7.2 Cauchy distribution\nIf\u0017= 1, the Student distribution is known as the Cauchy orLorentz distribution. Its pdf is deﬁned\nby\nC(xj\u0016;\r) =1\n\r\u0019\"\n1 +\u0012x\u0000\u0016\n\r\u00132#\u00001\n(2.132)\nThis distribution has very heavy tails compared to a Gaussian. For example, 95% of the values from\na standard normal are between -1.96 and 1.96, but for a standard Cauchy they are between -12.7\nDraft of “Probabilistic Machine Learning: An Introduction”.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 196, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 989}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0197_2b149832", "text": "For example, 95% of the values from\na standard normal are between -1.96 and 1.96, but for a standard Cauchy they are between -12.7\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.7. Some other common univariate distributions * 61\nand 12.7. In fact the tails are so heavy that the integral that deﬁnes the mean does not converge. Thehalf Cauchy distribution is a version of the Cauchy (with \u0016= 0) that is “folded over” on itself,\nso all its probability density is on the positive reals. Thus it has the form\nC+(xj\r),2\n\u0019\r\"\n1 +\u0012x\n\r\u00132#\u00001\n(2.133)\nThis is useful in Bayesian modeling, where we want to use a distribution over positive reals with\nheavy tails, but ﬁnite density at the origin. 2.7.3 Laplace distribution\nAnother distribution with heavy tails is the Laplace distribution8, also known as the double\nsided exponential distribution. This has the following pdf:\nLap(yj\u0016;b),1\n2bexp\u0012\n\u0000jy\u0000\u0016j\nb\u0013\n(2.134)\nSee Figure 2.15 for a plot.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 197, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 961}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0198_55622986", "text": "This has the following pdf:\nLap(yj\u0016;b),1\n2bexp\u0012\n\u0000jy\u0000\u0016j\nb\u0013\n(2.134)\nSee Figure 2.15 for a plot. Here \u0016is a location parameter and b >0is a scale parameter. This\ndistribution has the following properties:\nmean =\u0016;mode =\u0016;var= 2b2(2.135)\nIn Section 11.6.1, we discuss how to use the Laplace distribution for robust linear regression, and\nin Section 11.4, we discuss how to use the Laplace distribution for sparse linear regression. 2.7.4 Beta distribution\nThebeta distribution has support over the interval [0;1]and is deﬁned as follows:\nBeta(xja;b) =1\nB(a;b)xa\u00001(1\u0000x)b\u00001(2.136)\nwhereB(a;b)is thebeta function , deﬁned by\nB(a;b),\u0000(a)\u0000(b)\n\u0000(a+b)(2.137)\nwhere \u0000(a)is the Gamma function deﬁned by\n\u0000(a),Z1\n0xa\u00001e\u0000xdx (2.138)\nSee Figure 2.17a for plots of some beta distributions. 8. Pierre-Simon Laplace (1749–1827) was a French mathematician, who played a key role in creating the ﬁeld of\nBayesian statistics. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n62 Chapter 2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 198, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 976}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0199_24615d40", "text": "8. Pierre-Simon Laplace (1749–1827) was a French mathematician, who played a key role in creating the ﬁeld of\nBayesian statistics. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n62 Chapter 2. Probability: Univariate Models\n0.0 0.2 0.4 0.6 0.8 1.00123456Beta distributions\na=0.1,b=0.1\na=0.1,b=1.0\na=1.0,b=1.0\na=2.0,b=2.0\na=2.0,b=8.0\n(a)\n0 1 2 3 4 5 6 70.000.250.500.751.001.251.501.752.00Gamma distributions\na=1.0,b=1.0\na=1.5,b=1.0\na=2.0,b=1.0\na=1.0,b=2.0\na=1.5,b=2.0\na=2.0,b=2.0 (b)\nFigure 2.17: (a) Some beta distributions. If a<1, we get a “spike” on the left, and if b<1, we get a “spike”\non the right. if a=b= 1, the distribution is uniform. If a >1andb >1, the distribution is unimodal. Generated by code at ﬁgures.probml.ai/book1/2.17. (b) Some gamma distributions. If a\u00141, the mode is at 0,\notherwise the mode is away from 0. As we increase the rate b, we reduce the horizontal scale, thus squeezing\neverything leftwards and upwards. Generated by code at ﬁgures.probml.ai/book1/2.17.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 199, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1003}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0200_73d76fa2", "text": "As we increase the rate b, we reduce the horizontal scale, thus squeezing\neverything leftwards and upwards. Generated by code at ﬁgures.probml.ai/book1/2.17. We require a;b > 0to ensure the distribution is integrable (i.e., to ensure B(a;b)exists). If\na=b= 1, we get the uniform distribution. If aandbare both less than 1, we get a bimodal\ndistribution with “spikes” at 0 and 1; if aandbare both greater than 1, the distribution is unimodal. For later reference, we note that the distribution has the following properties (Exercise 2.8):\nmean =a\na+b;mode =a\u00001\na+b\u00002;var=ab\n(a+b)2(a+b+ 1)(2.139)\n2.7.5 Gamma distribution\nThegamma distribution is a ﬂexible distribution for positive real valued rv’s,x>0.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 200, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 702}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0201_13e7fc15", "text": "It is deﬁned\nin terms of two parameters, called the shape a>0and the rate b>0:\nGa(xjshape =a;rate =b),ba\n\u0000(a)xa\u00001e\u0000xb(2.140)\nSometimes the distribution is parameterized in terms of the shape aand thescales= 1=b:\nGa(xjshape =a;scale =s),1\nsa\u0000(a)xa\u00001e\u0000x=s(2.141)\nSee Figure 2.17b for some plots of the gamma pdf. For reference, we note that the distribution has the following properties:\nmean =a\nb;mode =a\u00001\nb;var =a\nb2(2.142)\nThere are several distributions which are just special cases of the Gamma, which we discuss below. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.7. Some other common univariate distributions * 63\n•Exponential distribution . This is deﬁned by\nExpon(xj\u0015),Ga(xjshape = 1;rate=\u0015) (2.143)\nThis distribution describes the times between events in a Poisson process, i.e. a process in which\nevents occur continuously and independently at a constant average rate \u0015. •Chi-squared distribution .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 201, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 941}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0202_160c9f99", "text": "a process in which\nevents occur continuously and independently at a constant average rate \u0015. •Chi-squared distribution . This is deﬁned by\n\u001f2\n\u0017(x),Ga(xjshape =\u0017\n2;rate=1\n2) (2.144)\nwhere\u0017is called the degrees of freedom. This is the distribution of the sum of squared Gaussian\nrandom variables. More precisely, if Zi\u0018N(0;1), andS=P\u0017\ni=1Z2\ni, thenS\u0018\u001f2\n\u0017. •Theinverse Gamma distribution is deﬁned as follows:\nIG(xjshape =a;scale =b),ba\n\u0000(a)x\u0000(a+1)e\u0000b=x(2.145)\nThe distribution has these properties\nmean =b\na\u00001;mode =b\na+ 1;var=b2\n(a\u00001)2(a\u00002)(2.146)\nThe mean only exists if a >1. The variance only exists if a >2. Note: ifX\u0018Ga(shape =\na;rate=b), then 1=X\u0018IG(shape =a;scale =b). (Note that bplays two diﬀerent roles in this\ncase.)\n2.7.6 Empirical distribution\nSuppose we have a set of NsamplesD=fx(1);:::;x(N)g, derived from a distribution p(X), where\nX2R.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 202, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 852}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0203_57b2d4f1", "text": "(Note that bplays two diﬀerent roles in this\ncase.)\n2.7.6 Empirical distribution\nSuppose we have a set of NsamplesD=fx(1);:::;x(N)g, derived from a distribution p(X), where\nX2R. We can approximate the pdf using a set of delta functions (Section 2.6.5) or “spikes”, centered\non these samples:\n^pN(x) =1\nNNX\nn=1\u000ex(i)(x) (2.147)\nThis is called the empirical distribution of the datasetD. An example of this, with N= 5, is\nshown in Figure 2.18(a). The corresponding cdf is given by\n^PN(x) =1\nNNX\nn=1I\u0010\nx(i)\u0014x\u0011\n=1\nNNX\nn=1ux(i)(x) (2.148)\nwhereuy(x)is astep function atydeﬁned by\nuy(x) =(\n1ifx\u0015y\n0ifx<y(2.149)\nThis can be visualized as a “stair case”, as in Figure 2.18(b), where the jumps of height 1=Noccur at\nevery sample. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n64 Chapter 2. Probability: Univariate Models\n(a)\n (b)\nFigure 2.18: Illustration of the (a) empirical pdf and (b) empirical cdf derived from a set of N= 5samples. From https: // bit. ly/ 3hFgi0e .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 203, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 975}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0204_6878d6f7", "text": "Probability: Univariate Models\n(a)\n (b)\nFigure 2.18: Illustration of the (a) empirical pdf and (b) empirical cdf derived from a set of N= 5samples. From https: // bit. ly/ 3hFgi0e . Used with kind permission of Mauro Escudero. 2.8 Transformations of random variables *\nSupposex\u0018p()is some random variable, and y=f(x)is some deterministic transformation of it. In this section, we discuss how to compute p(y). 2.8.1 Discrete case\nIfXis a discrete rv, we can derive the pmf for Yby simply summing up the probability mass for all\nthex’s such that f(x) =y:\npy(y) =X\nx:f(x)=ypx(x) (2.150)\nFor example, if f(X) = 1ifXis even and f(X) = 0otherwise, and px(X)is uniform on the set\nf1;:::; 10g, thenpy(1) =P\nx2f2;4;6;8;10gpx(x) = 0:5, and hence py(0) = 0:5also. Note that in this\nexample,fis a many-to-one function. 2.8.2 Continuous case\nIfXis continuous, we cannot use Equation (2.150) since px(x)is a density, not a pmf, and we cannot\nsum up densities.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 204, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 945}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0205_8bc2abf0", "text": "Note that in this\nexample,fis a many-to-one function. 2.8.2 Continuous case\nIfXis continuous, we cannot use Equation (2.150) since px(x)is a density, not a pmf, and we cannot\nsum up densities. Instead, we work with cdf’s, as follows:\nPy(y),Pr(Y\u0014y) = Pr(f(X)\u0014y) = Pr(X2fxjf(x)\u0014yg) (2.151)\nIffis invertible, we can derive the pdf of yby diﬀerentiating the cdf, as we show below. If fis not\ninvertible, we can use numerical integration, or a Monte Carlo approximation. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.8. Transformations of random variables * 65\n2.8.3 Invertible transformations (bijections)\nIn this section, we consider the case of monotonic and hence invertible functions. (Note a function is\ninvertible iﬀ it is a bijector ). With this assumption, there is a simple formula for the pdf of y, as we\nwill see.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 205, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 852}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0206_05cc646f", "text": "(Note a function is\ninvertible iﬀ it is a bijector ). With this assumption, there is a simple formula for the pdf of y, as we\nwill see. (This can be generalized to invertible, but non-monotonic, functions, but we ignore this\ncase.)\n2.8.3.1 Change of variables: scalar case\nWe start with an example. Suppose x\u0018Unif(0;1), andy=f(x) = 2x+ 1. This function stretches\nand shifts the probability distribution, as shown in Figure 2.19(a). Now let us zoom in on a point x\nand another point that is inﬁnitesimally close, namely x+dx. We see this interval gets mapped to\n(y;y+dy). The probability mass in these intervals must be the same, hence p(x)dx=p(y)dy, and so\np(y) =p(x)dx=dy. However, since it does not matter (in terms of probability preservation) whether\ndx=dy> 0ordx=dy< 0, we get\npy(y) =px(x)jdx\ndyj (2.152)\nNow consider the general case for any px(x)and any monotonic function f:R!R. Letg=f\u00001,\nsoy=f(x)andx=g(y).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 206, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 915}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0207_a6c1fbd5", "text": "Letg=f\u00001,\nsoy=f(x)andx=g(y). If we assume that f:R!Ris monotonically increasing we get\nPy(y) = Pr(f(X)\u0014y) = Pr(X\u0014f\u00001(y)) =Px(f\u00001(y)) =Px(g(y)) (2.153)\nTaking derivatives we get\npy(y),d\ndyPy(y) =d\ndyPx(x) =dx\ndyd\ndxPx(x) =dx\ndypx(x) (2.154)\nWe can derive a similar expression (but with opposite signs) for the case where fis monotonically\ndecreasing. To handle the general case we take the absolute value to get\npy(y) =px(g(y))\f\fd\ndyg(y)\f\f (2.155)\nThis is called change of variables formula. 2.8.3.2 Change of variables: multivariate case\nWe can extend the previous results to multivariate distributions as follows. Let fbe an invertible\nfunction that maps RntoRn, with inverse g. Suppose we want to compute the pdf of y=f(x). By\nanalogy with the scalar case, we have\npy(y) =px(g(y))\f\fdet [Jg(y)]\f\f (2.156)\nwhere Jg=dg(y)\ndyTis the Jacobian of g, andjdetJ(y)jis the absolute value of the determinant of J\nevaluated at y.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 207, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 919}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0208_452b813d", "text": "By\nanalogy with the scalar case, we have\npy(y) =px(g(y))\f\fdet [Jg(y)]\f\f (2.156)\nwhere Jg=dg(y)\ndyTis the Jacobian of g, andjdetJ(y)jis the absolute value of the determinant of J\nevaluated at y. (See Section 7.8.5 for a discussion of Jacobians.) In Exercise 3.6 you will use this\nformula to derive the normalization constant for a multivariate Gaussian. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n66 Chapter 2. Probability: Univariate Models\n(a)\n (b)\nFigure 2.19: (a) Mapping a uniform pdf through the function f(x) = 2x+ 1. (b) Illustration of how two\nnearby points, xandx+dx, get mapped under f. Ifdy\ndx>0, the function is locally increasing, but ifdy\ndx<0,\nthe function is locally decreasing. From [Jan18]. Used with kind permission of Eric Jang. Figure 2.20: Illustration of an aﬃne transformation applied to a unit square, f(x) =Ax+b. (a) Here\nA=I. (b) Hereb=0. From [Jan18]. Used with kind permission of Eric Jang.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 208, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 936}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0209_f0f517bd", "text": "Figure 2.20: Illustration of an aﬃne transformation applied to a unit square, f(x) =Ax+b. (a) Here\nA=I. (b) Hereb=0. From [Jan18]. Used with kind permission of Eric Jang. Figure 2.20 illustrates this result in 2d, for the case where f(x) =Ax+b, where A=\u0012a b\nc d\u0013\n. We see that the area of the unit square changes by a factor of det(A) =ad\u0000bc, which is the area of\nthe parallelogram. As another example, consider transforming a density from Cartesian coordinates x= (x1;x2)to\npolar coordinates y=f(x1;x2), sog(r;\u0012) = (rcos\u0012;rsin\u0012). Then\nJg=\u0012@x1\n@r@x1\n@\u0012@x2\n@r@x2\n@\u0012\u0013\n=\u0012\ncos\u0012\u0000rsin\u0012\nsin\u0012 r cos\u0012\u0013\n(2.157)\njdet(Jg)j=jrcos2\u0012+rsin2\u0012j=jrj (2.158)\nHence\npr;\u0012(r;\u0012) =px1;x2(rcos\u0012;rsin\u0012)r (2.159)\nTo see this geometrically, notice that the area of the shaded patch in Figure 2.21 is given by\nPr(r\u0014R\u0014r+dr;\u0012\u0014\u0002\u0014\u0012+d\u0012) =pr;\u0012(r;\u0012)drd\u0012 (2.160)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.8.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 209, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 905}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0210_928a5859", "text": "August 27, 2021\n2.8. Transformations of random variables * 67\nrd+drr + drdr\nFigure 2.21: Change of variables from polar to Cartesian. The area of the shaded patch is rdrd\u0012. Adapted\nfrom Figure 3.16 of [Ric95]. In the limit, this is equal to the density at the center of the patch times the size of the patch, which\nis given by r dr d\u0012. Hence\npr;\u0012(r;\u0012)drd\u0012 =px1;x2(rcos\u0012;rsin\u0012)rdrd\u0012 (2.161)\n2.8.4 Moments of a linear transformation\nSupposefis an aﬃne function, so y=Ax+b. In this case, we can easily derive the mean and\ncovariance of yas follows. First, for the mean, we have\nE[y] =E[Ax+b] =A\u0016+b (2.162)\nwhere\u0016=E[x]. Iffis a scalar-valued function, f(x) =aTx+b, the corresponding result is\nE\u0002\naTx+b\u0003\n=aT\u0016+b (2.163)\nFor the covariance, we have\nCov [y] = Cov [ Ax+b] =A\u0006AT(2.164)\nwhere \u0006= Cov [x]. We leave the proof of this as an exercise.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 210, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 837}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0211_f28e2e5a", "text": "We leave the proof of this as an exercise. As a special case, if y=aTx+b, we get\nV[y] =V\u0002\naTx+b\u0003\n=aT\u0006a (2.165)\nFor example, to compute the variance of the sum of two scalar random variables, we can set a= [1;1]\nto get\nV[x1+x2] =\u00001 1\u0001\u0012\u000611\u000612\n\u000621\u000622\u0013\u00121\n1\u0013\n(2.166)\n= \u0006 11+ \u0006 22+ 2\u0006 12=V[x1] +V[x2] + 2Cov [x1;x2] (2.167)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n68 Chapter 2. Probability: Univariate Models\n- - 1 2 3 4 - -\n7 6 5 - - - - - z0=x0y0= 5\n- 7 6 5 - - - - z1=x0y1+x1y0= 16\n- - 7 6 5 - - - z2=x0y2+x1y1+x2y0= 34\n- - - 7 6 5 - - z3=x1y2+x2y1+x3y0= 52\n- - - - 7 6 5 - z4=x2y2+x3y1= 45\n- - - - - 7 6 5 z5=x3y2= 28\nTable 2.4: Discrete convolution of x= [1;2;3;4]withy= [5;6;7]to yieldz= [5;16;34;52;45;28]. In general,\nzn=P1\nk=\u00001xkyn\u0000k. We see that this operation consists of “ﬂipping” yand then “dragging” it over x,\nmultiplying elementwise, and adding up the results.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 211, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 890}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0212_865139d0", "text": "In general,\nzn=P1\nk=\u00001xkyn\u0000k. We see that this operation consists of “ﬂipping” yand then “dragging” it over x,\nmultiplying elementwise, and adding up the results. Note, however, thatalthoughsomedistributions(suchastheGaussian)arecompletelycharacterized\nby their mean and covariance, in general we must use the techniques described above to derive the\nfull distribution of y. 2.8.5 The convolution theorem\nLety=x1+x2, wherex1andx2are independent rv’s. If these are discrete random variables, we\ncan compute the pmf for the sum as follows:\np(y=j) =X\nkp(x1=k)p(x2=j\u0000k) (2.168)\nforj=:::;\u00002;\u00001;0;1;2;:::. Ifx1andx2have pdf’sp1(x1)andp2(x2), what is the distribution of y? The cdf for yis given by\nPy(y\u0003) = Pr(y\u0014y\u0003) =Z1\n\u00001p1(x1)dx1Zy\u0003\u0000x1\n\u00001p2(x2)dx2 (2.169)\nwhere we integrate over the region Rdeﬁned byx1+x2<y\u0003.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 212, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 806}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0213_7b5afe4c", "text": "The cdf for yis given by\nPy(y\u0003) = Pr(y\u0014y\u0003) =Z1\n\u00001p1(x1)dx1Zy\u0003\u0000x1\n\u00001p2(x2)dx2 (2.169)\nwhere we integrate over the region Rdeﬁned byx1+x2<y\u0003. Thus the pdf for yis\np(y) =\u0014d\ndy\u0003Py(y\u0003)\u0015\ny\u0003=y=Z\np1(x1)p2(y\u0000x1)dx1 (2.170)\nwhere we used the rule of diﬀerentiating under the integral sign :\nd\ndxZb(x)\na(x)f(t)dt=f(b(x))db(x)\ndx\u0000f(a(x))da(x)\ndx(2.171)\nWe can write Equation (2.170) as follows:\np=p1~p2 (2.172)\nwhere ~represents the convolution operator. For ﬁnite length vectors, the integrals become\nsums, and convolution can be thought of as a “ﬂip and drag” operation, as illustrated in Table 2.4. Consequently, Equation (2.170) is called the convolution theorem . Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.8. Transformations of random variables * 69\nS2 3 4 5 6 7 8 9 10 11 12p(S)\n0.16\n0.14\n0.12\n0.10\n0.08\n0.06\n0.04\n0.026 _\n36\n5 __\n36\n4 _\n36\n3 __\n36\n2 __\n36\n1 __\n36\nFigure 2.22: Distribution of the sum of two dice rolls, i.e., p(y)wherey=x1+x2andxi\u0018Unif(f1;2;:::; 6g).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 213, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 997}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0214_7cd7e65c", "text": "From https: // en. wikipedia. org/ wiki/ Probability_ distribution . Used with kind permission of\nWikipedia author Tim Stellmach. For example, suppose we roll two dice, so p1andp2are both the discrete uniform distributions\noverf1;2;:::; 6g. Lety=x1+x2be the sum of the dice. We have\np(y= 2) =p(x1= 1)p(x2= 1) =1\n61\n6=1\n36(2.173)\np(y= 3) =p(x1= 1)p(x2= 2) +p(x1= 2)p(x2= 1) =1\n61\n6+1\n61\n6=2\n36(2.174)\n\u0001\u0001\u0001 (2.175)\nContinuing in this way, we ﬁnd p(y= 4) = 3=36,p(y= 5) = 4=36,p(y= 6) = 5=36,p(y= 7) = 6=36,\np(y= 8) = 5=36,p(y= 9) = 4=36,p(y= 10) = 3=36,p(y= 11) = 2=36andp(y= 12) = 1=36. See\nFigure 2.22 for a plot. We see that the distribution looks like a Gaussian; we explain the reasons for\nthis in Section 2.8.6. We can also compute the pdf of the sum of two continuous rv’s.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 214, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 777}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0215_80a24b34", "text": "See\nFigure 2.22 for a plot. We see that the distribution looks like a Gaussian; we explain the reasons for\nthis in Section 2.8.6. We can also compute the pdf of the sum of two continuous rv’s. For example, in the case of\nGaussians, where x1\u0018N(\u00161;\u001b2\n1)andx2\u0018N(\u00162;\u001b2\n2), one can show (Exercise 2.4) that if y=x1+x2\nthen\np(y) =N(x1j\u00161;\u001b2\n1)\nN(x2j\u00162;\u001b2\n2) =N(yj\u00161+\u00162;\u001b2\n1+\u001b2\n2) (2.176)\nHence the convolution of two Gaussians is a Gaussian. 2.8.6 Central limit theorem\nNow consider Nrandom variables with pdf’s (not necessarily Gaussian) pn(x), each with mean\n\u0016and variance \u001b2. We assume each variable is independent and identically distributed or\niidfor short, which means Xn\u0018p(X)are independent samples from the same distribution. Let\nSN=PN\nn=1Xnbe the sum of the rv’s. One can show that, as Nincreases, the distribution of this\nsum approaches\np(SN=u) =1p\n2\u0019N\u001b2exp\u0012\n\u0000(u\u0000N\u0016)2\n2N\u001b2\u0013\n(2.177)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n70 Chapter 2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 215, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 959}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0216_5a81a81c", "text": "One can show that, as Nincreases, the distribution of this\nsum approaches\np(SN=u) =1p\n2\u0019N\u001b2exp\u0012\n\u0000(u\u0000N\u0016)2\n2N\u001b2\u0013\n(2.177)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n70 Chapter 2. Probability: Univariate Models\n0 0.5 10123N = 1\n(a)\n0 0.5 10123N = 5 (b)\nFigure 2.23: The central limit theorem in pictures. We plot a histogram of ^\u0016s\nN=1\nNPN\nn=1xns, where\nxns\u0018Beta(1;5), fors= 1 : 10000 . AsN!1, the distribution tends towards a Gaussian. (a) N= 1. (b)\nN= 5. Adapted from Figure 2.6 of [Bis06]. Generated by code at ﬁgures.probml.ai/book1/2.23. Hence the distribution of the quantity\nZN,SN\u0000N\u0016\n\u001bp\nN=X\u0000\u0016\n\u001b=p\nN(2.178)\nconverges to the standard normal, where X=SN=Nis the sample mean. This is called the central\nlimit theorem . See e.g., [Jay03, p222] or [Ric95, p169] for a proof. In Figure 2.23 we give an example in which we compute the sample mean of rv’s drawn from a\nbeta distribution. We see that the sampling distribution of this mean rapidly converges to a Gaussian\ndistribution.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 216, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 995}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0217_6ab6d40a", "text": "We see that the sampling distribution of this mean rapidly converges to a Gaussian\ndistribution. 2.8.7 Monte Carlo approximation\nSupposexis a random variable, and y=f(x)is some function of x. It is often diﬃcult to compute the\ninduced distribution p(y)analytically. One simple but powerful alternative is to draw a large number\nof samples from the x’s distribution, and then to use these samples (instead of the distribution) to\napproximate p(y). For example, suppose x\u0018Unif(\u00001;1)andy=f(x) =x2. We can approximate p(y)by drawing\nmany samples from p(x)(using a uniform random number generator ), squaring them, and\ncomputing the resulting empirical distribution, which is given by\npS(y),1\nNsNsX\ns=1\u000e(y\u0000ys) (2.179)\nThis is just an equally weighted “sum of spikes”, each centered on one of the samples (see Section 2.7.6). By using enough samples, we can approximate p(y)rather well. See Figure 2.24 for an illustration. This approach is called a Monte Carlo approximation to the distribution.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 217, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 990}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0218_01824bf2", "text": "By using enough samples, we can approximate p(y)rather well. See Figure 2.24 for an illustration. This approach is called a Monte Carlo approximation to the distribution. (The term “Monte\nCarlo” comes from the name of a famous gambling casino in Monaco.) Monte Carlo techniques were\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.9. Exercises 71\n−1 0 1−0.500.511.5\n00.5 10246\n00.5 100.050.10.150.20.25\nFigure 2.24: Computing the distribution of y=x2, wherep(x)is uniform (left). The analytic result is\nshown in the middle, and the Monte Carlo approximation is shown on the right. Generated by code at\nﬁgures.probml.ai/book1/2.24. ﬁrst developed in the area of statistical physics — in particular, during development of the atomic\nbomb — but are now widely used in statistics and machine learning as well. More details can be\nfound in the sequel to this book, [Mur22], as well as specialized books on the topic, such as [Liu01;\nRC04; KTB11; BZ20].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 218, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 977}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0219_865ce164", "text": "More details can be\nfound in the sequel to this book, [Mur22], as well as specialized books on the topic, such as [Liu01;\nRC04; KTB11; BZ20]. 2.9 Exercises\nExercise 2.1 [Conditional independence *]\n(Source: Koller.)\na.LetH2f1;:::;Kgbe a discrete random variable, and let e1ande2be the observed values of two other\nrandom variables E1andE2. Suppose we wish to calculate the vector\n~P(Hje1;e2) = (P(H= 1je1;e2);:::;P (H=Kje1;e2))\nWhich of the following sets of numbers are suﬃcient for the calculation? i.P(e1;e2),P(H),P(e1jH),P(e2jH)\nii.P(e1;e2),P(H),P(e1;e2jH)\niii.P(e1jH),P(e2jH),P(H)\nb.Now suppose we now assume E1?E2jH(i.e.,E1andE2are conditionally independent given H). Which\nof the above 3 sets are suﬃcient now? Show your calculations as well as giving the ﬁnal result. Hint: use Bayes rule.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 219, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 797}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0220_5e09602e", "text": "Which\nof the above 3 sets are suﬃcient now? Show your calculations as well as giving the ﬁnal result. Hint: use Bayes rule. Exercise 2.2 [Pairwise independence does not imply mutual independence]\nWe say that two random variables are pairwise independent if\np(X2jX1) =p(X2) (2.180)\nand hence\np(X2;X1) =p(X1)p(X2jX1) =p(X1)p(X2) (2.181)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n72 Chapter 2. Probability: Univariate Models\nWe say that nrandom variables are mutually independent if\np(XijXS) =p(Xi)8S\u0012f1;:::;ngnfig (2.182)\nand hence\np(X1:n) =nY\ni=1p(Xi) (2.183)\nShowthatpairwiseindependencebetweenallpairsofvariablesdoesnotnecessarilyimplymutualindependence. It suﬃces to give a counter example. Exercise 2.3 [Conditional independence iﬀ joint factorizes *]\nIn the text we said X?YjZiﬀ\np(x;yjz) =p(xjz)p(yjz) (2.184)\nfor allx;y;zsuch thatp(z)>0.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 220, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 860}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0221_4e54ca05", "text": "It suﬃces to give a counter example. Exercise 2.3 [Conditional independence iﬀ joint factorizes *]\nIn the text we said X?YjZiﬀ\np(x;yjz) =p(xjz)p(yjz) (2.184)\nfor allx;y;zsuch thatp(z)>0. Now prove the following alternative deﬁnition: X?YjZiﬀ there exist\nfunctionsgandhsuch that\np(x;yjz) =g(x;z)h(y;z) (2.185)\nfor allx;y;zsuch thatp(z)>0. Exercise 2.4 [Convolution of two Gaussians is a Gaussian]\nShow that the convolution of two Gaussians is a Gaussian, i.e.,\np(y) =N(x1j\u00161;\u001b2\n1)\nN(x2j\u00162;\u001b2\n2) =N(yj\u00161+\u00162;\u001b2\n1+\u001b2\n2) (2.186)\nwherey=x1+x2,x1\u0018N(\u00161;\u001b2\n1)andx2\u0018N(\u00162;\u001b2\n2). Exercise 2.5 [Expected value of the minimum of two rv’s *]\nSupposeX;Yare two points sampled independently and uniformly at random from the interval [0;1]. What\nis the expected location of the leftmost point? Exercise 2.6 [Variance of a sum]\nShow that the variance of a sum is\nV[X+Y] =V[X] +V[Y] + 2Cov [X;Y ]; (2.187)\nwhere Cov [X;Y ]is the covariance between XandY.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 221, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 934}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0222_e1470a97", "text": "Exercise 2.6 [Variance of a sum]\nShow that the variance of a sum is\nV[X+Y] =V[X] +V[Y] + 2Cov [X;Y ]; (2.187)\nwhere Cov [X;Y ]is the covariance between XandY. Exercise 2.7 [Deriving the inverse gamma density *]\nLetX\u0018Ga(a;b), andY= 1=X. Derive the distribution of Y. Exercise 2.8 [Mean, mode, variance for the beta distribution]\nSuppose\u0012\u0018Beta(a;b). Show that the mean, mode and variance are given by\nE[\u0012] =a\na+b(2.188)\nV[\u0012] =ab\n(a+b)2(a+b+ 1)(2.189)\nmode [\u0012] =a\u00001\na+b\u00002(2.190)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n2.9. Exercises 73\nExercise 2.9 [Bayes rule for medical diagnosis *]\nAfter your yearly checkup, the doctor has bad news and good news. The bad news is that you tested positive\nfor a serious disease, and that the test is 99% accurate (i.e., the probability of testing positive given that you\nhave the disease is 0.99, as is the probability of testing negative given that you don’t have the disease).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 222, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 949}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0223_5de4a3f5", "text": "The\ngood news is that this is a rare disease, striking only one in 10,000 people. What are the chances that you\nactually have the disease? (Show your calculations as well as giving the ﬁnal result.)\nExercise 2.10 [Legal reasoning]\n(Source: Peter Lee.) Suppose a crime has been committed. Blood is found at the scene for which there is no\ninnocent explanation. It is of a type which is present in 1% of the population. a. The prosecutor claims: “There is a 1% chance that the defendant would have the crime blood type if he\nwere innocent. Thus there is a 99% chance that he is guilty”. This is known as the prosecutor’s fallacy . What is wrong with this argument? b.The defender claims: “The crime occurred in a city of 800,000 people. The blood type would be found in\napproximately 8000 people. The evidence has provided a probability of just 1 in 8000 that the defendant\nis guilty, and thus has no relevance.” This is known as the defender’s fallacy . What is wrong with this\nargument?", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 223, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0224_a23a15d5", "text": "The evidence has provided a probability of just 1 in 8000 that the defendant\nis guilty, and thus has no relevance.” This is known as the defender’s fallacy . What is wrong with this\nargument? Exercise 2.11 [Probabilities are sensitive to the form of the question that was used to generate the answer *]\n(Source: Minka.) My neighbor has two children. Assuming that the gender of a child is like a coin ﬂip,\nit is most likely, a priori, that my neighbor has one boy and one girl, with probability 1/2. The other\npossibilities—two boys or two girls—have probabilities 1/4 and 1/4. a.Suppose I ask him whether he has any boys, and he says yes. What is the probability that one child is a\ngirl? b.Suppose instead that I happen to see one of his children run by, and it is a boy. What is the probability\nthat the other child is a girl? Exercise 2.12 [Normalization constant for a 1D Gaussian]\nThe normalization constant for a zero-mean Gaussian is given by\nZ=Zb\naexp\u0012\n\u0000x2\n2\u001b2\u0013\ndx (2.191)\nwherea=\u00001andb=1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 224, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 998}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0225_e1068499", "text": "Exercise 2.12 [Normalization constant for a 1D Gaussian]\nThe normalization constant for a zero-mean Gaussian is given by\nZ=Zb\naexp\u0012\n\u0000x2\n2\u001b2\u0013\ndx (2.191)\nwherea=\u00001andb=1. To compute this, consider its square\nZ2=Zb\naZb\naexp\u0012\n\u0000x2+y2\n2\u001b2\u0013\ndxdy (2.192)\nLet us change variables from cartesian (x;y)to polar (r;\u0012)usingx=rcos\u0012andy=rsin\u0012. Since\ndxdy =rdrd\u0012, andcos2\u0012+ sin2\u0012= 1, we have\nZ2=Z2\u0019\n0Z1\n0rexp\u0012\n\u0000r2\n2\u001b2\u0013\ndrd\u0012 (2.193)\nEvaluate this integral and hence show Z=p\n\u001b22\u0019. Hint 1: separate the integral into a product of two terms,\nthe ﬁrst of which (involving d\u0012) is constant, so is easy. Hint 2: if u=e\u0000r2=2\u001b2thendu=dr =\u00001\n\u001b2re\u0000r2=2\u001b2,\nso the second integral is also easy (sinceR\nu0(r)dr=u(r)). Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n\n3Probability: Multivariate Models\n3.1 Joint distributions for multiple random variables\nIn this section, we discuss various ways to measure the dependence of one or more variables on each\nother.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 225, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 943}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0226_10379f8a", "text": "3.1.1 Covariance\nThecovariance between two rv’sXandYmeasures the degree to which XandYare (linearly)\nrelated. Covariance is deﬁned as\nCov [X;Y ],E[(X\u0000E[X])(Y\u0000E[Y])] =E[XY]\u0000E[X]E[Y] (3.1)\nIfxis aD-dimensional random vector, its covariance matrix is deﬁned to be the following\nsymmetric, positive semi deﬁnite matrix:\nCov [x],Eh\n(x\u0000E[x])(x\u0000E[x])Ti\n,\u0006 (3.2)\n=0\nBBB@V[X1] Cov [ X1;X2]\u0001\u0001\u0001 Cov [X1;XD]\nCov [X2;X1]V[X2]\u0001\u0001\u0001 Cov [X2;XD]\n............ Cov [XD;X1] Cov [XD;X2]\u0001\u0001\u0001 V[XD]1\nCCCA(3.3)\nfrom which we get the important result\nE\u0002\nxxT\u0003\n=\u0006+\u0016\u0016T(3.4)\nAnother useful result is that the covariance of a linear transformation is given by\nCov [Ax+b] =ACov [x]AT(3.5)\nas shown in Exercise 3.4. Thecross-covariance between two random vectors is deﬁned as\nCov [x;y] =E\u0002\n(x\u0000E[x])(y\u0000E[y])T\u0003\n(3.6)\n76 Chapter 3. Probability: Multivariate Models\nFigure 3.1: Several sets of (x;y)points, with the correlation coeﬃcient of xandyfor each set.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 226, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 921}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0227_de20874f", "text": "Probability: Multivariate Models\nFigure 3.1: Several sets of (x;y)points, with the correlation coeﬃcient of xandyfor each set. Note that\nthe correlation reﬂects the noisiness and direction of a linear relationship (top row), but not the slope of\nthat relationship (middle), nor many aspects of nonlinear relationships (bottom). (Note: the ﬁgure in the\ncenter has a slope of 0 but in that case the correlation coeﬃcient is undeﬁned because the variance of Y\nis zero.) From https: // en. wikipedia. org/ wiki/ Pearson_ correlation_ coefficient . Used with kind\npermission of Wikipedia author Imagecreator. 3.1.2 Correlation\nCovariances can be between negative and positive inﬁnity. Sometimes it is more convenient to\nwork with a normalized measure, with a ﬁnite lower and upper bound. The (Pearson) correlation\ncoeﬃcient betweenXandYis deﬁned as\n\u001a,corr [X;Y ],Cov [X;Y ]p\nV[X]V[Y](3.7)\nOne can show (Exercise 3.2) that \u00001\u0014\u001a\u00141.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 227, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 924}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0228_bae248d4", "text": "The (Pearson) correlation\ncoeﬃcient betweenXandYis deﬁned as\n\u001a,corr [X;Y ],Cov [X;Y ]p\nV[X]V[Y](3.7)\nOne can show (Exercise 3.2) that \u00001\u0014\u001a\u00141. One can also show that corr [X;Y ]= 1if and only if Y=aX+b(anda>0) for some parameters\naandb, i.e., if there is a linearrelationship between XandY(see Exercise 3.3). Intuitively one might\nexpect the correlation coeﬃcient to be related to the slope of the regression line, i.e., the coeﬃcient a\nin the expression Y=aX+b. However, as we show in Equation (11.27), the regression coeﬃcient is\nin fact given by a=Cov [X;Y ]=V[X]. In Figure 3.1, we show that the correlation coeﬃcient can be\n0 for strong, but nonlinear, relationships. (Compare to Figure 6.6.) Thus a better way to think of\nthe correlation coeﬃcient is as a degree of linearity .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 228, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 782}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0229_275fbdfc", "text": "(Compare to Figure 6.6.) Thus a better way to think of\nthe correlation coeﬃcient is as a degree of linearity . (See code.probml.ai/book1/correlation2d for a\ndemo to illustrate this idea.)\nIn the case of a vector xof related random variables, the correlation matrix is given by\ncorr(x) =0\nBBBB@1E[(X1\u0000\u00161)(X2\u0000\u00162)]\n\u001b1\u001b2\u0001\u0001\u0001E[(X1\u0000\u00161)(XD\u0000\u0016D)]\n\u001b1\u001bDE[(X2\u0000\u00162)(X1\u0000\u00161)]\n\u001b2\u001b11\u0001\u0001\u0001E[(X2\u0000\u00162)(XD\u0000\u0016D)]\n\u001b2\u001bD............ E[(XD\u0000\u0016D)(X1\u0000\u00161)]\n\u001bD\u001b1E[(XD\u0000\u0016D)(X2\u0000\u00162)]\n\u001bD\u001b2\u0001\u0001\u0001 11\nCCCCA(3.8)\nThis can be written more compactly as\ncorr(x) = (diag( Kxx))\u00001\n2Kxx(diag( Kxx))\u00001\n2 (3.9)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n3.1. Joint distributions for multiple random variables 77\nFigure 3.2: Examples of spurious correlation between causally unrelated time series. Consumption of ice\ncream (red) and violent crime rate (yellow). over time. From http: // icbseverywhere. com/ blog/ 2014/\n10/ the-logic-of-causal-conclusions/ . Used with kind permission of Barbara Drescher.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 229, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 980}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0230_53652595", "text": "over time. From http: // icbseverywhere. com/ blog/ 2014/\n10/ the-logic-of-causal-conclusions/ . Used with kind permission of Barbara Drescher. where Kxxis theauto-covariance matrix\nKxx=\u0006=E\u0002\n(x\u0000E[x])(x\u0000E[x])T\u0003\n=Rxx\u0000\u0016\u0016T(3.10)\nandRxx=E\u0002\nxxT\u0003\nis theautocorrelation matrix . 3.1.3 Uncorrelated does not imply independent\nIfXandYare independent, meaning p(X;Y) =p(X)p(Y), then Cov [X;Y ]= 0, and hence\ncorr [X;Y ]= 0. So independent implies uncorrelated. However, the converse is not true: uncorrelated\ndoes not imply independent . For example, let X\u0018U(\u00001;1)andY=X2. ClearlyYis dependent on\nX(in fact,Yis uniquely determined by X), yet one can show (Exercise 3.1) that corr [X;Y ]= 0. Some striking examples of this fact are shown in Figure 3.1. This shows several data sets where\nthere is clear dependence between XandY, and yet the correlation coeﬃcient is 0. A more general\nmeasure of dependence between random variables is mutual information, discussed in Section 6.3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 230, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 967}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0231_852cb6cd", "text": "A more general\nmeasure of dependence between random variables is mutual information, discussed in Section 6.3. This is zero only if the variables truly are independent. 3.1.4 Correlation does not imply causation\nIt is well known that “ correlation does not imply causation ”. For example, consider Figure 3.2. In red, we plot x1:T, wherextis the amount of ice cream sold in month t. In yellow, we plot y1:T,\nwhereytis the violent crime rate in month t. (Quantities have been rescaled to make the plots\noverlap.) We see a strong correlation between these signals. Indeed, it is sometimes claimed that\n“eating ice cream causes murder” [Pet13]. Of course, this is just a spurious correlation , due to a\nhidden common cause , namely the weather. Hot weather increases ice cream sales, for obvious\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n78 Chapter 3. Probability: Multivariate Models\n(a)\n (b)\nFigure 3.3: Illustration of Simpson’s paradox. (a) Overall, ydecreasesywithx.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 231, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0232_75f347ec", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n78 Chapter 3. Probability: Multivariate Models\n(a)\n (b)\nFigure 3.3: Illustration of Simpson’s paradox. (a) Overall, ydecreasesywithx. (b) Within each group, y\nincreases with x. From https: // en. wikipedia. org/ wiki/ Simpson’s_ paradox . Used with kind permission\nof Wikipedia author Pace svwiki. reasons. Hot weather also increases violent crime; the reason for this is hotly (ahem) debated; some\nclaim it is due to an increase in anger [And01], but other claim it is merely due to more people being\noutside [Ash18], where most murders occur. Another famous example concerns the positive correlation between birth rates and the presence of\nstorks (a kind of bird). This has given rise to the urban legend that storks deliver babies [Mat00]. Of course, the true reason for the correlation is more likely due to hidden factors, such as increased\nliving standards and hence more food. Many more amusing examples of such spurious correlations\ncan be found in [Vig15].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 232, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1008}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0233_dba30b56", "text": "Many more amusing examples of such spurious correlations\ncan be found in [Vig15]. These examples serve as a “warning sign”, that we should not treat the ability for xto predictyas\nan indicator that xcausesy. 3.1.5 Simpson’s paradox\nSimpson’s paradox says that a statistical trend or relationship that appears in several diﬀerent\ngroups of data can disappear or reverse sign when these groups are combined. This results in\ncounterintuitive behavior if we misinterpret claims of statistical dependence in a causal way. A visualization of the paradox is given in Figure 3.3. Overall, we see that ydecreases with x, but\nwithin each subpopulation, yincreases with x. For a recent real-world example of Simpson’s paradox in the context of COVID-19, consider\nFigure 3.4(a). This shows that the case fatality rate (CFR) of COVID-19 in Italy is less than in\nChina in each age group, but is higher overall. The reason for this is that there are more older people\nin Italy, as shown in Figure 3.4(b).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 233, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 989}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0234_73a62320", "text": "The reason for this is that there are more older people\nin Italy, as shown in Figure 3.4(b). In other words, Figure 3.4(a) shows p(F= 1jA;C), whereA\nis age,Cis country, and F= 1is the event that someone dies from COVID-19, and Figure 3.4(b)\nshowsp(AjC), which is the probability someone is in age bucket Afor country C. Combining these,\nwe ﬁndp(F= 1jC=Italy)>p(F= 1jC=China ). See [KGS20] for more details. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n3.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 234, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 487}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0235_5c2a8cf8", "text": "Combining these,\nwe ﬁndp(F= 1jC=Italy)>p(F= 1jC=China ). See [KGS20] for more details. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n3.2. The multivariate Gaussian (normal) distribution 79\n0-9 10-19 20-29 30-39 40-49 50-59 60-69 70-79 80+ Total\nAge02468101214%Case fatality rates (CFRs) by age group\nChina, 17 February\nItaly, 9 March\n(a)\n0-9 10-19 20-29 30-39 40-49 50-59 60-69 70-79 80+\nAge05101520%Proportion of confirmed cases by age group\nChina, 17 February\nItaly, 9 March (b)\nFigure 3.4: Illustration of Simpson’s paradox using COVID-19, (a) Case fatality rates (CFRs) in Italy and\nChina by age group, and in aggregated form (“Total”, last pair of bars), up to the time of reporting (see legend). (b) Proportion of all conﬁrmed cases included in (a) within each age group by country. From Figure 1 of\n[KGS20]. Used with kind permission of Julius von Kügelgen.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 235, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 894}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0236_1e9206f6", "text": "(b) Proportion of all conﬁrmed cases included in (a) within each age group by country. From Figure 1 of\n[KGS20]. Used with kind permission of Julius von Kügelgen. 3.2 The multivariate Gaussian (normal) distribution\nThe most widely used joint probability distribution for continuous random variables is the multi-\nvariate Gaussian ormultivariate normal (MVN). This is mostly because it is mathematically\nconvenient, but also because the Gaussian assumption is fairly reasonable in many cases (see the\ndiscussion in Section 2.6.4). 3.2.1 Deﬁnition\nThe MVN density is deﬁned by the following:\nN(yj\u0016;\u0006),1\n(2\u0019)D=2j\u0006j1=2exp\u0014\n\u00001\n2(y\u0000\u0016)T\u0006\u00001(y\u0000\u0016)\u0015\n(3.11)\nwhere\u0016=E[y]2RDis the mean vector, and \u0006=Cov [y]is theD\u0002Dcovariance matrix ,\ndeﬁned as follows:\nCov [y],Eh\n(y\u0000E[y])(y\u0000E[y])Ti\n(3.12)\n=0\nBBB@V[Y1] Cov [ Y1;Y2]\u0001\u0001\u0001 Cov [Y1;YD]\nCov [Y2;Y1]V[Y2]\u0001\u0001\u0001 Cov [Y2;YD]\n............ Cov [YD;Y1] Cov [YD;Y2]\u0001\u0001\u0001 V[YD]1\nCCCA(3.13)\nwhere\nCov [Yi;Yj],E[(Yi\u0000E[Yi])(Yj\u0000E[Yj])] =E[YiYj]\u0000E[Yi]E[Yj] (3.14)\nandV[Yi] = Cov [Yi;Yi].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 236, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1001}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0237_cbf19f56", "text": "Cov [YD;Y1] Cov [YD;Y2]\u0001\u0001\u0001 V[YD]1\nCCCA(3.13)\nwhere\nCov [Yi;Yj],E[(Yi\u0000E[Yi])(Yj\u0000E[Yj])] =E[YiYj]\u0000E[Yi]E[Yj] (3.14)\nandV[Yi] = Cov [Yi;Yi]. From Equation (3.12), we get the important result\nE\u0002\nyyT\u0003\n=\u0006+\u0016\u0016T(3.15)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n80 Chapter 3. Probability: Multivariate Models\n(a)\n (b)\n (c)\nFigure 3.5: Visualization of a 2d Gaussian density as a surface plot. (a) Distribution using a full covariance\nmatrix can be oriented at any angle. (b) Distribution using a diagonal covariance matrix must be parallel to\nthe axis. (c) Distribution using a spherical covariance matrix must have a symmetric shape. Generated by\ncode at ﬁgures.probml.ai/book1/3.5. full\n-5 0 5-10-8-6-4-20246810\n(a)\ndiagonal\n-5 0 5-10-8-6-4-20246810 (b)\nspherical\n-5 0 5-5-4-3-2-1012345 (c)\nFigure 3.6: Visualization of a 2d Gaussian density in terms of level sets of constant probability density. (a) A\nfull covariance matrix has elliptical contours.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 237, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 960}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0238_0d52b9cf", "text": "(a) A\nfull covariance matrix has elliptical contours. (b) A diagonal covariance matrix is an axis aligned ellipse. (c) A spherical covariance matrix has a circular shape. Generated by code at ﬁgures.probml.ai/book1/3.6. The normalization constant in Equation (3.11) Z= (2\u0019)D=2j\u0006j1=2just ensures that the pdf\nintegrates to 1 (see Exercise 3.6). In 2d, the MVN is known as the bivariate Gaussian distribution. Its pdf can be represented as\ny\u0018N(\u0016;\u0006), wherey2R2,\u00162R2and\n\u0006=\u0012\n\u001b2\n1\u001b2\n12\n\u001b2\n21\u001b2\n2\u0013\n=\u0012\u001b2\n1\u001a\u001b1\u001b2\n\u001a\u001b1\u001b2\u001b2\n2\u0013\n(3.16)\nwhere\u001ais thecorrelation coeﬃcient , deﬁned by\ncorr [Y1;Y2],Cov [Y1;Y2]p\nV[Y1]V[Y2]=\u001b2\n12\n\u001b1\u001b2(3.17)\nOne can show (Exercise 3.2) that \u00001\u0014corr [Y1;Y2]\u00141. Expanding out the pdf in the 2d case gives\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n3.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 238, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 796}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0239_42505f27", "text": "Expanding out the pdf in the 2d case gives\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n3.2. The multivariate Gaussian (normal) distribution 81\nthe following rather intimidating-looking result:\np(y1;y2) =1\n2\u0019\u001b1\u001b2p\n1\u0000\u001a2exp\u0012\n\u00001\n2(1\u0000\u001a2)\u0002\n\u0014(y1\u0000\u00161)2\n\u001b2\n1+(y2\u0000\u00162)2\n\u001b2\n2\u00002\u001a(y1\u0000\u00161)\n\u001b1(y2\u0000\u00162)\n\u001b2\u0015\u0013\n(3.18)\nFigure 3.5 and Figure 3.6 plot some MVN densities in 2d for three diﬀerent kinds of covariance\nmatrices. A full covariance matrix hasD(D+ 1)=2parameters, where we divide by 2 since \u0006is\nsymmetric. (The reason for the elliptical shape is explained in Section 7.4.4, where we discuss the\ngeometry of quadratic forms.) A diagonal covariance matrix hasDparameters, and has 0s in the\noﬀ-diagonal terms. A spherical covariance matrix , also called isotropic covariance matrix ,\nhas the form \u0006=\u001b2ID, so it only has one free parameter, namely \u001b2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 239, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 864}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0240_e5bd0c35", "text": "A spherical covariance matrix , also called isotropic covariance matrix ,\nhas the form \u0006=\u001b2ID, so it only has one free parameter, namely \u001b2. 3.2.2 Mahalanobis distance\nIn this section, we attempt to gain some insights into the geometric shape of the Gaussian pdf\nin multiple dimensions. To do this, we will consider the shape of the level sets of constant (log)\nprobability. The log probability at a speciﬁc point yis given by\nlogp(yj\u0016;\u0006) =\u00001\n2(y\u0000\u0016)T\u0006\u00001(y\u0000\u0016) + const (3.19)\nThe dependence on ycan be expressed in terms of the Mahalanobis distance \u0001betweenyand\u0016,\nwhose square is deﬁned as follows:\n\u00012,(y\u0000\u0016)T\u0006\u00001(y\u0000\u0016) (3.20)\nThus contours of constant (log) probability are equivalent to contours of constant Mahalanobis\ndistance. To gain insight into the contours of constant Mahalanobis distance, we exploit the fact that \u0006,\nand hence \u0003=\u0006\u00001, are both positive deﬁnite matrices (by assumption).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 240, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 891}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0241_af59e839", "text": "To gain insight into the contours of constant Mahalanobis distance, we exploit the fact that \u0006,\nand hence \u0003=\u0006\u00001, are both positive deﬁnite matrices (by assumption). Consider the following\neigendecomposition (Section 7.4) of \u0006:\n\u0006=DX\nd=1\u0015duduT\nd (3.21)\nWe can similarly write\n\u0006\u00001=DX\nd=11\n\u0015duduT\nd (3.22)\nLet us deﬁne zd,uT\nd(y\u0000\u0016), soz=U(y\u0000\u0016). Then we can rewrite the Mahalanobis distance as\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n82 Chapter 3. Probability: Multivariate Models\nfollows:\n(y\u0000\u0016)T\u0006\u00001(y\u0000\u0016) = (y\u0000\u0016)T DX\nd=11\n\u0015duduT\nd! (y\u0000\u0016) (3.23)\n=DX\nd=11\n\u0015d(y\u0000\u0016)TuduT\nd(y\u0000\u0016) =DX\nd=1z2\nd\n\u0015d(3.24)\nAs we discuss in Section 7.4.4, this means we can interpret the Mahalanobis distance as Euclidean\ndistance in a new coordinate frame zin which we rotate ybyUand scale by \u0003. For example, in 2d, let us consider the set of points (z1;z2)that satisfy this equation:\nz2\n1\n\u00151+z2\n2\n\u00152=r (3.25)\nSince these points have the same Mahalanobis distance, they correspond to points of equal probability.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 241, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 999}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0242_7c4a9cc8", "text": "Hence we see that the contours of equal probability density of a 2d Gaussian lie along ellipses. This is\nillustrated in Figure 7.6. The eigenvectors determine the orientation of the ellipse, and the eigenvalues\ndetermine how elongated it is. 3.2.3 Marginals and conditionals of an MVN *\nSupposey= (y1;y2)is jointly Gaussian with parameters\n\u0016=\u0012\u00161\n\u00162\u0013\n;\u0006=\u0012\u000611\u000612\n\u000621\u000622\u0013\n;\u0003=\u0006\u00001=\u0012\u000311\u000312\n\u000321\u000322\u0013\n(3.26)\nwhere \u0003is theprecision matrix . Then the marginals are given by\np(y1) =N(y1j\u00161;\u000611)\np(y2) =N(y2j\u00162;\u000622)(3.27)\nand the posterior conditional is given by\np(y1jy2) =N(y1j\u00161j2;\u00061j2)\n\u00161j2=\u00161+\u000612\u0006\u00001\n22(y2\u0000\u00162)\n=\u00161\u0000\u0003\u00001\n11\u000312(y2\u0000\u00162)\n=\u00061j2(\u000311\u00161\u0000\u000312(y2\u0000\u00162))\n\u00061j2=\u000611\u0000\u000612\u0006\u00001\n22\u000621=\u0003\u00001\n11(3.28)\nThese equations are of such crucial importance in this book that we have put a box around them,\nso you can easily ﬁnd them later. For the derivation of these results (which relies on computing the\nSchur complement \u0006=\u000622=\u000611\u0000\u000612\u0006\u00001\n22\u000621), see Section 7.3.5. Draft of “Probabilistic Machine Learning: An Introduction”.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 242, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 999}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0243_5701feb1", "text": "For the derivation of these results (which relies on computing the\nSchur complement \u0006=\u000622=\u000611\u0000\u000612\u0006\u00001\n22\u000621), see Section 7.3.5. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n3.2. The multivariate Gaussian (normal) distribution 83\nWe see that both the marginal and conditional distributions are themselves Gaussian. For the\nmarginals, we just extract the rows and columns corresponding to y1ory2. For the conditional, we\nhave to do a bit more work. However, it is not that complicated: the conditional mean is just a\nlinear function of y2, and the conditional covariance is just a constant matrix that is independent of\ny2. We give three diﬀerent (but equivalent) expressions for the posterior mean, and two diﬀerent\n(but equivalent) expressions for the posterior covariance; each one is useful in diﬀerent circumstances. 3.2.4 Example: conditioning a 2d Gaussian\nLet us consider a 2d example.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 243, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 922}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0244_387a594b", "text": "3.2.4 Example: conditioning a 2d Gaussian\nLet us consider a 2d example. The covariance matrix is\n\u0006=\u0012\n\u001b2\n1\u001a\u001b1\u001b2\n\u001a\u001b1\u001b2\u001b2\n2\u0013\n(3.29)\nThe marginal p(y1)is a 1D Gaussian, obtained by projecting the joint distribution onto the y1line:\np(y1) =N(y1j\u00161;\u001b2\n1) (3.30)\nSuppose we observe Y2=y2; the conditional p(y1jy2)is obtained by “slicing” the joint distribution\nthrough the Y2=y2line:\np(y1jy2) =N\u0012\ny1j\u00161+\u001a\u001b1\u001b2\n\u001b2\n2(y2\u0000\u00162); \u001b2\n1\u0000(\u001a\u001b1\u001b2)2\n\u001b2\n2\u0013\n(3.31)\nIf\u001b1=\u001b2=\u001b, we get\np(y1jy2) =N\u0000\ny1j\u00161+\u001a(y2\u0000\u00162); \u001b2(1\u0000\u001a2)\u0001\n(3.32)\nFor example, suppose \u001a= 0:8,\u001b1=\u001b2= 1,\u00161=\u00162= 0, andy2= 1. We see that E[y1jy2= 1] =\n0:8, which makes sense, since \u001a= 0:8means that we believe that if y2increases by 1 (beyond its\nmean), then y1increases by 0.8. We also see V[y1jy2= 1] = 1\u00000:82= 0:36. This also makes sense:\nour uncertainty about y1has gone down, since we have learned something about y1(indirectly) by\nobservingy2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 244, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 886}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0245_0e253898", "text": "We also see V[y1jy2= 1] = 1\u00000:82= 0:36. This also makes sense:\nour uncertainty about y1has gone down, since we have learned something about y1(indirectly) by\nobservingy2. If\u001a= 0, we getp(y1jy2) =N\u0000\ny1j\u00161; \u001b2\n1\u0001\n, sincey2conveys no information about y1if\nthey are uncorrelated (and hence independent). 3.2.5 Example: Imputing missing values *\nAs an example application of the above results, suppose we observe some parts (dimensions) of y,\nwith the remaining parts being missing or unobserved. We can exploit the correlation amongst the\ndimensions (encoded by the covariance matrix) to infer the missing entries; this is called missing\nvalue imputation . Figure 3.7 shows a simple example. We sampled Nvectors from a D= 10-dimensional Gaussian,\nand then deliberately “hid” 50% of the data in each sample (row). We then inferred the missing\nentries given the observed entries and the true model parameters.1More precisely, for each row nof\n1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 245, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 940}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0246_9a8ebbdf", "text": "We then inferred the missing\nentries given the observed entries and the true model parameters.1More precisely, for each row nof\n1. In practice, we would need to estimate the parameters from the partially observed data. Unfortunately the MLE\nresults in Section 4.2.6 no longer apply, but we can use the EM algorithm to derive an approximate MLE in the\npresence of missing data. See the sequel to this book for details. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n84 Chapter 3. Probability: Multivariate Models\n01234567890\n1\n2\n3\n4\n5\n6\n7Observed\n(a)\n01234567890\n1\n2\n3\n4\n5\n6\n7Hidden truth (b)\n01234567890\n1\n2\n3\n4\n5\n6\n7Imputation with true params (c)\nFigure 3.7: Illustration of data imputation using an MVN. (a) Visualization of the data matrix. Blank entries\nare missing (not observed). Blue are positive, green are negative. Area of the square is proportional to the\nvalue. (This is known as a Hinton diagram , named after Geoﬀ Hinton, a famous ML researcher.) (b) True\ndata matrix (hidden).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 246, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1005}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0247_c55ea5f2", "text": "Area of the square is proportional to the\nvalue. (This is known as a Hinton diagram , named after Geoﬀ Hinton, a famous ML researcher.) (b) True\ndata matrix (hidden). (c) Mean of the posterior predictive distribution, based on partially observed data in\nthat row, using the true model parameters. Generated by code at ﬁgures.probml.ai/book1/3.7. the data matrix, we compute p(yn;hjyn;v;\u0012), wherevare the indices of the visible entries in that\nrow,hare the remaining indices of the hidden entries, and \u0012= (\u0016;\u0006). From this, we compute the\nmarginal distribution of each missing variable i2h,p(yn;ijyn;v;\u0012). From the marginal, we compute\nthe posterior mean, \u0016yn;i=E[yn;ijyn;v;\u0012]. The posterior mean represents our “best guess” about the true value of that entry, in the sense\nthat it minimizes our expected squared error, as explained in Chapter 5. We can use V[yn;ijyn;v;\u0012]\nas a measure of conﬁdence in this guess, although this is not shown.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 247, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 939}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0248_c263541d", "text": "We can use V[yn;ijyn;v;\u0012]\nas a measure of conﬁdence in this guess, although this is not shown. Alternatively, we could draw\nmultiple posterior samples from p(yn;hjyn;v;\u0012); this is called multiple imputation , and provides a\nmore robust estimate to downstream algorithms that consume the “ﬁlled in” data. 3.3 Linear Gaussian systems *\nIn Section 3.2.3, we conditioned on noise-free observations to infer the posterior over the hidden parts\nof a Gaussian random vector. In this section, we extend this approach to handle noisy observations. Letz2RLbe an unknown vector of values, and y2RDbe some noisy measurement of z. We\nassume these variables are related by the following joint distribution:\np(z) =N(zj\u0016z;\u0006z) (3.33)\np(yjz) =N(yjWz+b;\u0006y) (3.34)\nwhere Wis a matrix of size D\u0002L. This is an example of a linear Gaussian system .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 248, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 825}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0249_6dea67d7", "text": "This is an example of a linear Gaussian system . The corresponding joint distribution, p(z;y) =p(z)p(yjz), is aL+Ddimensional Gaussian, with\nmean and covariance given by\n\u0016=\u0012\n\u0016z\nW\u0016z+b\u0013\n(3.35)\n\u0006=\u0012\u0006z \u0006zWT\nW\u0006z\u0006y+W\u0006zWT\u0013\n(3.36)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n3.3. Linear Gaussian systems * 85\nBy applying the Gaussian conditioning formula in Equation (3.28) to the joint p(y;z)we can\ncompute the posterior p(zjy), as we explain below. This can be interpreted as inverting the z!y\narrow in the generative model from latents to observations. 3.3.1 Bayes rule for Gaussians\nThe posterior over the latent is given by\np(zjy) =N(zj\u0016zjy;\u0006zjy)\n\u0006\u00001\nzjy=\u0006\u00001\nz+WT\u0006\u00001\nyW\n\u0016zjy=\u0006zjy[WT\u0006\u00001\ny(y\u0000b) +\u0006\u00001\nz\u0016z](3.37)\nThis is known as Bayes’ rule for Gaussians .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 249, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 781}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0250_4434b7f1", "text": "Furthermore, the normalization constant of the\nposterior is given by\np(y) =Z\nN(zj\u0016z;\u0006z)N(yjWz+b;\u0006y)dz=N(yjW\u0016z+b;\u0006y+W\u0006zWT) (3.38)\nWe see that the Gaussian prior p(z), combined with the Gaussian likelihood p(yjz), results in a\nGaussian posterior p(zjy). Thus Gaussians are closed under Bayesian conditioning. To describe this\nmore generally, we say that the Gaussian prior is a conjugate prior for the Gaussian likelihood,\nsince the posterior distribution has the same type as the prior. We discuss the notion of conjugate\npriors in more detail in Section 4.6.1. In the sections below, we give various applications of this result. But ﬁrst, we give the derivation. 3.3.2 Derivation *\nWe now derive Equation 3.37. The basic idea is to derive the joint distribution, p(z;y) =p(z)p(yjz),\nand then to use the results from Section 3.2.3 for computing p(zjy). In more detail, we proceed as follows.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 250, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 890}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0251_7764832c", "text": "The basic idea is to derive the joint distribution, p(z;y) =p(z)p(yjz),\nand then to use the results from Section 3.2.3 for computing p(zjy). In more detail, we proceed as follows. The log of the joint distribution is as follows (dropping\nirrelevant constants):\nlogp(z;y) =\u00001\n2(z\u0000\u0016z)T\u0006\u00001\nz(z\u0000\u0016z)\u00001\n2(y\u0000Wz\u0000b)T\u0006\u00001\ny(y\u0000Wz\u0000b) (3.39)\nThis is clearly a joint Gaussian distribution, since it is the exponential of a quadratic form. Expanding out the quadratic terms involving zandy, and ignoring linear and constant terms, we\nhave\nQ=\u00001\n2zT\u0006\u00001\nzz\u00001\n2yT\u0006\u00001\nyy\u00001\n2(Wz)T\u0006\u00001\ny(Wz) +yT\u0006\u00001\nyWz (3.40)\n=\u00001\n2\u0012z\ny\u0013T\u0012\u0006\u00001\nz+WT\u0006\u00001\nyW\u0000WT\u0006\u00001\ny\n\u0000\u0006\u00001\nyW \u0006\u00001\ny\u0013\u0012z\ny\u0013\n(3.41)\n=\u00001\n2\u0012\nz\ny\u0013T\n\u0006\u00001\u0012\nz\ny\u0013\n(3.42)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n86 Chapter 3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 251, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 751}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0252_9ead684a", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n86 Chapter 3. Probability: Multivariate Models\nwhere the precision matrix of the joint is deﬁned as\n\u0006\u00001=\u0012\u0006\u00001\nz+WT\u0006\u00001\nyW\u0000WT\u0006\u00001\ny\n\u0000\u0006\u00001\nyW \u0006\u00001\ny\u0013\n,\u0003=\u0012\n\u0003xx\u0003xy\n\u0003yx\u0003yy\u0013\n(3.43)\nFrom Equation 3.28, and using the fact that \u0016y=W\u0016z+b, we have\np(zjy) =N(\u0016zjy;\u0006zjy) (3.44)\n\u0006zjy=\u0003\u00001\nxx= (\u0006\u00001\nz+WT\u0006\u00001\nyW)\u00001(3.45)\n\u0016zjy=\u0006zjy(\u0003xx\u0016z\u0000\u0003xy(y\u0000\u0016y)) (3.46)\n=\u0006zjy\u0000\n\u0006\u00001\nz\u0016z+WT\u0006\u00001\nyW\u0016z+WT\u0006\u00001\ny(y\u0000\u0016y)\u0001\n(3.47)\n=\u0006zjy\u0000\n\u0006\u00001\nz\u0016z+WT\u0006\u00001\ny(W\u0016z+y\u0000\u0016y)\u0001\n(3.48)\n=\u0006zjy\u0000\n\u0006\u00001\nz\u0016z+WT\u0006\u00001\ny(y\u0000b)\u0001\n(3.49)\n3.3.3 Example: Inferring an unknown scalar\nSuppose we make Nnoisy measurements yiof some underlying quantity z; let us assume the\nmeasurement noise has ﬁxed precision \u0015y= 1=\u001b2, so the likelihood is\np(yijz) =N(yijz;\u0015\u00001\ny) (3.50)\nNow let us use a Gaussian prior for the value of the unknown source:\np(z) =N(zj\u00160;\u0015\u00001\n0) (3.51)\nWe want to compute p(zjy1;:::;yN;\u001b2).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 252, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 861}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0253_072f6c75", "text": "We can convert this to a form that lets us apply Bayes\nrule for Gaussians by deﬁning y= (y1;:::;yN),W=1T\nN(an1\u0002Nrow vector of 1’s), and\n\u0006\u00001\ny= diag(\u0015yI). Then we get\np(zjy) =N(zj\u0016N;\u0015\u00001\nN) (3.52)\n\u0015N=\u00150+N\u0015y (3.53)\n\u0016N=N\u0015yy+\u00150\u00160\n\u0015N=N\u0015y\nN\u0015y+\u00150y+\u00150\nN\u0015y+\u00150\u00160 (3.54)\nThese equations are quite intuitive: the posterior precision \u0015Nis the prior precision \u00150plusNunits\nof measurement precision \u0015y. Also, the posterior mean \u0016Nis a convex combination of the MLE y\nand the prior mean \u00160. This makes it clear that the posterior mean is a compromise between the\nMLE and the prior. If the prior is weak relative to the signal strength ( \u00150is small relative to \u0015y), we\nput more weight on the MLE. If the prior is strong relative to the signal strength ( \u00150is large relative\nto\u0015y), we put more weight on the prior. This is illustrated in Figure 3.8. Note that the posterior mean is written in terms of N\u0015yy, so having Nmeasurements each of\nprecision\u0015yis like having one measurement with value yand precision N\u0015y.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 253, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 993}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0254_692c9f5e", "text": "This is illustrated in Figure 3.8. Note that the posterior mean is written in terms of N\u0015yy, so having Nmeasurements each of\nprecision\u0015yis like having one measurement with value yand precision N\u0015y. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n3.3. Linear Gaussian systems * 87\n5\n 0 50.00.10.20.30.40.50.6prior variance of 1\nprior\nlik\npost\n(a)\n5\n 0 50.00.10.20.30.40.50.6prior variance of 5\nprior\nlik\npost (b)\nFigure 3.8: Inference about zgiven a noisy observation y= 3. (a) Strong prior N(0;1). The posterior mean\nis “shrunk” towards the prior mean, which is 0. (b) Weak prior N(0;5). The posterior mean is similar to the\nMLE. Generated by code at ﬁgures.probml.ai/book1/3.8.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 254, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 706}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0255_9cdee08a", "text": "The posterior mean\nis “shrunk” towards the prior mean, which is 0. (b) Weak prior N(0;5). The posterior mean is similar to the\nMLE. Generated by code at ﬁgures.probml.ai/book1/3.8. We can rewrite the results in terms of the posterior variance, rather than posterior precision, as\nfollows:\np(zjD;\u001b2) =N(zj\u0016N;\u001c2\nN) (3.55)\n\u001c2\nN=1\nN\n\u001b2+1\n\u001c2\n0=\u001b2\u001c2\n0\nN\u001c2\n0+\u001b2(3.56)\n\u0016N=\u001c2\nN\u0012\u00160\n\u001c2\n0+Ny\n\u001b2\u0013\n=\u001b2\nN\u001c2\n0+\u001b2\u00160+N\u001c2\n0\nN\u001c2\n0+\u001b2y (3.57)\nwhere\u001c2\n0= 1=\u00150is the prior variance and \u001c2\nN= 1=\u0015Nis the posterior variance. We can also compute the posterior sequentially, by updating after each observation. If N= 1,\nwe can rewrite the posterior after seeing a single observation as follows (where we deﬁne \u0006y=\u001b2,\n\u00060=\u001c2\n0and\u00061=\u001c2\n1to be the variances of the likelihood, prior and posterior):\np(zjy) =N(zj\u00161;\u00061) (3.58)\n\u00061=\u00121\n\u00060+1\n\u0006y\u0013\u00001\n=\u0006y\u00060\n\u00060+ \u0006y(3.59)\n\u00161= \u0006 1\u0012\u00160\n\u00060+y\n\u0006y\u0013\n(3.60)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n88 Chapter 3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 255, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 930}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0256_1761efa2", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n88 Chapter 3. Probability: Multivariate Models\nWe can rewrite the posterior mean in 3 diﬀerent ways:\n\u00161=\u0006y\n\u0006y+ \u0006 0\u00160+\u00060\n\u0006y+ \u0006 0y (3.61)\n=\u00160+ (y\u0000\u00160)\u00060\n\u0006y+ \u0006 0(3.62)\n=y\u0000(y\u0000\u00160)\u0006y\n\u0006y+ \u0006 0(3.63)\nThe ﬁrst equation is a convex combination of the prior and the data. The second equation is the\nprior mean adjusted towards the data. The third equation is the data adjusted towards the prior\nmean; this is called shrinkage . These are all equivalent ways of expressing the tradeoﬀ between\nlikelihood and prior. If \u00060is small relative to \u0006y, corresponding to a strong prior, the amount of\nshrinkage is large (see Figure 3.8(a)), whereas if \u00060is large relative to \u0006y, corresponding to a weak\nprior, the amount of shrinkage is small (see Figure 3.8(b)).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 256, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 783}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0257_17e02125", "text": "Another way to quantify the amount of shrinkage is in terms of the signal-to-noise ratio , which\nis deﬁned as follows:\nSNR,E\u0002\nZ2\u0003\nE[\u000f2]=\u00060+\u00162\n0\n\u0006y(3.64)\nwherez\u0018N(\u00160;\u00060)is the true signal, y=z+\u000fis the observed signal, and \u000f\u0018N(0;\u0006y)is the\nnoise term. 3.3.4 Example: inferring an unknown vector\nSuppose we have an unknown quantity of interest, z2RD, which we endow with a Gaussian prior,\np(z) =N(\u0016z;\u0006z). If we “know nothing” about za priori, we can set \u0006z=1I, which means we are\ncompletely uncertain about what the value of zshould be. (In practice, we can use a large but ﬁnite\nvalue for the covariance.) By symmetry, it seems reasonable to set \u0016z=0. Now suppose we make Nnoisy but independent measurements of z,yn\u0018N(z;\u0006y), each of size\nD. We can represent the likelihood as follows:\np(Djz) =NY\nn=1N(ynjz;\u0006y) =N(yjz;1\nN\u0006y) (3.65)\nNote that we can replace the Nobservations with their average, y, provided we scale down the\ncovariance by 1=Nto compensate.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 257, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 952}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0258_f29ec719", "text": "Setting W=I,b=0, we can then use Bayes rule for Gaussian\nto compute the posterior over z:\np(zjy1;:::;yN) =N(zja\u0016;a\u0006) (3.66)\na\u0006\u00001=\u0006\u00001\nz+N\u0006\u00001\ny (3.67)\na\u0016=a\u0006(\u0006\u00001\ny(Ny) +\u0006\u00001\nz\u0016z) (3.68)\nwherea\u0016anda\u0006are the parameters of the posterior. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n3.3. Linear Gaussian systems * 89\n1.00\n 0.75\n 0.50\n 0.25\n 0.00 0.25 0.50 0.75 1.001.00\n0.75\n0.50\n0.25\n0.000.250.500.751.00\ndata\n(a)\n1.00\n 0.75\n 0.50\n 0.25\n 0.00 0.25 0.50 0.75 1.001.00\n0.75\n0.50\n0.25\n0.000.250.500.751.00prior (b)\n1.00\n 0.75\n 0.50\n 0.25\n 0.00 0.25 0.50 0.75 1.001.00\n0.75\n0.50\n0.25\n0.000.250.500.751.00post after 10 observation (c)\nFigure 3.9: Illustration of Bayesian inference for a 2d Gaussian random vector z. (a) The data is generated\nfromyn\u0018N(z;\u0006y), wherez= [0:5;0:5]Tand\u0006y= 0:1[2;1; 1;1]). We assume the sensor noise covariance\n\u0006yis known but zis unknown. The black cross represents z. (b) The prior is p(z) =N(zj0;0:1I2).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 258, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 952}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0259_5e189f8c", "text": "We assume the sensor noise covariance\n\u0006yis known but zis unknown. The black cross represents z. (b) The prior is p(z) =N(zj0;0:1I2). (c) We\nshow the posterior after 10 data points have been observed. Generated by code at ﬁgures.probml.ai/book1/3.9. Figure 3.9 gives a 2d example. We can think of zas representing the true, but unknown, location\nof an object in 2d space, such as a missile or airplane, and the ynas being noisy observations, such\nas radar “blips”. As we receive more blips, we are better able to localize the source. (In the sequel to\nthis book, [Mur22], we discuss the Kalman ﬁlter algorithm, which extends this idea to a temporal\nsequence of observations.)\nThe posterior uncertainty about each component of zlocation vector depends on how reliable the\nsensor is in each of these dimensions. In the above example, the measurement noise in dimension 1 is\nhigher than in dimension 2, so we have more posterior uncertainty about z1(horizontal axis) than\naboutz2(vertical axis).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 259, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 991}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0260_f05ddb93", "text": "In the above example, the measurement noise in dimension 1 is\nhigher than in dimension 2, so we have more posterior uncertainty about z1(horizontal axis) than\naboutz2(vertical axis). 3.3.5 Example: sensor fusion\nIn this section, we extend Section 3.3.4, to the case where we have multiple measurements, coming\nfrom diﬀerent sensors, each with diﬀerent reliabilities. That is, the model has the form\np(z;y) =p(z)MY\nm=1NmY\nn=1N(yn;mjz;\u0006m) (3.69)\nwhereMis the number of sensors (measurement devices), and Nmis the number of observations\nfrom sensor m, andy=y1:N;1:M2RK. Our goal is to combine the evidence together, to compute\np(zjy). This is known as sensor fusion . We now give a simple example, where there are just two sensors, so y1\u0018N (z;\u00061)andy2\u0018\nN(z;\u00062). Pictorially, we can represent this example as y1 z!y2. We can combine y1andy2\ninto a single vector y, so the model can be represented as z![y1;y2], wherep(yjz) =N(yjWz;\u0006y),\nwhere W= [I;I]and\u0006y= [\u00061;0;0;\u00062]are block-structured matrices.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 260, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 994}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0261_bdbbeffb", "text": "We can combine y1andy2\ninto a single vector y, so the model can be represented as z![y1;y2], wherep(yjz) =N(yjWz;\u0006y),\nwhere W= [I;I]and\u0006y= [\u00061;0;0;\u00062]are block-structured matrices. We can then apply Bayes’\nrule for Gaussians to compute p(zjy). Figure 3.10(a) gives a 2d example, where we set \u00061=\u00062= 0:01I2, so both sensors are equally\nreliable. In this case, the posterior mean is halfway between the two observations, y1andy2. In\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n90 Chapter 3. Probability: Multivariate Models\n0.2\n 0.0 0.2 0.4 0.6 0.8 1.0 1.21.2\n1.0\n0.8\n0.6\n0.4\n0.2\n0.00.2\n(a)\n0.50\n 0.25\n 0.00 0.25 0.50 0.75 1.00 1.251.50\n1.25\n1.00\n0.75\n0.50\n0.25\n0.000.25\n (b)\n0.75\n 0.50\n 0.25\n 0.00 0.25 0.50 0.75 1.00 1.251.25\n1.00\n0.75\n0.50\n0.25\n0.000.250.500.75\n (c)\nFigure 3.10: We observe y1= (0;\u00001)(red cross) and y2= (1;0)(green cross) and estimate E[zjy1;y2]\n(black cross). (a) Equally reliable sensors, so the posterior mean estimate is in between the two circles.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 261, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 987}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0262_091d0c93", "text": "(a) Equally reliable sensors, so the posterior mean estimate is in between the two circles. (b)\nSensor 2 is more reliable, so the estimate shifts more towards the green circle. (c) Sensor 1 is more reliable\nin the vertical direction, Sensor 2 is more reliable in the horizontal direction. The estimate is an appropriate\ncombination of the two measurements. Generated by code at ﬁgures.probml.ai/book1/3.10. Figure 3.10(b), we set \u00061= 0:05I2and\u00062= 0:01I2, so sensor 2 is more reliable than sensor 1. In\nthis case, the posterior mean is closer to y2. In Figure 3.10(c), we set\n\u00061= 0:01\u001210 1\n1 1\u0013\n;\u00062= 0:01\u00121 1\n1 10\u0013\n(3.70)\nso sensor 1 is more reliable in the second component (vertical direction), and sensor 2 is more reliable\nin the ﬁrst component (horizontal direction). In this case, the posterior mean uses y1’s vertical\ncomponent and y2’s horizontal component. 3.4 The exponential family *\nIn this section, we deﬁne the exponential family , which includes many common probability\ndistributions.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 262, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 998}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0263_86f22a2b", "text": "3.4 The exponential family *\nIn this section, we deﬁne the exponential family , which includes many common probability\ndistributions. The exponential family plays a crucial role in statistics and machine learning. In this\nbook, we mainly use it in the context of generalized linear models, which we discuss in Chapter 12. We will see more applications of the exponential family in the sequel to this book, [Mur22]. 3.4.1 Deﬁnition\nConsider a family of probability distributions parameterized by \u00112RKwith ﬁxed support over\nYD\u0012RD. We say that the distribution p(yj\u0011)is in theexponential family if its density can be\nwritten in the following way:\np(yj\u0011),1\nZ(\u0011)h(y) exp[\u0011TT(y)] =h(y) exp[\u0011TT(y)\u0000A(\u0011)] (3.71)\nwhereh(y)is a scaling constant (also known as the base measure , often 1),T(y)2RKare\nthesuﬃcient statistics ,\u0011are thenatural parameters orcanonical parameters ,Z(\u0011)is a\nnormalization constant known as the partition function , andA(\u0011) =logZ(\u0011)is thelog partition\nfunction .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 263, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 976}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0264_7a07de4b", "text": "One can show that Ais a convex function over the concave set \n,f\u00112RK:A(\u0011)<1g. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n3.4. The exponential family * 91\nIt is convenient if the natural parameters are independent of each other. Formally, we say that an\nexponential family is minimal if there is no\u00112RKnf0gsuch that\u0011TT(y) = 0. This last condition\ncan be violated in the case of multinomial distributions, because of the sum to one constraint on\nthe parameters; however, it is easy to reparameterize the distribution using K\u00001independent\nparameters, as we show below. Equation (3.71) can be generalized by deﬁning \u0011=f(\u001e), where\u001eis some other, possibly smaller,\nset of parameters. In this case, the distribution has the form\np(yj\u001e) =h(y) exp[f(\u001e)TT(y)\u0000A(f(\u001e))] (3.72)\nIf the mapping from \u001eto\u0011is nonlinear, we call this a curvedexponentialfamily . If\u0011=f(\u001e) =\u001e,\nthe model is said to be in canonical form .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 264, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 933}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0265_476ec3c4", "text": "If\u0011=f(\u001e) =\u001e,\nthe model is said to be in canonical form . If, in addition, T(y) =y, we say this is a natural\nexponential family orNEF. In this case, it can be written as\np(yj\u0011) =h(y) exp[\u0011Ty\u0000A(\u0011)] (3.73)\n3.4.2 Example\nAs a simple example, let us consider the Bernoulli distribution. We can write this in exponential\nfamily form as follows:\nBer(yj\u0016) =\u0016y(1\u0000\u0016)1\u0000y(3.74)\n= exp[ylog(\u0016) + (1\u0000y) log(1\u0000\u0016)] (3.75)\n= exp[T(y)T\u0011] (3.76)\nwhereT(y) = [I(y= 1);I(y= 0)],\u0011= [log(\u0016);log(1\u0000\u0016)], and\u0016is the mean parameter. However,\nthis is an over-complete representation since there is a linear dependence between the features. We can see this as follows:\n1TT(y) =I(y= 0) + I(y= 1) = 1 (3.77)\nIf the representation is overcomplete, \u0011is not uniquely identiﬁable. It is common to use a minimal\nrepresentation , which means there is a unique \u0011associated with the distribution.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 265, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 856}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0266_4dfeff04", "text": "It is common to use a minimal\nrepresentation , which means there is a unique \u0011associated with the distribution. In this case, we\ncan just deﬁne\nBer(yj\u0016) = exp\u0014\nylog\u0012\u0016\n1\u0000\u0016\u0013\n+ log(1\u0000\u0016)\u0015\n(3.78)\nWe can put this into exponential family form by deﬁning\n\u0011= log\u0012\u0016\n1\u0000\u0016\u0013\n(3.79)\nT(y) =y (3.80)\nA(\u0011) =\u0000log(1\u0000\u0016) = log(1 + e\u0011) (3.81)\nh(y) = 1 (3.82)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n92 Chapter 3. Probability: Multivariate Models\nWe can recover the mean parameter \u0016from the canonical parameter \u0011using\n\u0016=\u001b(\u0011) =1\n1 +e\u0000\u0011(3.83)\nwhich we recognize as the logistic (sigmoid) function. See the sequel to this book, [Mur22], for more examples. 3.4.3 Log partition function is cumulant generating function\nThe ﬁrst and second cumulants of a distribution are its mean E[Y]and variance V[Y], whereas the\nﬁrst and second moments are E[Y]andE\u0002\nY2\u0003\n. We can also compute higher order cumulants (and\nmoments).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 266, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 907}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0267_9ece308e", "text": "We can also compute higher order cumulants (and\nmoments). An important property of the exponential family is that derivatives of the log partition\nfunction can be used to generate all the cumulants of the suﬃcient statistics. In particular, the\nﬁrst and second cumulants are given by\nrA(\u0011) =E[T(y)] (3.84)\nr2A(\u0011) = Cov [T(y)] (3.85)\nFrom the above result, we see that the Hessian is positive deﬁnite, and hence A(\u0011)is convex in\u0011. Since the log likelihood has the form logp(yj\u0011) =\u0011TT(y)\u0000A(\u0011) +const, we see that this is concave,\nand hence the MLE has a unique global maximum. 3.4.4 Maximum entropy derivation of the exponential family\nSuppose we want to ﬁnd a distribution p(x)to describe some data, where all we know are the\nexpected values ( Fk) of certain features or functions fk(x):\nZ\ndxp(x)fk(x) =Fk (3.86)\nFor example, f1might compute x,f2might compute x2, makingF1the empirical mean and F2the\nempirical second moment. Our prior belief in the distribution is q(x).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 267, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 970}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0268_c9881c04", "text": "Our prior belief in the distribution is q(x). To formalize what we mean by “least number of assumptions”, we will search for the distribution\nthat is as close as possible to our prior q(x), in the sense of KL divergence (Section 6.2), while\nsatisfying our constraints:\np= argmin\npKL(pkq)subject to constraints (3.87)\nIf we use a uniform prior, q(x)/1, minimizing the KL divergence is equivalent to maximizing the\nentropy (Section 6.1):\np= argmax\npH(p)subject to constraints (3.88)\nThe result is called a maximum entropy model . Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n3.5. Mixture models 93\nTo minimize the KL subject to the constraints in Equation (3.86), and the constraint that p(x)\u00150\nandP\nxp(x) = 1, we will use Lagrange multipliers (see Section 8.5.1). The Lagrangian is given by\nJ(p;\u0015) =\u0000X\nxp(x) logp(x)\nq(x)+\u00150 \n1\u0000X\nxp(x)! +X\nk\u0015k \nFk\u0000X\nxp(x)fk(x)!", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 268, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 890}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0269_0efdadc6", "text": "The Lagrangian is given by\nJ(p;\u0015) =\u0000X\nxp(x) logp(x)\nq(x)+\u00150 \n1\u0000X\nxp(x)! +X\nk\u0015k \nFk\u0000X\nxp(x)fk(x)! (3.89)\nWe can use the calculus of variations to take derivatives wrt the function p, but we will adopt a\nsimpler approach and treat pas a ﬁxed length vector (since we are assuming that xis discrete). Then we have\n@J\n@pc=\u00001\u0000logp(x=c)\nq(x=c)\u0000\u00150\u0000X\nk\u0015kfk(x=c) (3.90)\nSetting@J\n@pc= 0for eachcyields\np(x) =q(x)\nZexp \n\u0000X\nk\u0015kfk(x)! (3.91)\nwhere we have deﬁned Z,e1+\u00150. Using the sum-to-one constraint, we have\n1 =X\nxp(x) =1\nZX\nxq(x) exp \n\u0000X\nk\u0015kfk(x)! (3.92)\nHence the normalization constant is given by\nZ=X\nxq(x) exp \n\u0000X\nk\u0015kfk(x)! (3.93)\nThis has exactly the form of the exponential family, where f(x)is the vector of suﬃcient statistics,\n\u0000\u0015are the natural parameters, and q(x)is our base measure. For example, if the features are f1(x) =xandf2(x) =x2, and we want to match the ﬁrst and\nsecond moments, we get the Gaussian disribution.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 269, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 925}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0270_5d162f3f", "text": "For example, if the features are f1(x) =xandf2(x) =x2, and we want to match the ﬁrst and\nsecond moments, we get the Gaussian disribution. 3.5 Mixture models\nOne way to create more complex probability models is to take a convex combination of simple\ndistributions. This is called a mixture model . This has the form\np(yj\u0012) =KX\nk=1\u0019kpk(y) (3.94)\nwherepkis thek’th mixture component, and \u0019kare the mixture weights which satisfy 0\u0014\u0019k\u00141\nandPK\nk=1\u0019k= 1. We can re-express this model as a hierarchical model, in which we introduce the discrete latent\nvariablez2f1;:::;Kg, which speciﬁes which distribution to use for generating the output y. The\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n94 Chapter 3. Probability: Multivariate Models\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.20.30.40.50.60.70.8\n(a)\n (b)\nFigure 3.11: A mixture of 3 Gaussians in 2d. (a) We show the contours of constant probability for each\ncomponent in the mixture. (b) A surface plot of the overall density.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 270, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 983}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0271_e1008a41", "text": "(a) We show the contours of constant probability for each\ncomponent in the mixture. (b) A surface plot of the overall density. Adapted from Figure 2.23 of [Bis06]. Generated by code at ﬁgures.probml.ai/book1/3.11\nprior on this latent variable is p(z=k) =\u0019k, and the conditional is p(yjz=k) =pk(y) =p(yj\u0012k). That is, we deﬁne the following joint model:\np(zj\u0012) = Cat(zj\u0019) (3.95)\np(yjz=k;\u0012) =p(yj\u0012k) (3.96)\nThe “generative story” for the data is that we ﬁrst sample a speciﬁc component z, and then we\ngenerate the observations yusing the parameters chosen according to the value of z. By marginalizing\noutz, we recover Equation (3.94):\np(yj\u0012) =KX\nk=1p(z=kj\u0012)p(yjz=k;\u0012) =KX\nk=1\u0019kp(yj\u0012k) (3.97)\nWe can create diﬀerent kinds of mixture model by varying the base distribution pk, as we illustrate\nbelow.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 271, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 796}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0272_b76d9acb", "text": "3.5.1 Gaussian mixture models\nAGaussian mixture model orGMM, also called a mixture of Gaussians (MoG), is deﬁned\nas follows:\np(y) =KX\nk=1\u0019kN(yj\u0016k;\u0006k) (3.98)\nIn Figure 3.11 we show the density deﬁned by a mixture of 3 Gaussians in 2d. Each mixture\ncomponent is represented by a diﬀerent set of elliptical contours. If we let the number of mixture\ncomponents grow suﬃciently large, a GMM can approximate any smooth distribution over RD. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n3.5. Mixture models 95\n8\n 6\n 4\n 2\n 0 2 42\n024\n(a)\n8\n 6\n 4\n 2\n 0 2 42\n024 (b)\nFigure 3.12: (a) Some data in 2d. (b) A possible clustering using K= 5clusters computed using a GMM. Generated by code at ﬁgures.probml.ai/book1/3.12. GMMs are often used for unsupervised clustering of real-valued data samples yn2RD. This\nworks in two stages. First we ﬁt the model e.g., by computing the MLE ^\u0012=argmax logp(Dj\u0012), where\nD=fyn:n= 1 :Ng.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 272, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 939}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0273_a5648305", "text": "GMMs are often used for unsupervised clustering of real-valued data samples yn2RD. This\nworks in two stages. First we ﬁt the model e.g., by computing the MLE ^\u0012=argmax logp(Dj\u0012), where\nD=fyn:n= 1 :Ng. (We discuss how to compute this MLE in Section 8.7.3.) Then we associate\neach data point ynwith a discrete latent or hidden variable zn2f1;:::;Kgwhich speciﬁes the\nidentity of the mixture component or cluster which was used to generate yn. These latent identities\nare unknown, but we can compute a posterior over them using Bayes rule:\nrnk,p(zn=kjxn;\u0012) =p(zn=kj\u0012)p(xnjzn=k;\u0012)PK\nk0=1p(zn=k0j\u0012)p(xnjzn=k0;\u0012)(3.99)\nThe quantity rnkis called the responsibility of clusterkfor data point n. Given the responsibilities,\nwe can compute the most probable cluster assignment as follows:\n^zn= arg max\nkrnk= arg max\nk[logp(xnjzn=k;\u0012) + logp(zn=kj\u0012)] (3.100)\nThis is known as hard clustering .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 273, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 882}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0274_6fbfb4a7", "text": "Given the responsibilities,\nwe can compute the most probable cluster assignment as follows:\n^zn= arg max\nkrnk= arg max\nk[logp(xnjzn=k;\u0012) + logp(zn=kj\u0012)] (3.100)\nThis is known as hard clustering . (If we use the responsibilities to fractionally assign each data\npoint to diﬀerent clusters, it is called soft clustering .) See Figure 3.12 for an example. If we have a uniform prior over zn, and we use spherical Gaussians with \u0006k=I, the hard clustering\nproblem reduces to\nzn= argmin\nkjjyn\u0000^\u0016kjj2\n2 (3.101)\nIn other words, we assign each data point to its closest centroid, as measured by Euclidean distance. This is the basis of the K-means clustering algorithm, which we discuss in Section 21.3. 3.5.2 Bernoulli mixture models\nIf the data is binary valued, we can use a Bernoulli mixture model orBMM(also called a\nmixture of Bernoullis ), where each mixture component has the following form:\np(yjz=k;\u0012) =DY\nd=1Ber(ydj\u0016dk) =DY\nd=1\u0016yd\ndk(1\u0000\u0016dk)1\u0000yd(3.102)\nAuthor: Kevin P. Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 274, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0275_05e06c10", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n96 Chapter 3. Probability: Multivariate Models\n0.05\n 0.04\n 0.06\n 0.06\n 0.02\n0.03\n 0.06\n 0.01\n 0.04\n 0.08\n0.07\n 0.06\n 0.08\n 0.06\n 0.07\n0.04\n 0.02\n 0.03\n 0.06\n 0.03\nFigure 3.13: We ﬁt a mixture of 20 Bernoullis to the binarized MNIST digit data. We visualize the estimated\ncluster means ^\u0016k. The numbers on top of each image represent the estimated mixing weights ^\u0019k. No labels\nwere used when training the model. Generated by code at ﬁgures.probml.ai/book1/3.13. Here\u0016dkis the probability that bit dturns on in cluster k. As an example, we ﬁt a BMM using K= 20components to the MNIST dataset (Section 3.5.2). (We\nuse the EM algorithm to do this ﬁtting, which is similar to EM for GMMs discussed in Section 8.7.3;\nhowever we can also use SGD to ﬁt the model, which is more eﬃcient for large datasets.2) The\nresulting parameters for each mixture component (i.e., \u0016kand\u0019k) are shown in Figure 3.13.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 275, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 937}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0276_27bda58e", "text": "We see\nthat the model has “discovered” a representation of each type of digit. (Some digits are represented\nmultiple times, since the model does not know the “true” number of classes. See Section 21.3.7 for\nmore information on how to choose the number Kof mixture components.)\n3.6 Probabilistic graphical models *\nI basically know of two principles for treating complicated systems in simple ways: the ﬁrst is\nthe principle of modularity and the second is the principle of abstraction. I am an apologist\nfor computational probability in machine learning because I believe that probability theory\nimplements these two principles in deep and intriguing ways — namely through factorization\nand through averaging. Exploiting these two mechanisms as fully as possible seems to me to\nbe the way forward in machine learning. — Michael Jordan, 1997 (quoted in [Fre98]). We have now introduced a few simple probabilistic building blocks.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 276, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 928}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0277_5352440b", "text": "— Michael Jordan, 1997 (quoted in [Fre98]). We have now introduced a few simple probabilistic building blocks. In Section 3.3, we showed\none way to combine some Gaussian building blocks to build a high dimensional distribution p(y)\nfrom simpler parts, namely the marginal p(y1)and the conditional p(y2jy1). This idea can be\nextended to deﬁne joint distributions over sets of many random variables. The key assumption we\nwill make is that some variables are conditionally independent of others. We will represent our\nCI assumptions using graphs, as we brieﬂy explain below. (See the sequel to this book, [Mur22], for\nmore information.)\n2. For the SGD code, see code.probml.ai/book1/mix_bernoulli_sgd_mnist. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n3.6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 277, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 786}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0278_660de2f6", "text": "For the SGD code, see code.probml.ai/book1/mix_bernoulli_sgd_mnist. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n3.6. Probabilistic graphical models * 97\nP (C=F) P(C=T)\n0.5 0.5\nC P(S=F) P(S=T)\nF 0.5 0.5\nT 0.9 0.1C P(R=F) P(R=T)\nF 0.8 0.2\nT 0.2 0.8\nS R P(W=F) P(W=T)\nF F 1.0 0.0\nT F 0.1 0.9\nF T 0.1 0.9\nT T 0.01 0.99Cloudy\nSprinkler Rain\nWet \nGrass\nFigure 3.14: Water sprinkler PGM with corresponding binary CPTs. T and F stand for true and false. 3.6.1 Representation\nAprobabilistic graphical model orPGMis a joint probability distribution that uses a graph\nstructure to encode conditional independence assumptions. When the graph is a directed acyclic\ngraphorDAG, the model is sometimes called a Bayesian network , although there is nothing\ninherently Bayesian about such models. The basic idea in PGMs is that each node in the graph represents a random variable, and each\nedge represents a direct dependency.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 278, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 941}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0279_89c38529", "text": "The basic idea in PGMs is that each node in the graph represents a random variable, and each\nedge represents a direct dependency. More precisely, each lack of edge represents a conditional\nindependency. In the DAG case, we can number the nodes in topological order (parents before\nchildren), and then we connect them such that each node is conditionally independent of all its\npredecessors given its parents:\nYi?Ypred(i)npa(i)jYpa(i) (3.103)\nwhere pa(i)are the parents of node i, and pred(i)are the predecessors of node iin the ordering. (This\nis called the ordered Markov property .) Consequently, we can represent the joint distribution as\nfollows:\np(Y1:V) =VY\ni=1p(YijYpa(i)) (3.104)\nwhereVis the number of nodes in the graph. 3.6.1.1 Example: water sprinkler network\nSuppose we want to model the dependencies between 4 random variables: C(whether it is cloudy\nseason or not), R(whether it is raining or not), S(whether the water sprinkler is on or not), and W\nAuthor: Kevin P. Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 279, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1003}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0280_c89e0eb0", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n98 Chapter 3. Probability: Multivariate Models\ny1y2y3···\n(a)\ny1y2y3··· (b)\nFigure 3.15: Illustration of ﬁrst and second order autoregressive (Markov) models. (whether the grass is wet or not). We know that the cloudy season makes rain more likely, so we add\naC!Rarc. We know that the cloudy season makes turning on a water sprinkler less likely, so we\nadd aC!Sarc. Finally, we know that either rain or sprinklers can cause the grass to get wet, so\nwe addS!WandR!Wedges. Formally, this deﬁnes the following joint distribution:\np(C;S;R;W ) =p(C)p(SjC)p(RjC;\u0013S)p(WjS;R;\u0000C) (3.105)\nwhere we strike through terms that are not needed due to the conditional independence properties of\nthe model. Each termp(YijYpa(i))is a called the conditional probability distribution orCPDfor node\ni. This can be any kind of distribution we like.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 280, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 868}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0281_78ce5a5d", "text": "Each termp(YijYpa(i))is a called the conditional probability distribution orCPDfor node\ni. This can be any kind of distribution we like. In Figure 3.14, we assume each CPD is a conditional\ncategorical distribution, which can be represented as a conditional probability table orCPT. We can represent the i’th CPT as follows:\n\u0012ijk,p(Yi=kjYpa(i)=j) (3.106)\nThis satisﬁes the properties 0\u0014\u0012ijk\u00141andPKi\nk=1\u0012ijk= 1for each row j. Hereiindexes nodes,\ni2[V];kindexes node states, k2[Ki], whereKiis the number of states for node i; andjindexes\njoint parent states, j2[Ji], whereJi=Q\np2pa(i)Kp. For example, the wet grass node has 2 binary\nparents, so there are 4 parent states. 3.6.1.2 Example: Markov chain\nSuppose we want to create a joint probability distribution over variable-length sequences, p(y1:T).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 281, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 798}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0282_8b21b861", "text": "3.6.1.2 Example: Markov chain\nSuppose we want to create a joint probability distribution over variable-length sequences, p(y1:T). If\neach variable ytrepresents a word from a vocabulary with Kpossible values, so yt2f1;:::;Kg, the\nresulting model represents a distribution over possible sentences of length T; this is often called a\nlanguage model . By the chain rule of probability, we can represent any joint distribution over Tvariables as follows:\np(y1:T) =p(y1)p(y2jy1)p(y3jy2;y1)p(y4jy3;y2;y1):::=TY\nt=1p(ytjy1:t\u00001) (3.107)\nUnfortunately, the number of parameters needed to represent each conditional distribution p(ytjy1:t\u00001)\ngrows exponentially with t. However, suppose we make the conditional independence assumption that\nthe future,yt+1:T, is independent of the past, y1:t\u00001, given the present, yt. This is called the ﬁrst\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n3.6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 282, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 911}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0283_ab160668", "text": "This is called the ﬁrst\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n3.6. Probabilistic graphical models * 99\norder Markov condition , and is repesented by the PGM in Figure 3.15(a). With this assumption,\nwe can write the joint distribution as follows:\np(y1:T) =p(y1)p(y2jy1)p(y3jy2)p(y4jy3):::=p(y1)TY\nt=2p(ytjyt\u00001) (3.108)\nThis is called a Markov chain ,Markov model orautoregressive model of order 1. The function p(ytjyt\u00001)is called the transition function ,transition kernel orMarkov kernel . This is just a conditional distribution over the states at time tgiven the state at time t\u00001, and hence\nit satisﬁes the conditions p(ytjyt\u00001)\u00150andPK\nk=1p(yt=kjyt\u00001=j) = 1. We can represent this\nCPT as a stochastic matrix ,Ajk=p(yt=kjyt\u00001=j), where each row sums to 1. This is known\nas thestate transition matrix . We assume this matrix is the same for all time steps, so the model\nis said to be homogeneous ,stationary , ortime-invariant .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 283, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 968}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0284_dfc20866", "text": "This is known\nas thestate transition matrix . We assume this matrix is the same for all time steps, so the model\nis said to be homogeneous ,stationary , ortime-invariant . This is an example of parameter\ntying, since the same parameter is shared by multiple variables. This assumption allows us to model\nan arbitrary number of variables using a ﬁxed number of parameters. The ﬁrst-order Markov assumption is rather strong. Fortunately, we can easily generalize ﬁrst-order\nmodels to depend on the last Mobservations, thus creating a model of order (memory length) M:\np(y1:T) =p(y1:M)TY\nt=M+1p(ytjyt\u0000M:t\u00001) (3.109)\nThis is called an M’th order Markov model . For example, if M= 2,ytdepends on yt\u00001and\nyt\u00002, as shown in Figure 3.15(b). This is called a trigram model , since it models the distribution\nover word triples. If we use M= 1, we get a bigram model , which models the distribution over\nword pairs.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 284, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 904}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0285_09156ee7", "text": "This is called a trigram model , since it models the distribution\nover word triples. If we use M= 1, we get a bigram model , which models the distribution over\nword pairs. Forlargevocabularysizes, thenumberofparametersneededtoestimatetheconditionaldistributions\nforM-gram models for large Mcan become prohibitive. In this case, we need to make additional\nassumptions beyond conditional independence. For example, we can assume that p(ytjyt\u0000M:t\u00001)can\nbe represented as a low-rank matrix, or in terms of some kind of neural network. This is called a\nneural language model . See Chapter 15 for details. 3.6.2 Inference\nA PGM deﬁnes a joint probability distribution. We can therefore use the rules of marginalization\nand conditioning to compute p(YijYj=yj)for any sets of variables iandj. Eﬃcient algorithms to\nperform this computation are discussed in the sequel to this book, [Mur22]. For example, consider the water sprinkler example in Figure 3.14. Our prior belief that it has\nrained is given by p(R= 1) = 0:5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 285, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1011}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0286_d1b5b566", "text": "For example, consider the water sprinkler example in Figure 3.14. Our prior belief that it has\nrained is given by p(R= 1) = 0:5. If we see that the grass is wet, then our posterior belief that it has\nrained changes to p(R= 1jW= 1) = 0:7079. Now suppose we also notice the water sprinkler was\nturned on: our belief that it rained goes down to p(R= 1jW= 1;S= 1) = 0:3204. This negative\nmutual interaction between multiple causes of some observations is called the explaining away\neﬀect, also known as Berkson’s paradox . (See code.probml.ai/book1/sprinkler_pgm for some code\nthat reproduces these calculations.)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n100 Chapter 3. Probability: Multivariate Models\n3.6.3 Learning\nIf the parameters of the CPDs are unknown, we can view them as additional random variables, add\nthem as nodes to the graph, and then treat them as hidden variables to be inferred.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 286, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 912}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0287_a0f8d26c", "text": "Figure 3.16(a)\nshows a simple example, in which we have Niid random variables, yn, all drawn from the same\ndistribution with common parameter \u0012. (Theshaded nodes represent observed values, whereas the\nunshaded (hollow) nodes represent latent variables or parameters.)\nMore precisely, the model encodes the following “generative story” about the data:\n\u0012\u0018p(\u0012) (3.110)\nyn\u0018p(yj\u0012) (3.111)\nwherep(\u0012)is some (unspeciﬁed) prior over the parameters, and p(yj\u0012)is some speciﬁed likelihood\nfunction. The corresponding joint distribution has the form\np(D;\u0012) =p(\u0012)p(Dj\u0012) (3.112)\nwhereD= (y1;:::;yN). By virtue of the iid assumption, the likelihood can be rewritten as follows:\np(Dj\u0012) =NY\nn=1p(ynj\u0012) (3.113)\nNotice that the order of the data vectors is not important for deﬁning this model, i.e., we can permute\nthe numbering of the leaf nodes in the PGM. When this property holds, we say that the data is\nexchangeable . 3.6.3.1 Plate notation\nIn Figure 3.16(a), we see that the ynodes are repeated Ntimes.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 287, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0288_82d9b831", "text": "When this property holds, we say that the data is\nexchangeable . 3.6.3.1 Plate notation\nIn Figure 3.16(a), we see that the ynodes are repeated Ntimes. To avoid visual clutter, it is common\nto use a form of syntactic sugar calledplates. This is a notational convention in which we draw a\nlittle box around the repeated variables, with the understanding that nodes within the box will get\nrepeated when the model is unrolled . We often write the number of copies or repetitions in the\nbottom right corner of the box. This is illustrated in Figure 3.16(b). This notation is widely used to\nrepresent certain kinds of Bayesian model. Figure 3.17 shows a more interesting example, in which we represent a GMM (Section 3.5.1) as a\ngraphical model. We see that this encodes the joint distribution\np(y1:N;z1:N;\u0012) =p(\u0019)\"KY\nk=1p(\u0016k)p(\u0006k)#\"NY\nn=1p(znj\u0019)p(ynjzn;\u00161:K;\u00061:K)#\n(3.114)\nWe see that the latent variables znas well as the unknown paramters, \u0012= (\u0019;\u00161:K;\u00061:K), are all\nshown as unshaded nodes.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 288, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 988}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0289_283be1f8", "text": "3.7 Exercises\nExercise 3.1 [Uncorrelated does not imply independent *]\nLetX\u0018U(\u00001;1)andY=X2. ClearlyYis dependent on X(in fact,Yis uniquely determined by X). However, show that \u001a(X;Y ) = 0. Hint: ifX\u0018U(a;b)thenE[X] = (a+b)=2andV[X] = (b\u0000a)2=12. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n3.7. Exercises 101\nθ\ny1yN\nNθ\nyn\nFigure 3.16: Left: data points ynare conditionally independent given \u0012. Right: Same model, using plate\nnotation. This represents the same model as the one on the left, except the repeated ynnodes are inside a\nbox, known as a plate; the number in the lower right hand corner, N, speciﬁes the number of repetitions of\ntheynnode. KN\nµkΣkynznπ\nFigure 3.17: A Gaussian mixture model represented as a graphical model. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n102 Chapter 3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 289, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 838}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0290_7a25737e", "text": "KN\nµkΣkynznπ\nFigure 3.17: A Gaussian mixture model represented as a graphical model. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n102 Chapter 3. Probability: Multivariate Models\nExercise 3.2 [Correlation coeﬃcient is between -1 and +1]\nProve that\u00001\u0014\u001a(X;Y )\u00141\nExercise 3.3 [Correlation coeﬃcient for linearly related variables is \u00061*]\nShow that, if Y=aX+bfor some parameters a>0andb, then\u001a(X;Y ) = 1. Similarly show that if a<0,\nthen\u001a(X;Y ) =\u00001. Exercise 3.4 [Linear combinations of random variables]\nLetxbe a random vector with mean mand covariance matrix \u0006. Let AandBbe matrices. a. Derive the covariance matrix of Ax. b. Show that tr(AB) = tr( BA). c. Derive an expression for E\u0002\nxTAx\u0003\n. Exercise 3.5 [Gaussian vs jointlyGaussian ]\nLetX\u0018N(0;1)andY=WX, wherep(W=\u00001) =p(W= 1) = 0:5. It is clear that XandYare not\nindependent, since Yis a function of X. a. ShowY\u0018N(0;1). b.Show Cov [X;Y ]= 0. ThusXandYare uncorrelated but dependent, even though they are Gaussian.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 290, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 978}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0291_9ad5b9ac", "text": "It is clear that XandYare not\nindependent, since Yis a function of X. a. ShowY\u0018N(0;1). b.Show Cov [X;Y ]= 0. ThusXandYare uncorrelated but dependent, even though they are Gaussian. Hint: use the deﬁnition of covariance\nCov [X;Y ] =E[XY]\u0000E[X]E[Y] (3.115)\nand therule of iterated expectation\nE[XY] =E[E[XYjW]] (3.116)\nExercise 3.6 [Normalization constant for a multidimensional Gaussian]\nProve that the normalization constant for a d-dimensional Gaussian is given by\n(2\u0019)d=2j\u0006j1\n2=Z\nexp(\u00001\n2(x\u0000\u0016)T\u0006\u00001(x\u0000\u0016))dx (3.117)\nHint: diagonalize \u0006and use the fact that j\u0006j=Q\ni\u0015ito write the joint pdf as a product of done-dimensional\nGaussians in a transformed coordinate system. (You will need the change of variables formula.) Finally, use\nthe normalization constant for univariate Gaussians. Exercise 3.7 [Sensor fusion with known variances in 1d]\nSuppose we have two sensors with known (and diﬀerent) variances v1andv2, but unknown (and the same)\nmean\u0016.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 291, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 944}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0292_0c95e779", "text": "Exercise 3.7 [Sensor fusion with known variances in 1d]\nSuppose we have two sensors with known (and diﬀerent) variances v1andv2, but unknown (and the same)\nmean\u0016. Suppose we observe n1observations y(1)\ni\u0018N (\u0016;v1)from the ﬁrst sensor and n2observations\ny(2)\ni\u0018N(\u0016;v2)from the second sensor. (For example, suppose \u0016is the true temperature outside, and sensor\n1 is a precise (low variance) digital thermosensing device, and sensor 2 is an imprecise (high variance) mercury\nthermometer.) Let Drepresent all the data from both sensors. What is the posterior p(\u0016jD), assuming a\nnon-informative prior for \u0016(which we can simulate using a Gaussian with a precision of 0)? Give an explicit\nexpression for the posterior mean and variance. Exercise 3.8 [Show that the Student distribution can be written as a Gaussian scale mixture]\nShow that a Student distribution can be written as a Gaussian scale mixture , where we use a Gamma\nmixing distribution on the precision \u000b, i.e.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 292, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 964}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0293_3375245e", "text": "p(xj\u0016;a;b ) =Z1\n0N(xj\u0016;\u000b\u00001)Ga(\u000bja;b)d\u000b (3.118)\nThis can be viewed as an inﬁnite mixture of Gaussians, with diﬀerent precisions. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4Statistics\n4.1 Introduction\nIn Chapter 2–Chapter 3, we assumed all the parameters \u0012of our probability models were known. In\nthis chapter, we discuss how to learn these parameters from data. The process of estimating \u0012fromDis calledmodel ﬁtting , ortraining , and is at the heart of\nmachine learning. There are many methods for producing such estimates, but most boil down to an\noptimization problem of the form\n^\u0012= argmin\n\u0012L(\u0012) (4.1)\nwhereL(\u0012)is some kind of loss function or objective function. We discuss several diﬀerent loss\nfunctions in this chapter. In some cases, we also discuss how to solve the optimization problem in\nclosed form. In general, however, we will need to use some kind of generic optimization algorithm,\nwhich we discuss in Chapter 8.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 293, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 962}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0294_e431c268", "text": "In some cases, we also discuss how to solve the optimization problem in\nclosed form. In general, however, we will need to use some kind of generic optimization algorithm,\nwhich we discuss in Chapter 8. In addition to computing a point estimate ,^\u0012, we discuss how to model our uncertainty or\nconﬁdence in this estimate. In statistics, the process of quantifying uncertainty about an unknown\nquantity estimated from a ﬁnite sample of data is called inference . We will discuss both Bayesian\nand frequentist approaches to inference.1\n4.2 Maximum likelihood estimation (MLE)\nThe most common approach to parameter estimation is to pick the parameters that assign the highest\nprobability to the training data; this is called maximum likelihood estimation orMLE. We give\nmore details below, and then give a series of worked examples. 4.2.1 Deﬁnition\nWe deﬁne the MLE as follows:\n^\u0012mle,argmax\n\u0012p(Dj\u0012) (4.2)\n1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 294, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 902}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0295_4e35833c", "text": "We give\nmore details below, and then give a series of worked examples. 4.2.1 Deﬁnition\nWe deﬁne the MLE as follows:\n^\u0012mle,argmax\n\u0012p(Dj\u0012) (4.2)\n1. In the deep learning community, the term “inference” refers to what we will call “prediction”, namely computing\np(yjx;^\u0012). 104 Chapter 4. Statistics\nWe usually assume the training examples are independently sampled from the same distribution, so\nthe (conditional) likelihood becomes\np(Dj\u0012) =NY\nn=1p(ynjxn;\u0012) (4.3)\nThis is known as the iidassumption, which stands for “independent and identically distributed”. We\nusually work with the log likelihood , which is given by\nLL(\u0012),logp(Dj\u0012) =NX\nn=1logp(ynjxn;\u0012) (4.4)\nThis decomposes into a sum of terms, one per example.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 295, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 712}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0296_91b36054", "text": "We\nusually work with the log likelihood , which is given by\nLL(\u0012),logp(Dj\u0012) =NX\nn=1logp(ynjxn;\u0012) (4.4)\nThis decomposes into a sum of terms, one per example. Thus the MLE is given by\n^\u0012mle= argmax\n\u0012NX\nn=1logp(ynjxn;\u0012) (4.5)\nSince most optimization algorithms (such as those discussed in Chapter 8) are designed to minimize\ncost functions, we can redeﬁne the objective function to be the (conditional) negative log\nlikelihood orNLL:\nNLL(\u0012),\u0000logp(Dj\u0012) =\u0000NX\nn=1logp(ynjxn;\u0012) (4.6)\nMinimizing this will give the MLE. If the model is unconditional (unsupervised), the MLE becomes\n^\u0012mle= argmin\n\u0012\u0000NX\nn=1logp(ynj\u0012) (4.7)\nsince we have outputs ynbut no inputs xn.2\nAlternatively we may want to maximize the jointlikelihood of inputs and outputs. The MLE in\nthis case becomes\n^\u0012mle= argmin\n\u0012\u0000NX\nn=1logp(yn;xnj\u0012) (4.8)\n4.2.2 Justiﬁcation for MLE\nThere are several ways to justify the method of MLE.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 296, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 887}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0297_ea723a86", "text": "The MLE in\nthis case becomes\n^\u0012mle= argmin\n\u0012\u0000NX\nn=1logp(yn;xnj\u0012) (4.8)\n4.2.2 Justiﬁcation for MLE\nThere are several ways to justify the method of MLE. One way is to view it as simple point\napproximation to the Bayesian posterior p(\u0012jD)using a uniform prior, as explained in Section 4.6.7.1. 2. In statistics, it is standard to use yto represent variables whose generative distribution we choose to model, and to\nusexto represent exogoneous inputs which are given but not generated. Thus supervised learning concerns ﬁtting\nconditional models of the form p(yjx), and unsupervised learning is the special case where x=;, so we are just ﬁtting\nthe unconditional distribution p(y). In the ML literature, supervised learning treats yas generated and xas given,\nbut in the unsupervised case, it often switches to using xto represent generated variables. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 297, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 928}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0298_b4ff2573", "text": "Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.2. Maximum likelihood estimation (MLE) 105\nIn particular, suppose we approximate the posterior by a delta function, p(\u0012jD) =\u000e(\u0012\u0000^\u0012map), where\n^\u0012mapis the posterior mode, given by\n^\u0012map= argmax\n\u0012logp(\u0012jD) = argmax\n\u0012logp(Dj\u0012) + logp(\u0012) (4.9)\nIf we use a uniform prior, p(\u0012)/1, the MAP estimate becomes equal to the MLE, ^\u0012map=^\u0012mle. Another way to justify the use of the MLE is that the resulting predictive distribution p(yj^\u0012mle)is\nas close as possible (in a sense to be deﬁned below) to the empirical distribution of the data. In\nthe unconditional case, the empirical distribution is deﬁned by\npD(y),1\nNNX\nn=1\u000e(y\u0000yn) (4.10)\nWe see that the empirical distribution is a series of delta functions or “spikes” at the observed training\npoints. We want to create a model whose distribution q(y) =p(yj\u0012)is similar to pD(y).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 298, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 895}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0299_7d374018", "text": "We want to create a model whose distribution q(y) =p(yj\u0012)is similar to pD(y). A standard way to measure the (dis)similarity between probability distributions pandqis the\nKullback Leibler divergence , orKL divergence . We give the details in Section 6.2, but in brief\nthis is deﬁned as\nKL(pkq) =X\nyp(y) logp(y)\nq(y)(4.11)\n=X\nyp(y) logp(y)\n|{z}\n\u0000H(p)\u0000X\nyp(y) logq(y)\n|{z}\nH(p;q)(4.12)\nwhereH(p)is the entropy of p(see Section 6.1), and H(p;q)is the cross-entropy of pandq(see\nSection 6.1.2). One can show that KL(pkq)\u00150, with equality iﬀ p=q. If we deﬁne q(y) =p(yj\u0012), and setp(y) =pD(y), then the KL divergence becomes\nKL(pkq) =X\nypD(y) logpD(y)\u0000pD(y) logq(y) (4.13)\n=\u0000H(pD)\u00001\nNNX\nn=1logp(ynj\u0012) (4.14)\n= const + NLL( \u0012) (4.15)\nThe ﬁrst term is a constant which we can ignore, leaving just the NLL. Thus minimizing the KL is\nequivalent to minimizing the NLL which is equivalent to computing the MLE, as in Equation (4.7).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 299, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 919}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0300_82da75c9", "text": "Thus minimizing the KL is\nequivalent to minimizing the NLL which is equivalent to computing the MLE, as in Equation (4.7). We can generalize the above results to the supervised (conditional) setting by using the following\nempirical distribution:\npD(x;y) =pD(yjx)pD(x) =1\nNNX\nn=1\u000e(x\u0000xn)\u000e(y\u0000yn) (4.16)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n106 Chapter 4. Statistics\nThe expected KL then becomes\nEpD(x)[KL(pD(Yjx)kq(Yjx))] =X\nxpD(x)\"X\nypD(yjx) logpD(yjx)\nq(yjx)#\n(4.17)\n= const\u0000X\nx;ypD(x;y) logq(yjx) (4.18)\n= const\u00001\nNNX\nn=1logp(ynjxn;\u0012) (4.19)\nMinimizing this is equivalent to minimizing the conditional NLL in Equation (4.6). 4.2.3 Example: MLE for the Bernoulli distribution\nSupposeYis a random variable representing a coin toss, where the event Y= 1corresponds to\nheads andY= 0corresponds to tails. Let \u0012=p(Y= 1)be the probability of heads. The probability\ndistribution for this rv is the Bernoulli, which we introduced in Section 2.4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 300, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 959}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0301_da261e0f", "text": "Let \u0012=p(Y= 1)be the probability of heads. The probability\ndistribution for this rv is the Bernoulli, which we introduced in Section 2.4. The NLL for the Bernoulli distribution is given by\nNLL(\u0012) =\u0000logNY\nn=1p(ynj\u0012) (4.20)\n=\u0000logNY\nn=1\u0012I(yn=1)(1\u0000\u0012)I(yn=0)(4.21)\n=\u0000NX\nn=1I(yn= 1) log\u0012+I(yn= 0) log(1\u0000\u0012) (4.22)\n=\u0000[N1log\u0012+N0log(1\u0000\u0012)] (4.23)\nwhere we have deﬁned N1=PN\nn=1I(yn= 1)andN0=PN\nn=1I(yn= 0), representing the number of\nheads and tails. (The NLL for the binomial is the same as for the Bernoulli, modulo an irrelevant\u0000N\nc\u0001\nterm, which is a constant independent of \u0012.) These two numbers are called the suﬃcient\nstatistics of the data, since they summarize everything we need to know about D. The total count,\nN=N0+N1, is called the sample size . The MLE can be found by solvingd\nd\u0012NLL(\u0012) = 0. The derivative of the NLL is\nd\nd\u0012NLL(\u0012) =\u0000N1\n\u0012+N0\n1\u0000\u0012(4.24)\nand hence the MLE is given by\n^\u0012mle=N1\nN0+N1(4.25)\nWe see that this is just the empirical fraction of heads, which is an intuitive result.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 301, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0302_f6566566", "text": "The derivative of the NLL is\nd\nd\u0012NLL(\u0012) =\u0000N1\n\u0012+N0\n1\u0000\u0012(4.24)\nand hence the MLE is given by\n^\u0012mle=N1\nN0+N1(4.25)\nWe see that this is just the empirical fraction of heads, which is an intuitive result. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.2. Maximum likelihood estimation (MLE) 107\n4.2.4 Example: MLE for the categorical distribution\nSupposewerolla K-sideddiceNtimes. LetYn2f1;:::;Kgbethen’thoutcome, where Yn\u0018Cat(\u0012). We want to estimate the probabilities \u0012from the datasetD=fyn:n= 1 :Ng. The NLL is given by\nNLL(\u0012) =\u0000X\nkNklog\u0012k (4.26)\nwhereNkis the number of times the event Y=kis observed. (The NLL for the multinomial is the\nsame, up to irrelevant scale factors.)\nTo compute the MLE, we have to minimize the NLL subject to the constraint thatPK\nk=1\u0012k= 1. To do this, we will use the method of Lagrange multipliers (see Section 8.5.1).3The Lagrangian is as\nfollows:\nL(\u0012;\u0015),\u0000X\nkNklog\u0012k\u0000\u0015 \n1\u0000X\nk\u0012k!", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 302, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 936}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0303_89edf293", "text": "To do this, we will use the method of Lagrange multipliers (see Section 8.5.1).3The Lagrangian is as\nfollows:\nL(\u0012;\u0015),\u0000X\nkNklog\u0012k\u0000\u0015 \n1\u0000X\nk\u0012k! (4.27)\nTaking derivatives with respect to \u0015yields the original constraint:\n@L\n@\u0015= 1\u0000X\nk\u0012k= 0 (4.28)\nTaking derivatives with respect to \u0012kyields\n@L\n@\u0012k=\u0000Nk\n\u0012k+\u0015= 0 =)Nk=\u0015\u0012k (4.29)\nWe can solve for \u0015using the sum-to-one constraint:\nX\nkNk=N=\u0015X\nk\u0012k=\u0015 (4.30)\nThus the MLE is given by\n^\u0012k=Nk\n\u0015=Nk\nN(4.31)\nwhich is just the empirical fraction of times event koccurs. 4.2.5 Example: MLE for the univariate Gaussian\nSupposeY\u0018N(\u0016;\u001b2)and letD=fyn:n= 1 :Ngbe an iid sample of size N. We can estimate\nthe parameters \u0012= (\u0016;\u001b2)using MLE as follows. First, we derive the NLL, which is given by\nNLL(\u0016;\u001b2) =\u0000NX\nn=1log\"\u00121\n2\u0019\u001b2\u00131\n2\nexp\u0012\n\u00001\n2\u001b2(yn\u0000\u0016)2\u0013#\n(4.32)\n=1\n2\u001b2NX\nn=1(yn\u0000\u0016)2+N\n2log(2\u0019\u001b2) (4.33)\n3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 303, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 823}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0304_1828be24", "text": "First, we derive the NLL, which is given by\nNLL(\u0016;\u001b2) =\u0000NX\nn=1log\"\u00121\n2\u0019\u001b2\u00131\n2\nexp\u0012\n\u00001\n2\u001b2(yn\u0000\u0016)2\u0013#\n(4.32)\n=1\n2\u001b2NX\nn=1(yn\u0000\u0016)2+N\n2log(2\u0019\u001b2) (4.33)\n3. We do not need to explicitly enforce the constraint that \u0012k\u00150since the gradient of the Lagrangian has the form\n\u0000Nk=\u0012k\u0000\u0015; so negative values of \u0012kwould increase the objective, rather than minimize it. (Of course, this does not\npreclude setting \u0012k= 0, and indeed this is the optimal solution if Nk= 0.)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n108 Chapter 4. Statistics\nTheminimumofthisfunctionmustsatisfythefollowingconditions, whichweexplaininSection8.1.1.1:\n@\n@\u0016NLL(\u0016;\u001b2) = 0;@\n@\u001b2NLL(\u0016;\u001b2) = 0 (4.34)\nSo all we have to do is to ﬁnd this stationary point.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 304, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 724}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0305_8c4f074e", "text": "Statistics\nTheminimumofthisfunctionmustsatisfythefollowingconditions, whichweexplaininSection8.1.1.1:\n@\n@\u0016NLL(\u0016;\u001b2) = 0;@\n@\u001b2NLL(\u0016;\u001b2) = 0 (4.34)\nSo all we have to do is to ﬁnd this stationary point. Some simple calculus (Exercise 4.1) shows that\nthe solution is given by the following:\n^\u0016mle=1\nNNX\nn=1yn=y (4.35)\n^\u001b2\nmle=1\nNNX\nn=1(yn\u0000^\u0016mle)2=1\nN\"NX\nn=1y2\nn+ ^\u00162\nmle\u00002yn^\u0016mle#\n=s2\u0000y2(4.36)\ns2,1\nNNX\nn=1y2\nn (4.37)\nThe quantities yands2are called the suﬃcient statistics of the data, since they are suﬃcient to\ncompute the MLE, without loss of information relative to using the raw data itself. Note that you might be used to seeing the estimate for the variance written as\n^\u001b2\nunb=1\nN\u00001NX\nn=1(yn\u0000^\u0016mle)2(4.38)\nwhere we divide by N\u00001. This is not the MLE, but is a diﬀerent kind of estimate, which happens\nto be unbiased (unlike the MLE); see Section 4.7.6.1 for details.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 305, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 870}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0306_75fbe109", "text": "This is not the MLE, but is a diﬀerent kind of estimate, which happens\nto be unbiased (unlike the MLE); see Section 4.7.6.1 for details. 4.2.6 Example: MLE for the multivariate Gaussian\nIn this section, we derive the maximum likelihood estimate for the parameters of a multivariate\nGaussian. First, let us write the log-likelihood, dropping irrelevant constants:\nLL(\u0016;\u0006) = logp(Dj\u0016;\u0006) =N\n2logj\u0003j\u00001\n2NX\nn=1(yn\u0000\u0016)T\u0003(yn\u0000\u0016) (4.39)\nwhere \u0003=\u0006\u00001is theprecision matrix (inverse covariance matrix). 4.2.6.1 MLE for the mean\nUsing the substitution zn=yn\u0000\u0016, the derivative of a quadratic form (Equation (7.264)) and the\nchain rule of calculus, we have\n@\n@\u0016(yn\u0000\u0016)T\u0006\u00001(yn\u0000\u0016) =@\n@znzT\nn\u0006\u00001zn@zn\n@\u0016T(4.40)\n=\u00001(\u0006\u00001+\u0006\u0000T)zn (4.41)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.2. Maximum likelihood estimation (MLE) 109\nsince@zn\n@\u0016T=\u0000I. Hence\n@\n@\u0016LL(\u0016;\u0006) =\u00001\n2NX\nn=1\u00002\u0006\u00001(yn\u0000\u0016) =\u0006\u00001NX\nn=1(yn\u0000\u0016) = 0 (4.42)\n^\u0016=1\nNNX\nn=1yn=y (4.43)\nSo the MLE of \u0016is just the empirical mean.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 306, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0307_abb8733e", "text": "Maximum likelihood estimation (MLE) 109\nsince@zn\n@\u0016T=\u0000I. Hence\n@\n@\u0016LL(\u0016;\u0006) =\u00001\n2NX\nn=1\u00002\u0006\u00001(yn\u0000\u0016) =\u0006\u00001NX\nn=1(yn\u0000\u0016) = 0 (4.42)\n^\u0016=1\nNNX\nn=1yn=y (4.43)\nSo the MLE of \u0016is just the empirical mean. 4.2.6.2 MLE for the covariance matrix\nWe can use the trace trick (Equation (7.36)) to rewrite the log-likelihood in terms of the precision\nmatrix \u0003=\u0006\u00001as follows:\nLL(^\u0016;\u0003) =N\n2logj\u0003j\u00001\n2X\nntr[(yn\u0000^\u0016)(yn\u0000^\u0016)T\u0003] (4.44)\n=N\n2logj\u0003j\u00001\n2tr [Sy\u0003] (4.45)\nSy,NX\nn=1(yn\u0000y)(yn\u0000y)T= X\nnynyT\nn! \u0000NyyT(4.46)\nwhere Syis thescatter matrix centered ony. We can rewrite the scatter matrix in a more compact form as follows:\nSy=~YT~Y=YTCT\nNCNY=YTCNY (4.47)\nwhere\nCN,IN\u00001\nN1N1T\nN (4.48)\nis thecentering matrix , which converts Yto~Yby subtracting the mean y=1\nNYT1Noﬀ every\nrow.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 307, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 751}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0308_44aded24", "text": "Using results from Section 7.8, we can compute derivatives of the loss with respect to \u0003to get\n@LL(^\u0016;\u0003)\n@\u0003=N\n2\u0003\u0000T\u00001\n2ST\ny=0 (4.49)\n\u0003\u0000T=\u0003\u00001=\u0006=1\nNSy (4.50)\n^\u0006=1\nNNX\nn=1(yn\u0000y)(yn\u0000y)T=1\nNYTCNY (4.51)\nThus the MLE for the covariance matrix is the empirical covariance matrix. See Figure 4.1a for an\nexample. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n110 Chapter 4. Statistics\nsepal_length sepal_width petal_length petal_widthsepal_length\nsepal_width\npetal_length\npetal_width0.69 -0.042 1.3 0.52\n-0.042 0.19 -0.33 -0.12\n1.3 -0.33 3.1 1.3\n0.52 -0.12 1.3 0.580.00.51.01.52.02.53.0\n(a)\nsepal_length sepal_width petal_length petal_widthsepal_length\nsepal_width\npetal_length\npetal_width-0.12\n0.87 -0.43\n0.82 -0.37 0.96\n0.4\n0.2\n0.00.20.40.60.81.0\n (b)\nFigure 4.1: (a) Covariance matrix for the features in the iris dataset from Section 1.2.1.1. (b) Correlation\nmatrix. We only show the lower triangle, since the matrix is symmetric and has a unit diagonal. Compare\nthis to Figure 1.3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 308, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 991}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0309_ba9b79e7", "text": "(b) Correlation\nmatrix. We only show the lower triangle, since the matrix is symmetric and has a unit diagonal. Compare\nthis to Figure 1.3. Generated by code at ﬁgures.probml.ai/book1/4.1. Sometimes it is more convenient to work with the correlation matrix deﬁned in Equation (3.8). This can be computed using\ncorr(Y) = (diag( \u0006))\u00001\n2\u0006(diag( \u0006))\u00001\n2 (4.52)\nwhere diag(\u0006)\u00001\n2is a diagonal matrix containing the entries 1=\u001bi. See Figure 4.1b for an example. Note, however, that the MLE may overﬁt or be numerically unstable, especially when the number\nof samples Nis small compared to the number of dimensions D. The main problem is that \u0006has\nO(D2)parameters, so we may need a lot of data to reliably estimate it. In particular, as we see from\nEquation (4.51), the MLE for a full covariance matrix is singular if N <D. And even when N >D,\nthe MLE can be ill-conditioned, meaning it is close to singular. We discuss solutions to this problem\nin Section 4.5.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 309, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 956}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0310_31d6cc22", "text": "And even when N >D,\nthe MLE can be ill-conditioned, meaning it is close to singular. We discuss solutions to this problem\nin Section 4.5.2. 4.2.7 Example: MLE for linear regression\nWe brieﬂy mentioned linear regression in Section 2.6.3. Recall that it corresponds to the following\nmodel:\np(yjx;\u0012) =N(yjwTx;\u001b2) (4.53)\nwhere\u0012= (w;\u001b2). Let us assume for now that \u001b2is ﬁxed, and focus on estimating the weights w. The negative log likelihood or NLL is given by\nNLL(w) =\u0000NX\nn=1log\"\u00121\n2\u0019\u001b2\u00131\n2\nexp\u0012\n\u00001\n2\u001b2(yn\u0000wTxn)2\u0013#\n(4.54)\nDropping the irrelevant additive constants gives the following simpliﬁed objective, known as the\nresidual sum of squares orRSS:\nRSS(w),NX\nn=1(yn\u0000wTxn)2=NX\nn=1r2\nn (4.55)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.3. Empirical risk minimization (ERM) 111\nwherernthen’thresidual error .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 310, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 838}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0311_4201ce47", "text": "August 27, 2021\n4.3. Empirical risk minimization (ERM) 111\nwherernthen’thresidual error . Scaling by the number of examples Ngives themean squared\nerrororMSE:\nMSE(w) =1\nNRSS(w) =1\nNNX\nn=1(yn\u0000wTxn)2(4.56)\nFinally, taking the square root gives the root mean squared error orRMSE:\nRMSE(w) =p\nMSE(w) =vuut1\nNNX\nn=1(yn\u0000wTxn)2 (4.57)\nWe can compute the MLE by minimizing the NLL, RSS, MSE or RMSE. All will give the same\nresults, since these objective functions are all the same, up to irrelevant constants\nLet us focus on the RSS objective. It can be written in matrix notation as follows:\nRSS(w) =NX\nn=1(yn\u0000wTxn)2=jjXw\u0000yjj2\n2= (Xw\u0000y)T(Xw\u0000y) (4.58)\nIn Section 11.2.2.1, we prove that the optimum, which occurs where rwRSS(w) =0, satisﬁes the\nfollowing equation:\n^wmle,argmin\nwRSS(w) = (XTX)\u00001XTy (4.59)\nThis is called the ordinary least squares orOLSestimate, and is equivalent to the MLE.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 311, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 884}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0312_2557d572", "text": "4.3 Empirical risk minimization (ERM)\nWe can generalize MLE by replacing the (conditional) log loss term in Equation (4.6), `(yn;\u0012;xn) =\n\u0000logp(ynjxn;\u0012), with any other loss function, to get\nL(\u0012) =1\nNNX\nn=1`(yn;\u0012;xn) (4.60)\nThis is known as empirical risk minimization orERM, since it is the expected loss where the\nexpectation is taken wrt the empirical distribution. See Section 5.4 for more details. 4.3.1 Example: minimizing the misclassiﬁcation rate\nIf we are solving a classiﬁcation problem, we might want to use 0-1 loss:\n`01(yn;\u0012;xn) =(\n0ifyn=f(xn;\u0012)\n1ifyn6=f(xn;\u0012)(4.61)\nwheref(x;\u0012)is some kind of predictor. The empirical risk becomes\nL(\u0012) =1\nNNX\nn=1`01(yn;\u0012;xn) (4.62)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n112 Chapter 4. Statistics\nThis is just the empirical misclassiﬁcation rate on the training set. Note that for binary problems, we can rewrite the misclassifcation rate in the following notation. Let~y2f\u0000 1;+1gbe the true label, and ^y2f\u0000 1;+1g=f(x;\u0012)be our prediction.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 312, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1007}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0313_d392002c", "text": "Note that for binary problems, we can rewrite the misclassifcation rate in the following notation. Let~y2f\u0000 1;+1gbe the true label, and ^y2f\u0000 1;+1g=f(x;\u0012)be our prediction. We deﬁne the 0-1\nloss as follows:\n`01(~y;^y) =I(~y6= ^y) =I(~y^y<0) (4.63)\nThe corresponding empirical risk becomes\nL(\u0012) =1\nNNX\nn=1`01(yn;^yn) =1\nNNX\nn=1I(~yn^yn<0) (4.64)\nwhere the dependence on xnand\u0012is implicit. 4.3.2 Surrogate loss\nUnfortunately, the 0-1 loss used in Section 4.3.1 is a non-smooth step function, as shown in Figure 4.2,\nmaking it diﬃcult to optimize. (In fact, it is NP-hard [BDEL03].) In this section we consider the use\nof asurrogate loss function [BJM06]. The surrogate is usually chosen to be a maximally tight\nconvex upper bound, which is then easy to minimize. For example, consider a probabilistic binary classiﬁer, which produces the following distribution\nover labels:\np(~yjx;\u0012) =\u001b(~y\u0011) =1\n1 +e\u0000~y\u0011(4.65)\nwhere\u0011=f(x;\u0012)is the log odds.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 313, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 937}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0314_a5583c28", "text": "For example, consider a probabilistic binary classiﬁer, which produces the following distribution\nover labels:\np(~yjx;\u0012) =\u001b(~y\u0011) =1\n1 +e\u0000~y\u0011(4.65)\nwhere\u0011=f(x;\u0012)is the log odds. Hence the log loss is given by\n`ll(~y;\u0011) =\u0000logp(~yj\u0011) = log(1 + e\u0000~y\u0011) (4.66)\nFigure 4.2 shows that this is a smooth upper bound to the 0-1 loss, where we plot the loss vs the\nquantity ~y\u0011, known as the margin, since it deﬁnes a “margin of safety” away from the threshold\nvalue of 0. Thus we see that minimizing the negative log likelihood is equivalent to minimizing a\n(fairly tight) upper bound on the empirical 0-1 loss. Another convex upper bound to 0-1 loss is the hinge loss , which is deﬁned as follows:\n`hinge(~y;\u0011) = max(0;1\u0000~y\u0011),(1\u0000~y\u0011)+ (4.67)\nThis is plotted in Figure 4.2; we see that it has the shape of a partially open door hinge. This is\nconvex upper bound to the 0-1 loss, although it is only piecewise diﬀerentiable, not everywhere\ndiﬀerentiable.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 314, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 942}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0315_f02d5ea1", "text": "This is\nconvex upper bound to the 0-1 loss, although it is only piecewise diﬀerentiable, not everywhere\ndiﬀerentiable. 4.4 Other estimation methods *\n4.4.1 The method of moments\nComputing the MLE requires solving the equation r\u0012NLL(\u0012) =0. Sometimes this is computationally\ndiﬃcult. In such cases, we may be able to use a simpler approach known as the method of moments\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.4. Other estimation methods * 113\n−2.0 −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 2.00.00.51.01.52.02.53.00-1\nhinge\nexp\nFigure 4.2: Illustration of various loss functions for binary classiﬁcation. The horizontal axis is the margin z=\n~y\u0011, the vertical axis is the loss. The log loss uses log base 2. Generated by code at ﬁgures.probml.ai/book1/4.2. (MOM). In this approach, we equate the theoretical moments of the distribution to the empirical\nmoments, and solve the resulting set of Ksimultaneous equations, where Kis the number of\nparameters.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 315, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 980}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0316_aab862d3", "text": "(MOM). In this approach, we equate the theoretical moments of the distribution to the empirical\nmoments, and solve the resulting set of Ksimultaneous equations, where Kis the number of\nparameters. The theoretical moments are given by \u0016k=E\u0002\nYk\u0003\n, fork= 1 :K, and the empirical\nmoments are given by\n^\u0016k=1\nNnX\nn=1yk\nn (4.68)\nso we just need to solve \u0016k= ^\u0016kfor eachk. We give some examples below. The method of moments is simple, but it is theoretically inferior to the MLE approach, since it\nmay not use all the data as eﬃciently. (For details on these theoretical results, see e.g., [CB02].)\nFurthermore, it can sometimes produce inconsistent results (see Section 4.4.1.2). However, when it\nproduces valid estimates, it can be used to initialize iterative algorithms that are used to optimize the\nNLL (see e.g., [AHK12]), thus combining the computational eﬃciency of MOM with the statistical\naccuracy of MLE.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 316, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 907}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0317_6a026d0c", "text": "4.4.1.1 Example: MOM for the univariate Gaussian\nFor example, consider the case of a univariate Gaussian distribution. From Section 4.2.5, we have\n\u00161=\u0016=y (4.69)\n\u00162=\u001b2+\u00162=s2(4.70)\nwhereyis the empirical mean and s2is the empirical average sum of squares. so ^\u0016=yand\n^\u001b2=s2\u0000y2. In this case, the MOM estimate is the same as the MLE, but this is not always the\ncase. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n114 Chapter 4. Statistics\n4.4.1.2 Example: MOM for the uniform distribution\nIn this section, we give an example of the MOM applied to the uniform distribution. Our presentation\nfollows the wikipedia page.4LetY\u0018Unif(\u00121;\u00122)be a uniform random variable, so\np(yj\u0012) =1\n\u00122\u0000\u00121I(\u00121\u0014y\u0014\u00122) (4.71)\nThe ﬁrst two moments are\n\u00161=E[Y] =1\n2(\u00121+\u00122) (4.72)\n\u00162=E\u0002\nY2\u0003\n=1\n3(\u00122\n1+\u00121\u00122+\u00122\n2) (4.73)\nInverting these equations gives\n(\u00121;\u00122) =\u0012\n\u00161\u0000q\n3(\u00162\u0000\u00162\n1);2\u00161\u0000\u00121\u0013\n(4.74)\nUnfortunately this estimator can sometimes give invalid results. For example, suppose D=\nf0;0;0;0;1g.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 317, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 976}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0318_8cacefde", "text": "For example, suppose D=\nf0;0;0;0;1g. The empirical moments are ^\u00161=1\n5and ^\u00162=1\n5, so the estimated parameters are\n^\u00121=1\n5\u00002p\n3\n5=\u00000:493and^\u00122=1\n5+2p\n3\n5= 0:893. However, these cannot possibly be the correct\nparameters, since if \u00122= 0:893, we cannot generate a sample as large as 1. By contrast, consider the MLE. Let y(1)\u0014y(2)\u0014\u0001\u0001\u0001\u0014y(N)be theorder statistics of the data\n(i.e., the values sorted in increasing order). Let \u0012=\u00122\u0000\u00121. Then the likelihood is given by\np(Dj\u0012) = (\u0012)\u0000NI\u0000\ny(1)\u0015\u00121\u0001\nI\u0000\ny(N)\u0014\u00122\u0001\n(4.75)\nWithin the permitted bounds for \u0012, the derivative of the log likelihood is given by\nd\nd\u0012logp(Dj\u0012) =\u0000N\n\u0012<0 (4.76)\nHence the likelihood is a decreasing function of \u0012, so we should pick\n^\u00121=y(1);^\u00122=y(N) (4.77)\nIn the above example, we get ^\u00121= 0and^\u00122= 1, as one would expect. 4.4.2 Online (recursive) estimation\nIf the entire dataset Dis available before training starts, we say that we are doing batch learning .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 318, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 920}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0319_161ebce3", "text": "4.4.2 Online (recursive) estimation\nIf the entire dataset Dis available before training starts, we say that we are doing batch learning . However, in some cases, the data set arrives sequentially, so D=fy1;y2;:::gin an unbounded\nstream. In this case, we want to perform online learning . Let^\u0012t\u00001be our estimate (e.g., MLE) given D1:t\u00001. To ensure our learning algorithm takes constant\ntime per update, we need to ﬁnd a learning rule of the form\n\u0012t=f(^\u0012t\u00001;yt) (4.78)\nThis is called a recursive update . Below we give some examples of such online learning methods. 4.https://en.wikipedia.org/wiki/Method_of_moments_(statistics) . Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.4. Other estimation methods * 115\n4.4.2.1 Example: recursive MLE for the mean of a Gaussian\nLet us reconsider the example from Section 4.2.5 where we computed the MLE for a univariate\nGaussian.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 319, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 901}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0320_c696661c", "text": "Other estimation methods * 115\n4.4.2.1 Example: recursive MLE for the mean of a Gaussian\nLet us reconsider the example from Section 4.2.5 where we computed the MLE for a univariate\nGaussian. We know that the batch estimate for the mean is given by\n^\u0016t=1\nttX\nn=1yn (4.79)\nThis is just a running sum of the data, so we can easily convert this into a recursive estimate as\nfollows:\n^\u0016t=1\nttX\nn=1yn=1\nt((t\u00001)^\u0016t\u00001+yt) (4.80)\n=^\u0016t\u00001+1\nt(yt\u0000^\u0016t\u00001) (4.81)\nThis is known as a moving average . We see from Equation (4.81) that the new estimate is the old estimate plus a correction term. The\nsize of the correction diminishes over time (i.e., as we get more samples). However, if the distribution\nis changing, we want to give more weight to more recent data examples. We discuss how to do this\nin Section 4.4.2.2. 4.4.2.2 Exponentially-weighted moving average\nEquation (4.81) shows how to compute the moving average of a signal.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 320, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 919}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0321_109bda44", "text": "We discuss how to do this\nin Section 4.4.2.2. 4.4.2.2 Exponentially-weighted moving average\nEquation (4.81) shows how to compute the moving average of a signal. In this section, we show\nhow to adjust this to give more weight to more recent examples. In particular, we will compute\nthe following exponentially weighted moving average orEWMA , also called an exponential\nmoving average orEMA:\n^\u0016t=\f\u0016t\u00001+ (1\u0000\f)yt (4.82)\nwhere 0< \f < 1. The contribution of a data point ksteps in the past is weighted by \fk(1\u0000\f). Thus the contribution from old data is exponentially decreasing. In particular, we have\n^\u0016t=\f\u0016t\u00001+ (1\u0000\f)yt (4.83)\n=\f2\u0016t\u00002+\f(1\u0000\f)yt\u00001+ (1\u0000\f)yt... (4.84)\n=\fty0+ (1\u0000\f)\ft\u00001y1+\u0001\u0001\u0001+ (1\u0000\f)\fyt\u00001+ (1\u0000\f)yt (4.85)\nThe sum of a geometric series is given by\n\ft+\ft\u00001+\u0001\u0001\u0001+\f1+\f0=1\u0000\ft+1\n1\u0000\f(4.86)\nHence\n(1\u0000\f)tX\nk=0\fk= (1\u0000\f)1\u0000\ft+1\n1\u0000\f= 1\u0000\ft+1(4.87)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n116 Chapter 4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 321, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 914}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0322_814451ef", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n116 Chapter 4. Statistics\n0 20 40 60 80 100 120 140 16010\n8\n6\n4\n2\n02\nbeta = 0.90\nEMA\nEMA with bias correction\n(a)\n0 20 40 60 80 100 120 140 16010\n8\n6\n4\n2\n02\nbeta = 0.99\nEMA\nEMA with bias correction (b)\nFigure 4.3: Illustration of exponentially-weighted moving average with and without bias correction. (a) Short\nmemory:\f= 0:9. (a) Long memory: \f= 0:99. Generated by code at ﬁgures.probml.ai/book1/4.3. Since 0<\f < 1, we have\ft+1!0ast!1, so smaller \fforgets the past more quickly, and adapts\nto the more recent data more rapidly. This is illustrated in Figure 4.3. Since the initial estimate starts from ^\u00160=0, there is an initial bias. This can be corrected by\nscaling as follows [KB15]:\n~\u0016t=^\u0016t\n1\u0000\ft(4.88)\n(Note that the update in Equation (4.82) is still applied to the uncorrected EMA, ^\u0016t\u00001, before being\ncorrected for the current time step.) The beneﬁt of this is illustrated in Figure 4.3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 322, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 938}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0323_ef09b05a", "text": "4.5 Regularization\nA fundamental problem with MLE, and ERM, is that it will try to pick parameters that minimize\nloss on the training set, but this may not result in a model that has low loss on future data. This is\ncalledoverﬁtting . As a simple example, suppose we want to predict the probability of heads when tossing a coin. We\ntoss itN= 3times and observe 3 heads. The MLE is ^\u0012mle=N1=(N0+N1) = 3=(3 + 0) = 1 (see\nSection 4.2.3). However, if we use Ber(yj^\u0012mle)to make predictions, we will predict that all future\ncoin tosses will also be heads, which seems rather unlikely. The core of the problem is that the model has enough parameters to perfectly ﬁt the observed\ntraining data, so it can perfectly match the empirical distribution. However, in most cases the\nempirical distribution is not the same as the true distribution, so putting all the probability mass on\nthe observed set of Nexamples will not leave over any probability for novel data in the future. That\nis, the model may not generalize .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 323, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1008}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0324_d68a2fa9", "text": "That\nis, the model may not generalize . The main solution to overﬁtting is to use regularization , which means to add a penalty term to\nthe NLL (or empirical risk). Thus we optimize an objective of the form\nL(\u0012;\u0015) =\"\n1\nNNX\nn=1`(yn;\u0012;xn)#\n+\u0015C(\u0012) (4.89)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.5. Regularization 117\nwhere\u0015\u00150is theregularization parameter , andC(\u0012)is some form of complexity penalty . A common complexity penalty is to use C(\u0012) =\u0000logp(\u0012), wherep(\u0012)is thepriorfor\u0012. If`is\nthe log loss, the regularized objective becomes\nL(\u0012;\u0015) =\u00001\nNNX\nn=1logp(ynjxn;\u0012)\u0000\u0015logp(\u0012) (4.90)\nBy setting\u0015= 1and rescaling p(\u0012)appropriately, we can equivalently minimize the following:\nL(\u0012;\u0015) =\u0000\"NX\nn=1logp(ynjxn;\u0012) + logp(\u0012)#\n=\u0000[logp(Dj\u0012) + logp(\u0012)] (4.91)\nMinimizing this is equivalent to maximizing the log posterior:\n^\u0012= argmax\n\u0012logp(\u0012jD) = argmax\n\u0012[logp(Dj\u0012) + logp(\u0012)\u0000const] (4.92)\nThis is known as MAP estimation , which stands for maximum a posterior estimation .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 324, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 995}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0325_a899dcdc", "text": "4.5.1 Example: MAP estimation for the Bernoulli distribution\nConsider again the coin tossing example. If we observe just one head, the MLE is \u0012mle= 1. To avoid\nthis, we can add a penalty to \u0012to discourage “extreme” values, such as \u0012= 0or\u0012= 1. We can do\nthis by using a beta distribution as our prior, p(\u0012) =Beta(\u0012ja;b), wherea;b> 1encourages values\nof\u0012near toa=(a+b)(see Section 2.7.4 for details). The log likelihood plus log prior becomes\nLL(\u0012) = logp(Dj\u0012) + logp(\u0012) (4.93)\n= [N1log\u0012+N0log(1\u0000\u0012)] + [(a\u00001) log(\u0012) + (b\u00001) log(1\u0000\u0012)] (4.94)\nUsing the method from Section 4.2.3 we ﬁnd that the MAP estimate is\n\u0012map=N1+a\u00001\nN1+N0+a+b\u00002(4.95)\nIf we seta=b= 2(which weakly favors a value of \u0012near 0.5), the estimate becomes\n\u0012map=N1+ 1\nN1+N0+ 2(4.96)\nThis is called add-one smoothing , and is a simple but widely used technique to avoid the zero\ncountproblem.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 325, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 851}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0326_551422d0", "text": "(See also Section 4.6.2.9.)\nThe zero-count problem, and overﬁtting more generally, is analogous to a problem in philosophy\ncalled the black swan paradox . This is based on the ancient Western conception that all swans\nwere white. In that context, a black swan was a metaphor for something that could not exist. (Black\nswans were discovered in Australia by European explorers in the 17th Century.) The term “black\nswan paradox” was ﬁrst coined by the famous philosopher of science Karl Popper; the term has also\nbeen used as the title of a recent popular book [Tal07]. This paradox was used to illustrate the\nproblem of induction , which is the problem of how to draw general conclusions about the future\nfrom speciﬁc observations from the past. The solution to the paradox is to admit that induction is in\ngeneral impossible, and that the best we can do is to make plausible guesses about what the future\nmight hold, by combining the empirical data with prior knowledge. Author: Kevin P. Murphy.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 326, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 995}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0327_3ac8887c", "text": "Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n118 Chapter 4. Statistics\n0 5 10 15 20 2500.511.5eigenvalueN=100, D=50\n \ntrue, k=10.00\nMLE, k= 71\nMAP, k=8.62\n0 5 10 15 20 2500.511.5eigenvalueN=50, D=50\n \ntrue, k=10.00\nMLE, k=1.7e+17\nMAP, k=8.85\n0 5 10 15 20 2500.511.5eigenvalueN=25, D=50\n \ntrue, k=10.00\nMLE, k=2.2e+18\nMAP, k=21.09\nFigure 4.4: Estimating a covariance matrix in D= 50dimensions using N2f100;50;25gsamples. We\nplot the eigenvalues in descending order for the true covariance matrix (solid black), the MLE (dotted blue)\nand the MAP estimate (dashed red), using Equation (4.98)with\u0015= 0:9. We also list the condition number\nof each matrix in the legend. We see that the MLE is often poorly conditioned, but the MAP estimate is\nnumerically well behaved. Adapted from Figure 1 of [SS05]. Generated by code at ﬁgures.probml.ai/book1/4.4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 327, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 859}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0328_7905b01d", "text": "We see that the MLE is often poorly conditioned, but the MAP estimate is\nnumerically well behaved. Adapted from Figure 1 of [SS05]. Generated by code at ﬁgures.probml.ai/book1/4.4. 4.5.2 Example: MAP estimation for the multivariate Gaussian *\nIn Section 4.2.6, we showed that the MLE for the mean of an MVN is the empirical mean, ^\u0016mle=y. We also showed that the MLE for the covariance is the empirical covariance, ^\u0006=1\nNSy. In high dimensions the estimate for \u0006can easily become singular. One solution to this is to\nperform MAP estimation, as we explain below. 4.5.2.1 Shrinkage estimate\nA convenient prior to use for \u0006is the inverse Wishart prior. This is a distribution over positive\ndeﬁnite matrices, where the parameters are deﬁned in terms of a prior scatter matrix,`S, and a prior\nsample size or strength`N. One can show that the resulting MAP estimate is given by\n^\u0006map=`S+Sy\n`N+N=`N\n`N+N`S\n`N+N\n`N+NSy\nN=\u0015\u00060+ (1\u0000\u0015)^\u0006mle (4.97)\nwhere\u0015=`N`N+Ncontrols the amount of regularization.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 328, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 987}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0329_8dce5844", "text": "One can show that the resulting MAP estimate is given by\n^\u0006map=`S+Sy\n`N+N=`N\n`N+N`S\n`N+N\n`N+NSy\nN=\u0015\u00060+ (1\u0000\u0015)^\u0006mle (4.97)\nwhere\u0015=`N`N+Ncontrols the amount of regularization. A common choice (see e.g., [FR07, p6]) for the prior scatter matrix is to use`S=`Ndiag(^\u0006mle). With this choice, we ﬁnd that the MAP estimate for \u0006is given by\n^\u0006map(i;j) =\u001a^\u0006mle(i;j) ifi=j\n(1\u0000\u0015)^\u0006mle(i;j)otherwise(4.98)\nThus we see that the diagonal entries are equal to their ML estimates, and the oﬀ-diagonal elements\nare “shrunk” somewhat towards 0. This technique is therefore called shrinkage estimation . The other parameter we need to set is \u0015, which controls the amount of regularization (shrink-\nage towards the MLE). It is common to set \u0015by cross validation (Section 4.5.5). Alternatively,\nwe can use the closed-form formula provided in [LW04a; LW04b; SS05], which is the optimal\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 329, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 943}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0330_a786e9ac", "text": "Alternatively,\nwe can use the closed-form formula provided in [LW04a; LW04b; SS05], which is the optimal\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.5. Regularization 119\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.07.5\n5.0\n2.5\n0.02.55.07.510.0L2 regularizer 0.00000\n(a)\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.07.5\n5.0\n2.5\n0.02.55.07.510.0L2 regularizer 0.00019 (b)\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.07.5\n5.0\n2.5\n0.02.55.07.510.0L2 regularizer 1.10776\n(c)\n109\n107\n105\n103\n101\n101\nL2 regularizer2.55.07.510.012.515.017.520.0mse\ntest\ntrain (d)\nFigure 4.5: (a-c) Ridge regression applied to a degree 14 polynomial ﬁt to 21 datapoints. (d) MSE vs strength\nof regularizer. The degree of regularization increases from left to right, so model complexity decreases from\nleft to right. Generated by code at ﬁgures.probml.ai/book1/4.5. frequentist estimate if we use squared loss.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 330, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 904}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0331_6ed7f2ba", "text": "The degree of regularization increases from left to right, so model complexity decreases from\nleft to right. Generated by code at ﬁgures.probml.ai/book1/4.5. frequentist estimate if we use squared loss. This is implemented in the sklearn function https://scikit-\nlearn.org/stable/modules/generated/sklearn.covariance.LedoitWolf.html. The beneﬁts of this approach are illustrated in Figure 4.4. We consider ﬁtting a 50-dimensional\nGaussian to N= 100,N= 50andN= 25data points. We see that the MAP estimate is always\nwell-conditioned, unlike the MLE (see Section 7.1.4.4 for a discussion of condition numbers). In\nparticular, we see that the eigenvalue spectrum of the MAP estimate is much closer to that of the\ntrue matrix than the MLE’s spectrum. The eigenvectors, however, are unaﬀected. 4.5.3 Example: weight decay\nIn Figure 1.7, we saw how using polynomial regression with too high of a degree can result in\noverﬁtting. One solution is to reduce the degree of the polynomial.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 331, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 977}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0332_6bb7e774", "text": "4.5.3 Example: weight decay\nIn Figure 1.7, we saw how using polynomial regression with too high of a degree can result in\noverﬁtting. One solution is to reduce the degree of the polynomial. However, a more general solution\nis to penalize the magnitude of the weights (regression coeﬃcients). We can do this by using a\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n120 Chapter 4. Statistics\nzero-mean Gaussian prior, p(w). The resulting MAP estimate is given by\n^wmap= argmin\nwNLL(w) +\u0015jjwjj2\n2 (4.99)\nwherejjwjj2\n2=PD\nd=1w2\nd. (We writewrather than\u0012, since it only really make sense to penalize the\nmagnitude of weight vectors, rather than other parameters, such as bias terms or noise variances.)\nEquation (4.99) is called `2regularization orweight decay . The larger the value of \u0015, the more\nthe parameters are penalized for being “large” (deviating from the zero-mean prior), and thus the\nless ﬂexible the model.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 332, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 929}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0333_8d612cbe", "text": "The larger the value of \u0015, the more\nthe parameters are penalized for being “large” (deviating from the zero-mean prior), and thus the\nless ﬂexible the model. In the case of linear regression, this kind of penalization scheme is called ridge regression . For\nexample, consider the polynomial regression example from Section 1.2.2.2, where the predictor has\nthe form\nf(x;w) =DX\nd=0wdxd=wT[1;x;x2;:::;xD] (4.100)\nSuppose we use a high degree polynomial, say D= 14, even though we have a small dataset with\njustN= 21examples. MLE for the parameters will enable the model to ﬁt the data very well, by\ncarefully adjusting the weights, but the resulting function is very “wiggly”, thus resulting in overﬁtting. Figure 4.5 illustrates how increasing \u0015can reduce overﬁtting. For more details on ridge regression,\nsee Section 11.3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 333, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 821}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0334_8bc4e4f9", "text": "Figure 4.5 illustrates how increasing \u0015can reduce overﬁtting. For more details on ridge regression,\nsee Section 11.3. 4.5.4 Picking the regularizer using a validation set\nA key question when using regularization is how to choose the strength of the regularizer \u0015: a small\nvalue means we will focus on minimizing empirical risk, which may result in overﬁtting, whereas a\nlarge value means we will focus on staying close to the prior, which may result in underﬁtting . In this section, we describe a simple but very widely used method for choosing \u0015. The basic idea\nis to partition the data into two disjoint sets, the training set Dtrainand avalidation setDvalid\n(also called a development set ). (Often we use about 80% of the data for the training set, and\n20% for the validation set.) We ﬁt the model on Dtrain(for each setting of \u0015) and then evaluate its\nperformance onDvalid. We then pick the value of \u0015that results in the best validation performance.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 334, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 955}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0335_cf666bd3", "text": "We then pick the value of \u0015that results in the best validation performance. (This optimization method is a 1d example of grid search, discussed in Section 8.8.)\nTo explain the method in more detail, we need some notation. Let us deﬁne the regularized\nempirical risk on a dataset as follows:\nR\u0015(\u0012;D) =1\njDjX\n(x;y)2D`(y;f(x;\u0012)) +\u0015C(\u0012) (4.101)\nFor each\u0015, we compute the parameter estimate\n^\u0012\u0015(Dtrain) = argmin\n\u0012R\u0015(\u0012;Dtrain) (4.102)\nWe then compute the validation risk :\nRval\n\u0015,R0(^\u0012\u0015(Dtrain);Dvalid) (4.103)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.5. Regularization 121\nFigure 4.6: Schematic of 5-fold cross validation. This is an estimate of the population risk , which is the expected loss under the true distribution\np\u0003(x;y).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 335, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 763}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0336_10d64803", "text": "August 27, 2021\n4.5. Regularization 121\nFigure 4.6: Schematic of 5-fold cross validation. This is an estimate of the population risk , which is the expected loss under the true distribution\np\u0003(x;y). Finally we pick\n\u0015\u0003= argmin\n\u00152SRval\n\u0015 (4.104)\n(This requires ﬁtting the model once for each value of \u0015inS, although in some cases, this can be\ndone more eﬃciently.)\nAfter picking \u0015\u0003, we can reﬁt the model to the entire dataset, D=Dtrain[Dvalid, to get\n^\u0012\u0003= argmin\n\u0012R\u0015\u0003(\u0012;D) (4.105)\n4.5.5 Cross-validation\nThe above technique in Section 4.5.4 can work very well. However, if the size of the training set\nis small, leaving aside 20% for a validation set can result in an unreliable estimate of the model\nparameters. A simple but popular solution to this is to use cross validation (CV). The idea is as follows: we\nsplit the training data into Kfolds; then, for each fold k2f1;:::;Kg, we train on all the folds but\nthek’th, and test on the k’th, in a round-robin fashion, as sketched in Figure 4.6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 336, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 993}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0337_57ecac3f", "text": "Formally, we have\nRcv\n\u0015,1\nKKX\nk=1R0(^\u0012\u0015(D\u0000k);Dk) (4.106)\nwhereDkis the data in the k’th fold, andD\u0000kis all the other data. This is called the cross-validated\nrisk. Figure 4.6 illustrates this procedure for K= 5. If we setK=N, we get a method known as\nleave-one-out cross-validation , since we always train on N\u00001items and test on the remaining\none. We can use the CV estimate as an objective inside of an optimization routine to pick the optimal\nhyperparameter, ^\u0015=argmin\u0015Rcv\n\u0015. Finally we combine all the available data (training and validation),\nand re-estimate the model parameters using ^\u0012=argmin\u0012R^\u0015(\u0012;D). See Section 5.4.3 for more details. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n122 Chapter 4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 337, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 721}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0338_64130e0b", "text": "See Section 5.4.3 for more details. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n122 Chapter 4. Statistics\n10−910−710−510−310−1101\nlog lambda2.55.07.510.012.515.017.520.0mean squared error\ntrain_mse\ntest_mse\n(a)\n10−910−710−510−310−1101\nlog lambda51015202530mean squared error\n5-fold cross validation, ntrain = 21 (b)\nFigure 4.7: Ridge regression is applied to a degree 14 polynomial ﬁt to 21 datapoints shown in Figure 4.5\nfor diﬀerent values of the regularizer \u0015. The degree of regularization increases from left to right, so model\ncomplexity decreases from left to right. (a) MSE on train (blue) and test (red) vs log(\u0015). (b) 5-fold cross-\nvalidation estimate of test MSE; error bars are standard error of the mean. Vertical line is the point chosen\nby the one standard error rule. Generated by code at ﬁgures.probml.ai/book1/4.7. 4.5.5.1 The one standard error rule\nCV gives an estimate of ^R\u0015, but does not give any measure of uncertainty.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 338, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 958}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0339_547aa640", "text": "Generated by code at ﬁgures.probml.ai/book1/4.7. 4.5.5.1 The one standard error rule\nCV gives an estimate of ^R\u0015, but does not give any measure of uncertainty. A standard frequentist\nmeasure of uncertainty of an estimate is the standard error of the mean , which is the mean of\nthe sampling distribution of the estimate (see Section 4.7.1). We can compute this as follows. First\nletLn=`(yn;f(xn;^\u0012\u0015(D\u0000n))be the loss on the n’th example, where we use the parameters that\nwere estimated using whichever training fold excludes n. (Note that Lndepends on \u0015, but we drop\nthis from the notation.) Next let ^\u0016=1\nNPN\nn=1Lnbe the empirical mean and ^\u001b2=1\nNPN\nn=1(Ln\u0000^\u0016)2\nbe the empirical variance. Given this, we deﬁne our estimate to be ^\u0016, and the standard error of\nthis estimate to be se(^\u0016) =^\u001bp\nN. Note that \u001bmeasures the intrinsic variability of Lnacross samples,\nwhereas se(^\u0016)measures our uncertainty about the mean ^\u0016.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 339, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 918}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0340_046d5b13", "text": "Note that \u001bmeasures the intrinsic variability of Lnacross samples,\nwhereas se(^\u0016)measures our uncertainty about the mean ^\u0016. Suppose we apply CV to a set of models and compute the mean and se of their estimated risks. A common heuristic for picking a model from these noisy estimates is to pick the value which\ncorresponds to the simplest model whose risk is no more than one standard error above the risk of\nthe best model; this is called the one-standard error rule [HTF01, p216]. 4.5.5.2 Example: ridge regression\nAs an example, consider picking the strength of the `2regularizer for the ridge regression problem\nin Section 4.5.3. In Figure 4.7a, we plot the error vs log(\u0015)on the train set (blue) and test set\n(red curve). We see that the test error has a U-shaped curve, where it decreases as we increase the\nregularizer, and then increases as we start to underﬁt. In Figure 4.7b, we plot the 5-fold CV estimate\nof the test MSE vs log(\u0015).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 340, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 943}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0341_78192823", "text": "In Figure 4.7b, we plot the 5-fold CV estimate\nof the test MSE vs log(\u0015). We see that the minimum CV error is close the optimal value for the test\nset (although it does underestimate the spike in the test error for large lambda, due to the small\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.5. Regularization 123\n0 10 20 30 40 50\nEpochs0.10.20.30.40.50.60.7Loss\nTraining and validation loss\nTraining loss\nValidation loss\n0 10 20 30 40 50\nEpochs0.60.70.80.91.0Accuracy\nTraining and validation accuracy\nTraining acc\nValidation acc\nFigure 4.8: Performance of a text classiﬁer (a neural network applied to a bag of word embeddings using\naverage pooling) vs number of training epochs on the IMDB movie sentiment dataset. Blue = train, red =\nvalidation. (a) Cross entropy loss. Early stopping is triggered at about epoch 25. (b) Classiﬁcation accuracy. Generated by code at ﬁgures.probml.ai/book1/4.8.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 341, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 928}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0342_c7e76040", "text": "Blue = train, red =\nvalidation. (a) Cross entropy loss. Early stopping is triggered at about epoch 25. (b) Classiﬁcation accuracy. Generated by code at ﬁgures.probml.ai/book1/4.8. sample size.)\n4.5.6 Early stopping\nA very simple form of regularization, which is often very eﬀective in practice (especially for complex\nmodels), is known as early stopping . This leverages the fact that optimization algorithms are\niterative, and so they take many steps to move away from the initial parameter estimates. If we detect\nsigns of overﬁtting (by monitoring performance on the validation set), we can stop the optimization\nprocess, to prevent the model memorizing too much information about the training set. See Figure 4.8\nfor an illustration. 4.5.7 Using more data\nAs the amount of data increases, the chance of overﬁtting (for a model of ﬁxed complexity) decreases\n(assuming the data contains suitably informative examples, and is not too redundant). This is\nillustrated in Figure 4.9.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 342, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 981}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0343_889bc924", "text": "This is\nillustrated in Figure 4.9. We show the MSE on the training and test sets for four diﬀerent models\n(polynomials of increasing degree) as a function of the training set size N. (A plot of error vs training\nset size is known as a learning curve .) The horizontal black line represents the Bayes error , which\nis the error of the optimal predictor (the true model) due to inherent noise. (In this example, the\ntrue model is a degree 2 polynomial, and the noise has a variance of \u001b2= 4; this is called the noise\nﬂoor, since we cannot go below it.)\nWe notice several interesting things. First, the test error for degree 1 remains high, even as N\nincreases, since the model is too simple to capture the truth; this is called underﬁtting. The test\nerror for the other models decreases to the optimal level (the noise ﬂoor), but it decreases more\nrapidly for the simpler models, since they have fewer parameters to estimate.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 343, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 923}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0344_2a23b0c4", "text": "The test\nerror for the other models decreases to the optimal level (the noise ﬂoor), but it decreases more\nrapidly for the simpler models, since they have fewer parameters to estimate. The gap between the\ntest error and training error is larger for more complex models, but decreases as Ngrows. Another interesting thing we can note is that the training error (blue line) initially increases with\nN, at least for the models that are suﬃciently ﬂexible. The reason for this is as follows: as the data\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n124 Chapter 4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 344, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 574}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0345_1005c0e0", "text": "The reason for this is as follows: as the data\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n124 Chapter 4. Statistics\n0 25 50 75 100 125 150 175 200\nsize of training set0.02.55.07.510.012.515.017.520.0mse\ntruth = degree 2, model = degree 1\ntest\ntrain\n(a)\n0 25 50 75 100 125 150 175 200\nsize of training set0.02.55.07.510.012.515.017.520.0mse\ntruth = degree 2, model = degree 2\ntest\ntrain (b)\n0 25 50 75 100 125 150 175 200\nsize of training set0.02.55.07.510.012.515.017.520.0mse\ntruth = degree 2, model = degree 10\ntest\ntrain\n(c)\n0 25 50 75 100 125 150 175 200\nsize of training set0.02.55.07.510.012.515.017.520.0mse\ntruth = degree 2, model = degree 20\ntest\ntrain (d)\nFigure 4.9: MSE on training and test sets vs size of training set, for data generated from a degree 2 polynomial\nwith Gaussian noise of variance \u001b2= 4. We ﬁt polynomial models of varying degree to this data. Generated\nby code at ﬁgures.probml.ai/book1/4.9.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 345, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 939}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0346_d0c756c2", "text": "We ﬁt polynomial models of varying degree to this data. Generated\nby code at ﬁgures.probml.ai/book1/4.9. set gets larger, we observe more distinct input-output pattern combinations, so the task of ﬁtting the\ndata becomes harder. However, eventually the training set will come to resemble the test set, and\nthe error rates will converge, and will reﬂect the optimal performance of that model. 4.6 Bayesian statistics *\nSo far, we have discussed several ways to estimate parameters from data. However, these approaches\nignore any uncertainty in the estimates, which can be important for some applications, such as\nactive learning, or avoiding overﬁtting, or just knowing how much to trust the estimate of some\nscientiﬁcally meaningful quantity. In statistics, modeling uncertainty about parameters using a\nprobability distribution (as opposed to just computing a point estimate) is known as inference . In this section, we use the posterior distribution to represent out uncertainty.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 346, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 981}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0347_2fcac1dc", "text": "In this section, we use the posterior distribution to represent out uncertainty. This is the\napproach adopted in the ﬁeld of Bayesian statistics . We give a brief introduction here, but more\ndetails can be found in the sequel to this book, [Mur22], as well as other good books, such as [McE20;\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.6. Bayesian statistics * 125\nGel+14]. To compute the posterior, we start with a priordistribution p(\u0012), which reﬂects what we know\nbefore seeing the data. We then deﬁne a likelihood function p(Dj\u0012), which reﬂects the data we\nexpect to see for each setting of the parameters. We then use Bayes rule to condition the prior on\nthe observed data to compute the posterior p(\u0012jD)as follows:\np(\u0012jD) =p(\u0012)p(Dj\u0012)\np(D)=p(\u0012)p(Dj\u0012)R\np(\u00120)p(Dj\u00120)d\u00120(4.107)\nThe denominator p(D)is called the marginal likelihood , since it is computed by marginalizing\nover(orintegrating out ) the unknown \u0012.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 347, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 949}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0348_a86d9582", "text": "This can be interpreted as the average probability of\nthe data, where the average is wrt the prior. Note, however, that p(D)is a constant, independent of\n\u0012, so we will often ignore it when we just want to infer the relative probabilities of \u0012values. Equation (4.107) is analogous to the use of Bayes rule for COVID-19 testing in Section 2.3.1. The\ndiﬀerence is that the unknowns correspond to parameters of a statistical model, rather than the\nunknown disease state of a patient. In addition, we usually condition on a set of observations D, as\nopposed to a single observation (such as a single test outcome). In particular, for a supervised or\nconditional model, the observed data has the form D=f(xn;yn) :n= 1 :Ng. For an unsupervised\nor unconditional model, the observed data has the form D=f(yn) :n= 1 :Ng. Once we have computed the posterior over the parameters, we can compute the posterior\npredictive distribution over outputs given inputs by marginalizing out the unknown parameters.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 348, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 991}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0349_e7fd345c", "text": "Once we have computed the posterior over the parameters, we can compute the posterior\npredictive distribution over outputs given inputs by marginalizing out the unknown parameters. In the supervised/ conditional case, this becomes\np(yjx;D) =Z\np(yjx;\u0012)p(\u0012jD)d\u0012 (4.108)\nThis can be viewed as a form of Bayes model averaging (BMA), since we are making predictions\nusing an inﬁnite set of models (parameter values), each one weighted by how likely it is. The use of\nBMA reduces the chance of overﬁtting (Section 1.2.3), since we are not just using the single best\nmodel. 4.6.1 Conjugate priors\nIn this section, we consider a set of (prior, likelihood) pairs for which we can compute the posterior\nin closed form. In particular, we will use priors that are “conjugate” to the likelihood. We say that\na priorp(\u0012)2Fis aconjugate prior for a likelihood function p(Dj\u0012)if the posterior is in the\nsame parameterized family as the prior, i.e., p(\u0012jD)2F. In other words, Fis closed under Bayesian\nupdating.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 349, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 994}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0350_2a430f72", "text": "In other words, Fis closed under Bayesian\nupdating. If the family Fcorresponds to the exponential family (deﬁned in Section 3.4), then the\ncomputations can be performed in closed form. In the sections below, we give some common examples of this framework, which we will use later in\nthe book. For simplicity, we focus on unconditional models (i.e., there are only outcomes or targets\ny, and no inputs or features x); we relax this assumption in Section 4.6.7. 4.6.2 The beta-binomial model\nSuppose we toss a coin Ntimes, and want to infer the probability of heads. Let yn= 1denote the\nevent that the n’th trial was heads, yn= 0represent the event that the n’th trial was tails, and let\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n126 Chapter 4. Statistics\nD=fyn:n= 1 :Ngbe all the data. We assume yn\u0018Ber(\u0012), where\u00122[0;1]is the rate parameter\n(probability of heads). In this section, we discuss how to compute p(\u0012jD).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 350, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 932}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0351_caca02c5", "text": "Statistics\nD=fyn:n= 1 :Ngbe all the data. We assume yn\u0018Ber(\u0012), where\u00122[0;1]is the rate parameter\n(probability of heads). In this section, we discuss how to compute p(\u0012jD). 4.6.2.1 Bernoulli likelihood\nWe assume the data are iidorindependent and identically distributed . Thus the likelihood\nhas the form\np(Dj\u0012) =NY\nn=1\u0012yn(1\u0000\u0012)1\u0000yn=\u0012N1(1\u0000\u0012)N0(4.109)\nwhere we have deﬁned N1=PN\nn=1I(yn= 1)andN0=PN\nn=1I(yn= 0), representing the number of\nheads and tails. These counts are called the suﬃcient statistics of the data, since this is all we\nneed to know about Dto infer\u0012. The total count, N=N0+N1, is called the sample size. 4.6.2.2 Binomial likelihood\nNote that we can also consider a Binomial likelihood model, in which we perform Ntrials and observe\nthe number of heads, y, rather than observing a sequence of coin tosses. Now the likelihood has the\nfollowing form:\np(Dj\u0012) = Bin(yjN;\u0012) =\u0012N\ny\u0013\n\u0012y(1\u0000\u0012)N\u0000y(4.110)\nThe scaling factor\u0000N\ny\u0001\nis independent of \u0012, so we can ignore it.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 351, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 973}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0352_0d52577d", "text": "Now the likelihood has the\nfollowing form:\np(Dj\u0012) = Bin(yjN;\u0012) =\u0012N\ny\u0013\n\u0012y(1\u0000\u0012)N\u0000y(4.110)\nThe scaling factor\u0000N\ny\u0001\nis independent of \u0012, so we can ignore it. Thus this likelihood is proportional\nto the Bernoulli likelihood in Equation (4.109), so our inferences about \u0012will be the same for both\nmodels. 4.6.2.3 Prior\nTo simplify the computations, we will assume that the prior p(\u0012)2Fis a conjugate prior for the\nlikelihood function p(yj\u0012). This means that the posterior is in the same parameterized family as the\nprior, i.e.,p(\u0012jD)2F. To ensure this property when using the Bernoulli (or Binomial) likelihood, we should use a prior\nof the following form:\np(\u0012)/\u0012`\u000b\u00001(1\u0000\u0012)`\f\u00001= Beta(\u0012j`\u000b;`\f) (4.111)\nWe recognize this as the pdf of a beta distribution (see Section 2.7.4).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 352, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 766}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0353_aeaaf5b3", "text": "4.6.2.4 Posterior\nIf we multiply the Bernoulli likelihood in Equation (4.109) with the beta prior in Equation (2.136)\nwe get a beta posterior:\np(\u0012jD)/\u0012N1(1\u0000\u0012)N0\u0012`\u000b\u00001(1\u0000\u0012)`\f\u00001(4.112)\n/Beta(\u0012j`\u000b+N1;`\f+N0) (4.113)\n= Beta(\u0012ja\u000b;a\f) (4.114)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.6. Bayesian statistics * 127\n0 0.2 0.4 0.6 0.8 100.511.522.53\nprior Be(2.0, 2.0)\nlik Be(5.0, 2.0)\npost Be(6.0, 3.0)\n(a)\n0 0.2 0.4 0.6 0.8 100.511.522.5\nprior Be(1.0, 1.0)\nlik Be(5.0, 2.0)\npost Be(5.0, 2.0) (b)\nFigure 4.10: Updating a Beta prior with a Bernoulli likelihood with suﬃcient statistics N1= 4;N0= 1. (a)\nBeta(2,2) prior. (b) Uniform Beta(1,1) prior. Generated by code at ﬁgures.probml.ai/book1/4.10. wherea\u000b,`\u000b+N1anda\f,`\f+N0are the parameters of the posterior. Since the posterior has the same\nfunctional form as the prior, we say that the beta distribution is a conjugate prior for the Bernoulli\nlikelihood. The parameters of the prior are called hyper-parameters .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 353, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 990}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0354_9ff78304", "text": "The parameters of the prior are called hyper-parameters . It is clear that (in this example) the\nhyper-parameters play a role analogous to the suﬃcient statistics; they are therefore often called\npseudo counts . We see that we can compute the posterior by simply adding the observed counts\n(from the likelihood) to the pseudo counts (from the prior). The strength of the prior is controlled by`N=`\u000b+`\f; this is called the equivalent sample size ,\nsince it plays a role analogous to the observed sample size, N=N0+N1. 4.6.2.5 Example\nFor example, suppose we set`\u000b=`\f= 2. This is like saying we believe we have already seen two heads\nand two tails before we see the actual data; this is a very weak preference for the value of \u0012= 0:5. The eﬀect of using this prior is illustrated in Figure 4.10a. We see the posterior (blue line) is a\n“compromise” between the prior (red line) and the likelihood (black line).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 354, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 907}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0355_02f59872", "text": "The eﬀect of using this prior is illustrated in Figure 4.10a. We see the posterior (blue line) is a\n“compromise” between the prior (red line) and the likelihood (black line). If we set`\u000b=`\f= 1, the corresponding prior becomes the uniform distribution:\np(\u0012) = Beta(\u0012j1;1)/\u00120(1\u0000\u0012)0= Unif(\u0012j0;1) (4.115)\nThe eﬀect of using this prior is illustrated in Figure 4.10b. We see that the posterior has exactly the\nsame shape as the likelihood, since the prior was “ uninformative ”. 4.6.2.6 Posterior mode (MAP estimate)\nThe most probable value of the parameter is given by the MAP estimate\n^\u0012map= arg max\n\u0012p(\u0012jD) (4.116)\n= arg max\n\u0012logp(\u0012jD) (4.117)\n= arg max\n\u0012logp(\u0012) + logp(Dj\u0012) (4.118)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n128 Chapter 4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 355, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 755}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0356_04c96030", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n128 Chapter 4. Statistics\nOne can show that this is given by\n^\u0012map=`\u000b+N1\u00001\n`\u000b+N1\u00001+`\f+N0\u00001(4.119)\nIf we use a Beta(\u0012j2;2)prior, this amounts to add-one smoothing :\n^\u0012map=N1+ 1\nN1+ 1 +N0+ 1=N1+ 1\nN+ 2(4.120)\nIf we use a uniform prior, p(\u0012)/1, the MAP estimate becomes the MLE, since logp(\u0012) = 0:\n^\u0012mle= arg max\n\u0012logp(Dj\u0012) (4.121)\nWhen we use a Beta prior, the uniform distribution is`\u000b=`\f= 1. In this case, the MAP estimate\nreduces to the MLE:\n^\u0012mle=N1\nN1+N0=N1\nN(4.122)\nIfN1= 0, we will estimate that p(Y= 1) = 0:0, which says that we do not predict any future\nobservations to be 1. This is a very extreme estimate, that is likely due to insuﬃcient data. We can\nsolve this problem using a MAP estimate with a stronger prior, or using a fully Bayesian approach,\nin which we marginalize out \u0012instead of estimating it, as explained in Section 4.6.2.9.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 356, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 891}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0357_e466fad8", "text": "We can\nsolve this problem using a MAP estimate with a stronger prior, or using a fully Bayesian approach,\nin which we marginalize out \u0012instead of estimating it, as explained in Section 4.6.2.9. 4.6.2.7 Posterior mean\nThe posterior mode can be a poor summary of the posterior, since it corresponds to a single point. The posterior mean is a more robust estimate, since it integrates over the whole space. Ifp(\u0012jD) = Beta(\u0012ja\u000b;a\f), then the posterior mean is given by\n\u0012,E[\u0012jD] =a\u000b\na\f+a\u000b=a\u000b\naN(4.123)\nwhereaN=a\f+a\u000bis the strength (equivalent sample size) of the posterior. We will now show that the posterior mean is a convex combination of the prior mean, m=`\u000b=`N\n(where`N,`\u000b+`\fis the prior strength), and the MLE: ^\u0012mle=N1\nN:\nE[\u0012jD] =`\u000b+N1\n`\u000b+N1+`\f+N0=`Nm+N1\nN+`N=`N\nN+`Nm+N\nN+`NN1\nN=\u0015m+ (1\u0000\u0015)^\u0012mle(4.124)\nwhere\u0015=`NaNis the ratio of the prior to posterior equivalent sample size. So the weaker the prior,\nthe smaller is \u0015, and hence the closer the posterior mean is to the MLE.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 357, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 976}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0358_7b4378d5", "text": "So the weaker the prior,\nthe smaller is \u0015, and hence the closer the posterior mean is to the MLE. 4.6.2.8 Posterior variance\nTo capture some notion of uncertainty in our estimate, a common approach is to compute the\nstandard error of our estimate, which is just the posterior standard deviation:\nse(\u0012) =p\nV[\u0012jD] (4.125)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.6. Bayesian statistics * 129\nIn the case of the Bernoulli model, we showed that the posterior is a beta distribution. The variance\nof the beta posterior is given by\nV[\u0012jD] =a\u000ba\f\n(a\u000b+a\f)2(a\u000b+a\f+1)=E[\u0012jD]2a\f\na\u000b(1+a\u000b+a\f)(4.126)\nwherea\u000b=`\u000b+N1anda\f=`\f+N0. IfN\u001d`\u000b+`\f, this simpliﬁes to\nV[\u0012jD]\u0019N1N0\nN3=^\u0012(1\u0000^\u0012)\nN(4.127)\nwhere ^\u0012is the MLE. Hence the standard error is given by\n\u001b=p\nV[\u0012jD]\u0019s\n^\u0012(1\u0000^\u0012)\nN(4.128)\nWe see that the uncertainty goes down at a rate of 1=p\nN. We also see that the uncertainty (variance)\nis maximized when ^\u0012= 0:5, and is minimized when ^\u0012is close to 0 or 1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 358, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 971}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0359_bec1a056", "text": "We also see that the uncertainty (variance)\nis maximized when ^\u0012= 0:5, and is minimized when ^\u0012is close to 0 or 1. This makes sense, since it is\neasier to be sure that a coin is biased than to be sure that it is fair. 4.6.2.9 Posterior predictive\nSuppose we want to predict future observations. A very common approach is to ﬁrst compute an\nestimate of the parameters based on training data, ^\u0012(D), and then to plug that parameter back into\nthe model and use p(yj^\u0012)to predict the future; this is called a plug-in approximation . However,\nthis can result in overﬁtting. As an extreme example, suppose we have seen N= 3heads in a row. The MLE is ^\u0012= 3=3 = 1. However, if we use this estimate, we would predict that tails are impossible. One solution to this is to compute a MAP estimate, and plug that in, as we discussed in Section 4.5.1. Here we discuss a fully Bayesian solution, in which we marginalize out \u0012.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 359, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 911}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0360_b46ddeac", "text": "One solution to this is to compute a MAP estimate, and plug that in, as we discussed in Section 4.5.1. Here we discuss a fully Bayesian solution, in which we marginalize out \u0012. Bernoulli model\nFor the Bernoulli model, the resulting posterior predictive distribution has the form\np(y= 1jD) =Z1\n0p(y= 1j\u0012)p(\u0012jD)d\u0012 (4.129)\n=Z1\n0\u0012Beta(\u0012ja\u000b;a\f)d\u0012=E[\u0012jD] =a\u000b\na\u000b+a\f(4.130)\nIn Section 4.5.1, we had to use the Beta(2,2) prior to recover add-one smoothing, which is a\nrather unnatural prior. In the Bayesian approach, we can get the same eﬀect using a uniform prior,\np(\u0012) = Beta(\u0012j1;1), since the predictive distribution becomes\np(y= 1jD) =N1+ 1\nN1+N0+ 2(4.131)\nThis is known as Laplace’s rule of succession . See Figure 4.11 for an illustration of this in the\nsequential setting. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n130 Chapter 4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 360, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 846}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0361_a8b68567", "text": "See Figure 4.11 for an illustration of this in the\nsequential setting. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n130 Chapter 4. Statistics\n(0,0)\n(1,0) (0,1)\n(2,0) (1,1) (0,2)\n(0,3) (1,2) (2,1) (3,0)\n(3,1) (2,2) (1,3)\nFigure 4.11: Illustration of sequential Bayesian updating for the beta-Bernoulli model. Each colored box\nrepresents the predicted distribution p(xtjht), whereht= (N1;t;N0;t)is the suﬃcient statistic derived from\nhistory of observations up until time t, namely the total number of heads and tails. The probability of heads\n(blue bar) is given by p(xt= 1jht) = (Nt;1+ 1)=(t+ 2), assuming we start with a uniform Beta(\u0012j1;1)prior. From Figure 3 of [Ort+19]. Used with kind permission of Pedro Ortega. Binomial model\nNow suppose we were interested in predicting the number of heads in M > 1future coin tossing\ntrials, i.e., we are using the binomial model instead of the Bernoulli model.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 361, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 918}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0362_e331699b", "text": "Binomial model\nNow suppose we were interested in predicting the number of heads in M > 1future coin tossing\ntrials, i.e., we are using the binomial model instead of the Bernoulli model. The posterior over \u0012is\nthe same as before, but the posterior predictive distribution is diﬀerent:\np(yjD;M) =Z1\n0Bin(yjM;\u0012)Beta(\u0012ja\u000b;a\f)d\u0012 (4.132)\n=\u0012M\ny\u00131\nB(a\u000b;a\f)Z1\n0\u0012y(1\u0000\u0012)M\u0000y\u0012a\u000b\u00001(1\u0000\u0012)a\f\u00001d\u0012 (4.133)\nWe recognize the integral as the normalization constant for a Beta(a\u000b+y;M\u0000y+a\f)distribution. Hence\nZ1\n0\u0012y+a\u000b\u00001(1\u0000\u0012)M\u0000y+a\f\u00001d\u0012=B(y+a\u000b;M\u0000y+a\f) (4.134)\nThus we ﬁnd that the posterior predictive is given by the following, known as the (compound)\nbeta-binomial distribution:\nBb(xjM;a\u000b;a\f),\u0012M\nx\u0013B(x+a\u000b;M\u0000x+a\f)\nB(a\u000b;a\f)(4.135)\nIn Figure 4.12(a), we plot the posterior predictive density for M= 10after seeing N1= 4heads\nandN0= 1tails, when using a uniform Beta(1,1) prior.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 362, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 852}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0363_c8cb29a6", "text": "In Figure 4.12(b), we plot the plug-in\napproximation, given by\np(\u0012jD)\u0019\u000e(\u0012\u0000^\u0012) (4.136)\np(yjD;M) =Z1\n0Bin(yjM;\u0012)p(\u0012jD)d\u0012= Bin(yjM;^\u0012) (4.137)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.6. Bayesian statistics * 131\n0 1 2 3 4 5 6 7 8 9 1000.020.040.060.080.10.120.140.160.180.2posterior predictive\n(a)\n0 1 2 3 4 5 6 7 8 9 1000.050.10.150.20.250.30.35plugin predictive (b)\nFigure 4.12: (a) Posterior predictive distributions for 10 future trials after seeing N1= 4heads andN0= 1\ntails. (b) Plug-in approximation based on the same data. In both cases, we use a uniform prior. Generated by\ncode at ﬁgures.probml.ai/book1/4.12. where ^\u0012is the MAP estimate. Looking at Figure 4.12, we see that the Bayesian prediction has\nlonger tails, spreading its probablity mass more widely, and is therefore less prone to overﬁtting and\nblack-swan type paradoxes.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 363, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 877}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0364_4cc1290a", "text": "Looking at Figure 4.12, we see that the Bayesian prediction has\nlonger tails, spreading its probablity mass more widely, and is therefore less prone to overﬁtting and\nblack-swan type paradoxes. (Note that we use a uniform prior in both cases, so the diﬀerence is not\narising due to the use of a prior; rather, it is due to the fact that the Bayesian approach integrates\nout the unknown parameters when making its predictions.)\n4.6.2.10 Marginal likelihood\nThemarginal likelihood orevidence for a modelMis deﬁned as\np(DjM ) =Z\np(\u0012jM)p(Dj\u0012;M)d\u0012 (4.138)\nWhen performing inference for the parameters of a speciﬁc model, we can ignore this term, since it is\nconstant wrt \u0012. However, this quantity plays a vital role when choosing between diﬀerent models,\nas we discuss in Section 5.2.2. It is also useful for estimating the hyperparameters from data (an\napproach known as empirical Bayes), as we discuss in Section 4.6.5.3. In general, computing the marginal likelihood can be hard.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 364, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 977}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0365_a8702ca0", "text": "It is also useful for estimating the hyperparameters from data (an\napproach known as empirical Bayes), as we discuss in Section 4.6.5.3. In general, computing the marginal likelihood can be hard. However, in the case of the beta-\nBernoulli model, the marginal likelihood is proportional to the ratio of the posterior normalizer to\nthe prior normalizer. To see this, recall that the posterior for the beta-binomial models is given by\np(\u0012jD) =Beta(\u0012ja0;b0), wherea0=a+N1andb0=b+N0. We know the normalization constant of\nthe posterior is B(a0;b0). Hence\np(\u0012jD) =p(Dj\u0012)p(\u0012)\np(D)(4.139)\n=1\np(D)\u00141\nB(a;b)\u0012a\u00001(1\u0000\u0012)b\u00001\u0015\u0014\u0012N\nN1\u0013\n\u0012N1(1\u0000\u0012)N0\u0015\n(4.140)\n=\u0012\nN\nN1\u00131\np(D)1\nB(a;b)\u0002\n\u0012a+N1\u00001(1\u0000\u0012)b+N0\u00001\u0003\n(4.141)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n132 Chapter 4. Statistics\nSo\n1\nB(a+N1;b+N0)=\u0012N\nN1\u00131\np(D)1\nB(a;b)(4.142)\np(D) =\u0012N\nN1\u0013B(a+N1;b+N0)\nB(a;b)(4.143)\nThe marginal likelihood for the beta-Bernoulli model is the same as above, except it is missing the\u0012N\nN1\u0013\nterm.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 365, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 973}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0366_9e6e8629", "text": "4.6.2.11 Mixtures of conjugate priors\nThe beta distribution is a conjugate prior for the binomial likelihood, which enables us to easily\ncompute the posterior in closed form, as we have seen. However, this prior is rather restrictive. For\nexample, suppose we want to predict the outcome of a coin toss at a casino, and we believe that the\ncoin may be fair, but may equally likely be biased towards heads. This prior cannot be represented\nby a beta distribution. Fortunately, it can be represented as a mixture of beta distributions . For example, we might use\np(\u0012) = 0:5 Beta(\u0012j20;20) + 0:5 Beta(\u0012j30;10) (4.144)\nIf\u0012comes from the ﬁrst distribution, the coin is fair, but if it comes from the second, it is biased\ntowards heads. We can represent a mixture by introducing a latent indicator variable h, whereh=kmeans that\n\u0012comes from mixture component k. The prior has the form\np(\u0012) =X\nkp(h=k)p(\u0012jh=k) (4.145)\nwhere each p(\u0012jh=k)is conjugate, and p(h=k)are called the (prior) mixing weights.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 366, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 990}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0367_8614ebd2", "text": "The prior has the form\np(\u0012) =X\nkp(h=k)p(\u0012jh=k) (4.145)\nwhere each p(\u0012jh=k)is conjugate, and p(h=k)are called the (prior) mixing weights. One can\nshow (Exercise 4.6) that the posterior can also be written as a mixture of conjugate distributions as\nfollows:\np(\u0012jD) =X\nkp(h=kjD)p(\u0012jD;h=k) (4.146)\nwherep(h=kjD)are the posterior mixing weights given by\np(h=kjD) =p(h=k)p(Djh=k)P\nk0p(h=k0)p(Djh=k0)(4.147)\nHere the quantity p(Djh=k)is the marginal likelihood for mixture component k(see Section 4.6.2.10). Returning to our example above, if we have the prior in Equation (4.144), and we observe N1= 20\nheads andN0= 10tails, then, using Equation (4.143), the posterior becomes\np(\u0012jD) = 0:346 Beta(\u0012j40;30) + 0:654 Beta(\u0012j30;20) (4.148)\nSee Figure 4.13 for an illustration. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 367, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 847}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0368_62f0c97c", "text": "Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.6. Bayesian statistics * 133\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.511.522.533.544.55mixture of Beta distributions\n \nprior\nposterior\nFigure 4.13: A mixture of two Beta distributions. Generated by code at ﬁgures.probml.ai/book1/4.13. We can compute the posterior probability that the coin is biased towards heads as follows:\nPr(\u0012>0:5jD) =X\nkPr(\u0012>0:5jD;h=k)p(h=kjD) = 0:9604 (4.149)\nIf we just used a single Beta(20,20) prior, we would get a slightly smaller value of Pr(\u0012>0:5jD) =\n0:8858. So if we were “suspicious” initially that the casino might be using a biased coin, our fears\nwould be conﬁrmed more quickly than if we had to be convinced starting with an open mind. 4.6.3 The Dirichlet-multinomial model\nIn this section, we generalize the results from Section 4.6.2 from binary variables (e.g., coins) to\nK-ary variables (e.g., dice).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 368, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 921}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0369_b23b536b", "text": "4.6.3 The Dirichlet-multinomial model\nIn this section, we generalize the results from Section 4.6.2 from binary variables (e.g., coins) to\nK-ary variables (e.g., dice). 4.6.3.1 Likelihood\nLetY\u0018Cat(\u0012)be a discrete random variable drawn from a categorical distribution. The likelihood\nhas the form\np(Dj\u0012) =NY\nn=1Cat(ynj\u0012) =NY\nn=1CY\nc=1\u0012I(yn=c)\nc =CY\nc=1\u0012Nc\nc (4.150)\nwhereNc=P\nnI(yn=c). 4.6.3.2 Prior\nThe conjugate prior for a categorical distribution is the Dirichlet distribution , which is a mul-\ntivariate generalization of the beta distribution. This has support over the probability simplex ,\ndeﬁned by\nSK=f\u0012: 0\u0014\u0012k\u00141;KX\nk=1\u0012k= 1g (4.151)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n134 Chapter 4. Statistics\n1\n2\n3\n(a)\n (b)\n(c)\n (d)\nFigure 4.14: (a) The Dirichlet distribution when K= 3deﬁnes a distribution over the simplex, which can be\nrepresented by the triangular surface. Points on this surface satisfy 0\u0014\u0012k\u00141andP3\nk=1\u0012k= 1. Generated\nby code at ﬁgures.probml.ai/book1/4.14.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 369, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 999}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0370_c1ffe8c5", "text": "Points on this surface satisfy 0\u0014\u0012k\u00141andP3\nk=1\u0012k= 1. Generated\nby code at ﬁgures.probml.ai/book1/4.14. (b) Plot of the Dirichlet density for`\u000b= (20;20;20). (c) Plot of the\nDirichlet density for`\u000b= (3;3;20). (d) Plot of the Dirichlet density for`\u000b= (0:1;0:1;0:1). Generated by code\nat ﬁgures.probml.ai/book1/4.14. The pdf of the Dirichlet is deﬁned as follows:\nDir(\u0012j`\u000b),1\nB(`\u000b)KY\nk=1\u0012`\u000bk\u00001\nkI(\u00122SK) (4.152)\nwhereB(`\u000b)is the multivariate beta function,\nB(`\u000b),QK\nk=1\u0000(`\u000bk)\n\u0000(PK\nk=1`\u000bk)(4.153)\nFigure 4.14 shows some plots of the Dirichlet when K= 3. We see that`\u000b0=P\nk`\u000bkcontrols the\nstrength of the distribution (how peaked it is), and the`\u000bkcontrol where the peak occurs. For example,\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 370, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 765}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0371_28e7b763", "text": "For example,\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.6. Bayesian statistics * 135\n1 2 3 4 50.00.51.0\n1 2 3 4 50.00.51.0\n1 2 3 4 50.00.51.0\n1 2 3 4 50.00.51.0\n1 2 3 4 50.00.51.0Samples from Dir (alpha=0.1)\n(a)\n1 2 3 4 50.00.51.0\n1 2 3 4 50.00.51.0\n1 2 3 4 50.00.51.0\n1 2 3 4 50.00.51.0\n1 2 3 4 50.00.51.0Samples from Dir (alpha=1.0) (b)\nFigure 4.15: Samples from a 5-dimensional symmetric Dirichlet distribution for diﬀerent parameter values. (a)`\u000b= (0:1;:::; 0:1). This results in very sparse distributions, with many 0s. (b)`\u000b= (1;:::; 1). This results\nin more uniform (and dense) distributions. Generated by code at ﬁgures.probml.ai/book1/4.15. Dir(1;1;1)is a uniform distribution, Dir(2;2;2)is a broad distribution centered at (1=3;1=3;1=3),\nandDir(20;20;20)is a narrow distribution centered at (1=3;1=3;1=3).Dir(3;3;20)is an asymmetric\ndistribution that puts more density in one of the corners. If`\u000bk<1for allk, we get “spikes” at\nthe corners of the simplex.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 371, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1000}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0372_eb34bc8f", "text": "If`\u000bk<1for allk, we get “spikes” at\nthe corners of the simplex. Samples from the distribution when`\u000bk<1will be sparse, as shown in\nFigure 4.15. 4.6.3.3 Posterior\nWe can combine the multinomial likelihood and Dirichlet prior to compute the posterior, as follows:\np(\u0012jD)/p(Dj\u0012)Dir(\u0012j`\u000b) (4.154)\n=\"Y\nk\u0012Nk\nk#\"Y\nk\u0012`\u000bk\u00001\nk#\n(4.155)\n= Dir(\u0012j`\u000b1+N1;:::;`\u000bK+NK) (4.156)\n= Dir(\u0012ja\u000b) (4.157)\nwherea\u000bk=`\u000bk+Nkare the parameters of the posterior. So we see that the posterior can be computed\nby adding the empirical counts to the prior counts. The posterior mean is given by\n\u0012k=a\u000bkPK\nk0=1a\u000bk0(4.158)\nThe posterior mode, which corresponds to the MAP estimate, is given by\n^\u0012k=a\u000bk\u00001PK\nk0=1(a\u000bk0\u00001)(4.159)\nIf we use`\u000bk= 1, corresponding to a uniform prior, the MAP becomes the MLE:\n^\u0012k=Nk=N (4.160)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n136 Chapter 4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 372, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 856}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0373_07928eff", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n136 Chapter 4. Statistics\n(See Section 4.2.4 for a more direct derivation of this result.)\n4.6.3.4 Posterior predictive\nThe posterior predictive distribution is given by\np(y=kjD) =Z\np(y=kj\u0012)p(\u0012jD)d\u0012 (4.161)\n=Z\n\u0012kp(\u0012kjD)d\u0012k=E[\u0012kjD] =a\u000bkP\nk0a\u000bk0(4.162)\nIn other words, the posterior predictive distribution is given by\np(yjD) = Cat(yj\u0012) (4.163)\nwhere\u0012,E[\u0012jD]are the posterior mean parameters. If instead we plug-in the MAP estimate, we\nwill suﬀer from the zero-count problem. The only way to get the same eﬀect as add-one smoothing is\nto use a MAP estimate with`\u000bc= 2. Equation (4.162) gives the probability of a single future event, conditioned on past observations\ny= (y1;:::;yN). In some cases, we want to know the probability of observing a batch of future data,\nsay~y= (~y1;:::; ~yM).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 373, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 830}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0374_9b5d8e6e", "text": "In some cases, we want to know the probability of observing a batch of future data,\nsay~y= (~y1;:::; ~yM). We can compute this as follows:\np(~yjy) =p(~y;y)\np(y)(4.164)\nThe denominator is the marginal likelihood of the training data, and the numerator is the marginal\nlikelihood of the training and future test data. We discuss how to compute such marginal likelihoods\nin Section 4.6.3.5. 4.6.3.5 Marginal likelihood\nBy the same reasoning as in Section 4.6.2.10, one can show that the marginal likelihood for the\nDirichlet-categorical model is given by\np(D) =B(N+\u000b)\nB(\u000b)(4.165)\nwhere\nB(\u000b) =QK\nk=1\u0000(\u000bk)\n\u0000(P\nk\u000bk)(4.166)\nHence we can rewrite the above result in the following form, which is what is usually presented in\nthe literature:\np(D) =\u0000(P\nk\u000bk)\n\u0000(N+P\nk\u000bk)Y\nk\u0000(Nk+\u000bk)\n\u0000(\u000bk)(4.167)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 374, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 862}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0375_b5815559", "text": "August 27, 2021\n4.6. Bayesian statistics * 137\n4.6.4 The Gaussian-Gaussian model\nIn this section, we derive the posterior for the parameters of a Gaussian distribution. For simplicity,\nwe assume the variance is known. (The general case is discussed in the sequel to this book, [Mur22],\nas well as other standard references on Bayesian statistics.)\n4.6.4.1 Univariate case\nIf\u001b2is a known constant, the likelihood for \u0016has the form\np(Dj\u0016)/exp \n\u00001\n2\u001b2NX\nn=1(yn\u0000\u0016)2! (4.168)\nOne can show that the conjugate prior is another Gaussian, N(\u0016j`m;`\u001c2). Applying Bayes’ rule for\nGaussians, as in Section 4.6.4.1, we ﬁnd that the corresponding posterior is given by\np(\u0016jD;\u001b2) =N(\u0016jam;a\u001c2) (4.169)\na\u001c2=1\nN\n\u001b2+1`\u001c2=\u001b2`\u001c2\nN`\u001c2+\u001b2(4.170)\nam=a\u001c2\u0012`m\n`\u001c2+Ny\n\u001b2\u0013\n=\u001b2\nN`\u001c2+\u001b2`m+N`\u001c2\nN`\u001c2+\u001b2y (4.171)\nwherey,1\nNPN\nn=1ynis the empirical mean. This result is easier to understand if we work in terms of the precision parameters, which are\njust inverse variances.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 375, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 938}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0376_195be4e4", "text": "This result is easier to understand if we work in terms of the precision parameters, which are\njust inverse variances. Speciﬁcally, let \u0014= 1=\u001b2be the observation precision, and`\u0015= 1=`\u001c2be the\nprecision of the prior. We can then rewrite the posterior as follows:\np(\u0016jD;\u0014) =N(\u0016jam;a\u0015\u00001) (4.172)\na\u0015=`\u0015+N\u0014 (4.173)\nam=N\u0014y+`\u0015`m\na\u0015=N\u0014\nN\u0014+`\u0015y+`\u0015\nN\u0014+`\u0015`m (4.174)\nThese equations are quite intuitive: the posterior precisiona\u0015is the prior precision`\u0015plusNunits of\nmeasurement precision \u0014. Also, the posterior meanamis a convex combination of the empirical mean\nyand the prior mean`m. This makes it clear that the posterior mean is a compromise between the\nempirical mean and the prior. If the prior is weak relative to the signal strength (`\u0015is small relative\nto\u0014), we put more weight on the empirical mean. If the prior is strong relative to the signal strength\n(`\u0015is large relative to \u0014), we put more weight on the prior. This is illustrated in Figure 4.16.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 376, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 949}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0377_ba714dde", "text": "If the prior is strong relative to the signal strength\n(`\u0015is large relative to \u0014), we put more weight on the prior. This is illustrated in Figure 4.16. Note\nalso that the posterior mean is written in terms of N\u0014y, so having Nmeasurements each of precision\n\u0014is like having one measurement with value yand precision N\u0014. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n138 Chapter 4. Statistics\n5\n 0 50.00.10.20.30.40.50.6prior variance of 1\nprior\nlik\npost\n(a)\n5\n 0 50.00.10.20.30.40.50.6prior variance of 5\nprior\nlik\npost (b)\nFigure 4.16: Inferring the mean of a univariate Gaussian with known \u001b2given observation y= 3. (a)\nUsing strong prior, p(\u0016) =N(\u0016j0;1). (b) Using weak prior, p(\u0016) =N(\u0016j0;5). Generated by code at\nﬁgures.probml.ai/book1/4.16. Posterior after seeing N= 1examples\nTo gain further insight into these equations, consider the posterior after seeing a single data point y\n(soN= 1).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 377, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 907}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0378_3b2e189b", "text": "Generated by code at\nﬁgures.probml.ai/book1/4.16. Posterior after seeing N= 1examples\nTo gain further insight into these equations, consider the posterior after seeing a single data point y\n(soN= 1). Then the posterior mean can be written in the following equivalent ways:\nam=`\u0015\na\u0015`m+\u0014\na\u0015y (4.175)\n=`m+\u0014\na\u0015(y\u0000`m) (4.176)\n=y\u0000`\u0015\na\u0015(y\u0000`m) (4.177)\nThe ﬁrst equation is a convex combination of the prior mean and the data. The second equation\nis the prior mean adjusted towards the data y. The third equation is the data adjusted towards\nthe prior mean; this is called a shrinkage estimate. This is easier to see if we deﬁne the weight\nw=`\u0015=a\u0015, which is the ratio of the prior to posterior precision. Then we have\nam=y\u0000w(y\u0000`m) = (1\u0000w)y+w`m (4.178)\nNote that, for a Gaussian, the posterior mean and posterior mode are the same. Thus we can use\nthe above equations to perform MAP estimation. See Exercise 4.2 for a simple example.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 378, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 923}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0379_a0f0f63a", "text": "Thus we can use\nthe above equations to perform MAP estimation. See Exercise 4.2 for a simple example. Posterior variance\nIn addition to the posterior mean or mode of \u0016, we might be interested in the posterior variance,\nwhich gives us a measure of conﬁdence in our estimate. The square root of this is called the standard\nerror of the mean :\nse(\u0016),p\nV[\u0016jD] (4.179)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.6. Bayesian statistics * 139\nSuppose we use an uninformative prior for \u0016by setting`\u0015= 0(see Section 4.6.5.1). In this case, the\nposterior mean is equal to the MLE,am=y. Suppose, in addition, that we approximate \u001b2by the\nsample variance\ns2,1\nNNX\nn=1(yn\u0000y)2(4.180)\nHencea\u0015=N^\u0014=N=s2, so the SEM becomes\nse(\u0016) =p\nV[\u0016jD] =1pa\u0015=sp\nN(4.181)\nThus we see that the uncertainty in \u0016is reduced at a rate of 1=p\nN.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 379, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 843}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0380_221d123e", "text": "In addition, we can use the fact that 95% of a Gaussian distribution is contained within 2 standard\ndeviations of the mean to approximate the 95% credible interval for\u0016using\nI:95(\u0016jD) =y\u00062sp\nN(4.182)\n4.6.4.2 Multivariate case\nForD-dimensional data, the likelihood has the form\np(Dj\u0016) =NY\nn=1N(ynj\u0016;\u0006) (4.183)\n=N\n(2\u0019)D=2j\u0006j1\n2exp\"\n\u00001\n2NX\nn=1(yn\u0000\u0016)T\u0006\u00001(yn\u0000\u0016)#\n(4.184)\n=N(yj\u0016;1\nN\u0006) (4.185)\nwherey=1\nNPN\nn=1yn. Thus we replace the set of observations with their mean, and scale down the\ncovariance by a factor of N. For simplicity, we will use a conjugate prior, which in this case is a Gaussian, namely\np(\u0016) =N(\u0016j`m;`V) (4.186)\nWe can derive a Gaussian posterior for \u0016based on the results in Section 3.3.1 We get\np(\u0016jD;\u0006) =N(\u0016jam;aV) (4.187)\naV\u00001=`V\u00001+N\u0006\u00001(4.188)\nam=aV(\u0006\u00001(Ny)+`V\u00001`m) (4.189)\nFigure 4.17 gives a 2d example of these results. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n140 Chapter 4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 380, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 914}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0381_6b35b138", "text": "Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n140 Chapter 4. Statistics\n−1 0 1−1−0.500.51data prior\n−1 0 1−1−0.500.51post after 10 obs\n−1 0 1−1−0.500.51\nFigure 4.17: Illustration of Bayesian inference for the mean of a 2d Gaussian. (a) The data is generated from\nyn\u0018N(\u0016;\u0006), where\u0016= [0:5;0:5]Tand\u0006= 0:1[2;1; 1;1]). (b) The prior is p(\u0016) =N(\u0016j0;0:1I2). (c) We\nshow the posterior after 10 data points have been observed. Generated by code at ﬁgures.probml.ai/book1/4.17. 4.6.5 Beyond conjugate priors\nWe have seen various examples of conjugate priors, all of which have come from the exponential\nfamily (see Section 3.4). These priors have the advantage of being easy to interpret (in terms of\nsuﬃcient statistics from a virtual prior dataset), and easy to compute with. However, for most\nmodels, there is no prior in the exponential family that is conjugate to the likelihood. Furthermore,\neven where there is a conjugate prior, the assumption of conjugacy may be too limiting.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 381, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 989}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0382_6caefce8", "text": "Furthermore,\neven where there is a conjugate prior, the assumption of conjugacy may be too limiting. Therefore in\nthe sections below, we brieﬂy discuss various other kinds of priors. 4.6.5.1 Noninformative priors\nWhen we have little or no domain speciﬁc knowledge, it is desirable to use an uninformative ,\nnoninformative orobjective priors, to “let the data speak for itself”. For example, if we want to\ninfer a real valued quantity, such as a location parameter \u00162R, we can use a ﬂat priorp(\u0016)/1. This can be viewed as an “inﬁnitely wide” Gaussian. Unfortunately, there is no unique way to deﬁne uninformative priors, and they all encode some\nkind of knowledge. It is therefore better to use the term diﬀuse prior ,minimally informative\npriorordefault prior . See the sequel to this book, [Mur22], for more details. 4.6.5.2 Hierarchical priors\nBayesian models require specifying a prior p(\u0012)for the parameters. The parameters of the prior are\ncalledhyperparameters , and will be denoted by \u001e.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 382, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 994}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0383_5c8449f1", "text": "4.6.5.2 Hierarchical priors\nBayesian models require specifying a prior p(\u0012)for the parameters. The parameters of the prior are\ncalledhyperparameters , and will be denoted by \u001e. If these are unknown, we can put a prior on\nthem; this deﬁnes a hierarchical Bayesian model , ormulti-level model , which can visualize\nlike this:\u001e!\u0012!D. We assume the prior on the hyper-parameters is ﬁxed (e.g., we may use some\nkind of minimally informative prior), so the joint distribution has the form\np(\u001e;\u0012;D) =p(\u001e)p(\u0012j\u001e)p(Dj\u0012) (4.190)\nThe hope is that we can learn the hyperparameters by treating the parameters themselves as\ndatapoints. This is useful when we have multiple related parameters that need to be estimated (e.g.,\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.6. Bayesian statistics * 141\nfrom diﬀerent subpopulations, or muliple tasks); this provides a learning signal to the top level of the\nmodel. See the sequel to this book, [Mur22], for details.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 383, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 978}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0384_97ad53ce", "text": "Bayesian statistics * 141\nfrom diﬀerent subpopulations, or muliple tasks); this provides a learning signal to the top level of the\nmodel. See the sequel to this book, [Mur22], for details. 4.6.5.3 Empirical priors\nIn Section 4.6.5.2, we discussed hierarchical Bayes as a way to infer parameters from data. Unfortu-\nnately, posterior inference in such models can be computationally challenging. In this section, we\ndiscuss a computationally convenient approximation, in which we ﬁrst compute a point estimate of\nthe hyperparameters, ^\u001e, and then compute the conditional posterior, p(\u0012j^\u001e;D), rather than the joint\nposterior,p(\u0012;\u001ejD). To estimate the hyper-parameters, we can maximize the marginal likelihood:\n^\u001emml(D) = argmax\n\u001ep(Dj\u001e) = argmax\n\u001eZ\np(Dj\u0012)p(\u0012j\u001e)d\u0012 (4.191)\nThis technique is known as type II maximum likelihood , since we are optimizing the hyperparam-\neters, rather than the parameters. Once we have estimated ^\u001e, we compute the posterior p(\u0012j^\u001e;D)in\nthe usual way.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 384, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 978}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0385_79b59887", "text": "Once we have estimated ^\u001e, we compute the posterior p(\u0012j^\u001e;D)in\nthe usual way. Since we are estimating the prior parameters from data, this approach is empirical Bayes (EB)\n[CL96]. This violates the principle that the prior should be chosen independently of the data. However, we can view it as a computationally cheap approximation to inference in the full hierarchical\nBayesian model, just as we viewed MAP estimation as an approximation to inference in the one level\nmodel\u0012!D. In fact, we can construct a hierarchy in which the more integrals one performs, the\n“more Bayesian” one becomes, as shown below. Method Deﬁnition\nMaximum likelihood ^\u0012= argmax\u0012p(Dj\u0012)\nMAP estimation ^\u0012= argmax\u0012p(Dj\u0012)p(\u0012j\u001e)\nML-II (Empirical Bayes) ^\u001e= argmax\u001eR\np(Dj\u0012)p(\u0012j\u001e)d\u0012\nMAP-II ^\u001e= argmax\u001eR\np(Dj\u0012)p(\u0012j\u001e)p(\u001e)d\u0012\nFull Bayes p(\u0012;\u001ejD)/p(Dj\u0012)p(\u0012j\u001e)p(\u001e)\nNote that ML-II is less likely to overﬁt than “regular” maximum likelihood, because there are\ntypically fewer hyper-parameters \u001ethan there are parameters \u0012.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 385, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0386_f3dff282", "text": "See the sequel to this book, [Mur22],\nfor details. 4.6.6 Credible intervals\nA posterior distribution is (usually) a high dimensional object that is hard to visualize and work\nwith. A common way to summarize such a distribution is to compute a point estimate, such as the\nposterior mean or mode, and then to compute a credible interval , which quantiﬁes the uncertainty\nassociated with that estimate. (A credible interval is not the same as a conﬁdence interval, which is\na concept from frequentist statistics which we discuss in Section 4.7.4.)\nMore precisely, we deﬁne a 100(1\u0000\u000b)% credible interval to be a (contiguous) region C= (`;u)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n142 Chapter 4. Statistics\n0 0.2 0.4 0.6 0.8 100.511.522.533.5\n(a)\n0 0.2 0.4 0.6 0.8 100.511.522.533.5 (b)\nFigure 4.18: (a) Central interval and (b) HPD region for a Beta(3,9) posterior. The CI is (0.06, 0.52) and the\nHPD is (0.04, 0.48). Adapted from Figure 3.6 of [Hof09].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 386, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 970}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0387_d9059f7b", "text": "The CI is (0.06, 0.52) and the\nHPD is (0.04, 0.48). Adapted from Figure 3.6 of [Hof09]. Generated by code at ﬁgures.probml.ai/book1/4.18. (standing for lower and upper) which contains 1\u0000\u000bof the posterior probability mass, i.e.,\nC\u000b(D) = (`;u) :P(`\u0014\u0012\u0014ujD) = 1\u0000\u000b (4.192)\nThere may be many intervals that satisfy Equation (4.192), so we usually choose one such that there\nis(1\u0000\u000b)=2mass in each tail; this is called a central interval . If the posterior has a known functional\nform, we can compute the posterior central interval using `=F\u00001(\u000b=2)andu=F\u00001(1\u0000\u000b=2), where\nFis the cdf of the posterior, and F\u00001is the inverse cdf. For example, if the posterior is Gaussian,\np(\u0012jD) =N(0;1), and\u000b= 0:05, then we have `= \b\u00001(\u000b=2) =\u00001:96, andu= \b\u00001(1\u0000\u000b=2) = 1:96,\nwhere \bdenotes the cdf of the Gaussian. This is illustrated in Figure 2.2b.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 387, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 824}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0388_29b71949", "text": "This is illustrated in Figure 2.2b. This justiﬁes the common\npractice of quoting a credible interval in the form of \u0016\u00062\u001b, where\u0016represents the posterior mean,\n\u001brepresents the posterior standard deviation, and 2 is a good approximation to 1.96. In general, it is often hard to compute the inverse cdf of the posterior. In this case, a sim-\nple alternative is to draw samples from the posterior, and then to use a Monte Carlo approxi-\nmation to the posterior quantiles: we simply sort the Ssamples, and ﬁnd the one that occurs\nat location \u000b=Salong the sorted list. As S! 1, this converges to the true quantile. See\ncode.probml.ai/book1/beta_credible_int_demo for a demo of this. A problem with central intervals is that there might be points outside the central interval which\nhave higher probability than points that are inside, as illustrated in Figure 4.18(a).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 388, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 861}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0389_cf48484a", "text": "A problem with central intervals is that there might be points outside the central interval which\nhave higher probability than points that are inside, as illustrated in Figure 4.18(a). This motivates\nan alternative quantity known as the highest posterior density orHPDregion, which is the set\nof points which have a probability above some threshold. More precisely we ﬁnd the threshold p\u0003on\nthe pdf such that\n1\u0000\u000b=Z\n\u0012:p(\u0012jD)>p\u0003p(\u0012jD)d\u0012 (4.193)\nand then deﬁne the HPD as\nC\u000b(D) =f\u0012:p(\u0012jD)\u0015p\u0003g (4.194)\nIn 1d, the HPD region is sometimes called a highest density interval orHDI. For example,\nFigure 4.18(b) shows the 95% HDI of a Beta(3;9)distribution, which is (0:04;0:48). We see that\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.6. Bayesian statistics * 143\n−4 −2 0 2 4 6 8 100.00.10.20.30.40.5\nα/2 α/2\n(a)\n−4 −2 0 2 4 6 810pMIN (b)\nFigure 4.19: (a) Central interval and (b) HPD region for a hypothetical multimodal posterior. Adapted from\nFigure 2.2 of [Gel+04].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 389, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 994}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0390_07879391", "text": "Adapted from\nFigure 2.2 of [Gel+04]. Generated by code at ﬁgures.probml.ai/book1/4.19. this is narrower than the central interval, even though it still contains 95% of the mass; furthermore,\nevery point inside of it has higher density than every point outside of it. For a unimodal distribution, the HDI will be the narrowest interval around the mode containing\n95% of the mass. To see this, imagine “water ﬁlling” in reverse, where we lower the level until 95%\nof the mass is revealed, and only 5% is submerged. This gives a simple algorithm for computing\nHDIs in the 1d case: simply search over points such that the interval contains 95% of the mass\nand has minimal width. This can be done by 1d numerical optimization if we know the inverse\nCDF of the distribution, or by search over the sorted data points if we have a bag of samples (see\ncode.probml.ai/book1/betaHPD for some code). If the posterior is multimodal, the HDI may not even be a connected region: see Figure 4.19(b) for\nan example.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 390, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 998}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0391_84c17115", "text": "If the posterior is multimodal, the HDI may not even be a connected region: see Figure 4.19(b) for\nan example. However, summarizing multimodal posteriors is always diﬃcult. 4.6.7 Bayesian machine learning\nSo far, we have focused on unconditional models of the form p(yj\u0012). In supervised machine learning,\nwe use conditional models of the form p(yjx;\u0012). The posterior over the parameters is now p(\u0012jD),\nwhereD=f(xn;yn) :n= 1 :Ng. Computing this posterior can be done using the principles we\nhave already discussed. This approach is called Bayesian machine learning , since we are “being\nBayesian” about the model parameters. 4.6.7.1 Plugin approximation\nOnce we have computed the posterior over the parameters, we can compute the posterior predictive\ndistribution over outputs given inputs by marginalizing out the unknown parameters:\np(yjx;D) =Z\np(yjx;\u0012)p(\u0012jD)d\u0012 (4.195)\nOf course, computing this integral is often intractable.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 391, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 927}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0392_db8fc06e", "text": "A very simple approximation is to assume\nthere is just a single best model, ^\u0012, such as the MLE. This is equivalent to approximating the\nposterior as an inﬁnitely narrow, but inﬁnitely tall, “spike” at the chosen value. We can write this as\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n144 Chapter 4. Statistics\nfollows:\np(\u0012jD) =\u000e(\u0012\u0000^\u0012) (4.196)\nwhere\u000eis the Dirac delta function (see Section 2.6.5). If we use this approximation, then the\npredictive distribution can be obtained by simply “plugging in” the point estimate into the likelihood:\np(yjx;D) =Z\np(yjx;\u0012)p(\u0012jD)d\u0012\u0019Z\np(yjx;\u0012)\u000e(\u0012\u0000^\u0012)d\u0012=p(yjx;^\u0012) (4.197)\nThis follows from the sifting property of delta functions (Equation (2.129)). The approach in Equation (4.197) is called a plug-in approximation . This approach is equivalent\nto the standard approach used in most of machine learning, in which we ﬁrst ﬁt the model (i.e. compute a point estimate ^\u0012) and then use it to make predicitons.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 392, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 960}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0393_09b4a348", "text": "This approach is equivalent\nto the standard approach used in most of machine learning, in which we ﬁrst ﬁt the model (i.e. compute a point estimate ^\u0012) and then use it to make predicitons. However, the standard (plug-in)\napproach can suﬀer from overﬁtting and overconﬁdence, as we discussed in Section 1.2.3. The\nfully Bayesian approach avoids this by marginalizing out the parameters, but can be expensive. Fortunately, even simple approximations, in which we average over a few plausible parameter values,\ncan improve performance. We give some examples of this below. 4.6.7.2 Example: scalar input, binary output\nSuppose we want to perform binary classiﬁcation, so y2f0;1g. We will use a model of the form\np(yjx;\u0012) = Ber(yj\u001b(wTx+b)) (4.198)\nwhere\n\u001b(a),ea\n1 +ea(4.199)\nis thesigmoid orlogisticfunction which maps R![0;1], and Ber(yj\u0016)is the Bernoulli distribution\nwith mean \u0016(see Section 2.4 for details).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 393, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 906}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0394_8994cb4c", "text": "In other words,\np(y= 1jx;\u0012) =\u001b(wTx+b) =1\n1 +e\u0000(wTx+b)(4.200)\nThis model is called logistic regression . (We discuss this in more detail in Chapter 10.)\nLet us apply this model to the task of determining if an iris ﬂower is of type Setosa or Versicolor,\nyn2f0;1g, given information about the sepal length, xn. (See Section 1.2.1.1 for a description of\nthe iris dataset.)\nWe ﬁrst ﬁt a 1d logistic regression model of the following form\np(y= 1jx;\u0012) =\u001b(b+wx) (4.201)\nto the datasetD=f(xn;yn)gusing maximum likelihood estimation. (See Section 10.2.3 for details\non how to compute the MLE for this model.) Figure 4.20a shows the plugin approximation to the\nposterior predictive, p(y= 1jx;^\u0012), where ^\u0012is the MLE of the parameters. We see that we become\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 394, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 827}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0395_da90ab73", "text": "We see that we become\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.6. Bayesian statistics * 145\n4.5 5.0 5.5 6.0 6.5 7.0\nsepal_length0.00.20.40.60.81.0p(y=1)\n(a)\n4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5\nsepal_length0.00.20.40.60.81.0\np(y=1)\n (b)\nFigure 4.20: (a) Logistic regression for classifying if an Iris ﬂower is Versicolor ( y= 1) or setosa ( y= 0) using\na single input feature xcorresponding to sepal length. Labeled points have been (vertically) jittered to avoid\noverlapping too much. Vertical line is the decision boundary. Generated by code at ﬁgures.probml.ai/book1/4.20. (b) Same as (a) but showing posterior distribution. Adapted from Figure 4.4 of [Mar18]. Generated by code\nat ﬁgures.probml.ai/book1/4.20. more conﬁdent that the ﬂower is of type Versicolor as the sepal length gets larger, as represented by\nthe sigmoidal (S-shaped) logistic function. Thedecision boundary is deﬁned to be the input value x\u0003wherep(y= 1jx\u0003;^\u0012) = 0:5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 395, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 972}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0396_c0426ff7", "text": "Thedecision boundary is deﬁned to be the input value x\u0003wherep(y= 1jx\u0003;^\u0012) = 0:5. We can\nsolve for this value as follows:\n\u001b(b+wx\u0003) =1\n1 +e\u0000(b+wx\u0003)=1\n2(4.202)\nb+wx\u0003= 0 (4.203)\nx\u0003=\u0000b\nw(4.204)\nFrom Figure 4.20a, we see that x\u0003\u00195:5cm. However, the above approach does not model the uncertainty in our estimate of the parameters, and\ntherefore ignores the induced uncertainty in the output probabilities, and the location of the decision\nboundary. To capture this additional uncertainty, we can use a Bayesian approach to approximate\nthe posterior p(\u0012jD). (See Section 10.5 for details.) Given this, we can approximate the posterior\npredictive distribution using a Monte Carlo approximation:\np(y= 1jx;D)\u00191\nSSX\ns=1p(y= 1jx;\u0012s) (4.205)\nwhere\u0012s\u0018p(\u0012jD)is a posterior sample. Figure 4.20b plots the mean and 95% credible interval of\nthis function. We see that there is now a range of predicted probabilities for each input.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 396, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 912}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0397_874d4244", "text": "Figure 4.20b plots the mean and 95% credible interval of\nthis function. We see that there is now a range of predicted probabilities for each input. We can\nalso compute a distribution over the location of the decision boundary by using the Monte Carlo\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n146 Chapter 4. Statistics\nDEADLINE\nETA for Company B\nETA for Company A\nTime \nFigure 4.21: Distribution of arrival times for two diﬀerent shipping companies. ETA is the expected time of\narrival. A’s distribution has greater uncertainty, and may be too risky. From https: // bit. ly/ 39bc4XL . Used with kind permission of Brendan Hasz. approximation\np(x\u0003jD)\u00191\nSSX\ns=1\u000e\u0012\nx\u0003\u0000(\u0000bs\nws)\u0013\n(4.206)\nwhere (bs;ws) =\u0012s. The 95% credible interval for this distribution is shown by the “fat” vertical line\nin Figure 4.20b. Although carefully modeling our uncertainty may not matter for this application, it can be\nimportant in risk-sensitive applications, such as health care and ﬁnance, as we discuss in Chapter 5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 397, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1013}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0398_1c6637ad", "text": "Although carefully modeling our uncertainty may not matter for this application, it can be\nimportant in risk-sensitive applications, such as health care and ﬁnance, as we discuss in Chapter 5. 4.6.7.3 Example: binary input, scalar output\nNow suppose we want to predict the delivery time for a package, y2R, if shipped by company A vs\nB. We can encode the company id using a binary feature x2f0;1g, wherex= 0means company A\nandx= 1means company B. We will use the following discriminative model for this problem:\np(yjx;\u0012) =N(yj\u0016x;\u001b2\nx) (4.207)\nwhereN(yj\u0016;\u001b2)is the Gaussian distribution\nN(yj\u0016;\u001b2),1p\n2\u0019\u001b2e\u00001\n2\u001b2(y\u0000\u0016)2(4.208)\nand\u0012= (\u00160;\u00161;\u001b0;\u001b1)are the parameters of the model. We can ﬁt this model using maximum\nlikelihood estimation as we discuss in Section 4.2.5; alternatively, we can adopt a Bayesian approach,\nas we discuss in Section 4.6.4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 398, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 845}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0399_3af791da", "text": "We can ﬁt this model using maximum\nlikelihood estimation as we discuss in Section 4.2.5; alternatively, we can adopt a Bayesian approach,\nas we discuss in Section 4.6.4. The advantage of the Bayesian approach is that by capturing uncertainty in the parameters \u0012, we\nalso capture uncertainty in our forecasts p(yjx;D), whereas using a plug-in approximation p(yjx;^\u0012)\nwould underestimate this uncertainty. For example, suppose we have only used each company once, so\nour training set has the form D=f(x1= 0;y1= 15);(x2= 1;y2= 20)g. As we show in Section 4.2.5,\nthe MLE for the means will be the empirical means, ^\u00160= 15and ^\u00161= 20, but the MLE for the\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.6. Bayesian statistics * 147\nstandard deviations will be zero, ^\u001b0=^\u001b1= 0, since we only have a single sample from each “class”. The resulting plug-in prediction will therefore not capture any uncertainty. To see why modeling the uncertainty is important, consider Figure 4.21.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 399, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1005}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0400_92dbbccb", "text": "The resulting plug-in prediction will therefore not capture any uncertainty. To see why modeling the uncertainty is important, consider Figure 4.21. We see that the expected\ntime of arrival (ETA) for company A is less than for company B; however, the variance of A’s\ndistribution is larger, which makes it a risky choice if you want to be conﬁdent the package will\narrive by the speciﬁed deadline. (For more details on how to choose optimal actions in the presence\nof uncertainty, see Chapter 5.)\nOf course, the above example is extreme, because we assumed we only had one example from each\ndelivery company. However, this kind of problem occurs whenever we have few examples of a given\nkind of input, as can happen whenever the data has a long tail of novel patterns, such as a new\ncombination of words or categorical features. 4.6.7.4 Scaling up\nThe above examples were both extremely simple, involving 1d input and 1d output, and just 2–4\nparameters.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 400, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 953}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0401_b7677c71", "text": "4.6.7.4 Scaling up\nThe above examples were both extremely simple, involving 1d input and 1d output, and just 2–4\nparameters. Mostpracticalproblemsinvolvehighdimensionalinputs, andsometimeshighdimensional\noutputs, and therefore use models with lots of parameters. Unfortunately, computing the posterior,\np(\u0012jD), and the posterior predictive, p(yjx;D), can be computationally challenging for many models. We discuss this issue in Section 4.6.8. 4.6.8 Computational issues\nGiven a likelihood p(Dj\u0012)and a prior p(\u0012), we can compute the posterior p(\u0012jD)using Bayes’ rule. However, actually performing this computation is usually intractable, except for simple special cases,\nsuch as conjugate models (Section 4.6.1), or models where all the latent variables come from a small\nﬁnite set of possible values. We therefore need to approximate the posterior. There are a large variety\nof methods for performing approximate posterior inference , which trade oﬀ accuracy, simplicity,\nand speed.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 401, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 982}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0402_2f1afa8c", "text": "We therefore need to approximate the posterior. There are a large variety\nof methods for performing approximate posterior inference , which trade oﬀ accuracy, simplicity,\nand speed. We brieﬂy discuss some of these algorithms below, but go into more detail in the sequel\nto this book, [Mur22]. (See also [MFR20] for a review of various approximate inference methods,\nstarting with Bayes’ original method in 1763.)\nAs a running example, we will use the problem of approximating the posterior of a beta-Bernoulli\nmodel. Speciﬁcally, the goal is to approximate\np(\u0012jD)/\"NY\nn=1Bin(ynj\u0012)#\nBeta(\u0012j1;1) (4.209)\nwhereDconsists of 10 heads and 1 tail (so the total number of observations is N= 11), and we\nuse a uniform prior. Although we can compute this posterior exactly (see Figure 4.22), using the\nmethod discussed in Section 4.6.2, this serves as a useful pedagogical example since we can compare\nthe approximation to the exact answer.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 402, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 930}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0403_3479de73", "text": "Also, since the target distribution is just 1d, it is easy to\nvisualize the results. (Note, however, that the problem is not completely trivial, since the posterior is\nhighly skewed, due to the use of an imbalanced sample of 10 heads and 1 tail.)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n148 Chapter 4. Statistics\n0.0 0.2 0.4 0.6 0.8 1.0\ngrid approximation\n(a)\n0.0 0.2 0.4 0.6 0.8 1.0\nQuadratic approximation\nLaplace\nexact (b)\nFigure 4.22: Approximating the posterior of a beta-Bernoulli model. (a) Grid approximation using 20 grid\npoints. (b) Laplace approximation. Generated by code at ﬁgures.probml.ai/book1/4.22.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 403, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 635}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0404_33107932", "text": "(a) Grid approximation using 20 grid\npoints. (b) Laplace approximation. Generated by code at ﬁgures.probml.ai/book1/4.22. 4.6.8.1 Grid approximation\nThe simplest approach to approximate posterior inference is to partition the space of possible values\nfor the unknowns into a ﬁnite set of possibilities, call them \u00121;:::;\u0012K, and then to approximate the\nposterior by brute-force enumeration, as follows:\np(\u0012=\u0012kjD)\u0019p(Dj\u0012k)p(\u0012k)\np(D)=p(Dj\u0012k)p(\u0012k)PK\nk0=1p(D;\u0012k0)(4.210)\nThis is called a grid approximation . In Figure 4.22a, we illustrate this method applied to our 1d\nproblem. We see that it is easily able to capture the skewed posterior. Unfortunately, this approach\ndoes not scale to problems in more than 2 or 3 dimensions, because the number of grid points grows\nexponentially with the number of dimensions.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 404, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 808}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0405_1579e73c", "text": "Unfortunately, this approach\ndoes not scale to problems in more than 2 or 3 dimensions, because the number of grid points grows\nexponentially with the number of dimensions. 4.6.8.2 Quadratic (Laplace) approximation\nIn this section, we discuss a simple way to approximate the posterior using a multivariate Gaussian;\nthis is known as a Laplace approximation or aquadratic approximation (see e.g., [TK86;\nRMC09]). To derive this, suppose we write the posterior as follows:\np(\u0012jD) =1\nZe\u0000 (\u0012)(4.211)\nwhere (\u0012) =\u0000logp(\u0012;D)is called an energy function, and Z=p(D)is the normalization constant. Performing a Taylor series expansion around the mode ^\u0012(i.e., the lowest energy state) we get\n (\u0012)\u0019 (^\u0012) + (\u0012\u0000^\u0012)Tg+1\n2(\u0012\u0000^\u0012)TH(\u0012\u0000^\u0012) (4.212)\nwheregis the gradient at the mode, and His the Hessian. Since ^\u0012is the mode, the gradient term is\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.6. Bayesian statistics * 149\nzero.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 405, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 940}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0406_589c7ad3", "text": "Since ^\u0012is the mode, the gradient term is\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.6. Bayesian statistics * 149\nzero. Hence\n^p(\u0012;D) =e\u0000 (^\u0012)exp\u0014\n\u00001\n2(\u0012\u0000^\u0012)TH(\u0012\u0000^\u0012)\u0015\n(4.213)\n^p(\u0012jD) =1\nZ^p(\u0012;D) =N(\u0012j^\u0012;H\u00001) (4.214)\nZ=e\u0000 (^\u0012)(2\u0019)D=2jHj\u00001\n2 (4.215)\nThe last line follows from normalization constant of the multivariate Gaussian. The Laplace approximation is easy to apply, since we can leverage existing optimization algorithms\nto compute the MAP estimate, and then we just have to compute the Hessian at the mode. (In high\ndimensional spaces, we can use a diagonal approximation.)\nIn Figure 4.22b, we illustrate this method applied to our 1d problem. Unfortunately we see that it\nis not a particularly good approximation. This is because the posterior is skewed, whereas a Gaussian\nis symmetric. In addition, the parameter of interest lies in the constrained interval \u00122[0;1], whereas\nthe Gaussian assumes an unconstrained space, \u00122R.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 406, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 969}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0407_8ab137f8", "text": "In addition, the parameter of interest lies in the constrained interval \u00122[0;1], whereas\nthe Gaussian assumes an unconstrained space, \u00122R. Fortunately, we can solve this latter problem\nby using a change of variable. For example, in this case we can apply the Laplace approximation to\n\u000b= logit(\u0012). This is a common trick to simplify the job of inference. 4.6.8.3 Variational approximation\nIn Section 4.6.8.2, we discussed the Laplace approximation, which uses an optimization procedure\nto ﬁnd the MAP estimate, and then approximates the curvature of the posterior at that point\nbased on the Hessian. In this section, we discuss variational inference (VI), which is another\noptimization-based approach to posterior inference, but which has much more modeling ﬂexibility\n(and thus can give a much more accurate approximation).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 407, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 823}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0408_10d9f48f", "text": "VI attempts to approximate an intractable probability distribution, such as p(\u0012jD), with one that\nis tractable, q(\u0012), so as to minimize some discrepancy Dbetween the distributions:\nq\u0003= argmin\nq2QD(q;p) (4.216)\nwhereQis some tractable family of distributions (e.g., multivariate Gaussian). If we deﬁne Dto be\nthe KL divergence (see Section 6.2), then we can derive a lower bound to the log marginal likelihood;\nthis quantity is known as the evidence lower bound orELBO. By maximizing the ELBO, we can\nimprove the quality of the posterior approximation. See the sequel to this book, [Mur22], for details. 4.6.8.4 Markov Chain Monte Carlo (MCMC) approximation\nAlthough VI is a fast, optimization-based method, it can give a biased approximation to the posterior,\nsince it is restricted to a speciﬁc function form q2Q. A more ﬂexible approach is to use a non-\nparametric approximation in terms of a set of samples, q(\u0012)\u00191\nSPS\ns=1\u000e(\u0012\u0000\u0012s). This is called a\nMonte Carlo approximation to the posterior.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 408, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 994}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0409_979f4621", "text": "A more ﬂexible approach is to use a non-\nparametric approximation in terms of a set of samples, q(\u0012)\u00191\nSPS\ns=1\u000e(\u0012\u0000\u0012s). This is called a\nMonte Carlo approximation to the posterior. The key issue is how to create the posterior samples\n\u0012s\u0018p(\u0012jD)eﬃciently, without having to evaluate the normalization constant p(D) =R\np(\u0012;D)d\u0012. A common approach to this problem is known as Markov chain Monte Carlo orMCMC . If\nwe augment this algorithm with gradient-based information, derived from rlogp(\u0012;D), we can\nsigniﬁcantly speed up the method; this is called Hamiltonian Monte Carlo orHMC. See the\nsequel to this book, [Mur22], for details. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n150 Chapter 4. Statistics\n4.7 Frequentist statistics *\nThe approach to statistical inference that we described in Section 4.6 is called Bayesian statistics. It treats parameters of models just like any other unknown random variable, and applies the rules\nof probability theory to infer them from data.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 409, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 991}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0410_3ba08709", "text": "It treats parameters of models just like any other unknown random variable, and applies the rules\nof probability theory to infer them from data. Attempts have been made to devise approaches to\nstatistical inference that avoid treating parameters like random variables, and which thus avoid\nthe use of priors and Bayes rule. This alternative approach is known as frequentist statistics ,\nclassical statistics ororthodox statistics . The basic idea (formalized in Section 4.7.1) is to to represent uncertainty by calculating how a\nquantity estimated from data (such as a parameter or a predicted label) would change if the data\nwere changed. It is this notion of variation across repeated trials that forms the basis for modeling\nuncertainty used by the frequentist approach. By contrast, the Bayesian approach views probability in\nterms of information rather than repeated trials. This allows the Bayesian to compute the probability\nof one-oﬀ events, as we discussed in Section 2.1.1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 410, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 983}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0411_bbebdc19", "text": "This allows the Bayesian to compute the probability\nof one-oﬀ events, as we discussed in Section 2.1.1. Perhaps more importantly, the Bayesian approach\navoids certain paradoxes that plague the frequentist approach (see Section 4.7.5 and Section 5.5.4). These pathologies led the famous statistician George Box to say:\nI believe that it would be very diﬃcult to persuade an intelligent person that current [frequentist]\nstatistical practice was sensible, but that there would be much less diﬃculty with an approach\nvia likelihood and Bayes’ theorem. — George Box, 1962 (quoted in [Jay76]). Nevertheless, it is useful to be familiar with frequentist statistics, since it is widely used, and has\nsome key concepts that are useful even for Bayesians [Rub84]. 4.7.1 Sampling distributions\nIn frequentist statistics, uncertainty is not represented by the posterior distribution of a random\nvariable, but instead by the sampling distribution of anestimator .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 411, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 951}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0412_2d5a45f8", "text": "4.7.1 Sampling distributions\nIn frequentist statistics, uncertainty is not represented by the posterior distribution of a random\nvariable, but instead by the sampling distribution of anestimator . (We deﬁne these two terms\nbelow.)\nAs explained in the section on decision theory in Section 5.1, an estimator is a decision procedure\nthat speciﬁes what action to take given some observed data. In the context of parameter estimation,\nwhere the action space is to return a parameter vector, we will denote this by ^\u0012=\u0019(D). For example,\n^\u0012could be the maximum likelihood estimate, the MAP estimate, or the method of moments estimate. The sampling distribution of an estimator is the distribution of results we would see if we applied\nthe estimator multiple times to diﬀerent datasets sampled from some distribution; in the context of\nparameter estimation, it is the distribution of ^\u0012, viewed as a random variable that depends on the\nrandom sampleD.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 412, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 944}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0413_b18255b0", "text": "In more detail, imagine sampling Sdiﬀerent data sets, each of size N, from some\ntrue model p(xj\u0012\u0003)to generate\n~D(s)=fxn\u0018p(xnj\u0012\u0003) :n= 1 :Ng (4.217)\nWe denote this by D(s)\u0018\u0012\u0003for brevity. Now apply the estimator to each D(s)to get a set of estimates,\nf^\u0012(D(s))g. As we let S!1, the distribution induced by this set is the sampling distribution of the\nestimator. More precisely, we have\np(\u0019(~D) =\u0012j~D\u0018\u0012\u0003)\u00191\nSSX\ns=1\u000e(\u0012=\u0019(D(s))) (4.218)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.7. Frequentist statistics * 151\nIn some cases, we can compute this analytically, as we discuss in Section 4.7.2, although typically we\nneed to approximate it by Monte Carlo, as we discuss in Section 4.7.3. 4.7.2 Gaussian approximation of the sampling distribution of the MLE\nThe most common estimator is the MLE. When the sample size becomes large, the sampling\ndistribution of the MLE for certain models becomes Gaussian. This is known as the asymptotic\nnormality of the sampling distribution.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 413, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1003}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0414_46a7a8e2", "text": "When the sample size becomes large, the sampling\ndistribution of the MLE for certain models becomes Gaussian. This is known as the asymptotic\nnormality of the sampling distribution. More formally, we have the following result:\nTheorem 4.7.1. If the parameters are identiﬁable, then\np(\u0019(~D) =^\u0012j~D\u0018\u0012\u0003)!N (^\u0012j\u0012\u0003;(NF(\u0012\u0003))\u00001) (4.219)\nwhere F(\u0012\u0003)is theFisher information matrix , deﬁned in Equation (4.220). The Fisher information matrix measures the amount of curvature of the log-likelihood surface at\nits peak, as we show below. More formally, the Fisher information matrix (FIM) is deﬁned to be the covariance of the\ngradient of the log likelihood (also called the score function ):\nF,Ex\u0018p(xj\u0012)\u0002\nrlogp(xj\u0012)rlogp(xj\u0012)T\u0003\n(4.220)\nHence the (i;j)’th entry has the form\nFij=Ex\u0018\u0012\u0014\u0012@\n@\u0012ilogp(xj\u0012)\u0013\u0012@\n@\u0012jlogp(xj\u0012)\u0013\u0015\n(4.221)\nOne can show the following result. Theorem 4.7.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 414, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 864}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0415_b9417afa", "text": "Theorem 4.7.2. Iflogp(xj\u0012)is twice diﬀerentiable, and under certain regularity conditions, the\nFIM is equal to the expected Hessian of the NLL, i.e.,\nFij=\u0000Ex\u0018\u0012\u0014@2\n@\u0012i\u0012jlogp(xj\u0012)\u0015\n(4.222)\nThus we can interpret the FIM as the Hessian of the NLL. This helps us understand the result in Equation (4.219): a log-likelihood function with high\ncurvature (large Hessian) will result in a low variance estimate, since the parameters are “well\ndetermined” by the data, and hence robust to repeated sampling. 4.7.3 Bootstrap approximation of the sampling distribution of any estimator\nIn cases where the estimator is a complex function of the data (e.g., not just an MLE), or when the\nsample size is small, we can approximate its sampling distribution using a Monte Carlo technique\nknown as the bootstrap . The idea is simple. If we knew the true parameters \u0012\u0003, we could generate many (say S) fake\ndatasets, each of size N, from the true distribution, using ~D(s)=fxn\u0018p(xnj\u0012\u0003) :n= 1 :Ng.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 415, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 976}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0416_764a7bfd", "text": "The idea is simple. If we knew the true parameters \u0012\u0003, we could generate many (say S) fake\ndatasets, each of size N, from the true distribution, using ~D(s)=fxn\u0018p(xnj\u0012\u0003) :n= 1 :Ng. We\ncould then compute our estimator from each sample, ^\u0012s=\u0019(~D(s))and use the empirical distribution\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n152 Chapter 4. Statistics\n0 0.2 0.4 0.6 0.8 105001000150020002500300035004000\nBoot: true = 0.70, n=10, mle = 0.90, se = 0.001\n(a)\n0 0.2 0.4 0.6 0.8 10500100015002000250030003500\nBoot: true = 0.70, n=100, mle = 0.70, se = 0.000\n (b)\n0 0.2 0.4 0.6 0.8 1050010001500200025003000\nBayes: true = 0.70, n=10, post mean = 0.83, se = 0.001\n(c)\n0 0.2 0.4 0.6 0.8 1050010001500200025003000\nBayes: true = 0.70, n=100, post mean = 0.70, se = 0.000\n (d)\nFigure 4.23: Bootstrap (top row) vs Bayes (bottom row). The Ndata cases were generated from Ber(\u0012= 0:7). Left column: N= 10. Right column: N= 100.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 416, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 928}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0417_61ba0f05", "text": "The Ndata cases were generated from Ber(\u0012= 0:7). Left column: N= 10. Right column: N= 100. (a-b) A bootstrap approximation to the sampling distribution\nof the MLE for a Bernoulli distribution. We show the histogram derived from B= 10;000bootstrap samples. (c-d) Histogram of 10,000 samples from the posterior distribution using a uniform prior. Generated by code\nat ﬁgures.probml.ai/book1/4.23. of the resulting ^\u0012sas our estimate of the sampling distribution, as in Equation (4.218). Since \u0012\u0003is\nunknown, the idea of the parametric bootstrap is to generate each sampled dataset using ^\u0012=\u0019(D)\ninstead of\u0012\u0003, i.e., we use ~D(s)=fxn\u0018p(xnj^\u0012) :n= 1 :Ngin Equation (4.218). This is a plug-in\napproximation to the sampling distribution. An alternative, called the non-parametric bootstrap , is to sample Ndata points from the\noriginal data with replacement. This creates a new distribution D(s)which has the same size as the\noriginal.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 417, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 927}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0418_ad20af2e", "text": "An alternative, called the non-parametric bootstrap , is to sample Ndata points from the\noriginal data with replacement. This creates a new distribution D(s)which has the same size as the\noriginal. However, the number of unique data points in a bootstrap sample is just 0:632\u0002N, on\naverage. (To see this, note that the probability an item is picked at least once is (1\u0000(1\u00001=N)N),\nwhich approaches 1\u0000e\u00001\u00190:632for largeN.)\nFigure 4.23(a-b) shows an example where we compute the sampling distribution of the MLE for a\nBernoulli using the parametric bootstrap. (Results using the non-parametric bootstrap are essentially\nthe same.) When N= 10, we see that the sampling distribution is asymmetric, and therefore quite\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.7. Frequentist statistics * 153\nfar from Gaussian, but when N= 100, the distribution looks more Gaussian, as theory suggests (see\nSection 4.7.2).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 418, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 936}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0419_5eb8cdee", "text": "August 27, 2021\n4.7. Frequentist statistics * 153\nfar from Gaussian, but when N= 100, the distribution looks more Gaussian, as theory suggests (see\nSection 4.7.2). 4.7.3.1 Bootstrap is a “poor man’s” posterior\nA natural question is: what is the connection between the parameter estimates ^\u0012s=\u0019(D(s))computed\nby the bootstrap and parameter values sampled from the posterior, \u0012s\u0018p(\u0001jD)? Conceptually they\nare quite diﬀerent. But in the common case that the estimator is MLE and the prior is not very\nstrong, they can be quite similar. For example, Figure 4.23(c-d) shows an example where we compute\nthe posterior using a uniform Beta(1,1) prior, and then sample from it. We see that the posterior\nand the sampling distribution are quite similar. So one can think of the bootstrap distribution as a\n“poor man’s” posterior [HTF01, p235]. However, perhaps surprisingly, bootstrap can be slower than posterior sampling.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 419, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 913}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0420_d402d697", "text": "So one can think of the bootstrap distribution as a\n“poor man’s” posterior [HTF01, p235]. However, perhaps surprisingly, bootstrap can be slower than posterior sampling. The reason is that\nthe bootstrap has to generate Ssampled datasets, and then ﬁt a model to each one. By contrast, in\nposterior sampling, we only have to “ﬁt” a model once given a single dataset. (Some methods for\nspeeding up the bootstrap when applied to massive data sets are discussed in [Kle+11].)\n4.7.4 Conﬁdence intervals\nIn frequentist statistics, we use the variability induced by the sampling distribution as a way to\nestimate uncertainty of a parameter estimate. More precisely, we deﬁne a 100(1\u0000\u000b)%conﬁdence\ninterval for a parameter estimate \u0012as any interval I(~D) = (`(~D);u(~D))derived from a hypothetical\ndataset ~Dsuch that\nPr(\u00122I(~D)j~D\u0018\u0012) = 1\u0000\u000b (4.223)\nIt is common to set \u000b= 0:05, which yields a 95% CI.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 420, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 890}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0421_6cbbcdb1", "text": "This means that, if we repeatedly sampled\ndata, and compute I(~D)for each such dataset, then about 95% of such intervals will contain the true\nparameter\u0012. Note, however, that Equation (4.223) does notmean that for any particular dataset that \u00122I(D)\nwith 95% probability; this is what a Bayesian credible interval computes (Section 4.6.6), but is not\nwhat a frequentist conﬁdence interval computes. For more details on this important distinction, see\nSection 4.7.5. Let us put aside such “philosophical” concerns, and discuss how to compute a conﬁdence interval. Suppose that ^\u0012is an estimate of the parameter \u0012. Let\u0012\u0003be its true but unknown value. Also,\nsuppose that the sampling distribution of \u0001 = ^\u0012\u0000\u0012\u0003is known. Let \u000eand\u000edenote its\u000b=2and\n1\u0000\u000b=2quantiles. Hence\nPr(\u000e\u0014^\u0012\u0000\u0012\u0003\u0014\u000e) = 1\u0000\u000b (4.224)\nRearranging we get\nPr(^\u0012\u0000\u000e\u0014\u0012\u0003\u0014^\u0012\u0000\u000e) = 1\u0000\u000b (4.225)\nAnd hence\nI(~D) = ( ^\u0012(~D)\u0000\u000e(~D);^\u0012(~D) +\u000e(~D)) (4.226)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n154 Chapter 4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 421, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 971}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0422_9e5341b1", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n154 Chapter 4. Statistics\nis a100(1\u0000\u000b)%conﬁdence interval. In some cases, we can analytically compute the distribution of \u0001 = ^\u0012\u0000\u0012\u0003. This can be used to\nderive exact conﬁdence intervals. However, it is more common to assume a Gaussian approximation\nto the sampling distribution, as in Section 4.7.2. In this case, we haveq\nNF(^\u0012)(^\u0012\u0000\u0012\u0003)\u0018N(0;1). Hence we can compute an approximate CI using\n^\u0012\u0006z\u000b=2^ se (4.227)\nwherez\u000b=2is the\u000b=2quantile of the Gaussian cdf, and ^ se= 1=q\nNF(^\u0012)is the estimated standard\nerror. If we set \u000b= 0:05, we havez\u000b=2= 1:96, which justiﬁes the common approximation ^\u0012\u00062 ^ se. If the Gaussian approximation is not a good one, we can use a bootstrap approximation (see\nSection 4.7.3). In particular, we sample Sdatasets from ^\u0012(D), and apply the estimator to each one\nto get ^\u0012(D(s)); we then use the empirical distribution of ^\u0012(D)\u0000^\u0012(D(s))as an approximation to the\nsampling distribution of \u0001.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 422, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 960}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0423_161bdeed", "text": "4.7.5 Caution: Conﬁdence intervals are not credible\nA 95% frequentist conﬁdence interval for a parameter \u0012is deﬁned as any interval I(~D)such that\nPr(\u00122I(~D)j~D\u0018\u0012) = 0:95, as we explain in Section 4.7.4. This does notmean that the parameter is\n95% likely to live inside this interval given the observed data. That quantity — which is usually\nwhat we want to compute — is instead given by the Bayesian credible interval p(\u00122IjD), as we\nexplain in Section 4.6.6. These concepts are quite diﬀerent: In the frequentist approach, \u0012is treated\nas an unknown ﬁxed constant, and the data is treated as random. In the Bayesian approach, we\ntreat the data as ﬁxed (since it is known) and the parameter as random (since it is unknown). This counter-intuitive deﬁnition of conﬁdence intervals can lead to bizarre results. Consider the\nfollowing example from [Ber85, p11].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 423, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 858}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0424_88b068ec", "text": "This counter-intuitive deﬁnition of conﬁdence intervals can lead to bizarre results. Consider the\nfollowing example from [Ber85, p11]. Suppose we draw two integers D= (y1;y2)from\np(yj\u0012) =8\n<\n:0:5ify=\u0012\n0:5ify=\u0012+ 1\n0otherwise(4.228)\nIf\u0012= 39, we would expect the following outcomes each with probability 0.25:\n(39;39);(39;40);(40;39);(40;40) (4.229)\nLetm= min(y1;y2)and deﬁne the following interval:\n[`(D);u(D)] = [m;m ] (4.230)\nFor the above samples this yields\n[39;39];[39;39];[39;39];[40;40] (4.231)\nHence Equation (4.230) is clearly a 75% CI, since 39is contained in 3/4 of these intervals. However,\nif we observeD= (39;40)thenp(\u0012= 39jD) = 1:0, so we know that \u0012must be 39, yet we only have\n75% “conﬁdence” in this fact. We see that the CI will “cover” the true parameter 75% of the time,\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.7.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 424, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 870}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0425_526c448d", "text": "We see that the CI will “cover” the true parameter 75% of the time,\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.7. Frequentist statistics * 155\nif we compute multiple CIs from diﬀerent randomly sampled datasets, but if we just have a single\nobserved dataset, and hence a single CI, then the frequentist “coverage” probability can be very\nmisleading. Another, less contrived, example is as follows. Suppose we want to estimate the parameter \u0012\nof a Bernoulli distribution. Let y=1\nNPN\nn=1ynbe the sample mean. The MLE is ^\u0012=y. An\napproximate 95% conﬁdence interval for a Bernoulli parameter is y\u00061:96p\ny(1\u0000y)=N(this is called\naWald interval and is based on a Gaussian approximation to the Binomial distribution; compare\nto Equation (4.128)). Now consider a single trial, where N= 1andy1= 0. The MLE is 0, which\noverﬁts, as we saw in Section 4.5.1. But our 95% conﬁdence interval is also (0;0), which seems even\nworse.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 425, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 949}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0426_0e1e3849", "text": "Now consider a single trial, where N= 1andy1= 0. The MLE is 0, which\noverﬁts, as we saw in Section 4.5.1. But our 95% conﬁdence interval is also (0;0), which seems even\nworse. It can be argued that the above ﬂaw is because we approximated the true sampling distribution\nwith a Gaussian, or because the sample size was too small, or the parameter “too extreme”. However,\nthe Wald interval can behave badly even for large N, and non-extreme parameters [BCD01]. By\ncontrast, a Bayesian credible interval with a non-informative Jeﬀreys prior behaves in the way we\nwould expect. Several more interesting examples, along with Python code, can be found at [Van14]. See\nalso [Hoe+14; Mor+16; Lyu+20; Cha+19b], who show that many people, including professional\nstatisticians, misunderstand and misuse frequentist conﬁdence intervals in practice, whereas Bayesian\ncredible intervals do not suﬀer from these problems.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 426, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 906}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0427_30cb1581", "text": "4.7.6 The bias-variance tradeoﬀ\nAn estimator is a procedure applied to data which returns an estimand. Let ^\u0012()be the estimator, and\n^\u0012(D)be the estimand. In frequentist statistics, we treat the data as a random variable, drawn from\nsome true but unknown distribution, p\u0003(D); this induces a distribution over the estimand, p\u0003(^\u0012(D)),\nknown as the sampling distribution (see Section 4.7.1). In this section, we discuss two key properties\nof this distribution, its bias and its variance, which we deﬁne below. 4.7.6.1 Bias of an estimator\nThebiasof an estimator is deﬁned as\nbias(^\u0012(\u0001)),Eh\n^\u0012(D)i\n\u0000\u0012\u0003(4.232)\nwhere\u0012\u0003is the true parameter value, and the expectation is wrt “nature’s distribution” p(Dj\u0012\u0003). If\nthe bias is zero, the estimator is called unbiased . For example, the MLE for a Gaussian mean is\nunbiased:\nbias(^\u0016) =E[x]\u0000\u0016=E\"\n1\nNNX\nn=1xn#\n\u0000\u0016=N\u0016\nN\u0000\u0016= 0 (4.233)\nwherexis the sample mean. However, the MLE for a Gaussian variance, \u001b2\nmle=1\nNPN\nn=1(xn\u0000x)2, is not an unbiased estimator\nof\u001b2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 427, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 993}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0428_b638a3f0", "text": "However, the MLE for a Gaussian variance, \u001b2\nmle=1\nNPN\nn=1(xn\u0000x)2, is not an unbiased estimator\nof\u001b2. In fact, one can show (Exercise 4.7) that\nE\u0002\n\u001b2\nmle\u0003\n=N\u00001\nN\u001b2(4.234)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n156 Chapter 4. Statistics\nso the ML estimator slightly underestimates the variance. Intuitively, this is because we “use up”\none of the data points to estimate the mean, so if we have a sample size of 1, we will estimate the\nvariance to be 0. If, however, \u0016is known, the ML estimator is unbiased (see Exercise 4.8). Now consider the following estimator\n\u001b2\nunb,1\nN\u00001NX\nn=1(xn\u0000x)2=N\nN\u00001\u001b2\nmle (4.235)\nThis is an unbiased estimator, which we can easily prove as follows:\nE\u0002\n\u001b2\nunb\u0003\n=N\nN\u00001E\u0002\n\u001b2\nmle\u0003\n=N\nN\u00001N\u00001\nN\u001b2=\u001b2(4.236)\n4.7.6.2 Variance of an estimator\nIt seems intuitively reasonable that we want our estimator to be unbiased. However, being unbiased is\nnotenough. Forexample, supposewewanttoestimatethemeanofaGaussianfrom D=fx1;:::;xNg.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 428, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 969}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0429_1c062b0d", "text": "However, being unbiased is\nnotenough. Forexample, supposewewanttoestimatethemeanofaGaussianfrom D=fx1;:::;xNg. The estimator that just looks at the ﬁrst data point, ^\u0012(D) =x1, is an unbiased estimator, but will\ngenerally be further from \u0012\u0003than the empirical mean x(which is also unbiased). So the variance of\nan estimator is also important. We deﬁne the variance of an estimator as follows:\nVh\n^\u0012i\n,Eh\n^\u00122i\n\u0000\u0010\nEh\n^\u0012i\u00112\n(4.237)\nwhere the expectation is taken wrt p(Dj\u0012\u0003). This measures how much our estimate will change as\nthe data changes. We can extend this to a covariance matrix for vector valued estimators. 4.7.6.3 The bias-variance tradeoﬀ\nIn this section, we discuss a fundamental tradeoﬀ that needs to be made when picking a method\nfor parameter estimation, assuming our goal is to minimize the mean squared error (MSE) of our\nestimate. Let ^\u0012=^\u0012(D)denote the estimate, and \u0012=Eh\n^\u0012i\ndenote the expected value of the estimate\n(as we varyD).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 429, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 947}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0430_71aeafcf", "text": "Let ^\u0012=^\u0012(D)denote the estimate, and \u0012=Eh\n^\u0012i\ndenote the expected value of the estimate\n(as we varyD). (All expectations and variances are wrt p(Dj\u0012\u0003), but we drop the explicit conditioning\nfor notational brevity.) Then we have\nEh\n(^\u0012\u0000\u0012\u0003)2i\n=E\u0014h\n(^\u0012\u0000\u0012) + (\u0012\u0000\u0012\u0003)i2\u0015\n(4.238)\n=E\u0014\u0010\n^\u0012\u0000\u0012\u00112\u0015\n+ 2(\u0012\u0000\u0012\u0003)Eh\n^\u0012\u0000\u0012i\n+ (\u0012\u0000\u0012\u0003)2(4.239)\n=E\u0014\u0010\n^\u0012\u0000\u0012\u00112\u0015\n+ (\u0012\u0000\u0012\u0003)2(4.240)\n=Vh\n^\u0012i\n+bias2(^\u0012) (4.241)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.7. Frequentist statistics * 157\n-1-0.5 00.5 11.5 22.500.10.20.30.40.50.60.70.8sampling distribution, truth = 1.0, prior = 0.0, n = 5\npostMean0\npostMean1\npostMean2\npostMean3\n(a)\nsample size51015202530354045relative MSE\n0.50.60.70.80.911.1MSE of postmean / MSE of MLE\npostMean0\npostMean1\npostMean2\npostMean3 (b)\nFigure 4.24: Left: Sampling distribution of the MAP estimate (equivalent to the posterior mean) under a\nN(\u00120= 0;\u001b2=\u00140)prior with diﬀerent prior strengths \u00140.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 430, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 923}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0431_617f12d3", "text": "(If we set \u0014= 0, the MAP estimate reduces to the\nMLE.) The data is n= 5samples drawn from N(\u0012\u0003= 1;\u001b2= 1). Right: MSE relative to that of the MLE\nversus sample size. Adapted from Figure 5.6 of [Hof09]. Generated by code at ﬁgures.probml.ai/book1/4.24. In words,\nMSE = variance + bias2(4.242)\nThis is called the bias-variance tradeoﬀ (see e.g., [GBD92]). What it means is that it might be\nwise to use a biased estimator, so long as it reduces our variance by more than the square of the bias,\nassuming our goal is to minimize squared error. 4.7.6.4 Example: MAP estimator for a Gaussian mean\nLet us give an example, based on [Hof09, p79]. Suppose we want to estimate the mean of a Gaussian\nfromx= (x1;:::;xN). We assume the data is sampled from xn\u0018N (\u0012\u0003= 1;\u001b2). An obvious\nestimate is the MLE. This has a bias of 0 and a variance of\nV[xj\u0012\u0003] =\u001b2\nN(4.243)\nBut we could also use a MAP estimate.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 431, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 889}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0432_c5a57a93", "text": "We assume the data is sampled from xn\u0018N (\u0012\u0003= 1;\u001b2). An obvious\nestimate is the MLE. This has a bias of 0 and a variance of\nV[xj\u0012\u0003] =\u001b2\nN(4.243)\nBut we could also use a MAP estimate. In Section 4.6.4.2, we show that the MAP estimate under a\nGaussian prior of the form N(\u00120;\u001b2=\u00140)is given by\n~x,N\nN+\u00140x+\u00140\nN+\u00140\u00120=wx+ (1\u0000w)\u00120 (4.244)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n158 Chapter 4. Statistics\nwhere 0\u0014w\u00141controls how much we trust the MLE compared to our prior. The bias and variance\nare given by\nE[~x]\u0000\u0012\u0003=w\u0012\u0003+ (1\u0000w)\u00120\u0000\u0012\u0003= (1\u0000w)(\u00120\u0000\u0012\u0003) (4.245)\nV[~x] =w2\u001b2\nN(4.246)\nSo although the MAP estimate is biased (assuming w<1), it has lower variance. Let us assume that our prior is slightly misspeciﬁed, so we use \u00120= 0, whereas the truth is \u0012\u0003= 1. In Figure 4.24(a), we see that the sampling distribution of the MAP estimate for \u00140>0is biased\naway from the truth, but has lower variance (is narrower) than that of the MLE. In Figure 4.24(b), we plot mse(~x)=mse(x)vsN.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 432, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0433_d7363fcc", "text": "In Figure 4.24(b), we plot mse(~x)=mse(x)vsN. We see that the MAP estimate has lower MSE\nthan the MLE for \u001402f1;2g. The case \u00140= 0corresponds to the MLE, and the case \u00140= 3\ncorresponds to a strong prior, which hurts performance because the prior mean is wrong. Thus we\nsee that, provided the prior strength is properly “tuned”, a MAP estimate can outperform an ML\nestimate in terms of minimizing MSE. 4.7.6.5 Example: MAP estimator for linear regression\nAnother important example of the bias-variance tradeoﬀ arises in ridge regression, which we discuss\nin Section 11.3. In brief, this corresponds to MAP estimation for linear regression under a Gaussian\nprior,p(w) =N(wj0;\u0015\u00001I)The zero-mean prior encourages the weights to be small, which reduces\noverﬁtting; the precision term, \u0015, controls the strength of this prior. Setting \u0015= 0results in the\nMLE; using \u0015>0results in a biased estimate.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 433, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 890}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0434_1d7deacc", "text": "Setting \u0015= 0results in the\nMLE; using \u0015>0results in a biased estimate. To illustrate the eﬀect on the variance, consider a\nsimple example where we ﬁt a 1d ridge regression model using 2 diﬀerent values of \u0015. Figure 4.25 on\nthe left plots each individual ﬁtted curve, and on the right plots the average ﬁtted curve. We see\nthat as we increase the strength of the regularizer, the variance decreases, but the bias increases. See also Figure 4.26 where we give a cartoon sketch of the bias variance tradeoﬀ in terms of model\ncomplexity. 4.7.6.6 Bias-variance tradeoﬀ for classiﬁcation\nIf we use 0-1 loss instead of squared error, the frequentist risk is no longer expressible as squared bias\nplus variance. In fact, one can show (Exercise 7.2 of [HTF09]) that the bias and variance combine\nmultiplicatively. If the estimate is on the correct side of the decision boundary, then the bias is\nnegative, and decreasing the variance will decrease the misclassiﬁcation rate.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 434, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 965}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0435_17195340", "text": "If the estimate is on the correct side of the decision boundary, then the bias is\nnegative, and decreasing the variance will decrease the misclassiﬁcation rate. But if the estimate\nis on the wrong side of the decision boundary, then the bias is positive, so it pays to increase the\nvariance [Fri97a]. This little known fact illustrates that the bias-variance tradeoﬀ is not very useful\nfor classiﬁcation. It is better to focus on expected loss, not directly on bias and variance. We can\napproximate the expected loss using cross validation, as we discuss in Section 4.5.5. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.7. Frequentist statistics * 159\n0 0.2 0.4 0.6 0.8 1−1.5−1−0.500.511.5ln(λ) = 5\n0 0.2 0.4 0.6 0.8 1−1−0.500.51ln(λ) = 5\n0 0.2 0.4 0.6 0.8 1−1.5−1−0.500.511.5ln(λ) = −5\n0 0.2 0.4 0.6 0.8 1−1−0.500.51ln(λ) = −5\nFigure 4.25: Illustration of bias-variance tradeoﬀ for ridge regression. We generate 100 data sets from the\ntrue function, shown in solid green.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 435, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1003}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0436_4e95081f", "text": "We generate 100 data sets from the\ntrue function, shown in solid green. Left: we plot the regularized ﬁt for 20 diﬀerent data sets. We use linear\nregression with a Gaussian RBF expansion, with 25 centers evenly spread over the [0;1]interval. Right: we\nplot the average of the ﬁts, averaged over all 100 datasets. Top row: strongly regularized: we see that the\nindividual ﬁts are similar to each other (low variance), but the average is far from the truth (high bias). Bottom row: lightly regularized: we see that the individual ﬁts are quite diﬀerent from each other (high\nvariance), but the average is close to the truth (low bias). Adapted from [Bis06] Figure 3.5. Generated by\ncode at ﬁgures.probml.ai/book1/4.25. Figure 4.26: Cartoon illustration of the bias variance tradeoﬀ. From http: // scott. fortmann-roe. com/\ndocs/ BiasVariance. html . Used with kind permission of Scott Fortmann-Roe. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n160 Chapter 4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 436, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 971}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0437_96427f8d", "text": "From http: // scott. fortmann-roe. com/\ndocs/ BiasVariance. html . Used with kind permission of Scott Fortmann-Roe. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n160 Chapter 4. Statistics\n4.8 Exercises\nExercise 4.1 [MLE for the univariate Gaussian *]\nShow that the MLE for a univariate Gaussian is given by\n^\u0016=1\nNNX\nn=1yn (4.247)\n^\u001b2=1\nNNX\nn=1(yn\u0000^\u0016)2(4.248)\nExercise 4.2 [MAP estimation for 1D Gaussians *]\n(Source: Jaakkola.)\nConsider samples x1;:::;xnfrom a Gaussian random variable with known variance \u001b2and unknown mean \u0016. We further assume a prior distribution (also Gaussian) over the mean, \u0016\u0018N(m;s2), with ﬁxed mean mand\nﬁxed variance s2. Thus the only unknown is \u0016. a.Calculate the MAP estimate ^\u0016MAP. You can state the result without proof. Alternatively, with a lot\nmore work, you can compute derivatives of the log posterior, set to zero and solve. b.Show that as the number of samples nincrease, the MAP estimate converges to the maximum likelihood\nestimate. c.Supposenis small and ﬁxed.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 437, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1014}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0438_5f91d52a", "text": "b.Show that as the number of samples nincrease, the MAP estimate converges to the maximum likelihood\nestimate. c.Supposenis small and ﬁxed. What does the MAP estimator converge to if we increase the prior variance\ns2? d.Supposenis small and ﬁxed. What does the MAP estimator converge to if we decrease the prior variance\ns2? Exercise 4.3 [Gaussian posterior credible interval]\n(Source: DeGroot.) Let X\u0018N (\u0016;\u001b2= 4)where\u0016is unknown but has prior \u0016\u0018N (\u00160;\u001b2\n0= 9). The\nposterior after seeing nsamples is\u0016\u0018N(\u0016n;\u001b2\nn). (This is called a credible interval, and is the Bayesian\nanalog of a conﬁdence interval.) How big does nhave to be to ensure\np(`\u0014\u0016n\u0014ujD)\u00150:95 (4.249)\nwhere (`;u)is an interval (centered on \u0016n) of width 1 and Dis the data? Hint: recall that 95% of the\nprobability mass of a Gaussian is within \u00061:96\u001bof the mean. Exercise 4.4 [BIC for Gaussians *]\n(Source: Jaakkola.)\nThe Bayesian information criterion (BIC) is a penalized log-likelihood function that can be used for model\nselection.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 438, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 997}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0439_7c344619", "text": "Exercise 4.4 [BIC for Gaussians *]\n(Source: Jaakkola.)\nThe Bayesian information criterion (BIC) is a penalized log-likelihood function that can be used for model\nselection. It is deﬁned as\nBIC = logp(Dj^\u0012ML)\u0000d\n2log(N) (4.250)\nwheredis the number of free parameters in the model and Nis the number of samples. In this question,\nwe will see how to use this to choose between a full covariance Gaussian and a Gaussian with a diagonal\ncovariance. Obviously a full covariance Gaussian has higher likelihood, but it may not be “worth” the extra\nparameters if the improvement over a diagonal covariance matrix is too small. So we use the BIC score to\nchoose the model. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n4.8. Exercises 161\nWe can write\nlogp(Dj^\u0006;^\u0016) =\u0000N\n2tr\u0010\n^\u0006\u00001^S\u0011\n\u0000N\n2log(^j\u0006j) (4.251)\n^S=1\nNNX\ni=1(xi\u0000x)(xi\u0000x)T(4.252)\nwhere ^Sis the scatter matrix (empirical covariance), the trace of a matrix is the sum of its diagonals, and we\nhave used the trace trick.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 439, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 994}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0440_63f68143", "text": "a.Derive the BIC score for a Gaussian in Ddimensions with full covariance matrix. Simplify your answer as\nmuch as possible, exploiting the form of the MLE. Be sure to specify the number of free parameters d. b.Derive the BIC score for a Gaussian in Ddimensions with a diagonal covariance matrix. Be sure to specify\nthe number of free parameters d. Hint: for the digaonal case, the ML estimate of \u0006is the same as ^\u0006ML\nexcept the oﬀ-diagonal terms are zero:\n^\u0006diag= diag( ^\u0006ML(1;1);:::; ^\u0006ML(D;D )) (4.253)\nExercise 4.5 [BIC for a 2d discrete distribution]\n(Source: Jaakkola.)\nLetx2f0;1gdenote the result of a coin toss ( x= 0for tails,x= 1for heads). The coin is potentially biased,\nso that heads occurs with probability \u00121. Suppose that someone else observes the coin ﬂip and reports to you\nthe outcome, y. But this person is unreliable and only reports the result correctly with probability \u00122; i.e.,\np(yjx;\u00122)is given by\ny= 0y= 1\nx= 0\u00122 1\u0000\u00122\nx= 1 1\u0000\u00122\u00122\nAssume that \u00122is independent of xand\u00121. a.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 440, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 998}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0441_b2c29530", "text": "But this person is unreliable and only reports the result correctly with probability \u00122; i.e.,\np(yjx;\u00122)is given by\ny= 0y= 1\nx= 0\u00122 1\u0000\u00122\nx= 1 1\u0000\u00122\u00122\nAssume that \u00122is independent of xand\u00121. a. Write down the joint probability distribution p(x;yj\u0012)as a2\u00022table, in terms of \u0012= (\u00121;\u00122). b.Suppose have the following dataset: x= (1;1;0;1;1;0;0),y= (1;0;0;0;1;0;1). What are the MLEs for\n\u00121and\u00122? Justify your answer. Hint: note that the likelihood function factorizes,\np(x;yj\u0012) =p(yjx;\u00122)p(xj\u00121) (4.254)\nWhat isp(Dj^\u0012;M2)whereM2denotes this 2-parameter model? (You may leave your answer in fractional\nform if you wish.)\nc.Now consider a model with 4 parameters, \u0012= (\u00120;0;\u00120;1;\u00121;0;\u00121;1), representing p(x;yj\u0012) =\u0012x;y. (Only\n3 of these parameters are free to vary, since they must sum to one.) What is the MLE of \u0012? What is\np(Dj^\u0012;M4)whereM4denotes this 4-parameter model? d.Suppose we are not sure which model is correct.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 441, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 916}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0442_86bbc6b0", "text": "What is\np(Dj^\u0012;M4)whereM4denotes this 4-parameter model? d.Suppose we are not sure which model is correct. We compute the leave-one-out cross validated log\nlikelihood of the 2-parameter model and the 4-parameter model as follows:\nL(m) =nX\ni=1logp(xi;yijm;^\u0012(D\u0000i)) (4.255)\nand ^\u0012(D\u0000i))denotes the MLE computed on Dexcluding row i. Which model will CV pick and why? Hint: notice how the table of counts changes when you omit each training case one at a time. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n162 Chapter 4. Statistics\ne. Recall that an alternative to CV is to use the BIC score, deﬁned as\nBIC(M;D),logp(Dj^\u0012MLE)\u0000dof(M)\n2logN (4.256)\nwhere dof(M)is the number of free parameters in the model, Compute the BIC scores for both models\n(use log base e). Which model does BIC prefer? Exercise 4.6 [A mixture of conjugate priors is conjugate *]\nConsider a mixture prior\np(\u0012) =X\nkp(h=k)p(\u0012jz=k) (4.257)\nwhere each p(\u0012jz=k)is conjugate to the likelihood. Prove that this is a conjugate prior.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 442, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1008}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0443_9142566a", "text": "Prove that this is a conjugate prior. Exercise 4.7 [ML estimator \u001b2\nmleis biased]\nShow that ^\u001b2\nMLE =1\nNPN\nn=1(xn\u0000^\u0016)2is a biased estimator of \u001b2, i.e., show\nEX1;:::;Xn\u0018N(\u0016;\u001b)[^\u001b2(X1;:::; Xn)]6=\u001b2\nHint: note that X1;:::;XNare independent, and use the fact that the expectation of a product of independent\nrandom variables is the product of the expectations. Exercise 4.8 [Estimation of \u001b2when\u0016is known*]\nSuppose we sample x1;:::;xN\u0018N(\u0016;\u001b2)where\u0016is aknownconstant. Derive an expression for the MLE\nfor\u001b2in this case. Is it unbiased? Exercise 4.9 [Variance and MSE of estimators for Gaussian variance *]\nProve that the standard error for the MLE for a Gaussian variance is\nq\nV[\u001b2\nmle] =r\n2(N\u00001)\nN2\u001b2(4.258)\nHint: use the fact that\nN\u00001\n\u001b2\u001b2\nunb\u0018\u001f2\nN\u00001; (4.259)\nand that V\u0002\n\u001f2\nN\u00001\u0003\n= 2(N\u00001). Finally, show that MSE (\u001b2\nunb) =2N\u00001\nN2\u001b4and MSE (\u001b2\nmle) =2\nN\u00001\u001b4. Draft of “Probabilistic Machine Learning: An Introduction”.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 443, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 916}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0444_be3cae9f", "text": "Finally, show that MSE (\u001b2\nunb) =2N\u00001\nN2\u001b4and MSE (\u001b2\nmle) =2\nN\u00001\u001b4. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5Decision Theory\n5.1 Bayesian decision theory\nBayesian inference provides the optimal way to update our beliefs about hidden quantities Hgiven\nobserved data X=xby computing the posterior p(Hjx). However, at the end of the day, we\nneed to turn our beliefs into actions that we can perform in the world. How can we decide which\naction is best? This is where Bayesian decision theory comes in. In this chapter, we give a brief\nintroduction. For more details, see e.g., [DeG70; KWW22]. 5.1.1 Basics\nIn decision theory, we assume the decision maker, or agent, has a set of possible actions, A, to\nchoose from. For example, consider the case of a hypothetical doctor treating someone who may\nhave COVID-19. Suppose the actions are to do nothing, or to give the patient an expensive drug\nwith bad side eﬀects, but which can save their life.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 444, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 978}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0445_26964942", "text": "Suppose the actions are to do nothing, or to give the patient an expensive drug\nwith bad side eﬀects, but which can save their life. Each of these actions has costs and beneﬁts, which will depend on the underlying state of nature\nH2H. We can encode this information into a loss function `(h;a), that speciﬁes the loss we incur\nif we take action a2Awhen the state of nature is h2H. For example, suppose the state is deﬁned by the age of the patient (young vs old), and whether\nthey have COVID-19 or not. Note that the age can be observed directly, but the disease state must\nbe inferred from noisy observations, as we discussed in Section 2.3. Thus the state is partially\nobserved . Let us assume that the cost of administering a drug is the same, no matter what the state of the\npatient is. However, the beneﬁts will diﬀer.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 445, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 823}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0446_b61444f0", "text": "Thus the state is partially\nobserved . Let us assume that the cost of administering a drug is the same, no matter what the state of the\npatient is. However, the beneﬁts will diﬀer. If the patient is young, we expect them to live a long\ntime, so the cost of not giving the drug if they have COVID-19 is high; but if the patient is old, they\nhave fewer years to live, so the cost of not giving the drug if they have COVID-19 is arguably less\n(especially in view of the side eﬀects). In medical circles, a common unit of cost is quality-adjusted\nlife years orQALY. Suppose that the expected QALY for a young person is 60, and for an old\nperson is 10. Let us assume the drug costs the equivalent of 8 QALY, due to induced pain and\nsuﬀering from side eﬀects. Then we get the loss matrix shown in Table 5.1. These numbers reﬂect relative costs and beneﬁts, and will depend on many factors. The numbers\ncan be derived by asking the decision maker about their preferences about diﬀerent possible\noutcomes.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 446, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 997}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0447_597694fd", "text": "These numbers reﬂect relative costs and beneﬁts, and will depend on many factors. The numbers\ncan be derived by asking the decision maker about their preferences about diﬀerent possible\noutcomes. It is a theorem of decision theory that any consistent set of preferences can be converted\ninto an ordinal cost scale (see e.g., https://en.wikipedia.org/wiki/Preference_(economics) ). Once we have speciﬁed the loss function, we can compute the posterior expected loss orrisk\n164 Chapter 5. Decision Theory\nState Nothing Drugs\nNo COVID-19, young 0 8\nCOVID-19, young 60 8\nNo COVID-19, old 0 8\nCOVID-19, old 10 8\nTable 5.1: Hypothetical loss matrix for a decision maker, where there are 4 states of nature, and 2 possible\nactions. test age pr(covid) cost-noop cost-drugs action\n0 0 0.01 0.84 8.00 0\n0 1 0.01 0.14 8.00 0\n1 0 0.80 47.73 8.00 1\n1 1 0.80 7.95 8.00 0\nTable 5.2: Optimal policy for treating COVID-19 patients for each possible observation.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 447, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 944}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0448_c57c67c5", "text": "for each possible action:\nR(ajx),Ep(hjx)[`(h;a)] =X\nh2H`(h;a)p(hjx) (5.1)\nTheoptimal policy (also called the Bayes estimator ) speciﬁes what action to take for each\npossible observation so as to minimize the risk:\n\u0019\u0003(x) = argmin\na2AEp(hjx)[`(h;a)] (5.2)\nAn alternative, but equivalent, way of stating this result is as follows. Let us deﬁne a utility\nfunctionU(h;a)to be the desirability of each possible action in each possible state. If we set\nU(h;a) =\u0000`(h;a), then the optimal policy is as follows:\n\u0019\u0003(x) = argmax\na2AEh[U(h;a)] (5.3)\nThis is called the maximum expected utility principle . Let us return to our COVID-19 example. The observation xconsists of the age (young or old)\nand the test result (positive or negative). Using the results from Section 2.3.1 on Bayes rule for\nCOVID-19 diagnosis, we can convert the test result into a distribution over disease states (i.e.,\ncompute the probability the patient has COVID-19 or not).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 448, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 938}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0449_8146229e", "text": "Given this belief state, and the loss\nmatrix in Table 5.1, we can compute the optimal policy for each possible observation, as shown in\nTable 5.2. We see from Table 5.2 that the drug should only be given to young people who test positive. If,\nhowever, we reduce the cost of the drug from 8 units to 5, then the optimal policy changes: in this\ncase, we should give the drug to everyone who tests positive. The policy can also change depending\non the reliability of the test. For example, if we increase the sensitivity from 0.875 to 0.975, then\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5.1. Bayesian decision theory 165\nthe probability that someone has COVID-19 if they test positive increases from 0.80 to 0.81, which\nchanges the optimal policy to be one in which we should administer the drug to everyone who tests\npositive, even if the drug costs 8 QALY.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 449, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 891}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0450_24a4e54e", "text": "(See code.probml.ai/book1/dtheory for the code to reproduce\nthis example.)\nSo far, we have implicitly assumed that the agent is risk neutral . This means that their decision\nis not aﬀected by the degree of certainty in a set of outcomes. For example, such an agent would be\nindiﬀerent between getting $50 for sure, of a 50% chance of $100 or $0. By contrast, a risk averse\nagent would choose the ﬁrst. We can generalize the framework of Bayesian decision theory to risk\nsensitive applications, but we do not pursue the matter here. (See e.g., [Cho+15] for details.)\n5.1.2 Classiﬁcation problems\nIn this section, we use Bayesian decision theory to decide the optimal class label to predict given an\nobserved input x2X. 5.1.2.1 Zero-one loss\nSuppose the states of nature correspond to class labels, so H=Y=f1;:::;Cg. Furthermore,\nsuppose the actions also correspond to class labels, so A=Y.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 450, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 888}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0451_6cfbc431", "text": "5.1.2.1 Zero-one loss\nSuppose the states of nature correspond to class labels, so H=Y=f1;:::;Cg. Furthermore,\nsuppose the actions also correspond to class labels, so A=Y. In this setting, a very commonly used\nloss function is the zero-one loss `01(y\u0003;^y), deﬁned as follows:\n^y= 0 ^y= 1\ny\u0003= 0 0 1\ny\u0003= 1 1 0(5.4)\nWe can write this more concisely as follows:\n`01(y\u0003;^y) =I(y\u00036= ^y) (5.5)\nIn this case, the posterior expected loss is\nR(^yjx) =p(^y6=y\u0003jx) = 1\u0000p(y\u0003= ^yjx) (5.6)\nHence the action that minimizes the expected loss is to choose the most probable label:\n\u0019(x) = argmax\ny2Yp(yjx) (5.7)\nThis corresponds to the modeof the posterior distribution, also known as the maximum a\nposteriori orMAP estimate\n5.1.2.2 Cost-sensitive classiﬁcation\nConsider a binary classiﬁcation problem where the loss function is `(y\u0003;^y)is as follows:\n\u0012`00`01\n`10`11\u0013\n(5.8)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n166 Chapter 5. Decision Theory\nLetp0=p(y\u0003= 0jx)andp1= 1\u0000p0.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 451, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 973}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0452_e3215540", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n166 Chapter 5. Decision Theory\nLetp0=p(y\u0003= 0jx)andp1= 1\u0000p0. Thus we should choose label ^y= 0iﬀ\n`00p0+`10p1<`01p0+`11p1 (5.9)\nIf`00=`11= 0, this simpliﬁes to\np1<`01\n`01+`10(5.10)\nNow suppose `10=c`01, so a false negative costs ctimes more than a false positive. The decision rule\nfurther simpliﬁes to the following: pick a= 0iﬀp1<1=(1 +c). For example, if a false negative costs\ntwice as much as false positive, so c= 2, then we use a decision threshold of 1=3before declaring a\npositive. 5.1.2.3 Classiﬁcation with the “reject” option\nIn some cases, we may able to say “I don’t know” instead of returning an answer that we don’t really\ntrust; this is called picking the reject option (see e.g., [BW08]). This is particularly important in\ndomains such as medicine and ﬁnance where we may be risk averse. We can formalize the reject option as follows.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 452, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 893}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0453_15bc21e3", "text": "This is particularly important in\ndomains such as medicine and ﬁnance where we may be risk averse. We can formalize the reject option as follows. Suppose the states of nature are H=Y=f1;:::;Cg,\nand the actions are A=Y[f 0g, where action 0 represents the reject action. Now deﬁne the following\nloss function:\n`(y\u0003;a) =8\n<\n:0ify\u0003=aanda2f1;:::;Cg\n\u0015r ifa= 0\n\u0015e otherwise(5.11)\nwhere\u0015ris the cost of the reject action, and \u0015eis the cost of a classiﬁcation error. Exercise 5.1\nasks you to show that the optimal action is to pick the reject action if the most probable class has\na probability below \u0015\u0003= 1\u0000\u0015r\n\u0015e; otherwise you should just pick the most probable class. In other\nwords, the optimal policy is as follows:\na\u0003=(\ny\u0003ifp\u0003>\u0015\u0003\nreject otherwise(5.12)\nwhere\ny\u0003= argmax\ny2f1;:::;Cgp(yjx) (5.13)\np\u0003=p(y\u0003jx) = max\ny2f1;:::;Cgp(yjx) (5.14)\n\u0015\u0003= 1\u0000\u0015r\n\u0015e(5.15)\nSee Figure 5.1 for an illustration. One interesting application of the reject option arises when playing the TV game show Jeopardy.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 453, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 982}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0454_b45c6a8e", "text": "One interesting application of the reject option arises when playing the TV game show Jeopardy. In\nthis game, contestants have to solve various word puzzles and answer a variety of trivia questions, but\nif they answer incorrectly, they lose money. In 2011, IBM unveiled a computer system called Watson\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5.1. Bayesian decision theory 167\n1.0threshold\nReject RegionP(y = 1 | X)P(y = 2 | X)\n0.0X\nFigure 5.1: For some regions of input space, where the class posteriors are uncertain, we may prefer not to\nchoose class 1 or 2; instead we may prefer the reject option. Adapted from Figure 1.26 of [Bis06]. Estimate Row sum\n0 1\nTruth0TN FP N\n1FN TP P\nCol. sum ^N ^P\nTable 5.3: Class confusion matrix for a binary classiﬁcation problem.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 454, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 803}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0455_ff9f1f52", "text": "Adapted from Figure 1.26 of [Bis06]. Estimate Row sum\n0 1\nTruth0TN FP N\n1FN TP P\nCol. sum ^N ^P\nTable 5.3: Class confusion matrix for a binary classiﬁcation problem. TP is the number of true positives, FP\nis the number of false positives, TN is the number of true negatives, FN is the number of false negatives, Pis\nthe true number of positives, ^Pis the predicted number of positives, Nis the true number of negatives, ^Nis\nthe predicted number of negatives. which beat the top human Jeopardy champion. Watson uses a variety of interesting techniques\n[Fer+10], but the most pertinent one for our present discussion is that it contains a module that\nestimates how conﬁdent it is of its answer. The system only chooses to “buzz in” its answer if\nsuﬃciently conﬁdent it is correct. For some other methods and applications, see e.g., [Cor+16; GEY19].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 455, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 847}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0456_fc2f7296", "text": "The system only chooses to “buzz in” its answer if\nsuﬃciently conﬁdent it is correct. For some other methods and applications, see e.g., [Cor+16; GEY19]. 5.1.3 ROC curves\nIn Section 5.1.2.2, we showed that we can pick the optimal label in a binary classiﬁcation problem\nby thresholding the probability using a value \u001c, derived from the relative cost of a false positive\nand false negative. Instead of picking a single threshold, we can consider using a set of diﬀerent\nthresholds, and comparing the resulting performance, as we discuss below. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n168 Chapter 5. Decision Theory\nEstimate\n0 1\nTruth0TN/N=TNR=Spec FP/N =FPR=Type I = Fallout\n1FN/P=FNR=Miss=Type II TP/P=TPR=Sens=Recall\nTable 5.4: Class confusion matrix for a binary classiﬁcation problem normalized per row to get p(^yjy).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 456, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 841}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0457_96110958", "text": "Abbreviations: TNR = true negative rate, Spec = speciﬁcity, FPR = false positive rate, FNR = false negative\nrate, Miss = miss rate, TPR = true positive rate, Sens = sensitivity. Note FNR=1-TPR and FPR=1-TNR. Estimate\n0 1\nTruth0TN/ ^N=NPV FP/ ^P=FDR\n1FN/ ^N=FOR TP/ ^P=Prec=PPV\nTable 5.5: Class confusion matrix for a binary classiﬁcation problem normalized per column to get p(yj^y). Abbreviations: NPV = negative predictive value, FDR = false discovery rate, FOR = false omission rate,\nPPV = positive predictive value, Prec = precision. Note that FOR=1-NPV and FDR=1-PPV. 5.1.3.1 Class confusion matrices\nFor any ﬁxed threshold \u001c, we consider the following decision rule:\n^y\u001c(x) =I(p(y= 1jx)\u00151\u0000\u001c) (5.16)\nWe can compute the empirical number of false positives (FP) that arise from using this policy on a\nset ofNlabeled examples as follows:\nFP\u001c=NX\nn=1I(^y\u001c(xn) = 1;yn= 0) (5.17)\nSimilarly, we can compute the empirical number of false negatives (FN), true positives (TP), and\ntrue negatives (TN).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 457, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 995}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0458_8325d421", "text": "We can store these results in a 2\u00022class confusion matrix C, whereCijis\nthe number of times an item with true class label iwas (mis)classiﬁed as having label j. In the case\nof binary classiﬁcation problems, the resulting matrix will look like Table 5.3. From this table, we can compute p(^yjy)orp(yj^y), depending on whether we normalize across the\nrows or columns. We can derive various summary statistics from these distributions, as summarized in\nTable 5.4 and Table 5.5. For example, the true positive rate (TPR), also known as the sensitivity ,\nrecallorhit rate , is deﬁned as\nTPR\u001c=p(^y= 1jy= 1;\u001c) =TP\u001c\nTP\u001c+FN\u001c(5.18)\nand thefalse positive rate (FPR), also called the false alarm rate , or thetype I error rate , is\ndeﬁned as\nFPR\u001c=p(^y= 1jy= 0;\u001c) =FP\u001c\nFP\u001c+TN\u001c(5.19)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5.1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 458, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 850}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0459_2cbf1aa4", "text": "August 27, 2021\n5.1. Bayesian decision theory 169\n0.0 0.2 0.4 0.6 0.8 1.0\nFPR0.00.20.40.60.81.0TPRA\nB\n(a)\n0.0 0.2 0.4 0.6 0.8 1.0\nrecall0.00.20.40.60.81.0precisionA B (b)\nFigure 5.2: (a) ROC curves for two hypothetical classiﬁcation systems. The red curve for system A is better\nthan the blue curve for system B. We plot the true positive rate (TPR) vs the false positive rate (FPR) as we\nvary the threshold \u001c. We also indicate the equal error rate (EER) with the red and blue dots, and the area\nunder the curve (AUC) for classiﬁer B by the shaded area. Generated by code at ﬁgures.probml.ai/book1/5.2. (b) A precision-recall curve for two hypothetical classiﬁcation systems. The red curve for system A is better\nthan the blue curve for system B. Generated by code at ﬁgures.probml.ai/book1/5.2. We can now plot the TPR vs FPR as an implicit function of \u001c. This is called a receiver operating\ncharacteristic orROCcurve. See Figure 5.2(a) for an example.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 459, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 953}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0460_9c37ecac", "text": "We can now plot the TPR vs FPR as an implicit function of \u001c. This is called a receiver operating\ncharacteristic orROCcurve. See Figure 5.2(a) for an example. 5.1.3.2 Summarizing ROC curves as a scalar\nThe quality of a ROC curve is often summarized as a single number using the area under the\ncurveorAUC. Higher AUC scores are better; the maximum is obviously 1. Another summary\nstatistic that is used is the equal error rate orEER, also called the cross-over rate , deﬁned as\nthe value which satisﬁes FPR = FNR. Since FNR=1-TPR, we can compute the EER by drawing a\nline from the top left to the bottom right and seeing where it intersects the ROC curve (see points A\nand B in Figure 5.2(a)). Lower EER scores are better; the minimum is obviously 0 (corresponding to\nthe top left corner). 5.1.3.3 Class imbalance\nIn some problems, there is severe class imbalance . For example, in information retrieval, the set of\nnegatives (irrelevant items) is usually much larger than the set of positives (relevant items).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 460, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1009}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0461_3f326189", "text": "For example, in information retrieval, the set of\nnegatives (irrelevant items) is usually much larger than the set of positives (relevant items). The ROC\ncurve is unaﬀected by class imbalance, as the TPR and FPR are fractions within the positives and\nnegatives, respectively. However, the usefulness of an ROC curve may be reduced in such cases, since\na large change in the absolute number of false positives will not change the false positive ratevery\nmuch, since FPR is divided by FP+TN (see e.g., [SR15] for discussion). Thus all the “action” happens\nin the extreme left part of the curve. In such cases, we may choose to use other ways of summarizing\nthe class confusion matrix, such as precision-recall curves, which we discuss in Section 5.1.4. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n170 Chapter 5. Decision Theory\n5.1.4 Precision-recall curves\nIn some problems, the notion of a “negative” is not well-deﬁned.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 461, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 936}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0462_318e3107", "text": "Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n170 Chapter 5. Decision Theory\n5.1.4 Precision-recall curves\nIn some problems, the notion of a “negative” is not well-deﬁned. For example, consider detecting\nobjects in images: if the detector works by classifying patches, then the number of patches examined\n— and hence the number of true negatives — is a parameter of the algorithm, not part of the problem\ndeﬁnition. Similarly, information retrieval systems usually get to choose the initial set of candidate\nitems, which are then ranked for relevance; by specifying a cutoﬀ, we can partition this into a positive\nand negative set, but note that the size of the negative set depends on the total number of items\nretrieved, which is an algorithm parameter, not part of the problem speciﬁcation. In these kinds of situations, we may choose to use a precision-recall curve to summarize the\nperformance of our system, as we explain below.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 462, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 947}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0463_caf1f7e4", "text": "In these kinds of situations, we may choose to use a precision-recall curve to summarize the\nperformance of our system, as we explain below. (See [DG06] for a more detailed discussion of the\nconnection between ROC curves and PR curves.)\n5.1.4.1 Computing precision and recall\nThe key idea is to replace the FPR with a quantity that is computed just from positives, namely the\nprecision :\nP(\u001c),p(y= 1j^y= 1;\u001c) =TP\u001c\nTP\u001c+FP\u001c(5.20)\nThe precision measures what fraction of our detections are actually positive. We can compare this to\ntherecall(which is the same as the TPR), which measures what fraction of the positives we actually\ndetected:\nR(\u001c),p(^y= 1jy= 1;\u001c) =TP\u001c\nTP\u001c+FN\u001c(5.21)\nIf^yn2f0;1gis the predicted label, and yn2f0;1gis the true label, we can estimate precision\nand recall using\nP(\u001c) =P\nnyn^ynP\nn^yn(5.22)\nR(\u001c) =P\nnyn^ynP\nnyn(5.23)\nWe can now plot the precision vs recall as we vary the threshold \u001c. See Figure 5.2(b). Hugging\nthe top right is the best one can do.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 463, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 972}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0464_bcfa1b3d", "text": "See Figure 5.2(b). Hugging\nthe top right is the best one can do. 5.1.4.2 Summarizing PR curves as a scalar\nThe PR curve can be summarized as a single number in several ways. First, we can quote the\nprecision for a ﬁxed recall level, such as the precision of the ﬁrst K= 10entities recalled. This\nis called the precision at K score. Alternatively, we can compute the area under the PR curve. However, it is possible that the precision does not drop monotonically with recall. For example,\nsuppose a classiﬁer has 90% precision at 10% recall, and 96% precision at 20% recall. In this case,\nrather than measuring the precision ata recall of 10%, we should measure the maximum precision\nwe can achieve with at leasta recall of 10% (which would be 96%). This is called the interpolated\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5.1. Bayesian decision theory 171\nprecision .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 464, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 902}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0465_9f92f25d", "text": "This is called the interpolated\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5.1. Bayesian decision theory 171\nprecision . The average of the interpolated precisions is called the average precision ; it is equal\nto the area under the interpolated PR curve, but may not be equal to the area under the raw PR\ncurve.1Themean average precision ormAPis the mean of the AP over a set of diﬀerent PR\ncurves. 5.1.4.3 F-scores\nFor a ﬁxed threshold, corresponding to a single point on the PR curve, we can compute a single\nprecision and recall value, which we will denote by PandR. These are often combined into a single\nstatistic called the F\f, which weights recall as \f >0more important than precision.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 465, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 725}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0466_9a7f2b5c", "text": "These are often combined into a single\nstatistic called the F\f, which weights recall as \f >0more important than precision. This is deﬁned\nas follows:\n1\nF\f=1\n1 +\f21\nP+\f2\n1 +\f21\nR(5.24)\nor equivalently\nF\f,(1 +\f2)P\u0001R\n\f2P+R=(1 +\f2)TP\n(1 +\f2)TP+\f2FN+FP(5.25)\nIf we set\f= 1, we get the harmonic mean of precision and recall:\n1\nF1=1\n2\u00121\nP+1\nR\u0013\n(5.26)\nF1=2\n1=R+ 1=P= 2P\u0001R\nP+R=TP\nTP+1\n2(FP+FN)(5.27)\nTo understand why we use the harmonic mean instead of the arithmetic mean, (P+R)=2, consider\nthe following scenario. Suppose we recall all entries, so ^yn= 1for alln, andR= 1. In this case, the\nprecisionPwill be given by the prevalence ,p(y= 1) =P\nnI(yn=1)\nN. Suppose the prevalence is low,\nsayp(y= 1) = 10\u00004. The arithmetic mean of PandRis given by (P+R)=2 = (10\u00004+ 1)=2\u001950%. By contrast, the harmonic mean of this strategy is only2\u000210\u00004\u00021\n1+10\u00004\u00190:2%. In general, the harmonic\nmean is more conservative, and requires both precision and recall to be high. UsingF1score weights precision and recall equally.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 466, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 998}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0467_b9da4e1d", "text": "In general, the harmonic\nmean is more conservative, and requires both precision and recall to be high. UsingF1score weights precision and recall equally. However, if recall is more important, we may\nuse\f= 2, and if precision is more important, we may use \f= 0:5. 5.1.4.4 Class imbalance\nROC curves are insensitive to class imbalance, but PR curves are not, as noted in [Wil20]. To see this,\nlet the fraction of positives in the dataset be \u0019=P=(P+N), and deﬁne the ratio r=P=N =\u0019=(1\u0000\u0019). Letn=P+Nbe the population size. ROC curves are not aﬀected by changes in r, since the TPR\nis deﬁned as a ratio within the positive examples, and FPR is deﬁned as a ratio within the negative\nexamples. This means it does not matter which class we deﬁne as positive, and which we deﬁne as\nnegative. 1. For details, see https://sanchom.wordpress.com/tag/average-precision/ . Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n172 Chapter 5. Decision Theory\nNow consider PR curves.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 467, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 971}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0468_c415353b", "text": "1. For details, see https://sanchom.wordpress.com/tag/average-precision/ . Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n172 Chapter 5. Decision Theory\nNow consider PR curves. The precision can be written as\nPrec =TP\nTP+FP=P\u0001TPR\nP\u0001TPR +N\u0001FPR=TPR\nTPR +1\nrFPR(5.28)\nThusPrec!1as\u0019!1andr!1, andPrec!0as\u0019!0andr!0. For example, if we\nwe change from a balanced problem where r= 0:5to an imbalanced problem where r= 0:1(so\npositives are rarer), the precision at each threshold will drop, and the recall (aka TPR) will stay the\nsame, so the overall PR curve will be lower. Thus if we have multiple binary problems with diﬀerent\nprevalences (e.g., object detection of common or rare objects), we should be careful when averaging\ntheir precisions [HCD12]. The F-score is also aﬀected by class imbalance.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 468, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 806}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0469_533e2d83", "text": "The F-score is also aﬀected by class imbalance. To see this, note that we can rewrite the F-score\nas follows:\n1\nF\f=1\n1 +\f21\nP+\f2\n1 +\f21\nR(5.29)\n=1\n1 +\f2TPR +N\nPFPR\nTPR+\f2\n1 +\f21\nTPR(5.30)\nF\f=(1 +\f2)TPR\nTPR +1\nrFPR +\f2(5.31)\n5.1.5 Regression problems\nSo far, we have considered the case where there are a ﬁnite number of actions Aand states of nature\nH. In this section, we consider the case where the set of actions and states are both equal to the real\nline,A=H=R. We will specify various commonly used loss functions for this case (which can be\nextended to RDby computing the loss elementwise.) The resulting decision rules can be used to\ncompute the optimal parameters for an estimator to return, or the optimal action for a robot to take,\netc.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 469, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 747}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0470_b24aa30a", "text": "5.1.5.1 L2 loss\nThe most common loss for continuous states and actions is the `2loss, also called squared error\norquadratic loss , which is deﬁned as follows:\n`2(h;a) = (h\u0000a)2(5.32)\nIn this case, the risk is given by\nR(ajx) =E\u0002\n(h\u0000a)2jx\u0003\n=E\u0002\nh2jx\u0003\n\u00002aE[hjx] +a2(5.33)\nThe optimal action must satisfy the condition that the derivative of the risk (at that point) is zero\n(as explained in Chapter 8). Hence the optimal action is to pick the posterior mean:\n@\n@aR(ajx) =\u00002E[hjx] + 2a= 0)\u0019(x) =E[hjx] =Z\nhp(hjx)dh (5.34)\nThis is often called the minimum mean squared error estimate or MMSE estimate. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5.1. Bayesian decision theory 173\n−3 −2 −1 0 1 2 3−0.500.511.522.533.544.55\n \nL2\nL1\nhuber\nFigure 5.3: Illustration of `2,`1, and Huber loss functions with \u000e= 1:5. Generated by code at ﬁg-\nures.probml.ai/book1/5.3. 5.1.5.2 L1 loss\nThe`2loss penalizes deviations from the truth quadratically, and thus is sensitive to outliers.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 470, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 997}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0471_59011b30", "text": "Generated by code at ﬁg-\nures.probml.ai/book1/5.3. 5.1.5.2 L1 loss\nThe`2loss penalizes deviations from the truth quadratically, and thus is sensitive to outliers. A\nmorerobustalternative is the absolute or `1loss\n`1(h;a) =jh\u0000aj (5.35)\nThis is sketched in Figure 5.3. Exercise 5.4 asks you to show that the optimal estimate is the\nposterior median , i.e., a value asuch that Pr(h<ajx) =Pr(h\u0015ajx) = 0:5. We can use this for\nrobust regression as discussed in Section 11.6.1. 5.1.5.3 Huber loss\nAnother robust loss function is the Huber loss [Hub64], deﬁned as follows:\n`\u000e(h;a) =\u001ar2=2ifjrj\u0014\u000e\n\u000ejrj\u0000\u000e2=2ifjrj>\u000e(5.36)\nwherer=h\u0000a. This is equivalent to `2for errors that are smaller than \u000e, and is equivalent to `1\nfor larger errors. See Figure 5.3 for a plot. We can use this for robust regression as discussed in\nSection 11.6.3. 5.1.6 Probabilistic prediction problems\nIn Section 5.1.2, we assumed the set of possible actions was to pick a single class label (or possibly the\n“reject” or “do not know” action).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 471, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1004}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0472_88ce4cc4", "text": "5.1.6 Probabilistic prediction problems\nIn Section 5.1.2, we assumed the set of possible actions was to pick a single class label (or possibly the\n“reject” or “do not know” action). In Section 5.1.5, we assumed the set of possible actions was to pick\na real valued scalar. In this section, we assume the set of possible actions is to pick a probability\ndistribution over some value of interest. That is, we want to perform probabilistic prediction\norprobabilistic forecasting , rather than predicting a speciﬁc value. More precisely, we assume\nthe true “state of nature” is a distribution ,h=p(Yjx), the action is another distribution, a=q(Yjx),\nand we want to pick qto minimize E[`(p;q)]for a givenx. We discuss various possible loss functions\nbelow. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n174 Chapter 5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 472, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 826}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0473_8a2c34a7", "text": "We discuss various possible loss functions\nbelow. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n174 Chapter 5. Decision Theory\n5.1.6.1 KL, cross-entropy and log-loss\nA common form of loss functions for comparing two distributions is the Kullback Leibler diver-\ngence, orKL divergence , which is deﬁned as follows:\nKL(pkq),X\ny2Yp(y) logp(y)\nq(y)(5.37)\n(We have assumed the variable yis discrete, for notational simplicity, but this can be generalized\nto real-valued variables.) In Section 6.2, we show that the KL divergence satisﬁes the following\nproperties: KL(pkq)\u00150with equality iﬀ p=q. Note that it is an asymmetric function of its\narguments. We can expand the KL as follows:\nKL(pkq) =X\ny2Yp(y) logp(y)\u0000X\ny2Yp(y) logq(y) (5.38)\n=\u0000H(p) +H(p;q) (5.39)\nH(p),\u0000X\nyp(y) logp(y) (5.40)\nH(p;q),\u0000X\nyp(y) logq(y) (5.41)\nTheH(p)term is known as the entropy . This is a measure of uncertainty or variance of p; it is\nmaximal ifpis uniform, and is 0 if pis a degenerate or deterministic delta function.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 473, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1007}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0474_9f8ca217", "text": "This is a measure of uncertainty or variance of p; it is\nmaximal ifpis uniform, and is 0 if pis a degenerate or deterministic delta function. Entropy is often\nused in the ﬁeld of information theory , which is concerned with optimal ways of compressing and\ncommunicating data (see Chapter 6). The optimal coding scheme will allocate fewer bits to more\nfrequent symbols (i.e., values of Yfor whichp(y)is large), and more bits to less frequent symbols. A\nkey result states that the number of bits needed to compress a dataset generated by a distribution p\nis at least H(p); the entropy therefore provides a lower bound on the degree to which we can compress\ndata without losing information. The H(p;q)term is known as the cross-entropy . This measures\nthe expected number of bits we need to use to compress a dataset coming from distribution pif we\ndesign our code using distribution q. Thus the KL is the extra number of bits we need to use to\ncompress the data due to using the incorrect distribution q.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 474, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1002}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0475_1e6a567a", "text": "Thus the KL is the extra number of bits we need to use to\ncompress the data due to using the incorrect distribution q. If the KL is zero, it means that we can\ncorrectly predict the probabilities of all possible future events, and thus we have learned to predict\nthe future as well as an “oracle” that has access to the true distribution p. To ﬁnd the optimal distribution to use when predicting future data, we can minimize KL(pkq). SinceH(p)is a constant wrt q, it can be ignored, and thus we can equivalently minimize the\ncross-entropy:\nq\u0003(Yjx) = argmin\nqH(q(Yjx);p(Yjx)) (5.42)\nNow consider the special case in which the true state of nature is a degenerate distribution, which\nputs all its mass on a single outcome, say c, i.e.,h=p(Yjx) =I(Y=c). This is often called\na “one-hot” distribution, since it turns “on” the c’th element of the vector, and leaves the other\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 475, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 950}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0476_2c2238b2", "text": "This is often called\na “one-hot” distribution, since it turns “on” the c’th element of the vector, and leaves the other\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5.2. Bayesian hypothesis testing 175\nelements “oﬀ”, as shown in Figure 2.1. In this case, the cross entropy becomes\nH(\u000e(Y=c);q) =\u0000X\ny2Y\u000e(y=c) logq(y) =\u0000logq(c) (5.43)\nThis is known as the log loss of the predictive distribution qwhen given target label c. 5.1.6.2 Proper scoring rules\nCross-entropy loss is a very common choice for probabilistic forecasting, but is not the only possible\nmetric. The key property we desire is that the loss function is minimized iﬀ the decision maker picks\nthe distribution qthat matches the true distribution p, i.e.,`(p;p)\u0014`(p;q), with equality iﬀ p=q. Such a loss function `is called a proper scoring rule [GR07]. We can show that cross-entropy loss is a proper scoring rule by virtue of the fact that KL(pkp)\u0014\nKL(pkq).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 476, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 951}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0477_afeeef3a", "text": "Such a loss function `is called a proper scoring rule [GR07]. We can show that cross-entropy loss is a proper scoring rule by virtue of the fact that KL(pkp)\u0014\nKL(pkq). However, the logp(y)=q(y)term can be quite sensitive to errors for low probability events\n[QC+06]. A common alternative is to use the Brier score [Bri50], which is deﬁned as follows (for a\ndiscrete distribution with Cvalues):\n`(p;q),1\nCCX\nc=1(q(y=cjx)\u0000p(y=cjx))2(5.44)\nThis is just the squared error of the predictive distribution compared to the true distribution, when\nviewed as vectors. Since it based on squared error, the Brier score is less sensitive to extremely rare\nor extremely common classes. Fortunately, it is also a proper scoring rule. 5.2 Bayesian hypothesis testing\nSuppose we have two hypotheses or models, commonly called the null hypothesis ,M0, and the\nalternative hypothesis ,M1, and we want to know which one is more likely to be true. This is\ncalledhypothesis testing .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 477, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 961}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0478_d8dc1478", "text": "This is\ncalledhypothesis testing . If we use 0-1 loss, the optimal decision is to pick the alternative hypothesis iﬀ p(M1jD)>p(M0jD),\nor equivalently, if p(M1jD)=p(M0jD)>1. If we use a uniform prior, p(M0) =p(M1) = 0:5, the\ndecision rule becomes: select M1iﬀp(DjM1)=p(DjM0)>1. This quantity, which is the ratio of\nmarginal likelihoods of the two models, is known as the Bayes factor :\nB1;0,p(DjM1)\np(DjM0)(5.45)\nThis is like a likelihood ratio , except we integrate out the parameters, which allows us to compare\nmodels of diﬀerent complexity, due to the Bayesian Occam’s razor eﬀect explained in Section 5.2.3. IfB1;0>1then we prefer model 1, otherwise we prefer model 0. Of course, it might be that B1;0\nis only slightly greater than 1. In that case, we are not very conﬁdent that model 1 is better. Jeﬀreys\n[Jef61] proposed a scale of evidence for interpreting the magnitude of a Bayes factor, which is shown\nin Table 5.6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 478, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 925}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0479_fd278714", "text": "In that case, we are not very conﬁdent that model 1 is better. Jeﬀreys\n[Jef61] proposed a scale of evidence for interpreting the magnitude of a Bayes factor, which is shown\nin Table 5.6. This is a Bayesian alternative to the frequentist concept of a p-value (see Section 5.5.3). We give a worked example of how to compute Bayes factors in Section 5.2.1. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n176 Chapter 5. Decision Theory\nBayes factor BF(1;0) Interpretation\nBF <1\n100Decisive evidence for M0\nBF <1\n10Strong evidence for M0\n1\n10<BF <1\n3Moderate evidence for M0\n1\n3<BF < 1 Weak evidence for M0\n1<BF < 3 Weak evidence for M1\n3<BF < 10 Moderate evidence for M1\nBF > 10 Strong evidence for M1\nBF > 100 Decisive evidence for M1\nTable 5.6: Jeﬀreys scale of evidence for interpreting Bayes factors.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 479, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 813}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0480_42a58f24", "text": "01111122222222223333333333444445−1.8−1.6−1.4−1.2−1−0.8−0.6−0.4log10 p(D|M1)\n(a)\n01111122222222223333333333444445−2.5−2.45−2.4−2.35−2.3−2.25−2.2−2.15−2.1−2.05−2BIC approximation to log10 p(D|M1) (b)\nFigure 5.4: (a) Log marginal likelihood vs number of heads for the coin tossing example. (b) BIC approximation. (The vertical scale is arbitrary, since we are holding Nﬁxed.) Generated by code at ﬁgures.probml.ai/book1/5.4. 5.2.1 Example: Testing if a coin is fair\nAs an example, suppose we observe some coin tosses, and want to decide if the data was generated by\na fair coin, \u0012= 0:5, or a potentially biased coin, where \u0012could be any value in [0;1]. Let us denote\nthe ﬁrst model by M0and the second model by M1. The marginal likelihood under M0is simply\np(DjM0) =\u00121\n2\u0013N\n(5.46)\nwhereNis the number of coin tosses.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 480, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 812}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0481_eab0daf9", "text": "Let us denote\nthe ﬁrst model by M0and the second model by M1. The marginal likelihood under M0is simply\np(DjM0) =\u00121\n2\u0013N\n(5.46)\nwhereNis the number of coin tosses. From Equation (4.143), the marginal likelihood under M1,\nusing a Beta prior, is\np(DjM1) =Z\np(Dj\u0012)p(\u0012)d\u0012=B(\u000b1+N1;\u000b0+N0)\nB(\u000b1;\u000b0)(5.47)\nWe plot logp(DjM1)vs the number of heads N1in Figure 5.4(a), assuming N= 5and a uniform\nprior,\u000b1=\u000b0= 1. (The shape of the curve is not very sensitive to \u000b1and\u000b0, as long as the\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5.2. Bayesian hypothesis testing 177\nprior is symmetric, so \u000b0=\u000b1.) If we observe 2 or 3 heads, the unbiased coin hypothesis M0\nis more likely than M1, sinceM0is a simpler model (it has no free parameters) — it would be\na suspicious coincidence if the coin were biased but happened to produce almost exactly 50/50\nheads/tails. However, as the counts become more extreme, we favor the biased coin hypothesis.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 481, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 957}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0482_c5c15108", "text": "However, as the counts become more extreme, we favor the biased coin hypothesis. Note\nthat, if we plot the log Bayes factor, logB1;0, it will have exactly the same shape, since logp(DjM0)\nis a constant. 5.2.2 Bayesian model selection\nNow suppose we have a set Mof more than 2 models, and we want to pick the most likely. This\nis calledmodel selection . We can view this as a decision theory problem, where the action space\nrequires choosing one model, m2M. If we have a 0-1 loss, the optimal action is to pick the most\nprobable model:\n^m= argmax\nm2Mp(mjD) (5.48)\nwhere\np(mjD) =p(Djm)p(m)P\nm2Mp(Djm)p(m)(5.49)\nis the posterior over models. If the prior over models is uniform, p(m) = 1=jMj, then the MAP\nmodel is given by\n^m= argmax\nm2Mp(Djm) (5.50)\nThe quantity p(Djm)is given by\np(Djm) =Z\np(Dj\u0012;m)p(\u0012jm)d\u0012 (5.51)\nThis is known as the marginal likelihood , or theevidence for modelm. Intuitively, it is the\nlikelihood of the data averaged over all possible parameter values, weighted by the prior p(\u0012jm).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 482, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1004}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0483_6300aa14", "text": "Intuitively, it is the\nlikelihood of the data averaged over all possible parameter values, weighted by the prior p(\u0012jm). If\nall settings of \u0012assign high probability to the data, then this is probably a good model. 5.2.2.1 Example: polynomial regression\nAs an example of Bayesian model selection, we will consider polynomial regression in 1d. Figure 5.5\nshows the posterior over three diﬀerent models, corresponding to polynomials of degrees 1, 2 and 3 ﬁt\ntoN= 5data points. We use a uniform prior over models, and use empirical Bayes to estimate the\nprior over the regression weights (see Section 11.7.7). We then compute the evidence for each model\n(see Section 11.7 for details on how to do this). We see that there is not enough data to justify a\ncomplex model, so the MAP model is m= 1. Figure 5.6 shows the analogous plot for N= 30data\npoints. Now we see that the MAP model is m= 2; the larger sample size means we can safely pick a\nmore complex model. Author: Kevin P. Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 483, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 997}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0484_e5cdf2d3", "text": "Now we see that the MAP model is m= 2; the larger sample size means we can safely pick a\nmore complex model. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n178 Chapter 5. Decision Theory\n−2 0 2 4 6 8 10 12−100102030405060deg=1 | logev=-16.28\n(a)\n−2 0 2 4 6 8 10 12−200204060deg=2 | logev=-20.64 (b)\n−2 0 2 4 6 8 10 12−50050100150200250300deg=3 | logev=-24.95\n(c)\n1 2 3\nM0.00.20.40.60.81.0P(M|D) (d)\nFigure 5.5: Ilustration of Bayesian model selection for polynomial regression. (a-c) We ﬁt polynomials of\ndegrees 1, 2 and 3 ﬁt to N= 5data points. The solid green curve is the true function, the dashed red curve\nis the prediction (dotted blue lines represent \u0006\u001baround the mean). (d) We plot the posterior over models,\np(mjD), assuming a uniform prior p(m)/1. Generated by code at ﬁgures.probml.ai/book1/5.5. 5.2.3 Occam’s razor\nConsider two models, a simple one, m1, and a more complex one, m2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 484, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 907}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0485_83bda41f", "text": "Generated by code at ﬁgures.probml.ai/book1/5.5. 5.2.3 Occam’s razor\nConsider two models, a simple one, m1, and a more complex one, m2. Suppose that both can explain\nthe data by suitably optimizing their parameters, i.e., for which p(Dj^\u00121;m1)andp(Dj^\u00122;m2)are\nboth large. Intuitively we should prefer m1, since it is simpler and just as good as m2. This principle\nis known as Occam’s razor . Let us now see how ranking models based on their marginal likelihood, which involves averaging\nthe likelihood wrt the prior, will give rise to this behavior. The complex model will put less prior\nprobability on the “good” parameters that explain the data, ^\u00122, since the prior must integrate to\n1.0 over the entire parameter space. Thus it will take averages in parts of parameter space with\nlow likelihood. By contrast, the simpler model has fewer parameters, so the prior is concentrated\nover a smaller volume; thus its averages will mostly be in the good part of parameter space, near ^\u00121.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 485, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 985}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0486_59595031", "text": "By contrast, the simpler model has fewer parameters, so the prior is concentrated\nover a smaller volume; thus its averages will mostly be in the good part of parameter space, near ^\u00121. Hence we see that the marginal likelihood will prefer the simpler model. This is called the Bayesian\nOccam’s razor eﬀect [Mac95; MG05]. Another way to understand the Bayesian Occam’s razor eﬀect is to compare the relative predictive\nabilitiesofsimpleandcomplexmodels. Sinceprobabilitiesmustsumtoone, wehaveP\nD0p(D0jm) = 1,\nwhere the sum is over all possible datasets. Complex models, which can predict many things, must\nspread their predicted probability mass thinly, and hence will not obtain as large a probability for\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 486, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 786}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0487_e0595fea", "text": "August 27, 2021\n5.2. Bayesian hypothesis testing 179\n−2 0 2 4 6 8 10 120102030405060deg=1 | logev=-146.15\n(a)\n−2 0 2 4 6 8 10 12−10010203040506070deg=2 | logev=-72.37 (b)\n−2 0 2 4 6 8 10 12020406080100deg=3 | logev=-77.22\n(c)\n1 2 3\nM0.00.20.40.60.81.0P(M|D) (d)\nFigure 5.6: Same as Figure 5.5 except now N= 30. Generated by code at ﬁgures.probml.ai/book1/5.6. any given data set as simpler models. This is sometimes called the conservation of probability\nmassprinciple, and is illustrated in Figure 5.7. On the horizontal axis we plot all possible data sets\nin order of increasing complexity (measured in some abstract sense). On the vertical axis we plot the\npredictions of 3 possible models: a simple one, M1; a medium one, M2; and a complex one, M3. We\nalso indicate the actually observed data D0by a vertical line. Model 1 is too simple and assigns low\nprobability toD0.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 487, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 874}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0488_f32674b3", "text": "We\nalso indicate the actually observed data D0by a vertical line. Model 1 is too simple and assigns low\nprobability toD0. Model 3 also assigns D0relatively low probability, because it can predict many\ndata sets, and hence it spreads its probability quite widely and thinly. Model 2 is “just right”: it\npredicts the observed data with a reasonable degree of conﬁdence, but does not predict too many\nother things. Hence model 2 is the most probable model. 5.2.4 Connection between cross validation and marginal likelihood\nWe have seen how the marginal likelihood helps us choose models of the “right” complexity. In\nnon-Bayesian approaches to model selection, it is standard to use cross validation (Section 4.5.5) for\nthis purpose. It turns out that the marginal likelihood is closely related to the leave-one-out cross-validation\n(LOO-CV) estimate, as we now show.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 488, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 864}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0489_f679348a", "text": "It turns out that the marginal likelihood is closely related to the leave-one-out cross-validation\n(LOO-CV) estimate, as we now show. We start with the marginal likelihood, which we write in\nsequential form as follows:\np(Djm) =NY\nn=1p(ynjy1:n\u00001;x1:N;m) =NY\nn=1p(ynjxn;D1:n\u00001;m) (5.52)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n180 Chapter 5. Decision Theory\np(D)M1M2M3D0D\nFigure 5.7: A schematic illustration of the Bayesian Occam’s razor. The broad (green) curve corresponds to a\ncomplex model, the narrow (blue) curve to a simple model, and the middle (red) curve is just right. Adapted\nfrom Figure 3.13 of [Bis06]. See also [MG05, Figure 2] for a similar plot produced on real data.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 489, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 703}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0490_3d33cffd", "text": "Adapted\nfrom Figure 3.13 of [Bis06]. See also [MG05, Figure 2] for a similar plot produced on real data. where\np(yjx;D1:n\u00001;m) =Z\np(yjx;\u0012)p(\u0012jD1:n\u00001;m)d\u0012 (5.53)\nSuppose we use a plugin approximation to the above distribution to get\np(yjx;D1:n\u00001;m)\u0019Z\np(yjx;\u0012)\u000e(\u0012\u0000^\u0012m(D1:n\u00001))d\u0012=p(yjx;^\u0012m(D1:n\u00001)) (5.54)\nThen we get\nlogp(Djm)\u0019NX\nn=1logp(ynjxn;^\u0012m(D1:n\u00001)) (5.55)\nThis is very similar to a leave-one-out cross-validation estimate of the likelihood, except it is evaluated\nsequentially. A complex model will overﬁt the “early” examples and will then predict the remaining\nones poorly, and thus will get low marginal likelihood as well as low cross-validation score. See [FH20]\nfor further discussion. 5.2.5 Information criteria\nThe marginal likelihood, p(Djm) =R\np(Dj\u0012;m)p(\u0012)d\u0012, which is needed for Bayesian model selection\ndiscussed in Section 5.2.2, can be diﬃcult to compute, since it requires marginalizing over the entire\nparameter space.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 490, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 940}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0491_d5d8d670", "text": "Furthermore, the result can be quite sensitive to the choice of prior. In this section,\nwe discuss some other related metrics for model selection known as information criteria . We only\ngive a brief discussion; see e.g., [GHV14] for further details. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5.2. Bayesian hypothesis testing 181\n5.2.5.1 The Bayesian information criterion (BIC)\nTheBayesian information criterion orBIC[Sch78] can be thought of as a simple approximation\nto the log marginal likelihood. In particular, if we make a Gaussian approximation to the posterior,\nas discussed in Section 4.6.8.2, we get (from Equation (4.215)) the following:\nlogp(Djm)\u0019logp(Dj^\u0012map) + logp(^\u0012map)\u00001\n2logjHj (5.56)\nwhere His the Hessian of the negative log joint logp(D;\u0012)evaluated at the MAP estimate ^\u0012map. We\nsee that Equation (5.56) is the log likelihood plus some penalty terms.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 491, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 906}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0492_ea9992a8", "text": "We\nsee that Equation (5.56) is the log likelihood plus some penalty terms. If we have a uniform prior,\np(\u0012)/1, we can drop the prior term, and replace the MAP estimate with the MLE, ^\u0012, yielding\nlogp(Djm)\u0019logp(Dj^\u0012)\u00001\n2logjHj (5.57)\nWe now focus on approximating the logjHjterm, which is sometimes called the Occam factor ,\nsince it is a measure of model complexity (volume of the posterior distribution). We have H=PN\ni=1Hi, where Hi=rrlogp(Dij\u0012). Let us approximate each Hiby a ﬁxed matrix ^H. Then we\nhave\nlogjHj= logjN^Hj= log(NDj^Hj) =DlogN+ logj^Hj (5.58)\nwhereD=dim(\u0012)and we have assumed His full rank. We can drop the logj^Hjterm, since it is\nindependent of N, and thus will get overwhelmed by the likelihood.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 492, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 717}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0493_aff674fb", "text": "We can drop the logj^Hjterm, since it is\nindependent of N, and thus will get overwhelmed by the likelihood. Putting all the pieces together,\nwe get the BIC score that we want to maximize:\nJBIC(m) = logp(Djm)\u0019logp(Dj^\u0012;m)\u0000Dm\n2logN (5.59)\nWe can also deﬁne the BIC loss , that we want to minimize, by multiplying by -2:\nLBIC(m) =\u00002 logp(Dj^\u0012;m) +DmlogN (5.60)\n(The use of 2 as a scale factor is chosen to simplify the expression when using a model with a Gaussian\nlikelihood.)\n5.2.5.2 Akaike information criterion\nTheAkaike information criterion [Aka74] is closely related to the BIC. It has the form\nLAIC(m) =\u00002 logp(Dj^\u0012;m) + 2D (5.61)\nThis penalizes complex models less heavily than BIC, since the regularization term is independent of\nN. This estimator can be derived from a frequentist perspective. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n182 Chapter 5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 493, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 876}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0494_e6c2da6e", "text": "This estimator can be derived from a frequentist perspective. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n182 Chapter 5. Decision Theory\n5.2.5.3 Minimum description length (MDL)\nWe can think about the problem of scoring diﬀerent models in terms of information theory (Chapter 6). The goal is for the sender to communicate the data to the receiver. First the sender needs to specify\nwhich model mto use; this takes C(m) =\u0000logp(m)bits (see Section 6.1). Then the receiver can\nﬁt the model, by computing ^\u0012m, and can thus approximately reconstruct the data. To perfectly\nreconstruct the data, the sender needs to send the residual errors that cannot be explained by the\nmodel; this takes \u0000L(m) =\u0000logp(Dj^\u0012;m) =\u0000P\nnlogp(ynj^\u0012;m)bits. The total cost is\nLMDL(m) =\u0000logp(Dj^\u0012;m) +C(m) (5.62)\nWe see that has the same basic form as BIC/AIC. Choosing the model which minimizes J(m)is\nknown as the minimum description length orMDLprinciple. See e.g., [HY01] for details.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 494, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 975}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0495_38b8746a", "text": "Choosing the model which minimizes J(m)is\nknown as the minimum description length orMDLprinciple. See e.g., [HY01] for details. 5.3 Frequentist decision theory\nIn this section, we discuss frequentist decision theory . This is similar to Bayesian decision theory,\ndiscussed in Section 5.1, but diﬀers in that there is no prior, and hence no posterior, over the unknown\nstate of nature. Consequently we cannot deﬁne the risk as the posterior expected loss. We will\nconsider other deﬁnitions in Section 5.3.1. 5.3.1 Computing the risk of an estimator\nWe deﬁne the frequentist riskof an estimator \u0019given an unknown state of nature \u0012to be the\nexpected loss when applying that estimator to data xsampled from the likelihood function p(xj\u0012):\nR(\u0012;\u0019),Ep(xj\u0012)[`(\u0012;\u0019(x))] (5.63)\nWe give an example of this in Section 5.3.1.1. 5.3.1.1 Example\nLet us give an example, based on [BS94]. Consider the problem of estimating the mean of a Gaussian. We assume the data is sampled from xn\u0018N(\u0012\u0003;\u001b2= 1).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 495, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 981}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0496_7c9a25eb", "text": "5.3.1.1 Example\nLet us give an example, based on [BS94]. Consider the problem of estimating the mean of a Gaussian. We assume the data is sampled from xn\u0018N(\u0012\u0003;\u001b2= 1). If we use quadratic loss, `2(\u0012;^\u0012) = (\u0012\u0000^\u0012)2,\nthe corresponding risk function is the MSE. We now consider 5 diﬀerent estimators for computing \u0012:\n•\u00191(D) =x, the sample mean\n•\u00192(D) =median (D), the sample median\n•\u00193(D) =\u00120, a ﬁxed value\n•\u0019\u0014(D), the posterior mean under a N(\u0012j\u00120;\u001b2=\u0014)prior:\n\u0019\u0014(D) =N\nN+\u0014x+\u0014\nN+\u0014\u00120=wx+ (1\u0000w)\u00120 (5.64)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5.3. Frequentist decision theory 183\n3*-2-1.5-1-0.5 00.511.52R( 3*, /)\n00.050.10.150.20.250.30.350.40.450.5risk functions for n=5\nmle\nmedian\nfixed\npostmean1\npostmean5\n(a)\n3*-2-1.5-1-0.5 00.511.52R( 3*, /)\n00.020.040.060.080.10.120.140.160.18risk functions for n=20\nmle\nmedian\nfixed\npostmean1\npostmean5 (b)\nFigure 5.8: Risk functions for estimating the mean of a Gaussian.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 496, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 944}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0497_25dd0e25", "text": "Each curve represents R(^\u0012i(\u0001);\u0012\u0003)plotted\nvs\u0012\u0003, whereiindexes the estimator. Each estimator is applied to Nsamples fromN(\u0012\u0003;\u001b2= 1). The dark\nblue horizontal line is the sample mean (MLE); the red line horizontal line is the sample median; the black\ncurved line is the estimator ^\u0012=\u00120= 0; the green curved line is the posterior mean when \u0014= 1; the light blue\ncurved line is the posterior mean when \u0014= 5. (a)N= 5samples. (b) N= 20samples. Adapted from Figure\nB.1 of [BS94]. Generated by code at ﬁgures.probml.ai/book1/5.8. For\u0019\u0014, we use\u00120= 0, and consider a weak prior, \u0014= 1, and a stronger prior, \u0014= 5. Let^\u0012=^\u0012(x) =\u0019(x)be the estimated parameter. The risk of this estimator is given by the MSE. In Section 4.7.6.3, we show that the MSE can be decomposed into squared bias plus variance:\nMSE( ^\u0012j\u0012\u0003) =Vh\n^\u0012i\n+bias2(^\u0012) (5.65)\nwhere the bias is deﬁned as bias(^\u0012) =Eh\n^\u0012\u0000\u0012\u0003i\n. We now use this expression to derive the risk for\neach estimator. \u00191is the sample mean.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 497, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 962}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0498_ba13b60b", "text": "We now use this expression to derive the risk for\neach estimator. \u00191is the sample mean. This is unbiased, so its risk is\nMSE(\u00191j\u0012\u0003) =V[x] =\u001b2\nN(5.66)\n\u00192is the sample median. This is also unbiased. Furthermore, one can show that its variance is\napproximately \u0019=(2N), so the risk is\nMSE(\u00192j\u0012\u0003) =\u0019\n2N(5.67)\n\u00193returns the constant \u00120, so its bias is (\u0012\u0003\u0000\u00120)and its variance is zero. Hence the risk is\nMSE(\u00193j\u0012\u0003) = (\u0012\u0003\u0000\u00120)2(5.68)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n184 Chapter 5. Decision Theory\nFinally,\u00194is the posterior mean under a Gaussian prior. We can derive its MSE as follows:\nMSE(\u0019\u0014j\u0012\u0003) =Eh\n(wx+ (1\u0000w)\u00120\u0000\u0012\u0003)2i\n(5.69)\n=Eh\n(w(x\u0000\u0012\u0003) + (1\u0000w)(\u00120\u0000\u0012\u0003))2i\n(5.70)\n=w2\u001b2\nN+ (1\u0000w)2(\u00120\u0000\u0012\u0003)2(5.71)\n=1\n(N+\u0014)2\u0000\nN\u001b2+\u00142(\u00120\u0000\u0012\u0003)2\u0001\n(5.72)\nThese functions are plotted in Figure 5.8 for N2f5;20g. We see that in general, the best estimator\ndepends on the value of \u0012\u0003, which is unknown. If \u0012\u0003is very close to \u00120, then\u00193(which just predicts\n\u00120) is best.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 498, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 959}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0499_6ea780f0", "text": "We see that in general, the best estimator\ndepends on the value of \u0012\u0003, which is unknown. If \u0012\u0003is very close to \u00120, then\u00193(which just predicts\n\u00120) is best. If \u0012\u0003is within some reasonable range around \u00120, then the posterior mean, which combines\nthe prior guess of \u00120with the actual data, is best. If \u0012\u0003is far from \u00120, the MLE is best. 5.3.1.2 Bayes risk\nIn general, the true state of nature \u0012that generates the data xis unknown, so we cannot compute\nthe risk given in Equation (5.63). One solution to this is to assume a prior \u001afor\u0012, and then average\nit out. This gives us the Bayes risk , also called the integrated risk :\nR(\u001a;\u0019),E\u001a(\u0012)[R(\u0012;\u0019)] =Z\nd\u0012dx\u001a(\u0012)p(xj\u0012)`(\u0012;\u0019(x)) (5.73)\nA decision rule that minimizes the Bayes risk is known as a Bayes estimator .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 499, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 754}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0500_a1b6c4c2", "text": "This gives us the Bayes risk , also called the integrated risk :\nR(\u001a;\u0019),E\u001a(\u0012)[R(\u0012;\u0019)] =Z\nd\u0012dx\u001a(\u0012)p(xj\u0012)`(\u0012;\u0019(x)) (5.73)\nA decision rule that minimizes the Bayes risk is known as a Bayes estimator . This is equivalent to\nthe optimal policy recommended by Bayesian decision theory in Equation (5.2) since\n\u0019(x) = argmin\naZ\nd\u0012\u001a(\u0012)p(xj\u0012)`(\u0012;a) = argmin\naZ\nd\u0012p(\u0012jx)`(\u0012;a) (5.74)\nHence we see that picking the optimal action on a case-by-case basis (as in the Bayesian approach) is\noptimal on average (as in the frequentist approach). In other words, the Bayesian approach provides\na good way of achieving frequentist goals. See [BS94, p448] for further discussion of this point. 5.3.1.3 Maximum risk\nOf course the use of a prior might seem undesirable in the context of frequentist statistics. We can\ntherefore deﬁne the maximum risk as follows:\nRmax(\u0019),sup\n\u0012R(\u0012;\u0019) (5.75)\nA decision rule that minimizes the maximum risk is called a minimax estimator , and is denoted\n\u0019MM.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 500, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 966}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0501_21ce20fa", "text": "We can\ntherefore deﬁne the maximum risk as follows:\nRmax(\u0019),sup\n\u0012R(\u0012;\u0019) (5.75)\nA decision rule that minimizes the maximum risk is called a minimax estimator , and is denoted\n\u0019MM. For example, in Figure 5.9, we see that \u00191has lower worst-case risk than \u00192, ranging over all\npossible values of \u0012, so it is the minimax estimator. Minimaxestimatorshaveacertainappeal. However, computingthemcanbehard. Andfurthermore,\nthey are very pessimistic. In fact, one can show that all minimax estimators are equivalent to Bayes\nestimators under a least favorable prior . In most statistical situations (excluding game theoretic\nones), assuming nature is an adversary is not a reasonable assumption. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5.3. Frequentist decision theory 185\nFigure 5.9: Risk functions for two decision procedures, \u00191and\u00192. Since\u00191has lower worst case risk, it is\nthe minimax estimator, even though \u00192has lower risk for most values of \u0012.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 501, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 976}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0502_c5640ce2", "text": "Since\u00191has lower worst case risk, it is\nthe minimax estimator, even though \u00192has lower risk for most values of \u0012. Thus minimax estimators are\noverly conservative. 5.3.2 Consistent estimators\nSuppose we have a dataset D=fxn:n= 1 :Ngwhere the samples xn2Xare generated from a\ndistribution p(xj\u0012\u0003), where\u0012\u00032\u0002is the true parameter. Furthermore, suppose the parameters are\nidentiﬁable , meaning that p(Dj\u0012) =p(Dj\u00120)iﬀ\u0012=\u00120for any datasetD. Then we say that an\nestimator\u0019:XN!\u0002is aconsistent estimator if^\u0012(D)!\u0012\u0003asN!1(where the arrow denotes\nconvergence in probability). In other words, the procedure \u0019recovers the true parameter (or a subset\nof it) in the limit of inﬁnite data. This is equivalent to minimizing the 0-1 loss, L(\u0012\u0003;^\u0012) =I\u0010\n\u0012\u00036=^\u0012\u0011\n. An example of a consistent estimator is the maximum likelihood estimator (MLE). Note that an estimator can be unbiased but not consistent. For example, consider the estimator\n\u0019(fx1;:::;xNg) =xN. This is an unbiased estimator of the mean, since E[\u0019(D)]=E[x].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 502, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 999}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0503_4994eec0", "text": "Note that an estimator can be unbiased but not consistent. For example, consider the estimator\n\u0019(fx1;:::;xNg) =xN. This is an unbiased estimator of the mean, since E[\u0019(D)]=E[x]. But the\nsampling distribution of \u0019(D)does not converge to a ﬁxed value, so it cannot converge to the point\n\u0012\u0003. Although consistency is a desirable property, it is of somewhat limited usefulness in practice since\nmost real datasets do not come from our chosen model family (i.e., there is no \u0012\u0003such thatp(\u0001j\u0012\u0003)\ngenerates the observed data D). In practice, it is more useful to ﬁnd estimators that minimize some\ndiscrepancy measure between the empirical distribution pD(xjD)and the estimated distribution\np(xj^\u0012). If we use KL divergence as our discrepancy measure, our estimate becomes the MLE. 5.3.3 Admissible estimators\nIn Figure 5.8, we see that the sample median (dotted red line) always has higher risk than the sample\nmean (solid black line), at least under the Gaussian likelihood model.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 503, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 972}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0504_954d90a7", "text": "5.3.3 Admissible estimators\nIn Figure 5.8, we see that the sample median (dotted red line) always has higher risk than the sample\nmean (solid black line), at least under the Gaussian likelihood model. We therefore say that the\nsample mean dominates the sample median as an estimator. We say that \u00191dominates \u00192ifR(\u0012;\u00191)\u0014R(\u0012;\u00192)for all\u0012. The domination is said to be strict\nif the inequality is strict for some \u0012\u0003. An estimator is said to be admissible if it is not strictly\ndominated by any other estimator. In Figure 5.8, we see that the sample median (dotted red line) always has higher risk than the\nsample mean (solid black line). Therefore the sample median is not an admissible estimator for the\nmean. However, this conclusion only applies to the Gaussian likelihood model. If the true model for\np(xj\u0012)is something heavy-tailed, like a Laplace or Student distribution, or a Gaussian mixture, then\nthe sample median is likely to have lower risk, since it is more robust to outliers.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 504, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 987}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0505_3101334d", "text": "More surprisingly, one can show that the sample mean is not always an admissible estimator either,\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n186 Chapter 5. Decision Theory\neven under a Gaussian likelihood model with squared error loss (this is known as Stein’s paradox\n[Ste56]). It is natural to restrict our attention to admissable estimators. [Wal47] showed the following\nimportant result (known as the complete class theorem ), which essentially says that this restricts\nus to Bayesian estimators. Theorem 5.3.1 (Wald).Every admissable frequentist decision rule is a Bayes decision rule with\nrespect to some, possibly improper, prior distribution. However, the concept of admissibility is of somewhat limited usefulness. For example, it is easy to\nconstruct admissible but “silly” estimators, as we show in the following example. Theorem 5.3.2. LetX\u0018N(\u0012;1), and consider estimating \u0012under squared loss. Let \u00191(x) =\u00120, a\nconstant independent of the data . This is an admissible estimator.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 505, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1008}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0506_49a93d7d", "text": "Theorem 5.3.2. LetX\u0018N(\u0012;1), and consider estimating \u0012under squared loss. Let \u00191(x) =\u00120, a\nconstant independent of the data . This is an admissible estimator. Proof.Suppose not. Then there is some other estimator \u00192with smaller risk, so R(\u0012\u0003;\u00192)\u0014R(\u0012\u0003;\u00191),\nwhere the inequality must be strict for some \u0012\u0003. Consider the risk at \u0012\u0003=\u00120. We haveR(\u00120;\u00191) = 0,\nand\nR(\u00120;\u00192) =Z\n(\u00192(x)\u0000\u00120)2p(xj\u00120)dx (5.76)\nSince 0\u0014R(\u0012\u0003;\u00192)\u0014R(\u0012\u0003;\u00191)for all\u0012\u0003, andR(\u00120;\u00191) = 0, we haveR(\u00120;\u00192) = 0and hence\n\u00192(x) =\u00120=\u00191(x). Thus the only way \u00192can avoid having higher risk than \u00191at\u00120is by being\nequal to\u00191. Hence there is no other estimator \u00192with strictly lower risk, so \u00192is admissible. Note that the estimator \u00191(x) =\u00120is equivalent to a Bayes decision rule with respect to a prior\ndistribution concentrated on \u00120with certainty, and that this is why it is silly. 5.4 Empirical risk minimization\nIn this section, we consider how to apply frequentist decision theory in the context of supervised\nlearning.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 506, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 979}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0507_2078d4b8", "text": "5.4 Empirical risk minimization\nIn this section, we consider how to apply frequentist decision theory in the context of supervised\nlearning. 5.4.1 Empirical risk\nIn standard accounts of frequentist decision theory used in statistics textbooks, there is a single\nunknown “state of nature”, corresponding to the unknown parameters \u0012\u0003of some model, and we\ndeﬁne the risk as in Equation (5.63), namely R(\u0019;\u0012\u0003) =Ep(Dj\u0012\u0003)[`(\u0012\u0003;\u0019(D))]. In supervised learning, we have a diﬀerent unknown state of nature (namely the output y) for each\ninputx, and our estimator \u0019is a prediction function ^y=f(x), and the state of nature is the true\ndistribution p\u0003(x;y). Thus the risk of an estimator as follows:\nR(f;p\u0003) =R(f),Ep\u0003(x)p\u0003(yjx)[`(y;f(x)] (5.77)\nThis is called the population risk , since the expectations are taken wrt the true joint distribution\np\u0003(x;y). Of course, p\u0003is unknown, but we can approximate it using the empirical distribution with\nDraft of “Probabilistic Machine Learning: An Introduction”.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 507, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0508_77fd9da4", "text": "Of course, p\u0003is unknown, but we can approximate it using the empirical distribution with\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5.4. Empirical risk minimization 187\nNsamples:\npD(x;yjD),1\njDjX\n(xn;yn)2D\u000e(x\u0000xn)\u000e(y\u0000yn) (5.78)\nwherepD(x;y) =ptrain(x;y). Plugging this in gives us the empirical risk :\nR(f;D),EpD(x;y)[`(y;f(x)] =1\nNNX\nn=1`(yn;f(xn)) (5.79)\nNote thatR(f;D)is a random variable, since it depends on the training set. A natural way to choose the predictor is to use\n^fERM = argmin\nf2HR(f;D) = argmin\nf2H1\nNNX\nn=1`(yn;f(xn)) (5.80)\nwhere we optimize over a speciﬁc hypothesis space Hof functions. This is called empirical risk\nminimization (ERM). 5.4.1.1 Approximation error vs estimation error\nIn this section, we analyze the theoretical performance of functions that are ﬁt using the ERM\nprinciple. Let f\u0003\u0003=argminfR(f)be the function that achieves the minimal possible population risk,\nwhere we optimize over all possible functions.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 508, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 979}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0509_77f65029", "text": "Let f\u0003\u0003=argminfR(f)be the function that achieves the minimal possible population risk,\nwhere we optimize over all possible functions. Of course, we cannot consider all possible functions,\nso let us also deﬁne f\u0003=argminf2HR(f)to be the best function in our hypothesis space, H. Unfortunately we cannot compute f\u0003, since we cannot compute the population risk, so let us ﬁnally\ndeﬁne the prediction function that minimizes the empirical risk in our hypothesis space:\nf\u0003\nN= argmin\nf2HR(f;D) = argmin\nf2HEptrain[`(y;f(x))] (5.81)\nOne can show [BB08] that the risk of our chosen predictor compared to the best possible predictor\ncan be decomposed into two terms, as follows:\nEp\u0003[R(f\u0003\nN)\u0000R(f\u0003\u0003)] =R(f\u0003)\u0000R(f\u0003\u0003)|{z}\nEapp(H)+Ep\u0003[R(f\u0003\nN)\u0000R(f\u0003)]|{z}\nEest(H;N)(5.82)\nThe ﬁrst term,Eapp(H), is theapproximation error , which measures how closely Hcan model the\ntrue optimal function f\u0003\u0003.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 509, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 873}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0510_9f9a6044", "text": "The second term, Eest(H;N), is theestimation error orgeneralization\nerror, which measures the diﬀerence in estimated risks due to having a ﬁnite training set. We can\napproximate this by the diﬀerence between the training set error and the test set error, using two\nempirical distributions drawn from p\u0003:\nEp\u0003[R(f\u0003\nN)\u0000R(f\u0003)]\u0019Eptrain[`(y;f\u0003\nN(x))]\u0000Eptest[`(y;f\u0003\nN(x))] (5.83)\nThis diﬀerence is often called the generalization gap . We can decrease the approximation error by using a more expressive family of functions H, but\nthis usually increases the generalization error, due to overﬁtting. We discuss solutions to this tradeoﬀ\nbelow. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n188 Chapter 5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 510, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 709}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0511_ed5ad0c7", "text": "We discuss solutions to this tradeoﬀ\nbelow. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n188 Chapter 5. Decision Theory\n5.4.1.2 Regularized risk\nTo avoid the chance of overﬁtting, it is common to add a complexity penalty to the objective function,\ngiving us the regularized empirical risk :\nR\u0015(f;D) =R(f;D) +\u0015C(f) (5.84)\nwhereC(f)measures the complexity of the prediction function f(x;\u0012), and\u0015\u00150, which is known\nas ahyperparameter , controls the strength of the complexity penalty. (We discuss how to pick \u0015\nin Section 5.4.2.)\nIn practice, we usually work with parametric functions, and apply the regularizer to the parameters\nthemselves. This yields the following form of the objective:\nR\u0015(\u0012;D) =R(\u0012;D) +\u0015C(\u0012) (5.85)\nNote that, if the loss function is log loss, and the regularizer is a negative log prior, the regularized\nrisk is given by\nR\u0015(\u0012;D) =\u00001\nNNX\nn=1logp(ynjxn;\u0012)\u0000\u0015logp(\u0012) (5.86)\nMinimizing this is equivalent to MAP estimation.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 511, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 953}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0512_11444f09", "text": "5.4.2 Structural risk\nA natural way to estimate the hyperparameters is to minimize for the lowest achievable empirical\nrisk:\n^\u0015= argmin\n\u0015min\n\u0012R\u0015(\u0012;D) (5.87)\n(This is an example of bilevel optimization , also called nested optimization .) Unfortunately,\nthis technique will not work, since it will always pick the least amount of regularization, i.e., ^\u0015= 0. To see this, note that\nargmin\n\u0015min\n\u0012R\u0015(\u0012;D) = argmin\n\u0015min\n\u0012R(\u0012;D) +\u0015C(\u0012) (5.88)\nwhich is minimized by setting \u0015= 0The problem is that the empirical risk underestimates the\npopulation risk, resulting in overﬁtting when we choose \u0015. This is called optimism of the training\nerror. If we knew the regularized population risk R\u0015(\u0012), instead of the regularized empirical risk R\u0015(\u0012;D),\nwe could use it to pick a model of the right complexity (e.g., value of \u0015). This is known as structural\nrisk minimization [Vap98].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 512, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 867}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0513_78175133", "text": "This is known as structural\nrisk minimization [Vap98]. There are two main ways to estimate the population risk for a\ngiven model (value of \u0015), namely cross-validation (Section 5.4.3), and statistical learning theory\n(Section 5.4.4), which we discuss below. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5.4. Empirical risk minimization 189\n5.4.3 Cross-validation\nIn this section, we discuss a simple way to estimate the population risk for a supervised learning\nsetup. We simply partition the dataset into two, the part used for training the model, and a second\npart, called the validation set orholdout set , used for assessing the risk. We can ﬁt the model on\nthe training set, and use its performance on the validation set as an approximation to the population\nrisk. To explain the method in more detail, we need some notation.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 513, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 860}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0514_e719027d", "text": "We can ﬁt the model on\nthe training set, and use its performance on the validation set as an approximation to the population\nrisk. To explain the method in more detail, we need some notation. First we make the dependence of\nthe empirical risk on the dataset more explicit as follows:\nR\u0015(\u0012;D) =1\njDjX\n(x;y)2D`(y;f(x;\u0012)) +\u0015C(\u0012) (5.89)\nLet us also deﬁne ^\u0012\u0015(D) =argmin\u0012R\u0015(D;\u0012). Finally, letDtrainandDvalidbe a partition of D. (Often we use about 80% of the data for the training set, and 20% for the validation set.)\nFor each model \u0015, we ﬁt it to the training set to get ^\u0012\u0015(Dtrain). We then use the unregularized\nempirical risk on the validation set as an estimate of the population risk. This is known as the\nvalidation risk :\nRval\n\u0015,R0(^\u0012\u0015(Dtrain);Dvalid) (5.90)\nNote that we use diﬀerent data to train and evaluate the model. The above technique can work very well.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 514, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 866}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0515_019fe2b7", "text": "This is known as the\nvalidation risk :\nRval\n\u0015,R0(^\u0012\u0015(Dtrain);Dvalid) (5.90)\nNote that we use diﬀerent data to train and evaluate the model. The above technique can work very well. However, if the number of training cases is small, this\ntechnique runs into problems, because the model won’t have enough data to train on, and we won’t\nhave enough data to make a reliable estimate of the future performance. A simple but popular solution to this is to use cross validation (CV). The idea is as follows: we\nsplit the training data into Kfolds; then, for each fold k2f1;:::;Kg, we train on all the folds but\nthek’th, and test on the k’th, in a round-robin fashion, as sketched in Figure 4.6. Formally, we have\nRcv\n\u0015,1\nKKX\nk=1R0(^\u0012\u0015(D\u0000k);Dk) (5.91)\nwhereDkis the data in the k’th fold, andD\u0000kis all the other data. This is called the cross-validated\nrisk. Figure 4.6 illustrates this procedure for K= 5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 515, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 897}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0516_ff1bd2ce", "text": "This is called the cross-validated\nrisk. Figure 4.6 illustrates this procedure for K= 5. If we setK=N, we get a method known as\nleave-one-out cross-validation , since we always train on N\u00001items and test on the remaining\none. We can use the CV estimate as an objective inside of an optimization routine to pick the optimal\nhyperparameter, ^\u0015=argmin\u0015Rcv\n\u0015. Finally we combine all the available data (training and validation),\nand re-estimate the model parameters using ^\u0012= argmin\u0012R^\u0015(\u0012;D). 5.4.4 Statistical learning theory *\nThe principal problem with cross validation is that it is slow, since we have to ﬁt the model multiple\ntimes. This motivates the desire to compute analytic approximations or bounds on the population\nrisk. This is studied in the ﬁeld of statistical learning theory (SLT) (see e.g., [Vap98]). Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n190 Chapter 5. Decision Theory\nMore precisely, the goal of SLT is to upper bound the generalization error with a certain probability.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 516, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1009}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0517_a48e6c4f", "text": "Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n190 Chapter 5. Decision Theory\nMore precisely, the goal of SLT is to upper bound the generalization error with a certain probability. If the bound is satisﬁed, then we can be conﬁdent that a hypothesis that is chosen by minimizing\nthe empirical risk will have low population risk. In the case of binary classiﬁers, this means the\nhypothesis will make the correct predictions; in this case we say it is probably approximately\ncorrect, and that the hypothesis class is PAC learnable (see e.g., [KV94] for details). 5.4.4.1 Bounding the generalization error\nIn this section, we establish conditions under which we can prove that a hypothesis class is PAC\nlearnable. Letusinitiallyconsiderthecasewherethehypothesisspaceisﬁnite, withsize dim(H) =jHj. In other words, we are selecting a hypothesis from a ﬁnite list, rather than optimizing real-valued\nparameters. In this case, we can prove the following. Theorem 5.4.1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 517, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 974}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0518_032751e7", "text": "In other words, we are selecting a hypothesis from a ﬁnite list, rather than optimizing real-valued\nparameters. In this case, we can prove the following. Theorem 5.4.1. For any data distribution p\u0003, and any dataset Dof sizeNdrawn from p\u0003, the\nprobability that the generalization error of a binary classiﬁer will be more than \u000f, in the worst case,\nis upper bounded as follows:\nP\u0012\nmax\nh2HjR(h)\u0000R(h;D)j>\u000f\u0013\n\u00142 dim(H)e\u00002N\u000f2(5.92)\nwhereR(h;D) =1\nNPN\ni=1I(f(xi)6=y\u0003\ni)is the empirical risk, and R(h) =E[I(f(x)6=y\u0003)]is the\npopulation risk. Proof.Before we prove this, we introduce two useful results. First, Hoeﬀding’s inequality , which\nstates that if E1;:::;EN\u0018Ber(\u0012), then, for any \u000f>0,\nP(jE\u0000\u0012j>\u000f)\u00142e\u00002N\u000f2(5.93)\nwhereE=1\nNPN\ni=1Eiis the empirical error rate, and \u0012is the true error rate. Second, the union\nbound, which says that if A1;:::;Adare a set of events, then P([d\ni=1Ai)\u0014Pd\ni=1P(Ai). Using\nthese results, we have\nP\u0012\nmax\nh2HjR(h)\u0000R(h;D)j>\u000f\u0013\n=P [\nh2HjR(h)\u0000R(h;D)j>\u000f!", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 518, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 967}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0519_ef4479ac", "text": "Second, the union\nbound, which says that if A1;:::;Adare a set of events, then P([d\ni=1Ai)\u0014Pd\ni=1P(Ai). Using\nthese results, we have\nP\u0012\nmax\nh2HjR(h)\u0000R(h;D)j>\u000f\u0013\n=P [\nh2HjR(h)\u0000R(h;D)j>\u000f! (5.94)\n\u0014X\nh2HP(jR(h)\u0000R(h;D)j>\u000f) (5.95)\n\u0014X\nh2H2e\u00002N\u000f2= 2 dim(H)e\u00002N\u000f2(5.96)\nThis bound tells us that the optimism of the training error increases with dim(H)but decreases\nwithN=jDj, as is to be expected. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5.5. Frequentist hypothesis testing * 191\n(a)\n (b)\nFigure 5.10: (a) Illustration of the Neyman-Pearson hypothesis testing paradigm. Generated by code at\nﬁgures.probml.ai/book1/5.10. (b) Two hypothetical two-sided power curves. B dominates A. Adapted from\nFigure 6.3.5 of [LM86]. Generated by code at ﬁgures.probml.ai/book1/5.10. 5.4.4.2 VC dimension\nIf the hypothesis space His inﬁnite (e.g., we have real-valued parameters), we cannot use dim(H) =\njHj.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 519, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 917}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0520_1ea3d9a2", "text": "Generated by code at ﬁgures.probml.ai/book1/5.10. 5.4.4.2 VC dimension\nIf the hypothesis space His inﬁnite (e.g., we have real-valued parameters), we cannot use dim(H) =\njHj. Instead, we can use a quantity called the VC dimension of the hypothesis class, named after\nVapnik and Chervonenkis; this measures the degrees of freedom (eﬀective number of parameters) of\nthe hypothesis class. See e.g., [Vap98] for the details. Unfortunately, it is hard to compute the VC dimension for many interesting models, and the upper\nbounds are usually very loose, making this approach of limited practical value. However, various\nother, more practical, estimates of generalization error have recently been devised, especially for\nDNNs, such as [Jia+20]. 5.5 Frequentist hypothesis testing *\nSuppose we have two hypotheses, known as the nullhypothesis H0and analternativehypothesis\nH1, and we want to choose the one we think is correct on the basis of a dataset D.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 520, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 948}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0521_caa90e43", "text": "We could use a\nBayesian approach and compute the Bayes factor p(H0jD)=p(H1jD), as we discussed in Section 5.2. However, this requires integrating over all possible parameterizations of the models H0andH1, which\ncan be computationally diﬃcult, and which can be sensitive to the choice of prior. In this section, we\nconsider a frequentist approach to the problem. 5.5.1 Likelihood ratio test\nIf we use 0-1 loss, and assume p(H0) =p(H1), then the optimal decision rule is to accept H0iﬀ\np(DjH0)\np(DjH1)>1. This is called the likelihood ratio test . We give some examples of this below. 5.5.1.1 Example: comparing Gaussian means\nSuppose we are interested in testing whether some data comes from a Gaussian with mean \u00160or\nfrom a Gaussian with mean \u00161. (We assume a known shared variance \u001b2.) This is illustrated in\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n192 Chapter 5. Decision Theory\nFigure 5.10a, where we plot p(xjH0)andp(xjH1).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 521, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 947}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0522_70759738", "text": "(We assume a known shared variance \u001b2.) This is illustrated in\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n192 Chapter 5. Decision Theory\nFigure 5.10a, where we plot p(xjH0)andp(xjH1). We can derive the likelihood ratio as follows:\np(DjH0)\np(DjH1)=exp\u0010\n\u00001\n2\u001b2PN\nn=1(xn\u0000\u00160)2\u0011\nexp\u0010\n\u00001\n2\u001b2PN\nn=1(xn\u0000\u00161)2\u0011 (5.97)\n= exp\u00121\n2\u001b2(2Nx(\u00160\u0000\u00161) +N\u00162\n1\u0000N\u00162\n0)\u0013\n(5.98)\nWe see that this ratio only depends on the observed data via its mean, x. This is an example of a\ntest statistic \u001c(D), which is a scalar suﬃcient statistic for hypothesis testing. From Figure 5.10a,\nwe can see thatp(DjH0)\np(DjH1)>1iﬀx<x\u0003, wherex\u0003is the point where the two pdf’s intersect (we are\nassuming this point is unique). 5.5.1.2 Simple vs compound hypotheses\nIn Section 5.5.1.1, the parameters for the null and alternative hypotheses were either fully speciﬁed\n(\u00160and\u00161) or shared ( \u001b2). This is called a simple hypothesis test.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 522, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 906}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0523_38e53523", "text": "This is called a simple hypothesis test. In general, a hypothesis might\nnot fully specify all the parameters; this is called a compound hypothesis . In this case, we should\nintegrate out these unknown parameters, as in the Bayesian approach, since a hypothesis with more\nparameters will always have higher likelihood. As an approximation, we can “maximize them out”,\nwhich gives us the maximum likelihood ratio test:\np(H0jD)\np(H1jD)=R\n\u00122H0p(\u0012)p\u0012(D)R\n\u00122H1p(\u0012)p\u0012(D)\u0019max\u00122H0p\u0012(D)\nmax\u00122H1p\u0012(D)(5.99)\n5.5.2 Null hypothesis signiﬁcance testing (NHST)\nRather than assuming 0-1 loss, it is conventional to design the decision rule so that it has a type I\nerror rate (the probability of accidentally rejecting the null hypothesis H0) of\u000b. (See Section 5.1.3\nfor details on error rates of binary decision rules.) The error rate \u000bis called the signiﬁcance of the\ntest. Hence the overall approach is called null hypothesis signiﬁcance testing orNHST.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 523, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 938}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0524_70d27f44", "text": "Hence the overall approach is called null hypothesis signiﬁcance testing orNHST. In our Gaussian mean example, we see from Figure 5.10a that the type I error rate is the vertical\nshaded blue area:\n\u000b(\u00160) =p(rejectH0jH0is true ) (5.100)\n=p(X(~D)>x\u0003j~D\u0018H0) (5.101)\n=p\u0012X\u0000\u00160\n\u001b=p\nN>x\u0003\u0000\u00160\n\u001b=p\nN\u0013\n(5.102)\nHencex\u0003=z\u000b\u001b=p\nN+\u00160, wherez\u000bis the upper \u000bquantile of the standard Normal. The type II error rate is the probability we accidentally accept the null when the alternative is\ntrue:\n\f(\u00161) =p(type II error ) =p(acceptH0jH1is true ) =p(\u001c(~D)<\u001c\u0003j~D\u0018H1) (5.103)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5.5. Frequentist hypothesis testing * 193\nThis is shown by the horizontal shaded red area in Figure 5.10a. We deﬁne the powerof a test\nas1\u0000\f(\u00161); this is the probability that we reject H0given that H1is true. In other words, it is\nthe ability to correctly recognize that the null hypothesis is wrong.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 524, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 927}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0525_6660774b", "text": "We deﬁne the powerof a test\nas1\u0000\f(\u00161); this is the probability that we reject H0given that H1is true. In other words, it is\nthe ability to correctly recognize that the null hypothesis is wrong. Clearly the least power occurs\nif\u00161=\u00160(so the curves overlap); in this case, we have 1\u0000\f(\u00161) =\u000b(\u00160). As\u00161and\u00160become\nfurther apart, the power approaches 1 (because the shaded red area gets smaller, \f!0). If we have\ntwo tests,AandB, wherepower (B)\u0015power (A)for the same type I error rate, we say Bdominates\nA. See Figure 5.10b. A test with highest power under H1amongst all tests with signiﬁcance level \u000b\nis called a most powerful test . It turns out that the likelihood ratio test is a most powerful test, a\nresult known as the Neyman-Pearson lemma . 5.5.3 p-values\nWhen we reject H0we often say the result is statistically signiﬁcant at level\u000b. However, the\nresult may be statistically signiﬁcant but not practically signiﬁcant, depending on how far from the\ndecision boundary the test statistic is.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 525, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 994}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0526_d510b302", "text": "However, the\nresult may be statistically signiﬁcant but not practically signiﬁcant, depending on how far from the\ndecision boundary the test statistic is. Rather than arbitrarily declaring a result as signiﬁcant or not, it is preferable to quote the p-value. This is deﬁned as the probability, under the null hypothesis, of observing a test statistic that is as\nlarge or larger than that actually observed:\npval(\u001c(D)),Pr(\u001c(~D)\u0015\u001c(D)j~D\u0018H0) (5.104)\nIn other words, pval(\u001cobs),Pr(\u001cnull\u0015\u001cobs), where\u001cobs=\u001c(D)and\u001cnull=\u001c(~D), where ~D\u0018H0is\nhypothetical future data. To see the connection with hypothesis testing, suppose we pick a decision\nthresholdt\u0003such that Pr(\u001c(~D)\u0015t\u0003jH0) =\u000b. If we sett\u0003=\u001c(D), then\u000b= pval(\u001c(D)). Thus if we only accept hypotheses where the p-value is less than \u000b= 0:05, then 95% of the time we\nwill correctly reject the null hypothesis. However, this does notmean that the alternative hypothesis\nH1is true with probability 0.95.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 526, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 944}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0527_de59f5e3", "text": "However, this does notmean that the alternative hypothesis\nH1is true with probability 0.95. Indeed, even most scientists misinterpret p-values.2The quantity\nthat most people want to compute is the Bayesian posterior p(H1jD) = 0:95. For more on this\nimportant distinction, see Section 5.5.4. 5.5.4 p-values considered harmful\nA p-value is often interpreted as the likelihood of the data under the null hypothesis, so small values\nare interpreted to mean that H0is unlikely, and therefore that H1is likely. The reasoning is roughly\nas follows:\nIfH0is true, then this test statistic would probably not occur. This statistic did occur. ThereforeH0is probably false. However, this is invalid reasoning. To see why, consider the following example (from [Coh94]):\nIf a person is an American, then he is probably not a member of Congress. This person is a\nmember of Congress. Therefore he is probably not an American. This is obviously fallacious reasoning.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 527, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 949}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0528_1ee80ca5", "text": "This person is a\nmember of Congress. Therefore he is probably not an American. This is obviously fallacious reasoning. By contrast, the following logical argument is valid reasoning:\n2. See e.g., https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/ . Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n194 Chapter 5. Decision Theory\nIneﬀective Eﬀective\n“Not signiﬁcant” 171 4 175\n“Signiﬁcant” 9 16 25\n180 20 200\nTable 5.7: Some statistics of a hypothetical clinical trial. Source: [SAM04, p74]. If a person is a Martian, then he is not a member of Congress. This person is a member of\nCongress. Therefore he is not a Martian. The diﬀerence between these two cases is that the Martian example is using deduction , that is,\nreasoning forward from logical deﬁnitions to their consequences. More precisely, this example uses a\nrule from logic called modus tollens , in which we start out with a deﬁnition of the form P)Q;\nwhen we observe:Q, we can conclude :P.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 528, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 998}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0529_4e6f20d0", "text": "More precisely, this example uses a\nrule from logic called modus tollens , in which we start out with a deﬁnition of the form P)Q;\nwhen we observe:Q, we can conclude :P. By contrast, the American example concerns induction ,\nthat is, reasoning backwards from observed evidence to probable (but not necessarily true) causes\nusing statistical regularities, not logical deﬁnitions. To perform induction, we need to use probabilistic inference (as explained in detail in [Jay03]). In\nparticular, to compute the probability of the null hypothesis, we should use Bayes rule, as follows:\np(H0jD) =p(DjH0)p(H0)\np(DjH0)p(H0) +p(DjH1)p(H1)(5.105)\nIf the prior is uniform, so p(H0) =p(H1) = 0:5, this can be rewritten in terms of the likelihood\nratioLR=p(DjH0)=p(DjH1)as follows:\np(H0jD) =LR\nLR+ 1(5.106)\nIn the American Congress example, Dis the observation that the person is a member of Congress.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 529, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 888}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0530_5773f5d3", "text": "The null hypothesis H0is that the person is American, and the alternative hypothesis H1is that the\nperson is not American. We assume that p(DjH0)is low, since most Americans are not members of\nCongress. However, p(DjH1)is also low — in fact, in this example, it is 0, since only Americans can\nbe members of Congress. Hence LR=1, sop(H0jD) = 1:0, as intuition suggests. Note, however,\nthat NHST ignores p(DjH1)as well as the prior p(H0), so it gives the wrong results — not just in\nthis problem, but in many problems. In general there can be huge diﬀerences between p-values and p(H0jD). In particular, [SBB01]\nshow that even if the p-value is as low as 0.05, the posterior probability of H0can be as high as 30%\nor more, even with a uniform prior. Consider this concrete example from [SAM04, p74]. Suppose 200 clinical trials are carried out for\nsome drug. Suppose we perform a statistical test of whether the drug has a signiﬁcant eﬀect or not.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 530, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 945}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0531_e09292de", "text": "Consider this concrete example from [SAM04, p74]. Suppose 200 clinical trials are carried out for\nsome drug. Suppose we perform a statistical test of whether the drug has a signiﬁcant eﬀect or not. The test has a type I error rate of \u000b= 0:05and a type II error rate of \f= 0:2. The resulting data is\nshown in Table 5.7. We can compute the probability that the drug is not eﬀective, given that the result is supposedly\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5.5. Frequentist hypothesis testing * 195\nFigure 5.11: Cartoon illustrating the diﬀerence between frequentists and Bayesians. (The p<0:05comment\nis explained in Section 5.5.4. The betting comment is a reference to the Dutch book theorem, which essentially\nproves that the Bayesian approach to gambling (and other decision theory problems) is optimal, as explained\nin e.g., [Háj08].) From https: // xkcd. com/ 1132/ . Used with kind permission of Rundall Munroe (author\nof xkcd).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 531, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 971}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0532_1a97dac9", "text": "com/ 1132/ . Used with kind permission of Rundall Munroe (author\nof xkcd). “signiﬁcant”, as follows:\np(H0j’signiﬁcant’ ) =p(’signiﬁcant’jH0)p(H0)\np(’signiﬁcant’jH0)p(H0) +p(’signiﬁcant’jH1)p(H1)(5.107)\n=p(type I error )p(H0)\np(type I error )p(H0) + (1\u0000p(type II error ))p(H1)(5.108)\n=\u000bp(H0)\n\u000bp(H0) + (1\u0000\f)p(H1)(5.109)\nIf we have prior knowledge, based on past experience, that most (say 90%) drugs are ineﬀective,\nthen we ﬁnd p(H0j’signiﬁcant’ ) = 0:36, which is much more than the 5% probability people usually\nassociate with a p-value of \u000b= 0:05. Thus we should distrust claims of statistical signiﬁcance if they violate our prior knowledge. 5.5.5 Why isn’t everyone a Bayesian? In Section 4.7.5 and Section 5.5.4, we have seen that inference based on frequentist principles can\nexhibit various forms of counter-intuitive behavior that can sometimes contradict common sense\nreason, as has been pointed out in multiple articles (see e.g., [Mat98; MS11; Kru13; Gel16; Hoe+14;\nLyu+20; Cha+19b]).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 532, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 994}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0533_1277d21d", "text": "Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n196 Chapter 5. Decision Theory\nThe fundamental reason is that frequentist inference violates the likelihood principle [BW88],\nwhich says that inference should be based on the likelihood of the observed data, not on hypothetical\nfuture data that you have not observed. Bayes obviously satisﬁes the likelihood principle, and\nconsequently does not suﬀer from these pathologies. Given these fundamental ﬂaws of frequentist statistics, and the fact that Bayesian methods do not\nhave such ﬂaws, an obvious question to ask is: “Why isn’t everyone a Bayesian?” The (frequentist)\nstatistician Bradley Efron wrote a paper with exactly this title [Efr86]. His short paper is well worth\nreading for anyone interested in this topic. Below we quote his opening section:\nThe title is a reasonable question to ask on at least two counts. First of all, everyone used to\nbe a Bayesian.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 533, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 927}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0534_5f63112a", "text": "Below we quote his opening section:\nThe title is a reasonable question to ask on at least two counts. First of all, everyone used to\nbe a Bayesian. Laplace wholeheartedly endorsed Bayes’s formulation of the inference problem,\nand most 19th-century scientists followed suit. This included Gauss, whose statistical work is\nusually presented in frequentist terms. A second and more important point is the cogency of the Bayesian argument. Modern\nstatisticians, following the lead of Savage and de Finetti, have advanced powerful theoretical\narguments for preferring Bayesian inference. A byproduct of this work is a disturbing catalogue\nof inconsistencies in the frequentist point of view. Nevertheless, everyone is not a Bayesian. The current era (1986) is the ﬁrst century in which\nstatistics has been widely used for scientiﬁc reporting, and in fact, 20th-century statistics is\nmainly non-Bayesian. However, Lindley (1975) predicts a change for the 21st century. Time will tell whether Lindley was right.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 534, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1004}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0535_2441c62e", "text": "However, Lindley (1975) predicts a change for the 21st century. Time will tell whether Lindley was right. However, the trends seem to be going in this direction. For example, some journals have banned p-values [TM15; AGM19], and the journal The American\nStatistician (produced by the American Statistical Association) published a whole special issue\nwarning about the use of p-values and NHST [WSL19]. Traditionally, computation has been a barrier to using Bayesian methods, but this is less of an issue\nthese days, due to faster computers and better algorithms (which we will discuss in the sequel to this\nbook, [Mur22]). Another, more fundamental, concern is that the Bayesian approach is only as correct\nas its modeling assumptions. However, this criticism also applies to frequentist methods, since the\nsampling distribution of an estimator must be derived using assumptions about the data generating\nmechanism.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 535, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 915}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0536_3a7074a5", "text": "However, this criticism also applies to frequentist methods, since the\nsampling distribution of an estimator must be derived using assumptions about the data generating\nmechanism. (In fact [BT73] show that the sampling distributions for the MLE for common models\nare identical to the posterior distributions under a noninformative prior.) Fortunately, we can check\nmodeling assumptions empirically using cross validation (Section 4.5.5), calibration, and Bayesian\nmodel checking. We discuss these topics in the sequel to this book, [Mur22]. To summarize, it is worth quoting Donald Rubin, who wrote a paper [Rub84] called “Bayesianly\nJustiﬁable and Relevant Frequency Calculations for the Applied Statistician”. In it, he writes\nThe applied statistician should be Bayesian in principle and calibrated to the real world in\npractice. [They] should attempt to use speciﬁcations that lead to approximately calibrated pro-\ncedures under reasonable deviations from [their assumptions].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 536, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 979}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0537_4b10abeb", "text": "[They] should attempt to use speciﬁcations that lead to approximately calibrated pro-\ncedures under reasonable deviations from [their assumptions]. [They] should avoid models that\nare contradicted by observed data in relevant ways — frequency calculations for hypothetical\nreplications can model a model’s adequacy and help to suggest more appropriate models. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n5.6. Exercises 197\n5.6 Exercises\nExercise 5.1 [Reject option in classiﬁers]\n(Source: [DHS01, Q2.13].) In many classiﬁcation problems one has the option either of assigning xto classj\nor, if you are too uncertain, of choosing the reject option . If the cost for rejects is less than the cost of\nfalsely classifying the object, it may be the optimal action. Let \u000bimean you choose action i, fori= 1 :C+ 1,\nwhereCis the number of classes and C+ 1is the reject action. Let Y=jbe the true (but unknown) state\nof nature .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 537, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 950}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0538_a75b7656", "text": "Let \u000bimean you choose action i, fori= 1 :C+ 1,\nwhereCis the number of classes and C+ 1is the reject action. Let Y=jbe the true (but unknown) state\nof nature . Deﬁne the loss function as follows\n\u0015(\u000bijY=j) =8\n<\n:0ifi=jandi;j2f1;:::;Cg\n\u0015r ifi=C+ 1\n\u0015s otherwise(5.110)\nIn other words, you incur 0 loss if you correctly classify, you incur \u0015rloss (cost) if you choose the reject\noption, and you incur \u0015sloss (cost) if you make a substitution error (misclassiﬁcation). a.Show that the minimum risk is obtained if we decide Y=jifp(Y=jjx)\u0015p(Y=kjx)for allk(i.e.,jis\nthe most probable class) andifp(Y=jjx)\u00151\u0000\u0015r\n\u0015s; otherwise we decide to reject. b.Describe qualitatively what happens as \u0015r=\u0015sis increased from 0 to 1 (i.e., the relative cost of rejection\nincreases). Exercise 5.2 [Newsvendor problem *]\nConsider the following classic problem in decision theory / economics. Suppose you are trying to decide how\nmuch quantity Qof some product (e.g., newspapers) to buy to maximize your proﬁts.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 538, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 982}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0539_ea479ffa", "text": "Suppose you are trying to decide how\nmuch quantity Qof some product (e.g., newspapers) to buy to maximize your proﬁts. The optimal amount\nwill depend on how much demand Dyou think there is for your product, as well as its cost to you Cand its\nselling price P. SupposeDis unknown but has pdf f(D)and cdfF(D). We can evaluate the expected proﬁt\nby considering two cases: if D>Q, then we sell all Qitems, and make proﬁt \u0019= (P\u0000C)Q; but ifD<Q,\nwe only sell Ditems, at proﬁt (P\u0000C)D, but have wasted C(Q\u0000D)on the unsold items. So the expected\nproﬁt if we buy quantity Qis\nE\u0019(Q) =Z1\nQ(P\u0000C)Qf(D)dD+ZQ\n0(P\u0000C)Df(D)dD\u0000ZQ\n0C(Q\u0000D)f(D)dD (5.111)\nSimplify this expression, and then take derivatives wrt Qto show that the optimal quantity Q\u0003(which\nmaximizes the expected proﬁt) satisﬁes\nF(Q\u0003) =P\u0000C\nP(5.112)\nExercise 5.3 [Bayes factors and ROC curves *]\nLetB=p(DjH1)=p(DjH0)be the Bayes factor in favor of model 1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 539, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 896}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0540_5f7e689b", "text": "Suppose we plot two ROC curves, one\ncomputed by thresholding B, and the other computed by thresholding p(H1jD). Will they be the same or\ndiﬀerent? Explain why. Exercise 5.4 [Posterior median is optimal estimate under L1 loss]\nProve that the posterior median is the optimal estimate under L1 loss. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n\n6Information Theory\nIn this chapter, we introduce a few basic concepts from the ﬁeld of information theory . More\ndetails can be found in other books such as [Mac03; CT06], as well as the sequel to this book, [Mur22]. 6.1 Entropy\nTheentropy of a probability distribution can be interpreted as a measure of uncertainty, or lack\nof predictability, associated with a random variable drawn from a given distribution, as we explain\nbelow. We can also use entropy to deﬁne the information content of a data source. For example,\nsuppose we observe a sequence of symbols Xn\u0018pgenerated from distribution p.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 540, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 955}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0541_8a20a85d", "text": "We can also use entropy to deﬁne the information content of a data source. For example,\nsuppose we observe a sequence of symbols Xn\u0018pgenerated from distribution p. Ifphas high\nentropy, it will be hard to predict the value of each osbervation Xn. Hence we say that the dataset\nD= (X1;:::;Xn)has high information content. By contrast, if pis a degenerate distribution with 0\nentropy (the minimal value), then every Xnwill be the same, so Ddoes not contain much information.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 541, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 471}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0542_fccf6e43", "text": "By contrast, if pis a degenerate distribution with 0\nentropy (the minimal value), then every Xnwill be the same, so Ddoes not contain much information. (All of this can be formalized in terms of data compression, as we discuss in the sequel to this book.)\n6.1.1 Entropy for discrete random variables\nThe entropy of a discrete random variable Xwith distribution poverKstates is deﬁned by\nH(X),\u0000KX\nk=1p(X=k) log2p(X=k) =\u0000EX[logp(X)] (6.1)\n(Note that we use the notation H(X)to denote the entropy of the rv with distribution p, just as\npeople write V[X]to mean the variance of the distribution associated with X; we could alternatively\nwriteH(p).) Usually we use log base 2, in which case the units are called bits(short for binary\ndigits). For example, if X2f1;:::; 5gwith histogram distribution p= [0:25;0:25;0:2;0:15;0:15], we\nﬁndH= 2:29bits. If we use log base e, the units are called nats. The discrete distribution with maximum entropy is the uniform distribution.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 542, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 967}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0543_112d4a49", "text": "If we use log base e, the units are called nats. The discrete distribution with maximum entropy is the uniform distribution. Hence for a K-ary\nrandom variable, the entropy is maximized if p(x=k) = 1=K; in this case, H(X)=log2K. To see\nthis, note that\nH(X) =\u0000KX\nk=11\nKlog(1=K) =\u0000log(1=K) = log(K) (6.2)\n200 Chapter 6. Information Theory\n0 0.5 100.51\np(X = 1)H(X)\nFigure 6.1: Entropy of a Bernoulli random variable as a function of \u0012. The maximum entropy is log22 = 1. Generated by code at ﬁgures.probml.ai/book1/6.1. (a)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nSequence Position012Bits (b)\nFigure 6.2: (a) Some aligned DNA sequences. Each row is a sequence, each column is a location within the\nsequence. (b) The corresponding position weight matrix represented as a sequence logo. Each column\nrepresents a probablity distribution over the alphabet fA;C;G;Tgfor the corresponding location in the sequence. The size of the letter is proportional to the probability.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 543, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 959}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0544_b5666595", "text": "Each column\nrepresents a probablity distribution over the alphabet fA;C;G;Tgfor the corresponding location in the sequence. The size of the letter is proportional to the probability. The height of column tis given by 2\u0000Ht, where\n0\u0014Ht\u00142is the entropy (in bits) of the distribution pt. Thus deterministic distributions (with an entropy\nof 0, corresponding to highly conserved locations) have height 2, and uniform distributions (with an entropy\nof 2) have height 0. Generated by code at ﬁgures.probml.ai/book1/6.2. Conversely, the distribution with minimum entropy (which is zero) is any delta-function that puts all\nits mass on one state. Such a distribution has no uncertainty. For the special case of binary random variables, X2f0;1g, we can write p(X= 1) =\u0012and\np(X= 0) = 1\u0000\u0012. Hence the entropy becomes\nH(X) =\u0000[p(X= 1) log2p(X= 1) +p(X= 0) log2p(X= 0)] (6.3)\n=\u0000[\u0012log2\u0012+ (1\u0000\u0012) log2(1\u0000\u0012)] (6.4)\nThis is called the binary entropy function , and is also written H(\u0012). We plot this in Figure 6.1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 544, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0545_9d81c8aa", "text": "We plot this in Figure 6.1. We see that the maximum value of 1 bit occurs when the distribution is uniform, \u0012= 0:5. A fair coin\nrequires a single yes/no question to determine its state. As an interesting application of entropy, consider the problem of representing DNA sequence\nmotifs, which is a distribution over short DNA strings. We can estimate this distribution by aligning\na set of DNA sequences (e.g., from diﬀerent species), and then estimating the empirical distribution\nof each possible nucleotide from the 4 letter alphabet X\u0018fA;C;G;Tgat each location tin theith\nsequence as follows:\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n6.1. Entropy 201\nNt= NX\ni=1I(Xit=A);NX\ni=1I(Xit=C);NX\ni=1I(Xit=G);NX\ni=1I(Xit=T)! (6.5)\n^\u0012t=Nt=N; (6.6)\nThisNtis a length four vector counting the number of times each letter appears at each location\namongst the set of sequences. This ^\u0012tdistribution is known as a motif.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 545, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 943}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0546_654e70db", "text": "(6.5)\n^\u0012t=Nt=N; (6.6)\nThisNtis a length four vector counting the number of times each letter appears at each location\namongst the set of sequences. This ^\u0012tdistribution is known as a motif. We can also compute the\nmost probable letter in each location; this is called the consensus sequence . One way to visually summarize the data is by using a sequence logo , as shown in Figure 6.2(b). We plot the letters A, C, G and T, with the most probable letter on the top; the height of the t’th bar\nis deﬁned to be 2\u0000Ht, whereHtis the entropy of ^\u0012t(note that 2 is the maximum possible entropy\nfor a distribution over 4 letters). Thus tall bars correspond to nearly deterministic distributions,\nwhich are the locations that are conserved by evolution (e.g., because they are part of a gene coding\nregion). In this example, we see that column 13 is all G’s, and hence has height 2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 546, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 874}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0547_662fe373", "text": "In this example, we see that column 13 is all G’s, and hence has height 2. Estimating the entropy of a random variable with many possible states requires estimating its\ndistribution, which can require a lot of data. For example, imagine if Xrepresents the identity of\na word in an English document. Since there is a long tail of rare words, and since new words are\ninvented all the time, it can be diﬃcult to reliably estimate p(X)and hence H(X). For one possible\nsolution to this problem, see [VV13]. 6.1.2 Cross entropy\nThecross entropy between distribution pandqis deﬁned by\nH(p;q),\u0000KX\nk=1pklogqk (6.7)\nOne can show that the cross entropy is the expected number of bits needed to compress some data\nsamples drawn from distribution pusing a code based on distribution q. This can be minimized by\nsettingq=p, in which case the expected number of bits of the optimal code is H(p;p) =H(p)— this\nis known as Shannon’s source coding theorem (see e.g., [CT06]).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 547, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 957}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0548_27b3a77b", "text": "This can be minimized by\nsettingq=p, in which case the expected number of bits of the optimal code is H(p;p) =H(p)— this\nis known as Shannon’s source coding theorem (see e.g., [CT06]). 6.1.3 Joint entropy\nThe joint entropy of two random variables XandYis deﬁned as\nH(X;Y ) =\u0000X\nx;yp(x;y) log2p(x;y) (6.8)\nFor example, consider choosing an integer from 1 to 8, n2f1;:::; 8g. LetX(n) = 1ifnis even, and\nY(n) = 1ifnis prime:\nn1 2 3 4 5 6 7 8\nX0 1 0 1 0 1 0 1\nY0 1 1 0 1 0 1 0\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n202 Chapter 6. Information Theory\nThe joint distribution is\np(X;Y )Y= 0Y= 1\nX= 01\n83\n8\nX= 13\n81\n8\nso the joint entropy is given by\nH(X;Y ) =\u0000\u00141\n8log21\n8+3\n8log23\n8+3\n8log23\n8+1\n8log21\n8\u0015\n= 1:81bits (6.9)\nClearly the marginal probabilities are uniform: p(X= 1) =p(X= 0) =p(Y= 0) =p(Y=\n1) = 0:5, soH(X)=H(Y)= 1. Hence H(X;Y )= 1:81bits<H(X)+H(Y)= 2bits. In\nfact, this upper bound on the joint entropy holds in general.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 548, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 948}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0549_259ee8b8", "text": "Hence H(X;Y )= 1:81bits<H(X)+H(Y)= 2bits. In\nfact, this upper bound on the joint entropy holds in general. If XandYare independent, then\nH(X;Y )=H(X)+H(Y), so the bound is tight. This makes intuitive sense: when the parts are\ncorrelated in some way, it reduces the “degrees of freedom” of the system, and hence reduces the\noverall entropy. What is the lower bound on H(X;Y )? IfYis a deterministic function of X, thenH(X;Y )=H(X). So\nH(X;Y )\u0015maxfH(X);H(Y)g\u00150 (6.10)\nIntuitively this says combining variables together does not make the entropy go down: you cannot\nreduce uncertainty merely by adding more unknowns to the problem, you need to observe some data,\na topic we discuss in Section 6.1.4. We can extend the deﬁnition of joint entropy from two variables to nin the obvious way.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 549, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 784}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0550_3948660a", "text": "We can extend the deﬁnition of joint entropy from two variables to nin the obvious way. 6.1.4 Conditional entropy\nTheconditional entropy ofYgivenXis the uncertainty we have in Yafter seeing X, averaged\nover possible values for X:\nH(YjX),Ep(X)[H(p(YjX))] (6.11)\n=X\nxp(x)H(p(YjX=x)) =\u0000X\nxp(x)X\nyp(yjx) logp(yjx) (6.12)\n=\u0000X\nx;yp(x;y) logp(yjx) =\u0000X\nx;yp(x;y) logp(x;y)\np(x)(6.13)\n=\u0000X\nx;yp(x;y) logp(x;y)\u0000X\nxp(x) log1\np(x)(6.14)\n=H(X;Y )\u0000H(X) (6.15)\nIfYis a deterministic function of X, then knowing Xcompletely determines Y, soH(YjX) = 0. IfXandYare independent, knowing Xtells us nothing about YandH(YjX)=H(Y). Since\nH(X;Y )\u0014H(Y) +H(X), we have\nH(YjX)\u0014H(Y) (6.16)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n6.1. Entropy 203\nwith equality iﬀ XandYare independent. This shows that, on average, conditioning on data never\nincreases one’s uncertainty.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 550, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 877}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0551_5815eee9", "text": "August 27, 2021\n6.1. Entropy 203\nwith equality iﬀ XandYare independent. This shows that, on average, conditioning on data never\nincreases one’s uncertainty. The caveat “on average” is necessary because for any particular observation\n(value ofX), one may get more “confused” (i.e., H(Yjx)>H(Y)). However, in expectation, looking\nat the data is a good thing to do. (See also Section 6.3.8.)\nWe can rewrite Equation (6.15) as follows:\nH(X1;X2) =H(X1) +H(X2jX1) (6.17)\nThis can be generalized to get the chain rule for entropy :\nH(X1;X2;:::;Xn) =nX\ni=1H(XijX1;:::;Xi\u00001) (6.18)\n6.1.5 Perplexity\nTheperplexity of a discrete probability distribution pis deﬁned as\nperplexity(p),2H(p)(6.19)\nThis is often interpreted as a measure of predictability. For example, suppose pis a uniform\ndistribution over Kstates. In this case, the perplexity is K. Obviously the lower bound on perplexity\nis20= 1, which will be achieved if the distribution can perfectly predict outcomes.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 551, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 961}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0552_37a2c175", "text": "In this case, the perplexity is K. Obviously the lower bound on perplexity\nis20= 1, which will be achieved if the distribution can perfectly predict outcomes. Now suppose we have an empirical distribution based on data D:\npD(xjD) =1\nNNX\nn=1\u000exn(x) (6.20)\nWe can measure how well ppredictsDby computing\nperplexity(pD;p),2H(pD;p)(6.21)\nPerplexity is often used to evaluate the quality of statistical language models, which is a generative\nmodel for sequences of tokens. Suppose the data is a single long document xof lengthN, and\nsupposepis a simple unigram model. In this case, the cross entropy term is given by\nH=\u00001\nNNX\nn=1logp(xn) (6.22)\nand hence the perplexity is given by\nperplexity(pD;p) = 2H=NvuutNY\nn=11\np(xn)(6.23)\nThis is sometimes called the exponentiated cross entropy . We see that this is the geometric\nmean of the inverse predictive probabilities. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n204 Chapter 6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 552, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 936}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0553_03f09cc3", "text": "We see that this is the geometric\nmean of the inverse predictive probabilities. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n204 Chapter 6. Information Theory\nIn the case of language models, we usually condition on previous words when predicting the next\nword. For example, in a bigram model, we use a second order Markov model of the form p(xijxi\u00001). We deﬁne the branching factor of a language model as the number of possible words that can\nfollow any given word. We can thus interpret the perplexity as the weighted average branching factor. For example, suppose the model predicts that each word is equally likely, regardless of context, so\np(xijxi\u00001) = 1=K. Then the perplexity is ((1=K)N)\u00001=N=K. If some symbols are more likely than\nothers, and the model correctly reﬂects this, its perplexity will be lower than K. However, as we\nshow in Section 6.2, we have H(p\u0003)\u0014H(p\u0003;p), so we can never reduce the perplexity below the\nentropy of the underlying stochastic process p\u0003.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 553, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0554_44d17c8d", "text": "However, as we\nshow in Section 6.2, we have H(p\u0003)\u0014H(p\u0003;p), so we can never reduce the perplexity below the\nentropy of the underlying stochastic process p\u0003. See [JM08, p96] for further discussion of perplexity and its uses in language models. 6.1.6 Diﬀerential entropy for continuous random variables *\nIfXis a continuous random variable with pdf p(x), we deﬁne the diﬀerential entropy as\nh(X),\u0000Z\nXdxp(x) logp(x) (6.24)\nassuming this integral exists. For example, suppose X\u0018U(0;a). Then\nh(X) =\u0000Za\n0dx1\nalog1\na= loga (6.25)\nNote that, unlike the discrete case, diﬀerential entropy can be negative . This is because pdf’s can be\nbigger than 1. For example if X\u0018U(0;1=8), we haveh(X) = log2(1=8) =\u00003. One way to understand diﬀerential entropy is to realize that all real-valued quantities can only be\nrepresented to ﬁnite precision. It can be shown [CT91, p228] that the entropy of an n-bit quantization\nof a continuous random variable Xis approximately h(X) +n. For example, suppose X\u0018U(0;1\n8).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 554, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 991}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0555_237d0c06", "text": "It can be shown [CT91, p228] that the entropy of an n-bit quantization\nof a continuous random variable Xis approximately h(X) +n. For example, suppose X\u0018U(0;1\n8). Then in a binary representation of X, the ﬁrst 3 bits to the right of the binary point must be 0 (since\nthe number is\u00141=8). So to describe Xtonbits of accuracy only requires n\u00003bits, which agrees\nwithh(X) =\u00003calculated above. 6.1.6.1 Example: Entropy of a Gaussian\nThe entropy of a d-dimensional Gaussian is\nh(N(\u0016;\u0006)) =1\n2lnj2\u0019e\u0006j=1\n2ln[(2\u0019e)dj\u0006j] =d\n2+d\n2ln(2\u0019) +1\n2lnj\u0006j (6.26)\nIn the 1d case, this becomes\nh(N(\u0016;\u001b2)) =1\n2ln\u0002\n2\u0019e\u001b2\u0003\n(6.27)\n6.1.6.2 Connection with variance\nThe entropy of a Gaussian increases monotonically as the variance increases. However, this is not\nalways the case. For example, consider a mixture of two 1d Gaussians centered at -1 and +1. As we\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n6.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 555, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 914}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0556_cf3a3792", "text": "However, this is not\nalways the case. For example, consider a mixture of two 1d Gaussians centered at -1 and +1. As we\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n6.2. Relative entropy (KL divergence) * 205\nmove the means further apart, say to -10 and +10, the variance increases (since the average distance\nfrom the overall mean gets larger). However, the entropy remains more or less the same, since we are\nstill uncertain about where a sample might fall, even if we know that it will be near -10 or +10. (The\nexact entropy of a GMM is hard to compute, but a method to compute upper and lower bounds is\npresented in [Hub+08].)\n6.1.6.3 Discretization\nIn general, computing the diﬀerential entropy for a continuous random variable can be diﬃcult. A\nsimple approximation is to discretize orquantize the variables. There are various methods for this\n(see e.g., [DKS95; KK06] for a summary), but a simple approach is to bin the distribution based on\nits empirical quantiles.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 556, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1003}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0557_1d20517f", "text": "There are various methods for this\n(see e.g., [DKS95; KK06] for a summary), but a simple approach is to bin the distribution based on\nits empirical quantiles. The critical question is how many bins to use [LM04]. Scott [Sco79] suggested\nthe following heuristic:\nB=N1=3max(D)\u0000min(D)\n3:5\u001b(D)(6.28)\nwhere\u001b(D)is the empirical standard deviation of the data, and N=jDjis the number of datapoints\nin the empirical distribution. However, the technique of discretization does not scale well if Xis a\nmulti-dimensional random vector, due to the curse of dimensionality. 6.2 Relative entropy (KL divergence) *\nGiven two distributions pandq, it is often useful to deﬁne a distance metric to measure how “close”\nor “similar” they are. In fact, we will be more general and consider a divergence measure D(p;q)\nwhich quantiﬁes how far qis fromp, without requiring that Dbe a metric.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 557, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 868}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0558_f1a43e45", "text": "In fact, we will be more general and consider a divergence measure D(p;q)\nwhich quantiﬁes how far qis fromp, without requiring that Dbe a metric. More precisely, we say\nthatDis a divergence if D(p;q)\u00150with equality iﬀ p=q, whereas a metric also requires that Dbe\nsymmetric and satisfy the triangle inequality ,D(p;r)\u0014D(p;q)+D(q;r). There are many possible\ndivergence measures we can use. In this section, we focus on the Kullback-Leibler divergence\norKL divergence , also known as the information gain orrelative entropy , between two\ndistributions pandq. 6.2.1 Deﬁnition\nFor discrete distributions, the KL divergence is deﬁned as follows:\nKL(pkq),KX\nk=1pklogpk\nqk(6.29)\nThis naturally extends to continuous distributions as well:\nKL(pkq),Z\ndxp(x) logp(x)\nq(x)(6.30)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n206 Chapter 6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 558, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 841}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0559_369f1bca", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n206 Chapter 6. Information Theory\n6.2.2 Interpretation\nWe can rewrite the KL as follows:\nKL(pkq) =KX\nk=1pklogpk\n|{z}\n\u0000H(p)\u0000KX\nk=1pklogqk\n|{z}\nH(p;q)(6.31)\nWe recognize the ﬁrst term as the negative entropy, and the second term as the cross entropy. It can\nbe shown that the cross entropy H(p;q)is a lower bound on the number of bits needed to compress\ndata coming from distribution pif your code is designed based on distribution q; thus we can interpret\nthe KL divergence as the “extra number of bits” you need to pay when compressing data samples\nif you use the incorrect distribution qas the basis of your coding scheme compared to the true\ndistribution p. There are various other interpretations of KL divergence. See the sequel to this book, [Mur22], for\nmore information.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 559, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 820}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0560_903720aa", "text": "There are various other interpretations of KL divergence. See the sequel to this book, [Mur22], for\nmore information. 6.2.3 Example: KL divergence between two Gaussians\nFor example, one can show that the KL divergence between two multivariate Gaussian distributions\nis given by\nKL(N(xj\u00161;\u00061)kN(xj\u00162;\u00062))\n=1\n2\u0014\ntr(\u0006\u00001\n2\u00061) + (\u00162\u0000\u00161)T\u0006\u00001\n2(\u00162\u0000\u00161)\u0000D+ log\u0012det(\u00062)\ndet(\u00061)\u0013\u0015\n(6.32)\nIn the scalar case, this becomes\nKL(N(xj\u00161;\u001b1)kN(xj\u00162;\u001b2)) = log\u001b2\n\u001b1+\u001b2\n1+ (\u00161\u0000\u00162)2\n2\u001b2\n2\u00001\n2(6.33)\n6.2.4 Non-negativity of KL\nIn this section, we prove that the KL divergence is always non-negative. To do this, we use Jensen’s inequality . This states that, for any convex function f, we have\nthat\nf(nX\ni=1\u0015ixi)\u0014nX\ni=1\u0015if(xi) (6.34)\nwhere\u0015i\u00150andPn\ni=1\u0015i= 1. In words, this result says that fof the average is less than the\naverage of the f’s. This is clearly true for n= 2, since a convex function curves up above a straight\nline connecting the two end points (see Section 8.1.3). To prove for general n, we can use induction.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 560, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1005}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0561_d817f74b", "text": "This is clearly true for n= 2, since a convex function curves up above a straight\nline connecting the two end points (see Section 8.1.3). To prove for general n, we can use induction. For example, if f(x) = log(x), which is a concave function, we have\nlog(Exg(x))\u0015Exlog(g(x)) (6.35)\nWe use this result below. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n6.2. Relative entropy (KL divergence) * 207\nTheorem 6.2.1. (Information inequality) KL(pkq)\u00150with equality iﬀ p=q. Proof.We now prove the theorem following [CT06, p28]. Let A=fx:p(x)>0gbe the support of\np(x).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 561, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 593}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0562_43be2f89", "text": "Relative entropy (KL divergence) * 207\nTheorem 6.2.1. (Information inequality) KL(pkq)\u00150with equality iﬀ p=q. Proof.We now prove the theorem following [CT06, p28]. Let A=fx:p(x)>0gbe the support of\np(x). Using the concavity of the log function and Jensen’s inequality (Section 6.2.4), we have that\n\u0000KL(pkq) =\u0000X\nx2Ap(x) logp(x)\nq(x)=X\nx2Ap(x) logq(x)\np(x)(6.36)\n\u0014logX\nx2Ap(x)q(x)\np(x)= logX\nx2Aq(x) (6.37)\n\u0014logX\nx2Xq(x) = log 1 = 0 (6.38)\nSince log(x)is a strictly concave function ( \u0000log(x)is convex), we have equality in Equation (6.37)\niﬀp(x) =cq(x)for somecthat tracks the fraction of the whole space Xcontained in A. We have\nequality in Equation (6.38) iﬀP\nx2Aq(x) =P\nx2Xq(x) = 1, which implies c= 1. Hence KL(pkq)= 0\niﬀp(x) =q(x)for allx. This theorem has many important implications, as we will see throughout the book. For example,\nwe can show that the uniform distribution is the one that maximizes the entropy:\nCorollary 6.2.1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 562, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 936}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0563_08b70792", "text": "This theorem has many important implications, as we will see throughout the book. For example,\nwe can show that the uniform distribution is the one that maximizes the entropy:\nCorollary 6.2.1. (Uniform distribution maximizes the entropy) H(X)\u0014logjXj, wherejXjis the\nnumber of states for X, with equality iﬀ p(x)is uniform. Proof.Letu(x) = 1=jXj. Then\n0\u0014KL(pku) =X\nxp(x) logp(x)\nu(x)= logjXj\u0000H(X) (6.39)\n6.2.5 KL divergence and MLE\nSuppose we want to ﬁnd the distribution qthat is as close as possible to p, as measured by KL\ndivergence:\nq\u0003= arg min\nqKL(pkq) = arg min\nqZ\np(x) logp(x)dx\u0000Z\np(x) logq(x)dx (6.40)\nNow suppose pis the empirical distribution, which puts a probability atom on the observed training\ndata and zero mass everywhere else:\npD(x) =1\nNNX\nn=1\u000e(x\u0000xn) (6.41)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n208 Chapter 6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 563, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 850}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0564_10e1e679", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n208 Chapter 6. Information Theory\nUsing the sifting property of delta functions we get\nKL(pDkq) =\u0000Z\npD(x) logq(x)dx+C (6.42)\n=\u0000Z\"\n1\nNX\nn\u000e(x\u0000xn)#\nlogq(x)dx+C (6.43)\n=\u00001\nNX\nnlogq(xn) +C (6.44)\nwhereC=R\np(x)logp(x)is a constant independent of q. This is called the cross entropy objective,\nand is equal to the average negative log likelihood of qon the training set. Thus we see that\nminimizing KL divergence to the empirical distribution is equivalent to maximizing likelihood. This perspective points out the ﬂaw with likelihood-based training, namely that it puts too\nmuch weight on the training set. In most applications, we do not really believe that the empirical\ndistribution is a good representation of the true distribution, since it just puts “spikes” on a ﬁnite\nset of points, and zero density everywhere else.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 564, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 861}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0565_8003baed", "text": "Even if the dataset is large (say 1M images), the\nuniverse from which the data is sampled is usually even larger (e.g., the set of “all natural images” is\nmuch larger than 1M). We could smooth the empirical distribution using kernel density estimation\n(Section 16.3), but that would require a similar kernel on the space of images. An alternative,\nalgorithmic approach is to use data augmentation , which is a way of perturbing the observed\ndata samples in way that we believe reﬂects plausible “natural variation”. Applying MLE on this\naugmented dataset often yields superior results, especially when ﬁtting models with many parameters\n(see Section 19.1). 6.2.6 Forward vs reverse KL\nSuppose we want to approximate a distribution pusing a simpler distribution q. We can do this by\nminimizing KL(qkp)orKL(pkq). This gives rise to diﬀerent behavior, as we discuss below.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 565, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 869}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0566_a3156f5c", "text": "We can do this by\nminimizing KL(qkp)orKL(pkq). This gives rise to diﬀerent behavior, as we discuss below. First we consider the forwards KL , also called the inclusive KL , deﬁned by\nKL(pkq) =Z\np(x) logp(x)\nq(x)dx (6.45)\nMinimizing this wrt qis known as an M-projection ormoment projection . We can gain an understanding of the optimal qby considering inputs xfor whichp(x)>0but\nq(x) = 0. In this case, the term logp(x)=q(x)will be inﬁnite. Thus minimizing the KL will force q\nto include all the areas of space for which phas non-zero probability. Put another way, qwill be\nzero-avoiding ormode-covering , and will typically over-estimate the support of p. Figure 6.3(a)\nillustrates mode covering where pis a bimodal distribution but qis unimodal. Now consider the reverse KL , also called the exclusive KL :\nKL(qkp) =Z\nq(x) logq(x)\np(x)dx (6.46)\nMinimizing this wrt qis known as an I-projection orinformation projection . Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n6.3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 566, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1003}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0567_20b72e6a", "text": "Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n6.3. Mutual information * 209\n(a)\n (b)\n (c)\nFigure 6.3: Illustrating forwards vs reverse KL on a bimodal distribution. The blue curves are the contours of\nthe true distribution p. The red curves are the contours of the unimodal approximation q. (a) Minimizing\nforwards KL, KL(pkq), wrtqcausesqto “cover”p. (b-c) Minimizing reverse KL, KL(qkp)wrtqcauses\nqto “lock onto” one of the two modes of p. Adapted from Figure 10.3 of [Bis06]. Generated by code at\nﬁgures.probml.ai/book1/6.3. We can gain an understanding of the optimal qby consider inputs xfor whichp(x) = 0but\nq(x)>0. In this case, the term logq(x)=p(x)will be inﬁnite. Thus minimizing the exclusive KL will\nforceqto exclude all the areas of space for which phas zero probability. One way to do this is for q\nto put probability mass in very few parts of space; this is called zero-forcing ormode-seeking\nbehavior. In this case, qwill typically under-estimate the support of p.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 567, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1010}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0568_358e68c3", "text": "One way to do this is for q\nto put probability mass in very few parts of space; this is called zero-forcing ormode-seeking\nbehavior. In this case, qwill typically under-estimate the support of p. We illustrate mode seeking\nwhenpis bimodal but qis unimodal in Figure 6.3(b-c). 6.3 Mutual information *\nThe KL divergence gave us a way to measure how similar two distributions were. How should we\nmeasure how dependant two random variables are? One thing we could do is turn the question\nof measuring the dependence of two random variables into a question about the similarity of their\ndistributions. This gives rise to the notion of mutual information (MI) between two random\nvariables, which we deﬁne below.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 568, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 706}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0569_0f29ef0c", "text": "This gives rise to the notion of mutual information (MI) between two random\nvariables, which we deﬁne below. 6.3.1 Deﬁnition\nThe mutual information between rv’s XandYis deﬁned as follows:\nI(X;Y),KL(p(x;y)kp(x)p(y)) =X\ny2YX\nx2Xp(x;y) logp(x;y)\np(x)p(y)(6.47)\n(We write I(X;Y)instead of I(X;Y ), in caseXand/orYrepresent sets of variables; for example, we\ncan write I(X;Y;Z)to represent the MI between Xand(Y;Z).) For continuous random variables,\nwe just replace sums with integrals. It is easy to see that MI is always non-negative, even for continuous random variables, since\nI(X;Y) =KL(p(x;y)kp(x)p(y))\u00150 (6.48)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n210 Chapter 6. Information Theory\nWe achieve the bound of 0 iﬀ p(x;y) =p(x)p(y).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 569, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 753}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0570_cf7f9f0a", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n210 Chapter 6. Information Theory\nWe achieve the bound of 0 iﬀ p(x;y) =p(x)p(y). 6.3.2 Interpretation\nKnowing that the mutual information is a KL divergence between the joint and factored marginal\ndistributions tells is that the MI measures the information gain if we update from a model that treats\nthe two variables as independent p(x)p(y)to one that models their true joint density p(x;y). To gain further insight into the meaning of MI, it helps to re-express it in terms of joint and\nconditional entropies, as follows:\nI(X;Y) =H(X)\u0000H(XjY) =H(Y)\u0000H(YjX) (6.49)\nThus we can interpret the MI between XandYas the reduction in uncertainty about Xafter\nobservingY, or, by symmetry, the reduction in uncertainty about Yafter observing X. Incidentally,\nthis result gives an alternative proof that conditioning, on average, reduces entropy. In particular, we\nhave 0\u0014I(X;Y) =H(X)\u0000H(XjY), and hence H(XjY)\u0014H(X). We can also obtain a diﬀerent interpretation.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 570, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 993}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0571_9238cf22", "text": "In particular, we\nhave 0\u0014I(X;Y) =H(X)\u0000H(XjY), and hence H(XjY)\u0014H(X). We can also obtain a diﬀerent interpretation. One can show that\nI(X;Y) =H(X;Y )\u0000H(XjY)\u0000H(YjX) (6.50)\nFinally, one can show that\nI(X;Y) =H(X) +H(Y)\u0000H(X;Y ) (6.51)\nSee Figure 6.4 for a summary of these equations in terms of an information diagram . (Formally,\nthis is a signed measure mapping set expressions to their information-theoretic counterparts [Yeu91].)\n6.3.3 Example\nAs an example, let us reconsider the example concerning prime and even numbers from Section 6.1.3. Recall that H(X)=H(Y)= 1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 571, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 568}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0572_0282936f", "text": "Recall that H(X)=H(Y)= 1. The conditional distribution p(YjX)is given by normalizing each\nrow:\nY=0 Y=1\nX=01\n43\n4\nX=13\n41\n4\nHence the conditional entropy is\nH(YjX) =\u0000\u00141\n8log21\n4+3\n8log23\n4+3\n8log23\n4+1\n8log21\n4\u0015\n= 0:81bits (6.52)\nand the mutual information is\nI(X;Y) =H(Y)\u0000H(YjX) = (1\u00000:81)bits= 0:19bits (6.53)\nYou can easily verify that\nH(X;Y ) =H(XjY) +I(X;Y) +H(YjX) (6.54)\n= (0:81 + 0:19 + 0:81)bits= 1:81bits (6.55)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n6.3. Mutual information * 211\nFigure 6.4: The marginal entropy, joint entropy, conditional entropy and mutual information represented as\ninformation diagrams. Used with kind permission of Katie Everett.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 572, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 699}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0573_56edc3dc", "text": "Mutual information * 211\nFigure 6.4: The marginal entropy, joint entropy, conditional entropy and mutual information represented as\ninformation diagrams. Used with kind permission of Katie Everett. 6.3.4 Conditional mutual information\nWe can deﬁne the conditional mutual information in the obvious way\nI(X;YjZ),Ep(Z)[I(X;Y)jZ] (6.56)\n=Ep(x;y;z )\u0014\nlogp(x;yjz)\np(xjz)p(yjz)\u0015\n(6.57)\n=H(XjZ) +H(YjZ)\u0000H(X;YjZ) (6.58)\n=H(XjZ)\u0000H(XjY;Z) =H(YjZ)\u0000H(YjX;Z) (6.59)\n=H(X;Z) +H(Y;Z)\u0000H(Z)\u0000H(X;Y;Z ) (6.60)\n=I(Y;X;Z)\u0000I(Y;Z) (6.61)\nThe last equation tells us that the conditional MI is the extra (residual) information that Xtells us\naboutY, excluding what we already knew about YgivenZalone. We can rewrite Equation (6.61) as follows:\nI(Z;Y;X) =I(Z;X) +I(Y;XjZ) (6.62)\nGeneralizing to Nvariables, we get the chain rule for mutual information :\nI(Z1;:::;ZN;X) =NX\nn=1I(Zn;XjZ1;:::;Zn\u00001) (6.63)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n212 Chapter 6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 573, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 951}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0574_0580991f", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n212 Chapter 6. Information Theory\n6.3.5 MI as a “generalized correlation coeﬃcient”\nSuppose that (x;y)are jointly Gaussian:\n\u0012x\ny\u0013\n\u0018N\u0012\n0;\u0012\u001b2\u001a\u001b2\n\u001a\u001b2\u001b2\u0013\u0013\n(6.64)\nWe now show how to compute the mutual information between XandY. Using Equation (6.26), we ﬁnd that the entropy is\nh(X;Y ) =1\n2log\u0002\n(2\u0019e)2det \u0006\u0003\n=1\n2log\u0002\n(2\u0019e)2\u001b4(1\u0000\u001a2)\u0003\n(6.65)\nSinceXandYare individually normal with variance \u001b2, we have\nh(X) =h(Y) =1\n2log\u0002\n2\u0019e\u001b2\u0003\n(6.66)\nHence\nI(X;Y ) =h(X) +h(Y)\u0000h(X;Y ) (6.67)\n= log[2\u0019e\u001b2]\u00001\n2log[(2\u0019e)2\u001b4(1\u0000\u001a2)] (6.68)\n=1\n2log[(2\u0019e\u001b2)2]\u00001\n2log[(2\u0019e\u001b2)2(1\u0000\u001a2)] (6.69)\n=1\n2log1\n1\u0000\u001a2=\u00001\n2log[1\u0000\u001a2] (6.70)\nWe now discuss some interesting special cases. 1.\u001a= 1. In this case, X=Y, andI(X;Y ) =1, which makes sense. Observing Ytells us an inﬁnite\namount of information about X(as we know its real value exactly). 2.\u001a= 0. In this case, XandYare independent, and I(X;Y ) = 0, which makes sense. Observing Y\ntells us nothing about X. 3.\u001a=\u00001.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 574, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 969}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0575_f89adefd", "text": "2.\u001a= 0. In this case, XandYare independent, and I(X;Y ) = 0, which makes sense. Observing Y\ntells us nothing about X. 3.\u001a=\u00001. In this case, X=\u0000Y, andI(X;Y ) =1, which again makes sense. Observing Yallows\nus to predict Xto inﬁnite precision. Now consider the case where XandYare scalar, but not jointly Gaussian. In general it can be\ndiﬃcult to compute the mutual information between continuous random variables, because we have\nto estimate the joint density p(X;Y ). For scalar variables, a simple approximation is to discretize\norquantize them, by dividing the ranges of each variable into bins, and computing how many values\nfall in each histogram bin [Sco79]. We can then easily compute the MI using the empirical pmf. Unfortunately, the number of bins used, and the location of the bin boundaries, can have a\nsigniﬁcant eﬀect on the results. One way to avoid this is to use K-nearest neighbor distances to\nestimate densities in a non-parametric, adaptive way.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 575, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 963}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0576_3aabbf31", "text": "One way to avoid this is to use K-nearest neighbor distances to\nestimate densities in a non-parametric, adaptive way. This is the basis of the KSG estimator for MI\nproposed in [KSG04]. This is implemented in the sklearn.feature_selection.mutual_info_regression\nfunction. For papers related to this estimator, see [GOV18; HN19]. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n6.3. Mutual information * 213\n6.3.6 Normalized mutual information\nFor some applications, it is useful to have a normalized measure of dependence, between 0 and 1. We\nnow discuss one way to construct such a measure. First, note that\nI(X;Y) =H(X)\u0000H(XjY)\u0014H(X) (6.71)\n=H(Y)\u0000H(YjX)\u0014H(Y) (6.72)\nso\n0\u0014I(X;Y)\u0014min (H(X);H(Y)) (6.73)\nTherefore we can deﬁne the normalized mutual information as follows:\nNMI (X;Y ) =I(X;Y)\nmin (H(X);H(Y))\u00141 (6.74)\nThis normalized mutual information ranges from 0 to 1. When NMI (X;Y) = 0, we have\nI(X;Y) = 0, soXandYare independent.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 576, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 959}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0577_59ad707b", "text": "When NMI (X;Y) = 0, we have\nI(X;Y) = 0, soXandYare independent. When NMI (X;Y ) = 1, andH(X)<H(Y), we have\nI(X;Y) =H(X)\u0000H(XjY) =H(X) =)H(XjY) = 0 (6.75)\nand soXis a deterministic function of Y. For example, suppose Xis a discrete random variable\nwith pmf [0:5;0:25;0:25]. We have MI(X;X ) = 1:5(using log base 2), and H(X) = 1:5, so the\nnormalized MI is 1, as is to be expected. For continuous random variables, it is harder to normalize the mutual information, because of\nthe need to estimate the diﬀerential entropy, which is sensitive to the level of quantization. See\nSection 6.3.7 for further discussion. 6.3.7 Maximal information coeﬃcient\nAs we discussed in Section 6.3.6, it is useful to have a normalized estimate of the mutual information,\nbut this can be tricky to compute for real-valued data.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 577, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 805}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0578_fbe8fdcb", "text": "6.3.7 Maximal information coeﬃcient\nAs we discussed in Section 6.3.6, it is useful to have a normalized estimate of the mutual information,\nbut this can be tricky to compute for real-valued data. One approach, known as the maximal\ninformation coeﬃcient (MIC) [Res+11], is to deﬁne the following quantity:\nMIC(X;Y ) = max\nGI((X;Y )jG)\nlogjjGjj(6.76)\nwhereGis the set of 2d grids, and (X;Y)jGrepresents a discretization of the variables onto this\ngrid, andjjGjjismin(Gx;Gy), whereGxis the number of grid cells in the xdirection, and Gyis\nthe number of grid cells in the ydirection. (The maximum grid resolution depends on the sample\nsizen; they suggest restricting grids so that GxGy\u0014B(n), whereB(n) =n\u000b, where\u000b= 0:6.) The\ndenominator is the entropy of a uniform joint distribution; dividing by this ensures 0\u0014MIC\u00141. The intuition behind this statistic is the following: if there is a relationship between XandY,\nthen there should be some discrete gridding of the 2d input space that captures this.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 578, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 996}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0579_6d189ca3", "text": "The intuition behind this statistic is the following: if there is a relationship between XandY,\nthen there should be some discrete gridding of the 2d input space that captures this. Since we don’t\nknow the correct grid to use, MIC searches over diﬀerent grid resolutions (e.g., 2x2, 2x3, etc), as well\nas over locations of the grid boundaries. Given a grid, it is easy to quantize the data and compute\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n214 Chapter 6. Information Theory\nFigure 6.5: Illustration of how the maximal information coeﬃcient (MIC) is computed. (a) We search over\ndiﬀerent grid resolutions, and grid cell locations, and compute the MI for each. (b) For each grid resolution\n(k;l), we deﬁne set M(k;l)to be the maximum MI for any grid of that size, normalized by log(min(k;l)). (c)\nWe visualize the matrix M. The maximum entry (denoted by a star) is deﬁned to be the MIC. From Figure 1\nof [Res+11]. Used with kind permission of David Reshef. MI.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 579, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 980}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0580_a00cad73", "text": "(c)\nWe visualize the matrix M. The maximum entry (denoted by a star) is deﬁned to be the MIC. From Figure 1\nof [Res+11]. Used with kind permission of David Reshef. MI. We deﬁne the characteristic matrix M(k;l)to be the maximum MI achievable by any grid\nof size (k;l), normalized by log(min(k;l)). The MIC is then the maximum entry in this matrix,\nmaxkl\u0014B(n)M(k;l). See Figure 6.5 for a visualization of this process. In [Res+11], they show that this quantity exhibits a property known as equitability , which means\nthat it gives similar scores to equally noisy relationships, regardless of the type of relationship (e.g.,\nlinear, non-linear, non-functional). In [Res+16], they present an improved estimator, called MICe, which is more eﬃcient to compute,\nand only requires optimizing over 1d grids, which can be done in O(n)time using dynamic program-\nming.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 580, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 857}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0581_734c783c", "text": "In [Res+16], they present an improved estimator, called MICe, which is more eﬃcient to compute,\nand only requires optimizing over 1d grids, which can be done in O(n)time using dynamic program-\nming. They also present another quantity, called TICe(total information content), that has higher\npower to detect relationships from small sample sizes, but lower equitability. This is deﬁned to beP\nkl\u0014B(n)M(k;l). They recommend using TICe to screen a large number of candidate relationships,\nand then using MICe to quantify the strength of the relationship. For an eﬃcient implementation of\nboth of these metrics, see [Alb+18]. We can interpret MIC of 0 to mean there is no relationship between the variables, and 1 to represent\na noise-free relationship of any form. This is illustrated in Figure 6.6. Unlike correlation coeﬃcients,\nMIC is not restricted to ﬁnding linear relationships. For this reason, the MIC has been called “a\ncorrelation for the 21st century” [Spe11].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 581, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 968}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0582_4050c409", "text": "Unlike correlation coeﬃcients,\nMIC is not restricted to ﬁnding linear relationships. For this reason, the MIC has been called “a\ncorrelation for the 21st century” [Spe11]. In Figure 6.7, we give a more interesting example, from [Res+11]. The data consists of 357 variables\nmeasuring a variety of social, economic, health and political indicators, collected by the World Health\nOrganization (WHO). On the left of the ﬁgure, we see the correlation coeﬃcient (CC) plotted against\nthe MIC for all 63,546 variable pairs. On the right of the ﬁgure, we see scatter plots for particular\npairs of variables, which we now discuss:\n•The point marked C (near 0,0 on the plot) has a low CC and a low MIC. The corresponding\nscatter plot makes it clear that there is no relationship between these two variables (percentage of\nlives lost to injury and density of dentists in the population).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 582, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 875}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0583_9a135fe1", "text": "The corresponding\nscatter plot makes it clear that there is no relationship between these two variables (percentage of\nlives lost to injury and density of dentists in the population). •The points marked D and H have high CC (in absolute value) and high MIC, because they\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n6.3. Mutual information * 215\nPearson r=1.0\nMIC=1.0\nPearson r=0.8\nMIC=0.5\nPearson r=0.4\nMIC=0.2\nPearson r=0.0\nMIC=0.1\nPearson r=-0.4\nMIC=0.2\nPearson r=-0.8\nMIC=0.5\nPearson r=-1.0\nMIC=1.0\nPearson r=1.0\nMIC=1.0\nPearson r=1.0\nMIC=1.0\nPearson r=1.0\nMIC=1.0\nPearson r=-0.1\nMIC=0.2\nPearson r=-1.0\nMIC=1.0\nPearson r=-1.0\nMIC=1.0\nPearson r=-1.0\nMIC=1.0\nPearson r=0.1\nMIC=0.8\nPearson r=-0.0\nMIC=0.2\nPearson r=-0.0\nMIC=0.2\nPearson r=0.0\nMIC=0.4\nPearson r=-0.1\nMIC=0.4\nPearson r=0.0\nMIC=0.5\nPearson r=-0.0\nMIC=0.1\nFigure 6.6: Plots of some 2d distributions and the corresponding estimate of correlation coeﬃcient R2\nand the maximal information coeﬃcient (MIC).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 583, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 996}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0584_d849575e", "text": "Compare to Figure 3.1. Generated by code at ﬁg-\nures.probml.ai/book1/6.6. represent nearly linear relationships. •The points marked E, F, and G have low CC but high MIC. This is because they correspond\nto non-linear (and sometimes, as in the case of E and F, non-functional, i.e., one-to-many)\nrelationships between the variables. 6.3.8 Data processing inequality\nSuppose we have an unknown variable X, and we observe a noisy function of it, call it Y. If we\nprocess the noisy observations in some way to create a new variable Z, it should be intuitively obvious\nthat we cannot increase the amount of information we have about the unknown quantity, X. This is\nknown as the data processing inequality . We now state this more formally, and then prove it. Theorem 6.3.1. SupposeX!Y!Zforms a Markov chain, so that X?ZjY. Then I(X;Y)\u0015\nI(X;Z).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 584, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 838}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0585_9152284c", "text": "This is\nknown as the data processing inequality . We now state this more formally, and then prove it. Theorem 6.3.1. SupposeX!Y!Zforms a Markov chain, so that X?ZjY. Then I(X;Y)\u0015\nI(X;Z). Proof.By the chain rule for mutual information (Equation (6.62)), we can expand the mutual\ninformation in two diﬀerent ways:\nI(X;Y;Z) =I(X;Z) +I(X;YjZ) (6.77)\n=I(X;Y) +I(X;ZjY) (6.78)\nSinceX?ZjY, we have I(X;ZjY) = 0, so\nI(X;Z) +I(X;YjZ) =I(X;Y) (6.79)\nSinceI(X;YjZ)\u00150, we have I(X;Y)\u0015I(X;Z). Similarly one can prove that I(Y;Z)\u0015I(X;Z). Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n216 Chapter 6. Information Theory\nFigure 6.7: Left: Correlation coeﬃcient vs maximal information criterion (MIC) for all pairwise relationships\nin the WHO data. Right: scatter plots of certain pairs of variables. The red lines are non-parametric\nsmoothing regressions ﬁt separately to each trend. From Figure 4 of [Res+11]. Used with kind permission of\nDavid Reshef.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 585, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 950}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0586_b21b1a20", "text": "The red lines are non-parametric\nsmoothing regressions ﬁt separately to each trend. From Figure 4 of [Res+11]. Used with kind permission of\nDavid Reshef. 6.3.9 Suﬃcient Statistics\nAn important consequence of the DPI is the following. Suppose we have the chain \u0012!D!s(D). Then\nI(\u0012;s(D))\u0014I(\u0012;D) (6.80)\nIf this holds with equality, then we say that s(D)is asuﬃcient statistic of the dataDfor the\npurposes of inferring \u0012. In this case, we can equivalently write \u0012!s(D)!D, since we can\nreconstruct the data from knowing s(D)just as accurately as from knowing \u0012. An example of a suﬃcient statistic is the data itself, s(D) =D, but this is not very useful, since\nit doesn’t summarize the data at all. Hence we deﬁne a minimal suﬃcient statistic s(D)as one\nwhich is suﬃcient, and which contains no extra information about \u0012; thuss(D)maximally compresses\nthe dataDwithout losing information which is relevant to predicting \u0012.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 586, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 915}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0587_f916f05b", "text": "More formally, we say sis a\nminimal suﬃcient statistic for Dif for all suﬃcient statistics s0(D)there is some function fsuch that\ns(D) =f(s0(D)). We can summarize the situation as follows:\n\u0012!s(D)!s0(D)!D (6.81)\nHeres0(D)takess(D)and adds redundant information to it, thus creating a one-to-many mapping. For example, a minimal suﬃcient statistic for a set of NBernoulli trials is simply NandN1=P\nnI(Xn= 1), i.e., the number of successes. In other words, we don’t need to keep track of the\nentire sequence of heads and tails and their ordering, we only need to keep track of the total number\nof heads and tails. Similarly, for inferring the mean of a Gaussian distribution with known variance\nwe only need to know the empirical mean and number of samples. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n6.3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 587, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 835}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0588_eb2f17f9", "text": "Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n6.3. Mutual information * 217\n6.3.10 Fano’s inequality *\nA common method for feature selection is to pick input features Xdwhich have high mutual\ninformation with the response variable Y. Below we justify why this is a reasonable thing to do. In particular, we state a result, known as Fano’s inequality , which bounds the probability of\nmisclassiﬁcation (for any method) in terms of the mutual information between the features Xand\nthe class label Y. Theorem 6.3.2. (Fano’s inequality) Consider an estimator ^Y=f(X)such thatY!X!^Yforms\na Markov chain. Let Ebe the event ^Y6=Y, indicating that an error occured, and let Pe=P(Y6=^Y)\nbe the probability of error.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 588, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 736}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0589_1504e1fc", "text": "(Fano’s inequality) Consider an estimator ^Y=f(X)such thatY!X!^Yforms\na Markov chain. Let Ebe the event ^Y6=Y, indicating that an error occured, and let Pe=P(Y6=^Y)\nbe the probability of error. Then we have\nH(YjX)\u0014H\u0010\nYj^Y\u0011\n\u0014H(E) +PelogjYj (6.82)\nSinceH(E)\u00141, as we saw in Figure 6.1, we can weaken this result to get\n1 +PelogjYj\u0015H(YjX) (6.83)\nand hence\nPe\u0015H(YjX)\u00001\nlogjYj(6.84)\nThus minimizing H(YjX)(which can be done by maximizing I(X;Y)) will also minimize the lower\nbound onPe. Proof.(From [CT06, p38].) Using the chain rule for entropy, we have\nH\u0010\nE;Yj^Y\u0011\n=H\u0010\nYj^Y\u0011\n+H\u0010\nEjY;^Y\u0011\n|{z}\n=0(6.85)\n=H\u0010\nEj^Y\u0011\n+H\u0010\nYjE;^Y\u0011\n(6.86)\nSince conditioning reduces entropy (see Section 6.2.4), we have H\u0010\nEj^Y\u0011\n\u0014H(E).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 589, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 705}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0590_45f23a4e", "text": "The ﬁnal term\ncan be bounded as follows:\nH\u0010\nYjE;^Y\u0011\n=P(E= 0)H\u0010\nYj^Y;E = 0\u0011\n+P(E= 1)H\u0010\nYj^Y;E = 1\u0011\n(6.87)\n\u0014(1\u0000Pe)0 +PelogjYj (6.88)\nHence\nH\u0010\nYj^Y\u0011\n\u0014H\u0010\nEj^Y\u0011\n|{z}\n\u0014H(E)+H\u0010\nYjE;^Y\u0011\n|{z}\nPelogjYj(6.89)\nFinally, by the data processing inequality, we have I(Y;^Y)\u0014I(Y;X), soH(YjX)\u0014H\u0010\nYj^Y\u0011\n, which\nestablishes Equation (6.82). Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n218 Chapter 6. Information Theory\n6.4 Exercises\nExercise 6.1 [Expressing mutual information in terms of entropies *]\nProve the following identities:\nI(X;Y) =H(X)\u0000H(XjY) =H(Y)\u0000H(YjX) (6.90)\nand\nH(X;Y ) =H(XjY) +H(YjX) +I(X;Y) (6.91)\nExercise 6.2 [Relationship between D(pjjq)and\u001f2statistic]\n(Source: [CT91, Q12.2].)\nShow that, if p(x)\u0019q(x), then\nKL(pkq)\u00191\n2\u001f2(6.92)\nwhere\n\u001f2=X\nx(p(x)\u0000q(x))2\nq(x)(6.93)\nHint: write\np(x) = \u0001(x) +q(x) (6.94)\np(x)\nq(x)= 1 +\u0001(x)\nq(x)(6.95)\nand use the Taylor series expansion for log(1 +x). log(1 +x) =x\u0000x2\n2+x3\n3\u0000x4\n4\u0001\u0001\u0001 (6.96)\nfor\u00001<x\u00141.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 590, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 948}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0591_2c323121", "text": "log(1 +x) =x\u0000x2\n2+x3\n3\u0000x4\n4\u0001\u0001\u0001 (6.96)\nfor\u00001<x\u00141. Exercise 6.3 [Fun with entropies *]\n(Source: Mackay.) Consider the joint distribution p(X;Y )\nx\n1 2 3 4\n11=8 1=16 1=32 1=32\ny21=16 1=8 1=32 1=32\n31=16 1=16 1=16 1=16\n41=40 0 0\na. What is the joint entropy H(X;Y )? b. What are the marginal entropies H(X)andH(Y)? c. The entropy of Xconditioned on a speciﬁc value of yis deﬁned as\nH(XjY=y) =\u0000X\nxp(xjy) logp(xjy) (6.97)\nComputeH(Xjy)for each value of y. Does the posterior entropy on Xever increase given an observation\nofY? Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n6.4. Exercises 219\nd. The conditional entropy is deﬁned as\nH(XjY) =X\nyp(y)H(XjY=y) (6.98)\nCompute this. Does the posterior entropy on Xincrease or decrease when averaged over the possible\nvalues ofY? e. What is the mutual information between XandY? Exercise 6.4 [Forwards vs reverse KL divergence]\n(Source: Exercise33.7 of [Mac03].) Considera factored approximation q(x;y) =q(x)q(y)to ajoint distribution\np(x;y).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 591, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1009}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0592_66dfe2d3", "text": "Exercise 6.4 [Forwards vs reverse KL divergence]\n(Source: Exercise33.7 of [Mac03].) Considera factored approximation q(x;y) =q(x)q(y)to ajoint distribution\np(x;y). Show that to minimize the forwards KL KL(pkq)we should set q(x) =p(x)andq(y) =p(y), i.e.,\nthe optimal approximation is a product of marginals\nNow consider the following joint distribution, where the rows represent yand the columns x. 1 2 3 4\n11/8 1/8 0 0\n21/8 1/8 0 0\n30 0 1/4 0\n40 0 0 1/4\nShow that the reverse KL KL(qkp)for thisphas three distinct minima. Identify those minima and evaluate\nKL(qkp)at each of them. What is the value of KL(qkp)if we setq(x;y) =p(x)p(y)? Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n\n7Linear Algebra\nThis chapter is co-authored with Zico Kolter. 7.1 Introduction\nLinear algebra is the study of matrices and vectors. In this chapter, we summarize the key material\nthat we will need throughout the book.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 592, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 914}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0593_fee66c24", "text": "7.1 Introduction\nLinear algebra is the study of matrices and vectors. In this chapter, we summarize the key material\nthat we will need throughout the book. Much more information can be found in other sources, such\nas [Str09; Kle13; Mol04; TB97; Axl15; Tho17; Agg20]. 7.1.1 Notation\nIn this section, we deﬁne some notation. 7.1.1.1 Vectors\nAvectorx2Rnis a list ofnnumbers, usually written as a column vector\nx=2\n6664x1\nx2\n... xn3\n7775: (7.1)\nThe vector of all ones is denoted 1. The vector of all zeros is denoted 0. Theunit vector eiis a vector of all 0’s, except entry i, which has value 1:\nei= (0;:::; 0;1;0;:::; 0) (7.2)\nThis is also called a one-hot vector . 7.1.1.2 Matrices\nAmatrix A2Rm\u0002nwithmrows andncolumns is a 2d array of numbers, arranged as follows:\nA=2\n6664a11a12\u0001\u0001\u0001a1n\na21a22\u0001\u0001\u0001a2n\n............ am1am2\u0001\u0001\u0001amn3\n7775: (7.3)\n222 Chapter 7. Linear Algebra\nIfm=n, the matrix is said to be square. We use the notation AijorAi;jto denote the entry of Ain theith row and jth column.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 593, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 988}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0594_d448bcec", "text": "am1am2\u0001\u0001\u0001amn3\n7775: (7.3)\n222 Chapter 7. Linear Algebra\nIfm=n, the matrix is said to be square. We use the notation AijorAi;jto denote the entry of Ain theith row and jth column. We use\nthe notation Ai;:to denote the i’th row and A:;jto denote the j’th column. We treat all vectors as\ncolumn vectors by default (so Ai;:is viewed as a column vector with nentries). We use bold upper\ncase letters to denote matrices, bold lower case letters to denote vectors, and non-bold letters to\ndenote scalars. We can view a matrix as a set of columns stacked along the horizontal axis:\nA=2\n4j j j\nA:;1A:;2\u0001\u0001\u0001A:;n\nj j j3\n5: (7.4)\nFor brevity, we will denote this by\nA= [A:;1;A:;2;:::;A:;n] (7.5)\nWe can also view a matrix as a set of rows stacked along the vertical axis:\nA=2\n6664—AT\n1;:—\n—AT\n2;:—\n... —AT\nm;:—3\n7775: (7.6)\nFor brevity, we will denote this by\nA= [A1;:;A2;:;:::;Am;:] (7.7)\n(Note the use of a semicolon.)\nThetranspose ofamatrixresultsfrom“ﬂipping” therowsandcolumns.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 594, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 969}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0595_c00c69a0", "text": "—AT\nm;:—3\n7775: (7.6)\nFor brevity, we will denote this by\nA= [A1;:;A2;:;:::;Am;:] (7.7)\n(Note the use of a semicolon.)\nThetranspose ofamatrixresultsfrom“ﬂipping” therowsandcolumns. Givenamatrix A2Rm\u0002n,\nits transpose, written AT2Rn\u0002m, is deﬁned as\n(AT)ij=Aji (7.8)\nThe following properties of transposes are easily veriﬁed:\n(AT)T=A (7.9)\n(AB)T=BTAT(7.10)\n(A+B)T=AT+BT(7.11)\nIf a square matrix satisﬁes A=AT, it is called symmetric . We denote the set of all symmetric\nmatrices of size nasSn. 7.1.1.3 Tensors\nAtensor(in machine learning terminology) is just a generalization of a 2d array to more than 2\ndimensions, as illustrated in Figure 7.1. For example, the entries of a 3d tensor are denoted by Aijk. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.1. Introduction 223\nVector MatrixTensor\n4x4x4\n64\n8x8\nFigure 7.1: Illustration of a 1d vector, 2d matrix, and 3d tensor.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 595, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 902}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0596_3d7731f5", "text": "Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.1. Introduction 223\nVector MatrixTensor\n4x4x4\n64\n8x8\nFigure 7.1: Illustration of a 1d vector, 2d matrix, and 3d tensor. The colors are used to represent individual\nentries of the vector; this list of numbers can also be stored in a 2d matrix, as shown. (In this example, the\nmatrix is layed out in column-major order, which is the opposite of that used by Python.) We can also reshape\nthe vector into a 3d tensor, as shown. (a)\n (b)\nFigure 7.2: Illustration of (a) row-major vs (b) column-major order. From https: // commons. wikimedia. org/ wiki/ File: Row_ and_ column_ major_ order. svg . Used with kind permission of Wikipedia author\nCmglee. The number of dimensions is known as the orderorrankof the tensor.1In mathematics, tensors\ncan be viewed as a way to deﬁne multilinear maps, just as matrices can be used to deﬁne linear\nfunctions, although we will not need to use this interpretation.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 596, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 974}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0597_6aff9121", "text": "We canreshape a matrix into a vector by stacking its columns on top of each other, as shown in\nFigure 7.1. This is denoted by\nvec(A) = [A:;1;\u0001\u0001\u0001;A:;n]2Rmn\u00021(7.12)\nConversely, we can reshape a vector into a matrix. There are two choices for how to do this, known\nasrow-major order (used by languages such as Python and C++) and column-major order\n(used by languages such as Julia, Matlab, R and Fortran). See Figure 7.2 for an illustration of the\ndiﬀerence. 1. Note, however, that the rank of a 2d matrix is a diﬀerent concept, as discussed in Section 7.1.4.3. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n224 Chapter 7. Linear Algebra\n(a)\n (b)\nFigure 7.3: (a) Top: A vector v(blue) is added to another vector w(red). Bottom: wis stretched by a factor\nof 2, yielding the sum v+ 2w. From https: // en. wikipedia. org/ wiki/ Vector_ space .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 597, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 852}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0598_f8a74746", "text": "Bottom: wis stretched by a factor\nof 2, yielding the sum v+ 2w. From https: // en. wikipedia. org/ wiki/ Vector_ space . Used with kind\npermission of Wikipedia author IkamusumeFan (b) A vector vinR2(blue) expressed in terms of diﬀerent\nbases: using the standard basis of R2,v=xe1+ye2(black), and using a diﬀerent, non-orthogonal basis:\nv=f1+f2(red). From https: // en. wikipedia. org/ wiki/ Vector_ space . Used with kind permission of\nWikiepdia author Jakob.scholbach. 7.1.2 Vector spaces\nIn this section, we discuss some fundamental concepts in linear algebra. 7.1.2.1 Vector addition and scaling\nWe can view a vector x2Rnas deﬁning a point in n-dimensional Euclidean space. A vector space\nis a collection of such vectors, which can be added together, and scaled by scalars(1-dimensional\nnumbers), in order to create new points. These operations are deﬁned to operate elementwise, in\nthe obvious way, namely x+y= (x1+y1;:::;xn+yn)andcx= (cx1;:::;cxn), wherec2R. See\nFigure 7.3a for an illustration.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 598, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1000}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0599_4bb5d3cd", "text": "These operations are deﬁned to operate elementwise, in\nthe obvious way, namely x+y= (x1+y1;:::;xn+yn)andcx= (cx1;:::;cxn), wherec2R. See\nFigure 7.3a for an illustration. 7.1.2.2 Linear independence, spans and basis sets\nA set of vectorsfx1;x2;:::xngis said to be (linearly) independent if no vector can be represented\nas a linear combination of the remaining vectors. Conversely, a vector which canbe represented as a\nlinear combination of the remaining vectors is said to be (linearly) dependent . For example, if\nxn=n\u00001X\ni=1\u000bixi (7.13)\nfor somef\u000b1;:::;\u000bn\u00001gthenxnis dependent onfx1;:::;xn\u00001g; otherwise, it is independent of\nfx1;:::;xn\u00001g. Thespanof a set of vectors fx1;x2;:::;xngis the set of all vectors that can be expressed as a\nlinear combination of fx1;:::;xng. That is,\nspan(fx1;:::;xng),(\nv:v=nX\ni=1\u000bixi; \u000bi2R)\n: (7.14)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 599, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 911}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0600_e32ce656", "text": "That is,\nspan(fx1;:::;xng),(\nv:v=nX\ni=1\u000bixi; \u000bi2R)\n: (7.14)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.1. Introduction 225\nIt can be shown that if fx1;:::;xngis a set ofnlinearly independent vectors, where each xi2Rn,\nthen span(fx1;:::;xng) =Rn. In other words, anyvectorv2Rncan be written as a linear\ncombination of x1throughxn. AbasisBis a set of linearly independent vectors that spans the whole space, meaning that\nspan(B) =Rn. There are often multiple bases to choose from, as illustrated in Figure 7.3b. The\nstandard basis uses thecoordinate vectors e1= (1;0;:::; 0), up toen= (0;0;:::; 0;1). This\nlets us translate back and forth between viewing a vector in R2as an either an “arrow in the plane”,\nrooted at the origin, or as an ordered list of numbers (corresponding to the coeﬃcients for each basis\nvector). 7.1.2.3 Linear maps and matrices\nAlinear map orlinear transformation is any function f:V !W such thatf(v+w) =\nf(v) +f(w)andf(av) =a f(v)for allv;w2V.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 600, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1001}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0601_93355e5e", "text": "7.1.2.3 Linear maps and matrices\nAlinear map orlinear transformation is any function f:V !W such thatf(v+w) =\nf(v) +f(w)andf(av) =a f(v)for allv;w2V. Once the basis of Vis chosen, a linear map\nf:V!W is completely determined by specifying the images of the basis vectors, because any\nelement ofVcan be expressed uniquely as a linear combination of them. SupposeV=RnandW=Rm. We can compute f(vi)2Rmfor each basis vector in V, and\nstore these along the columns of an m\u0002nmatrix A. We can then compute y=f(x)2Rmfor any\nx2Rnas follows:\ny=0\n@nX\nj=1a1jxj;:::;nX\nj=1amjxj1\nA (7.15)\nThis corresponds to multiplying the vector xby the matrix A:\ny=Ax (7.16)\nSee Section 7.2 for more details. If the function is invertible, we can write\nx=A\u00001y (7.17)\nSee Section 7.3 for details. 7.1.2.4 Range and nullspace of a matrix\nSuppose we view a matrix A2Rm\u0002nas a set ofmvectors in Rn. Therange(sometimes also called\nthecolumn space ) of this matrix is the span of the columns of A.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 601, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 961}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0602_1c20d35b", "text": "7.1.2.4 Range and nullspace of a matrix\nSuppose we view a matrix A2Rm\u0002nas a set ofmvectors in Rn. Therange(sometimes also called\nthecolumn space ) of this matrix is the span of the columns of A. In other words,\nrange( A),fv2Rm:v=Ax;x2Rng: (7.18)\nThis can be thought of as the set of vectors that can be “reached” or “generated” by A; it is a\nsubspace of Rmwhose dimensionality is given by the rank of A(see Section 7.1.4.3). The nullspace\nof a matrix A2Rm\u0002nis the set of all vectors that get mapped to the null vector when multiplied\nbyA, i.e.,\nnullspace( A),fx2Rn:Ax=0g: (7.19)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n226 Chapter 7. Linear Algebra\nRRAAAANull spaceRangey= A xiix1x2x3x40y1y2nm\nFigure 7.4: Visualization of the nullspace and range of an m\u0002nmatrix A. Herey1=Ax1andy2=Ax4,\nsoy1andy2are in the range of A(are reachable from some x). Also Ax2=0andAx3=0, sox2andx3\nare in the nullspace of A(get mapped to 0).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 602, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 939}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0603_7aa97a4b", "text": "Herey1=Ax1andy2=Ax4,\nsoy1andy2are in the range of A(are reachable from some x). Also Ax2=0andAx3=0, sox2andx3\nare in the nullspace of A(get mapped to 0). We see that the range is often a subset of the input domain of\nthe mapping. The span of the rows of Ais the complement to the nullspace of A. See Figure 7.4 for an illustration of the range and nullspace of a matrix. We shall discuss how to\ncompute the range and nullspace of a matrix numerically in Section 7.5.4 below. 7.1.2.5 Linear projection\nTheprojection of a vectory2Rmonto the span offx1;:::;xng(here we assume xi2Rm) is the\nvectorv2span(fx1;:::;xng), such thatvis as close as possible to y, as measured by the Euclidean\nnormkv\u0000yk2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 603, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 694}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0604_7e6d0445", "text": "We denote the projection as Proj(y;fx1;:::;xng)and can deﬁne it formally as\nProj(y;fx1;:::;xng) = argminv2span(fx1;:::;xng)ky\u0000vk2: (7.20)\nGiven a (full rank) matrix A2Rm\u0002nwithm\u0015n, we can deﬁne the projection of a vector y2Rm\nonto the range of Aas follows:\nProj(y;A) = argminv2R(A)kv\u0000yk2=A(ATA)\u00001ATy: (7.21)\nThese are the same as the normal equations from Section 11.2.2.2. 7.1.3 Norms of a vector and matrix\nIn this section, we discuss ways of measuring the “size” of a vector and matrix. 7.1.3.1 Vector norms\nAnormof a vectorkxkis, informally, a measure of the “length” of the vector. More formally, a\nnorm is any function f:Rn!Rthat satisﬁes 4 properties:\n•For allx2Rn,f(x)\u00150(non-negativity). Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.1. Introduction 227\n•f(x) = 0if and only if x=0(deﬁniteness). •For allx2Rn,t2R,f(tx) =jtjf(x)(absolute value homogeneity). •For allx;y2Rn,f(x+y)\u0014f(x) +f(y)(triangle inequality).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 604, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 950}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0605_d5f6fdc8", "text": "August 27, 2021\n7.1. Introduction 227\n•f(x) = 0if and only if x=0(deﬁniteness). •For allx2Rn,t2R,f(tx) =jtjf(x)(absolute value homogeneity). •For allx;y2Rn,f(x+y)\u0014f(x) +f(y)(triangle inequality). Consider the following common examples:\np-normkxkp= (Pn\ni=1jxijp)1=p, forp\u00151. 2-normkxk2=pPn\ni=1x2\ni, also called Euclidean norm. Note that kxk2\n2=xTx. 1-normkxk1=Pn\ni=1jxij. Max-normkxk1= maxijxij. 0-normkxk0=Pn\ni=1I(jxij>0). This is a pseudo norm , since it does not satisfy homogeneity. It counts the number of non-zero elements in x. If we deﬁne 00= 0, we can write this as\nkxk0=Pn\ni=1x0\ni. 7.1.3.2 Matrix norms\nSuppose we think of a matrix A2Rm\u0002nas deﬁning a linear function f(x) =Ax. We deﬁne the\ninduced norm ofAas the maximum amount by which fcan lengthen any unit-norm input:\njjAjjp= max\nx6=0jjAxjjp\njjxjjp= max\njjxjj=1jjAxjjp (7.22)\nTypicallyp= 2, in which case\njjAjj2=q\n\u0015max(ATA) = max\ni\u001bi (7.23)\nwhere\u001biis thei’th singular value.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 605, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 937}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0606_a67bfa3c", "text": "Thenuclear norm , also called the trace norm , is deﬁned as\njjAjj\u0003= tr(p\nATA) =X\ni\u001bi (7.24)\nwherep\nATAis the matrix square root. Since the singular values are always non-negative, we have\njjAjj\u0003=X\nij\u001bij=jj\u001bjj1 (7.25)\nUsing this as a regularizer encourages many singular values to become zero, resulting in a low rank\nmatrix. More generally, we can deﬁne the Schattenp-normas\njjAjjp= X\ni\u001bp\ni(A)!1=p\n(7.26)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n228 Chapter 7. Linear Algebra\nIf we think of a matrix as a vector, we can deﬁne the matrix norm in terms of a vector norm,\njjAjj=jjvec(A)jj.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 606, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 605}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0607_32518c70", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n228 Chapter 7. Linear Algebra\nIf we think of a matrix as a vector, we can deﬁne the matrix norm in terms of a vector norm,\njjAjj=jjvec(A)jj. If the vector norm is the 2-norm, the corresponding matrix norm is the Frobenius\nnorm:\njjAjjF=vuutmX\ni=1nX\nj=1a2\nij=q\ntr(ATA) =jjvec(A)jj2 (7.27)\nIfAis expensive to evaluate, but Avis cheap (for a random vector v), we can create a stochastic\napproximation to the Frobenius norm by using the Hutchinson trace estimator from Equation (7.37)\nas follows:\njjAjj2\nF= tr(ATA) =E\u0002\nvTATAv\u0003\n=E\u0002\njjAvjj2\n2\u0003\n(7.28)\nwherev\u0018N(0;I). 7.1.4 Properties of a matrix\nIn this section, we discuss various scalar properties of matrices.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 607, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 697}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0608_78a7f2a5", "text": "7.1.4 Properties of a matrix\nIn this section, we discuss various scalar properties of matrices. 7.1.4.1 Trace of a square matrix\nThetraceof a square matrix A2Rn\u0002n, denoted tr(A), is the sum of diagonal elements in the\nmatrix:\ntr(A),nX\ni=1Aii: (7.29)\nThe trace has the following properties, where c2Ris a scalar, and A;B2Rn\u0002nare square\nmatrices:\ntr(A) = tr( AT) (7.30)\ntr(A+B) = tr( A) + tr( B) (7.31)\ntr(cA) =ctr(A) (7.32)\ntr(AB) = tr( BA) (7.33)\ntr(A) =nX\ni=1\u0015iwhere\u0015iare the eigenvalues of A (7.34)\nWe also have the following important cyclic permutation property : For A;B;Csuch that ABC\nis square,\ntr(ABC ) = tr( BCA ) = tr( CAB ) (7.35)\nFrom this, we can derive the trace trick , which rewrites the scalar inner product xTAxas follows\nxTAx= tr(xTAx) = tr(xxTA) (7.36)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.1. Introduction 229\nIn some cases, it may be expensive to evaluate the matrix A, but we may be able to cheaply\nevaluate matrix-vector products Av.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 608, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 997}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0609_0e6f6ae1", "text": "August 27, 2021\n7.1. Introduction 229\nIn some cases, it may be expensive to evaluate the matrix A, but we may be able to cheaply\nevaluate matrix-vector products Av. Supposevis a random vector such that E\u0002\nvvT\u0003\n=I. In this\ncase, we can create a Monte Carlo approximation to tr(A)using the following identity:\ntr(A) = tr( AE\u0002\nvvT\u0003\n) =E\u0002\ntr(AvvT)\u0003\n=E\u0002\ntr(vTAv)\u0003\n(7.37)\nThis is called the Hutchinson trace estimator [Hut90]. 7.1.4.2 Determinant of a square matrix\nThedeterminant of a square matrix, denoted det(A)orjAj, is a measure of how much it changes\na unit volume when viewed as a linear transformation.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 609, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 605}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0610_72f82917", "text": "7.1.4.2 Determinant of a square matrix\nThedeterminant of a square matrix, denoted det(A)orjAj, is a measure of how much it changes\na unit volume when viewed as a linear transformation. (The formal deﬁnition is rather complex and\nis not needed here.)\nThe determinant operator satisﬁes these properties, where A;B2Rn\u0002n\njAj=jATj (7.38)\njcAj=cnjAj (7.39)\njABj=jAjjBj (7.40)\njAj= 0iﬀAis singular (7.41)\njA\u00001j= 1=jAjifAis not singular (7.42)\njAj=nY\ni=1\u0015iwhere\u0015iare the eigenvalues of A (7.43)\nFor a positive deﬁnite matrix A, we can write A=LLT, where Lis the lower triangular Cholesky\ndecomposition. In this case, we have\ndet(A) = det( L) det(LT) = det( L)2(7.44)\nso\nlog det( A) = 2 log det( L) = 2 logY\niLii= 2trace(log(diag( L))) (7.45)\n7.1.4.3 Rank of a matrix\nThecolumn rank of a matrix Ais the dimension of the space spanned by its columns, and the row\nrankis the dimension of the space spanned by its rows.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 610, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 907}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0611_ae12f4a7", "text": "It is a basic fact of linear algebra (that can be\nshown using the SVD, discussed in Section 7.5) that for any matrix A,columnrank (A) =rowrank (A),\nand so this quantity is simply referred to as the rankofA, denoted as rank(A). The following are\nsome basic properties of the rank:\n•ForA2Rm\u0002n,rank(A)\u0014min(m;n). Ifrank(A) =min(m;n), then Ais said to be full rank ,\notherwise it is called rank deﬁcient . •ForA2Rm\u0002n,rank(A) = rank( AT) = rank( ATA) = rank( AAT). •ForA2Rm\u0002n,B2Rn\u0002p,rank(AB)\u0014min(rank( A);rank(B)). Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n230 Chapter 7. Linear Algebra\n•ForA;B2Rm\u0002n,rank(A+B)\u0014rank(A) + rank( B). One can show that a square matrix is invertible iﬀ it is full rank. 7.1.4.4 Condition numbers\nThecondition number of a matrix Ais a measure of how numerically stable any computations\ninvolving Awill be. It is deﬁned as follows:\n\u0014(A),jjAjj\u0001jjA\u00001jj (7.46)\nwherejjAjjis the norm of the matrix. We can show that \u0014(A)\u00151.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 611, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 957}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0612_c8fc4dfa", "text": "It is deﬁned as follows:\n\u0014(A),jjAjj\u0001jjA\u00001jj (7.46)\nwherejjAjjis the norm of the matrix. We can show that \u0014(A)\u00151. (The condition number depends\non which norm we use; we will assume the `2-norm unless stated otherwise.)\nWe say Aiswell-conditioned if\u0014(A)is small (close to 1), and ill-conditioned if\u0014(A)is large. A\nlarge condition number means Ais nearly singular. This is a better measure of nearness to singularity\nthan the size of the determinant. For example, suppose A= 0:1I100\u0002100. Then det(A) = 10\u0000100,\nwhich suggests Ais nearly singular, but \u0014(A) = 1, which means Ais well-conditioned, reﬂecting the\nfact that Axsimply scales the entries of xby 0.1. To get a better understanding of condition numbers, consider the linear system of equations\nAx=b. IfAis non-singular, the unique solution is x=A\u00001b. Suppose we change btob+ \u0001b;\nwhat eﬀect will that have on x?", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 612, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 863}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0613_348e4e12", "text": "IfAis non-singular, the unique solution is x=A\u00001b. Suppose we change btob+ \u0001b;\nwhat eﬀect will that have on x? The new solution must satisify\nA(x+ \u0001x) =b+ \u0001b (7.47)\nwhere\n\u0001x=A\u00001\u0001b (7.48)\nWe say that Ais well-conditioned if a small \u0001bresults in a small \u0001x; otherwise we say that Ais\nill-conditioned. For example, suppose\nA=1\n2\u00121 1\n1 + 10\u0000101\u000010\u000010\u0013\n;A\u00001=\u00121\u000010101010\n1 + 1010\u00001010\u0013\n(7.49)\nThe solution for b= (1;1)isx= (1;1). If we change bby\u0001b, the solution changes to\n\u0001x=A\u00001\u0001b=\u0012\n\u0001b1\u00001010(\u0001b1\u0000\u0001b2)\n\u0001b1+ 1010(\u0001b1\u0000\u0001b2)\u0013\n(7.50)\nSo a small change in bcan lead to an extremely large change in x, because Ais ill-conditioned\n(\u0014(A) = 2\u00021010). In the case of the `2-norm, the condition number is equal to the ratio of the largest to smallest\nsingular values (deﬁned in Section 7.5); furthermore, the singular values are the square roots of the\neigenvalues:\n\u0014(A) =\u001bmax=\u001bmin=r\n\u0015max\n\u0015min(7.51)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 613, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 962}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0614_62aadb81", "text": "August 27, 2021\n7.1. Introduction 231\nWe can gain further insight into condition numbers by considering a quadratic objective function\nf(x) =xTAx. If we plot the level set of this function, it will be elliptical, as shown in Section 7.4.4. As we increase the condition number of A, the ellipses become more and more elongated along certain\ndirections, corresponding to a very narrow valley in function space. If \u0014= 1(the minimum possible\nvalue), the level set will be circular. 7.1.5 Special types of matrices\nIn this section, we will list some common kinds of matrices with various forms of structure. 7.1.5.1 Diagonal matrix\nAdiagonal matrix is a matrix where all non-diagonal elements are 0. This is typically denoted\nD= diag(d1;d2;:::;dn), with\nD=0\nBBB@d1\nd2\n... dn1\nCCCA(7.52)\nTheidentity matrix , denoted I2Rn\u0002n, is a square matrix with ones on the diagonal and zeros\neverywhere else, I= diag(1;1;:::; 1).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 614, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 911}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0615_cdcbf64b", "text": "dn1\nCCCA(7.52)\nTheidentity matrix , denoted I2Rn\u0002n, is a square matrix with ones on the diagonal and zeros\neverywhere else, I= diag(1;1;:::; 1). It has the property that for all A2Rn\u0002n,\nAI=A=IA (7.53)\nwhere the size of Iis determined by the dimensions of Aso that matrix multiplication is possible. We can extract the diagonal vector from a matrix using d=diag(D). We can convert a vector\ninto a diagonal matrix by writing D= diag(d). Ablock diagonal matrix is one which contains matrices on its main diagonal, and is 0 everywhere\nelse, e.g.,\n\u0012A 0\n0 B\u0013\n(7.54)\nAband-diagonal matrix only has non-zero entries along the diagonal, and on ksides of the\ndiagonal, where kis the bandwidth. For example, a tridiagonal 6\u00026matrix looks like this:\n2\n66666666664A11A12 0\u0001\u0001\u0001 \u0001\u0001\u0001 0\nA21A22A23......... 0A32A33A34...... ......A43A44A45 0\n.........A54A55A56\n0\u0001\u0001\u0001 \u0001\u0001\u0001 0A65A663\n77777777775(7.55)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n232 Chapter 7.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 615, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 952}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0616_5552fec7", "text": "0A32A33A34...... ......A43A44A45 0\n.........A54A55A56\n0\u0001\u0001\u0001 \u0001\u0001\u0001 0A65A663\n77777777775(7.55)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n232 Chapter 7. Linear Algebra\n7.1.5.2 Triangular matrices\nAnupper triangular matrix only has non-zero entries on and above the diagonal. A lower\ntriangular matrix only has non-zero entries on and below the diagonal. Triangular matrices have the useful property that the diagonal entries of Aare the eigenvalues of\nA, and hence the determinant is the product of diagonal entries: det(A) =Q\niAii. 7.1.5.3 Positive deﬁnite matrices\nGiven a square matrix A2Rn\u0002nand a vectorx2Rn, the scalar value xTAxis called a quadratic\nform. Written explicitly, we see that\nxTAx=nX\ni=1nX\nj=1Aijxixj: (7.56)\nNote that,\nxTAx= (xTAx)T=xTATx=xT(1\n2A+1\n2AT)x (7.57)\nFor this reason, we often implicitly assume that the matrices appearing in a quadratic form are\nsymmetric.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 616, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 899}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0617_88186c44", "text": "We give the following deﬁnitions:\n•A symmetric matrix A2Snispositive deﬁnite iﬀ for all non-zero vectors x2Rn,xTAx>0. This is usually denoted A\u001f0(or just A>0). If it is possible that xTAx= 0, we say the matrix\nispositive semideﬁnite orpsd. We denote the set of all positive deﬁnite matrices by Sn\n++. •A symmetric matrix A2Snisnegative deﬁnite , denoted A\u001e0(or just A<0) iﬀ for all\nnon-zerox2Rn,xTAx<0. If it is possible that xTAx= 0, we say the matrix is negative\nsemideﬁnite . •A symmetric matrix A2Snisindeﬁnite , if it is neither positive semideﬁnite nor negative\nsemideﬁnite — i.e., if there exists x1;x22Rnsuch thatxT\n1Ax1>0andxT\n2Ax2<0. It should be obvious that if Ais positive deﬁnite, then \u0000Ais negative deﬁnite and vice versa. Likewise, if Ais positive semideﬁnite then \u0000Ais negative semideﬁnite and vice versa. If Ais\nindeﬁnite, then so is \u0000A. It can also be shown that positive deﬁnite and negative deﬁnite matrices\nare always invertible.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 617, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 951}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0618_5a99d881", "text": "If Ais\nindeﬁnite, then so is \u0000A. It can also be shown that positive deﬁnite and negative deﬁnite matrices\nare always invertible. In Section 7.4.3.1, we show that a symmmetric matrix is positive deﬁnite iﬀ its eigenvalues are\npositive. Note that if all elements of Aare positive, it does not mean Ais necessarily positive deﬁnite. For example, A=\u00124 3\n3 2\u0013\nis not positive deﬁnite. Conversely, a positive deﬁnite matrix can have\nnegative entries e.g., A=\u00122\u00001\n\u00001 2\u0013\nA suﬃcient condition for a (real, symmetric) matrix to be positive deﬁnite is that it is diagonally\ndominant , i.e., if in every row of the matrix, the magnitude of the diagonal entry in that row is\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.1. Introduction 233\nlarger than the sum of the magnitudes of all the other (non-diagonal) entries in that row. More\nprecisely,\njaiij>X\nj6=ijaijjfor alli (7.58)\nIn 2d, any real, symmmetric 2\u00022matrix\u0012a b\nb d\u0013\nis positive deﬁnite iﬀ a>0,d>0andad>b2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 618, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0619_ea33fd8a", "text": "More\nprecisely,\njaiij>X\nj6=ijaijjfor alli (7.58)\nIn 2d, any real, symmmetric 2\u00022matrix\u0012a b\nb d\u0013\nis positive deﬁnite iﬀ a>0,d>0andad>b2. Finally, there is one type of positive deﬁnite matrix that comes up frequently, and so deserves\nsome special mention. Given any matrix A2Rm\u0002n(not necessarily symmetric or even square), the\nGram matrix G=ATAis always positive semideﬁnite. Further, if m\u0015n(and we assume for\nconvenience that Ais full rank), then G=ATAis positive deﬁnite. 7.1.5.4 Orthogonal matrices\nTwo vectorsx;y2Rnareorthogonal ifxTy= 0. A vector x2Rnisnormalized ifkxk2= 1. A\nset of vectors that is pairwise orthogonal and normalized is called orthonormal . A square matrix\nU2Rn\u0002nisorthogonal if all its columns are orthonormal. (Note the diﬀerent meaning of the term\northogonal when talking about vectors versus matrices.) If the entries of Uare complex valued, we\nuse the term unitary instead of orthogonal.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 619, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 913}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0620_7ac6d99d", "text": "(Note the diﬀerent meaning of the term\northogonal when talking about vectors versus matrices.) If the entries of Uare complex valued, we\nuse the term unitary instead of orthogonal. It follows immediately from the deﬁnition of orthogonality and normality that Uis orthogonal iﬀ\nUTU=I=UUT: (7.59)\nIn other words, the inverse of an orthogonal matrix is its transpose. Note that if Uis not square —\ni.e.,U2Rm\u0002n; n<m — but its columns are still orthonormal, then UTU=I, but UUT6=I. We\ngenerally only use the term orthogonal to describe the previous case, where Uis square. An example of an orthogonal matrix is a rotation matrix (see Exercise 7.1). For example, a\nrotation in 3d by angle \u000babout thezaxis is given by\nR(\u000b) =0\n@cos(\u000b)\u0000sin(\u000b) 0\nsin(\u000b) cos(\u000b) 0\n0 0 11\nA (7.60)\nIf\u000b= 45\u000e, this becomes\nR(45) =0\n@1p\n2\u00001p\n20\n1p\n21p\n20\n0 0 11\nA (7.61)\nwhere1p\n2= 0:7071. We see that R(\u0000\u000b) =R(\u000b)\u00001=R(\u000b)T, so this is an orthogonal matrix.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 620, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 922}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0621_c2c1c2b5", "text": "We see that R(\u0000\u000b) =R(\u000b)\u00001=R(\u000b)T, so this is an orthogonal matrix. One nice property of orthogonal matrices is that operating on a vector with an orthogonal matrix\nwill not change its Euclidean norm, i.e.,\nkUxk2=kxk2 (7.62)\nfor any nonzero x2Rn, and orthogonal U2Rn\u0002n. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n234 Chapter 7. Linear Algebra\nSimilarly, one can show that the angle between two vectors is preserved after they are transformed\nby an orthogonal matrix. The cosine of the angle between xandyis given by\ncos(\u000b(x;y)) =xTy\njjxjjjjyjj(7.63)\nso\ncos(\u000b(Ux;Uy)) =(Ux)T(Uy)\njjUxjjjjUyjj=xTy\njjxjjjjyjj= cos(\u000b(x;y)) (7.64)\nIn summary, transformations by orthogonal matrices are generalizations of rotations (if det(U) = 1)\nand reﬂections (if det(U) =\u00001), since they preserve lengths and angles. Note that there is technique called Gram Schmidt orthogonalization which is a way to make any\nsquare matrix orthogonal, but we will not cover it here.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 621, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 963}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0622_b55ef29a", "text": "Note that there is technique called Gram Schmidt orthogonalization which is a way to make any\nsquare matrix orthogonal, but we will not cover it here. 7.2 Matrix multiplication\nThe product of two matrices A2Rm\u0002nandB2Rn\u0002pis the matrix\nC=AB2Rm\u0002p; (7.65)\nwhere\nCij=nX\nk=1AikBkj: (7.66)\nNote that in order for the matrix product to exist, the number of columns in Amust equal the\nnumber of rows in B. Matrix multiplication generally takes O(mnp)time, although faster methods exist. In addition,\nspecialized hardware, such as GPUs and TPUs, can be leveraged to speed up matrix multiplication\nsigniﬁcantly, by performing operations across the rows (or columns) in parallel. It is useful to know a few basic properties of matrix multiplication:\n•Matrix multiplication is associative :(AB)C=A(BC). •Matrix multiplication is distributive :A(B+C) =AB+AC. •Matrix multiplication is, in general, notcommutative ; that is, it can be the case that AB6=BA.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 622, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 941}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0623_c755e785", "text": "•Matrix multiplication is distributive :A(B+C) =AB+AC. •Matrix multiplication is, in general, notcommutative ; that is, it can be the case that AB6=BA. (In each of the above cases, we are assuming that the dimensions match.)\nThere are many important special cases of matrix multiplication, as we discuss below. 7.2.1 Vector–vector products\nGiven two vectors x;y2Rn, the quantity xTy, called the inner product ,dot product orscalar\nproduct of the vectors, is a real number given by\nhx;yi,xTy=nX\ni=1xiyi: (7.67)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.2. Matrix multiplication 235\nNote that it is always the case that xTy=yTx. Given vectors x2Rm,y2Rn(they no longer have to be the same size), xyTis called the outer\nproduct of the vectors. It is a matrix whose entries are given by (xyT)ij=xiyj, i.e.,\nxyT2Rm\u0002n=2\n6664x1y1x1y2\u0001\u0001\u0001x1yn\nx2y1x2y2\u0001\u0001\u0001x2yn\n............", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 623, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 897}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0624_397a74dc", "text": "It is a matrix whose entries are given by (xyT)ij=xiyj, i.e.,\nxyT2Rm\u0002n=2\n6664x1y1x1y2\u0001\u0001\u0001x1yn\nx2y1x2y2\u0001\u0001\u0001x2yn\n............ xmy1xmy2\u0001\u0001\u0001xmyn3\n7775: (7.68)\n7.2.2 Matrix–vector products\nGiven a matrix A2Rm\u0002nand a vector x2Rn, their product is a vector y=Ax2Rm. There are\na couple ways of looking at matrix-vector multiplication, and we will look at them both. If we write Aby rows, then we can express y=Axas follows:\ny=Ax=2\n6664—aT\n1—\n—aT\n2—\n... —aT\nm—3\n7775x=2\n6664aT\n1x\naT\n2x\n... aT\nmx3\n7775: (7.69)\nIn other words, the ith entry ofyis equal to the inner product of the ithrowofAandx,yi=aT\nix. Alternatively, let’s write Ain column form. In this case we see that\ny=Ax=2\n4j j j\na1a2\u0001\u0001\u0001an\nj j j3\n52\n6664x1\nx2\n... xn3\n7775=2\n4j\na1\nj3\n5x1+2\n4j\na2\nj3\n5x2+:::+2\n4j\nan\nj3\n5xn:(7.70)\nIn other words, yis alinear combination of thecolumns ofA, where the coeﬃcients of the\nlinear combination are given by the entries of x. We can view the columns of Aas a set of basis\nvectors deﬁning a linear subspace .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 624, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0625_617ebc4e", "text": "We can view the columns of Aas a set of basis\nvectors deﬁning a linear subspace . We can contstruct vectors in this subspace by taking linear\ncombinations of the basis vectors. See Section 7.1.2 for details. 7.2.3 Matrix–matrix products\nBelow we look at four diﬀerent (but, of course, equivalent) ways of viewing the matrix-matrix\nmultiplication C=AB. First we can view matrix-matrix multiplication as a set of vector-vector products. The most\nobvious viewpoint, which follows immediately from the deﬁnition, is that the i;jentry of Cis equal\nto the inner product of the ith row of Aand thejth column of B. Symbolically, this looks like the\nfollowing,\nC=AB=2\n6664—aT\n1—\n—aT\n2—\n... —aT\nm—3\n77752\n4j j j\nb1b2\u0001\u0001\u0001bp\nj j j3\n5=2\n6664aT\n1b1aT\n1b2\u0001\u0001\u0001aT\n1bp\naT\n2b1aT\n2b2\u0001\u0001\u0001aT\n2bp\n............ aT\nmb1aT\nmb2\u0001\u0001\u0001aT\nmbp3\n7775:(7.71)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n236 Chapter 7.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 625, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 893}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0626_fe64d003", "text": "aT\nmb1aT\nmb2\u0001\u0001\u0001aT\nmbp3\n7775:(7.71)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n236 Chapter 7. Linear Algebra\nAB\na1,1\na3,1a3,2a2,1a2,2\na4,1a4,2a1,2b1,2\nb2,2b1,3\nb2,3b1,1\nb2,1\nFigure 7.5: Illustration of matrix multiplication. From https: // en. wikipedia. org/ wiki/ Matrix_\nmultiplication . Used with kind permission of Wikipedia author Bilou. Remember that since A2Rm\u0002nandB2Rn\u0002p,ai2Rnandbj2Rn, so these inner products\nall make sense. This is the most “natural” representation when we represent Aby rows and Bby\ncolumns. See Figure 7.5 for an illustration. Alternatively, we can represent Aby columns, and Bby rows, which leads to the interpretation of\nABas a sum of outer products. Symbolically,\nC=AB=2\n4j j j\na1a2\u0001\u0001\u0001an\nj j j3\n52\n6664—bT\n1—\n—bT\n2—\n... —bT\nn—3\n7775=nX\ni=1aibT\ni: (7.72)\nPut another way, ABis equal to the sum, over all i, of the outer product of the ith column of A\nand theith row of B.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 626, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 919}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0627_c56e2aa1", "text": "—bT\nn—3\n7775=nX\ni=1aibT\ni: (7.72)\nPut another way, ABis equal to the sum, over all i, of the outer product of the ith column of A\nand theith row of B. Since, in this case, ai2Rmandbi2Rp, the dimension of the outer product\naibT\niism\u0002p, which coincides with the dimension of C. We can also view matrix-matrix multiplication as a set of matrix-vector products. Speciﬁcally, if\nwe represent Bby columns, we can view the columns of Cas matrix-vector products between Aand\nthe columns of B. Symbolically,\nC=AB=A2\n4j j j\nb1b2\u0001\u0001\u0001bp\nj j j3\n5=2\n4j j j\nAb1Ab2\u0001\u0001\u0001Abp\nj j j3\n5: (7.73)\nHere theith column of Cis given by the matrix-vector product with the vector on the right, ci=Abi. These matrix-vector products can in turn be interpreted using both viewpoints given in the previous\nsubsection. Finally, we have the analogous viewpoint, where we represent Aby rows, and view the rows of C\nas the matrix-vector product between the rows of Aand the matrix B. Symbolically,\nC=AB=2\n6664—aT\n1—\n—aT\n2—\n...", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 627, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 987}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0628_e0134b75", "text": "Symbolically,\nC=AB=2\n6664—aT\n1—\n—aT\n2—\n... —aT\nm—3\n7775B=2\n6664—aT\n1B—\n—aT\n2B—\n... —aT\nmB—3\n7775: (7.74)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.2. Matrix multiplication 237\nHere theith row of Cis given by the matrix-vector product with the vector on the left, cT\ni=aT\niB. It may seem like overkill to dissect matrix multiplication to such a large degree, especially when all\nthese viewpoints follow immediately from the initial deﬁnition we gave (in about a line of math) at\nthe beginning of this section. However, virtually all of linear algebra deals with matrix multiplications\nof some kind, and it is worthwhile to spend some time trying to develop an intuitive understanding\nof the viewpoints presented here. Finally, a word on notation. We write A2as shorthand for AA, which is the matrix product. To\ndenote elementwise squaring of the elements of a matrix, we write A\f2= [A2\nij].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 628, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 925}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0629_7bc9cf08", "text": "Finally, a word on notation. We write A2as shorthand for AA, which is the matrix product. To\ndenote elementwise squaring of the elements of a matrix, we write A\f2= [A2\nij]. (IfAis diagonal,\nthenA2=A\f2.)\nWe can also deﬁne the inverse of A2using thematrix square root : we say A=p\nMifA2=M. To denote elementwise square root of the elements of a matrix, we write [p\nMij]. 7.2.4 Application: manipulating data matrices\nAs an application of the above results, consider the case where Xis theN\u0002Ddesign matrix, whose\nrows are the data cases. There are various common preprocessing operations that we apply to this\nmatrix, which we summarize below. (Writing these operations in matrix form is useful because it is\nnotationally compact, and it allows us to implement the methods quickly using fast matrix code.)\n7.2.4.1 Summing slices of the matrix\nSuppose Xis anN\u0002Dmatrix.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 629, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 864}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0630_ff5c24c9", "text": "We can sum across the rows by premultiplying by a 1\u0002Nmatrix\nof ones to create a 1\u0002Dmatrix:\n1T\nNX=\u0000P\nnxn1\u0001\u0001\u0001P\nnxnD\u0001\n(7.75)\nHence the mean of the data vectors is given by\nxT=1\nN1T\nNX (7.76)\nWe can sum across the columns by postmultiplying by a D\u00021matrix of ones to create a N\u00021\nmatrix:\nX1D=0\nB@P\ndx1d\n...P\ndxNd1\nCA (7.77)\nWe can sum all entries in a matrix by pre and post multiplying by a vector of 1s:\n1T\nNX1D=X\nijXij (7.78)\nHence the overall mean is given by\nx=1\nND1T\nNX1D (7.79)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n238 Chapter 7. Linear Algebra\n7.2.4.2 Scaling rows and columns of a matrix\nWe often want to scale rows or columns of a data matrix (e.g., to standardize them). We now show\nhow to write this in matrix notation. If we pre-multiply Xby a diagonal matrix S=diag(s), wheresis anN-vector, then we just scale\neach row of Xby the corresponding scale factor in s:\ndiag(s)X=0\nB@s1\u0001\u0001\u0001 0\n... 0\u0001\u0001\u0001sN1\nCA0\nB@x1;1\u0001\u0001\u0001x1;D\n... xN;1\u0001\u0001\u0001xN;D1\nCA=0\nB@s1x1;1\u0001\u0001\u0001s1x1;D\n...", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 630, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 990}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0631_ab253e5f", "text": "0\u0001\u0001\u0001sN1\nCA0\nB@x1;1\u0001\u0001\u0001x1;D\n... xN;1\u0001\u0001\u0001xN;D1\nCA=0\nB@s1x1;1\u0001\u0001\u0001s1x1;D\n... sNxN:1\u0001\u0001\u0001sNxN;D1\nCA (7.80)\nIf we post-multiply Xby a diagonal matrix S=diag(s), wheresis aD-vector, then we just scale\neach column of Xby the corresponding element in s. Xdiag(s) =0\nB@x1;1\u0001\u0001\u0001x1;D\n... xN;1\u0001\u0001\u0001xN;D1\nCA0\nB@s1\u0001\u0001\u0001 0\n... 0\u0001\u0001\u0001sD1\nCA=0\nB@s1x1;1\u0001\u0001\u0001sDx1;D\n... s1xN;1\u0001\u0001\u0001sDxN;D1\nCA (7.81)\nThus we can rewrite the standardization operation from Section 10.2.8 in matrix form as follows:\nstandardize( X) = (X\u00001N\u0016T)diag(\u001b)\u00001(7.82)\nwhere\u0016=xis the empirical mean, and \u001bis a vector of the empirical standard deviations. 7.2.4.3 Sum of squares and scatter matrix\nThesum of squares matrix isD\u0002Dmatrix deﬁned by\nS0,XTX=NX\nn=1xnxT\nn=NX\nn=10\nB@x2\nn;1\u0001\u0001\u0001xn;1xn;D\n... xn;Dxn;1\u0001\u0001\u0001x2\nn;D1\nCA (7.83)\nThescatter matrix is aD\u0002Dmatrix deﬁned by\nSx,NX\nn=1(xn\u0000x)(xn\u0000x)T= X\nnxnxT\nn! \u0000NxxT(7.84)\nWe see that this is the sum of squares matrix applied to the mean-centered data.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 631, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 927}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0632_aa0897d5", "text": "xn;Dxn;1\u0001\u0001\u0001x2\nn;D1\nCA (7.83)\nThescatter matrix is aD\u0002Dmatrix deﬁned by\nSx,NX\nn=1(xn\u0000x)(xn\u0000x)T= X\nnxnxT\nn! \u0000NxxT(7.84)\nWe see that this is the sum of squares matrix applied to the mean-centered data. More precisely,\ndeﬁne ~Xto be a version of Xwhere we subtract the mean x=1\nNXT1Noﬀ every row. Hence we can\ncompute the centered data matrix using\n~X=X\u00001NxT=X\u00001\nN1N1T\nNX=CNX (7.85)\nwhere\nCN,IN\u00001\nN1N1T\nN (7.86)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.2. Matrix multiplication 239\nis thecentering matrix . The scatter matrix can now be computed as follows:\nSx=~XT~X=XTCT\nNCNX=XTCNX (7.87)\nwhere we exploited the fact that CNis symmetric and idempotent, i.e., Ck\nN=CNfork= 1;2;:::\n(since once we subtract the mean, subtracting it again has no eﬀect). 7.2.4.4 Gram matrix\nTheN\u0002Nmatrix XXTis a matrix of inner products called the Gram matrix :\nK,XXT=0\nB@xT\n1x1\u0001\u0001\u0001xT\n1xN\n...", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 632, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 904}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0633_4c565f17", "text": "7.2.4.4 Gram matrix\nTheN\u0002Nmatrix XXTis a matrix of inner products called the Gram matrix :\nK,XXT=0\nB@xT\n1x1\u0001\u0001\u0001xT\n1xN\n... xT\nnx1\u0001\u0001\u0001xT\nNxN1\nCA (7.88)\nSometimes we want to compute the inner products of the mean-centered data vectors, ~K=~X~XT. However, if we are working with a feature similarity matrix instead of raw features, we will only have\naccess to K, notX. (We will see examples of this in Section 20.4.4 and Section 20.4.6.) Fortunately,\nwe can compute ~KfromKusing the double centering trick :\n~K=~X~XT=CNKCN=K\u00001N\nNK\u0000K1N\nN+1N\nNK1N\nN(7.89)\nThis subtracts the row means and column means from K, and adds back the global mean that gets\nsubtracted twice, so that both row means and column means of ~Kare equal to zero. To see why Equation (7.89) is true, consider the scalar form:\n~Kij=~xT\ni~xj= (xi\u00001\nNNX\nk=1xk)(xj\u00001\nNNX\nl=1xl) (7.90)\n=xT\nixj\u00001\nNNX\nk=1xT\nixk\u00001\nNNX\nk=1xT\njxk+1\nN2NX\nk=1NX\nl=1xT\nkxl (7.91)\n7.2.4.5 Distance matrix\nLetXbeNx\u0002Ddatamatrix, and Ybe another Ny\u0002Ddatamatrix.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 633, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 987}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0634_fd9d566f", "text": "We can compute the squared\npairwise distances between these using\nDij= (xi\u0000yj)T(xi\u0000yj) =jjxijj2\u00002xT\niyj+jjyjjj2(7.92)\nLet us now write this in matrix form. Let ^x= [jjx1jj2;\u0001\u0001\u0001;jjxNxjj2] =diag(XXT)be a vector where\neach element is the squared norm of the examples in X, and deﬁne ^ysimilarly. Then we have\nD=^x1T\nNy\u00002XYT+1Nx^yT(7.93)\nIn the case that X=Y, we have\nD=^x1T\nN\u00002XXT+1N^xT(7.94)\nThis vectorized computation is often much faster than using for loops. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n240 Chapter 7. Linear Algebra\n7.2.5 Kronecker products *\nIfAis anm\u0002nmatrix and Bis ap\u0002qmatrix, then the Kronecker product A\nBis themp\u0002nq\nblock matrix\nA\nB=2\n64a11B\u0001\u0001\u0001a1nB\n.........", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 634, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 700}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0635_62583ee1", "text": "CC-BY-NC-ND license\n240 Chapter 7. Linear Algebra\n7.2.5 Kronecker products *\nIfAis anm\u0002nmatrix and Bis ap\u0002qmatrix, then the Kronecker product A\nBis themp\u0002nq\nblock matrix\nA\nB=2\n64a11B\u0001\u0001\u0001a1nB\n......... am1B\u0001\u0001\u0001amnB3\n75 (7.95)\nFor example,\n2\n4a11a12\na21a22\na31a323\n5\n\u0014b11b12b13\nb21b22b23\u0015\n=2\n6666664a11b11a11b12a11b13a12b11a12b12a12b13\na11b21a11b22a11b23a12b21a12b22a12b23\na21b11a21b12a21b13a22b11a22b12a22b13\na21b21a21b22a21b23a22b21a22b22a22b23\na31b11a31b12a31b13a32b11a32b12a32b13\na31b21a31b22a31b23a32b21a32b22a32b233\n7777775(7.96)\nHere are some useful identities:\n(A\nB)\u00001=A\u00001\nB\u00001(7.97)\n(A\nB)vec(C) = vec( BCAT) (7.98)\nSee [Loa00] for a list of other useful properties. 7.2.6 Einstein summation *\nEinstein summation , oreinsum for short, is a notational shortcut for working with tensors.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 635, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 788}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0636_1df2f8a6", "text": "7.2.6 Einstein summation *\nEinstein summation , oreinsum for short, is a notational shortcut for working with tensors. The\nconvention was introduced by Einstein [Ein16, sec 5], who later joked to a friend, “I have made a great\ndiscovery in mathematics; I have suppressed the summation sign every time that the summation\nmust be made over an index which occurs twice...” [Pai05, p.216]. For example, instead of writing\nmatrix multiplication as Cij=P\nkAikBkj, we can just write it as Cij=AikBkj, where we drop theP\nk. As a more complex example, suppose we have a 3d tensor Sntkwherenindexes examples in the\nbatch,tindexes locations in the sequence, and kindexes words in a one-hot representation. Let\nWkdbe an embedding matrix that maps sparse one-hot vectors Rkto dense vectors in Rd.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 636, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 783}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0637_f6ee03de", "text": "Let\nWkdbe an embedding matrix that maps sparse one-hot vectors Rkto dense vectors in Rd. We can\nconvert the batch of sequences of one-hots to a batch of sequences of embeddings as follows:\nEntd=X\nkSntkWkd (7.99)\nWe can compute the sum of the embedding vectors for each sequence (to get a global representation\nof each bag of words) as follows:\nEnd=X\nkX\ntSntkWkd (7.100)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.3. Matrix inversion 241\nFinally we can pass each sequence’s vector representation through another linear transform Vdcto\nmap to the logits over a classiﬁer with clabels:\nLnc=X\ndEndVdc=X\ndX\nkX\ntSntkWkdVdc (7.101)\nIn einsum notation, we write Lnc=SntkWkdVdc. We sum over kanddbecause those indices occur\ntwice on the RHS. We sum over tbecause that index does not occur on the LHS. Einsum is implemented in NumPy, Tensorﬂow, PyTorch, etc.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 637, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 883}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0638_4812b12d", "text": "We sum over kanddbecause those indices occur\ntwice on the RHS. We sum over tbecause that index does not occur on the LHS. Einsum is implemented in NumPy, Tensorﬂow, PyTorch, etc. What makes it particularly useful is\nthat it can perform the relevant tensor multiplications in complex expressions in an optimal order,\nso as to minimize time and intermediate memory allocation.2The library is best illustrated by the\nexamples in code.probml.ai/book1/einsum_demo. Note that the speed of einsum depends on the order in which the operations are performed, which\ndepends on the shapes of the relevant arguments. The optimal ordering minimizes the treewidth\nof the resulting computation graph, as explained in [GASG18]. In general, the time to compute\nthe optimal ordering is exponential in the number of arguments, so it is common to use a greedy\napproximation.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 638, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 854}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0639_c2bb44da", "text": "In general, the time to compute\nthe optimal ordering is exponential in the number of arguments, so it is common to use a greedy\napproximation. However, if we expect to repeat the same calculation many times, using tensors of\nthe same shape but potentially diﬀerent content, we can compute the optimal ordering once and\nreuse it multiple times. 7.3 Matrix inversion\nIn this section, we discuss how to invert diﬀerent kinds of matrices. 7.3.1 The inverse of a square matrix\nTheinverse of a square matrix A2Rn\u0002nis denoted A\u00001, and is the unique matrix such that\nA\u00001A=I=AA\u00001: (7.102)\nNote that A\u00001exists if and only if det(A)6= 0. Ifdet(A) = 0, it is called a singular matrix. The following are properties of the inverse; all assume that A;B2Rn\u0002nare non-singular:\n(A\u00001)\u00001=A (7.103)\n(AB)\u00001=B\u00001A\u00001(7.104)\n(A\u00001)T= (AT)\u00001,A\u0000T(7.105)\nFor the case of a 2\u00022matrix, the expression for A\u00001is simple enough to give explicitly.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 639, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 912}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0640_afa4f87d", "text": "We have\nA=\u0012a b\nc d\u0013\n;A\u00001=1\njAj\u0012d\u0000b\n\u0000c a\u0013\n(7.106)\nFor a block diagonal matrix, the inverse is obtained by simply inverting each block separately, e.g.,\n\u0012A 0\n0 B\u0013\u00001\n=\u0012A\u000010\n0 B\u00001\u0013\n(7.107)\n2. These optimizations are implemented in the opt-einsum library [GASG18]. Its core functionality is included in\nNumPy and JAX einsumfunctions, provided you set optimize=True parameter. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n242 Chapter 7. Linear Algebra\n7.3.2 Schur complements *\nIn this section, we review some useful results concerning block structured matrices. Theorem 7.3.1 (Inverse of a partitioned matrix) .Consider a general partitioned matrix\nM=\u0012E F\nG H\u0013\n(7.108)\nwhere we assume EandHare invertible. We have\nM\u00001=\u0012(M=H)\u00001\u0000(M=H)\u00001FH\u00001\n\u0000H\u00001G(M=H)\u00001H\u00001+H\u00001G(M=H)\u00001FH\u00001\u0013\n(7.109)\n=\u0012\nE\u00001+E\u00001F(M=E)\u00001GE\u00001\u0000E\u00001F(M=E)\u00001\n\u0000(M=E)\u00001GE\u00001(M=E)\u00001\u0013\n(7.110)\nwhere\nM=H,E\u0000FH\u00001G (7.111)\nM=E,H\u0000GE\u00001F (7.112)\nWe say that M=His theSchur complement ofMwrtH, and M=Eis the Schur complement of\nMwrtE.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 640, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 987}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0641_46c55db7", "text": "(The reason for this notation will be explained in Equation (7.133).)\nEquation (7.109)and Equation (7.110)are called the partitioned inverse formulae . Proof.If we could block diagonalize M, it would be easier to invert. To zero out the top right block\nofMwe can pre-multiply as follows\n\u0012I\u0000FH\u00001\n0 I\u0013\u0012E F\nG H\u0013\n=\u0012E\u0000FH\u00001G 0\nG H\u0013\n(7.113)\nSimilarly, to zero out the bottom left we can post-multiply as follows\n\u0012E\u0000FH\u00001G 0\nG H\u0013\u0012I 0\n\u0000H\u00001G I\u0013\n=\u0012E\u0000FH\u00001G 0\n0 H\u0013\n(7.114)\nPutting it all together we get\n\u0012I\u0000FH\u00001\n0 I\u0013\n|{z}\nX\u0012E F\nG H\u0013\n|{z}\nM\u0012I 0\n\u0000H\u00001G I\u0013\n|{z}\nZ=\u0012E\u0000FH\u00001G 0\n0 H\u0013\n|{z}\nW(7.115)\nTaking the inverse of both sides yields\nZ\u00001M\u00001X\u00001=W\u00001(7.116)\nM\u00001=ZW\u00001X (7.117)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 641, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 735}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0642_19e5f353", "text": "August 27, 2021\n7.3. Matrix inversion 243\nSubstituting in the deﬁnitions we get\n\u0012E F\nG H\u0013\u00001\n=\u0012I 0\n\u0000H\u00001G I\u0013\u0012(M=H)\u000010\n0 H\u00001\u0013\u0012I\u0000FH\u00001\n0 I\u0013\n(7.118)\n=\u0012\n(M=H)\u000010\n\u0000H\u00001G(M=H)\u00001H\u00001\u0013\u0012\nI\u0000FH\u00001\n0 I\u0013\n(7.119)\n=\u0012(M=H)\u00001\u0000(M=H)\u00001FH\u00001\n\u0000H\u00001G(M=H)\u00001H\u00001+H\u00001G(M=H)\u00001FH\u00001\u0013\n(7.120)\nAlternatively, we could have decomposed the matrix Min terms of EandM=E= (H\u0000GE\u00001F),\nyielding\n\u0012E F\nG H\u0013\u00001\n=\u0012E\u00001+E\u00001F(M=E)\u00001GE\u00001\u0000E\u00001F(M=E)\u00001\n\u0000(M=E)\u00001GE\u00001(M=E)\u00001\u0013\n(7.121)\n7.3.3 The matrix inversion lemma *\nEquating the top left block of the ﬁrst matrix in Equation (7.119) with the top left block of the\nmatrix in Equation (7.121)\n(M=H)\u00001= (E\u0000FH\u00001G)\u00001=E\u00001+E\u00001F(H\u0000GE\u00001F)\u00001GE\u00001(7.122)\nThis is known as the matrix inversion lemma or theSherman-Morrison-Woodbury formula . A typical application in machine learning is the following. Let Xbe anN\u0002Ddata matrix, and\n\u0006beN\u0002Ndiagonal matrix.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 642, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 834}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0643_9fcea42b", "text": "A typical application in machine learning is the following. Let Xbe anN\u0002Ddata matrix, and\n\u0006beN\u0002Ndiagonal matrix. Then we have (using the substitutions E=\u0006,F=GT=X, and\nH\u00001=\u0000I) the following result:\n(\u0006+XXT)\u00001=\u0006\u00001\u0000\u0006\u00001X(I+XT\u0006\u00001X)\u00001XT\u0006\u00001(7.123)\nThe LHS takes O(N3)time to compute, the RHS takes time O(D3)to compute. Another application concerns computing a rank one update of an inverse matrix. Let E=A,\nF=u,G=vT, andH=\u00001. Then we have\n(A+uvT)\u00001=A\u00001+A\u00001u(\u00001\u0000vTA\u00001u)\u00001vTA\u00001(7.124)\n=A\u00001\u0000A\u00001uvTA\u00001\n1 +vTA\u00001u(7.125)\nThis is known as the Sherman-Morrison formula . 7.3.4 Matrix determinant lemma *\nWe now use the above results to derive an eﬃcient way to compute the determinant of a block-\nstructured matrix. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n244 Chapter 7. Linear Algebra\nFrom Equation (7.115), we have\njXjjMjjZj=jWj=jE\u0000FH\u00001GjjHj (7.126)\nj\u0012\nE F\nG H\u0013\nj=jE\u0000FH\u00001GjjHj (7.127)\njMj=jM=HjjHj (7.128)\njM=Hj=jMj\njHj(7.129)\nSo we can see that M=Hacts somewhat like a division operator.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 643, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 995}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0644_204fb54d", "text": "Furthermore, we have\njMj=jM=HjjHj=jM=EjjEj (7.130)\njM=Hj=jM=EjjEj\njHj(7.131)\njE\u0000FH\u00001Gj=jH\u0000GE\u00001FjjH\u00001jjEj (7.132)\nHence (setting E=A,F=\u0000u,G=vT,H= 1) we have\njA+uvTj= (1 +vTA\u00001u)jAj (7.133)\nThis is known as the matrix determinant lemma . 7.3.5 Application: deriving the conditionals of an MVN *\nConsider a joint Gaussian of the form p(x1;x2) =N(xj\u0016;\u0006), where\n\u0016=\u0012\n\u00161\n\u00162\u0013\n;\u0006=\u0012\n\u000611\u000612\n\u000621\u000622\u0013\n(7.134)\nIn Section 3.2.3, we claimed that\np(x1jx2) =N(x1j\u00161+\u000612\u0006\u00001\n22(x2\u0000\u00162);\u000611\u0000\u000612\u0006\u00001\n22\u000621) (7.135)\nIn this section, we derive this result using Schur complenents. Let us factor the joint p(x1;x2)asp(x2)p(x1jx2)as follows:\np(x1;x2)/exp(\n\u00001\n2\u0012\nx1\u0000\u00161\nx2\u0000\u00162\u0013T\u0012\u000611\u000612\n\u000621\u000622\u0013\u00001\u0012x1\u0000\u00161\nx2\u0000\u00162\u0013)\n(7.136)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 644, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 767}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0645_e02028a3", "text": "August 27, 2021\n7.4. Eigenvalue decomposition (EVD) 245\nUsing Equation (7.118) the above exponent becomes\np(x1;x2)/exp(\n\u00001\n2\u0012x1\u0000\u00161\nx2\u0000\u00162\u0013T\u0012I 0\n\u0000\u0006\u00001\n22\u000621I\u0013\u0012(\u0006=\u000622)\u000010\n0 \u0006\u00001\n22\u0013\n(7.137)\n\u0002\u0012\nI\u0000\u000612\u0006\u00001\n22\n0 I\u0013\u0012x1\u0000\u00161\nx2\u0000\u00162\u0013\u001b\n(7.138)\n= exp\u001a\n\u00001\n2(x1\u0000\u00161\u0000\u000612\u0006\u00001\n22(x2\u0000\u00162))T(\u0006=\u000622)\u00001(7.139)\n(x1\u0000\u00161\u0000\u000612\u0006\u00001\n22(x2\u0000\u00162)) \n\u0002exp\u001a\n\u00001\n2(x2\u0000\u00162)T\u0006\u00001\n22(x2\u0000\u00162)\u001b\n(7.140)\nThis is of the form\nexp(quadratic form in x1;x2)\u0002exp(quadratic form in x2) (7.141)\nHence we have successfully factorized the joint as\np(x1;x2) =p(x1jx2)p(x2) (7.142)\n=N(x1j\u00161j2;\u00061j2)N(x2j\u00162;\u000622) (7.143)\nwhere the parameters of the conditional distribution can be read oﬀ from the above equations using\n\u00161j2=\u00161+\u000612\u0006\u00001\n22(x2\u0000\u00162) (7.144)\n\u00061j2=\u0006=\u000622=\u000611\u0000\u000612\u0006\u00001\n22\u000621 (7.145)\nWe can also use the fact that jMj=jM=HjjHjto check the normalization constants are correct:\n(2\u0019)(d1+d2)=2j\u0006j1\n2= (2\u0019)(d1+d2)=2(j\u0006=\u000622jj\u000622j)1\n2 (7.146)\n= (2\u0019)d1=2j\u0006=\u000622j1\n2(2\u0019)d2=2j\u000622j1\n2 (7.147)\nwhered1= dim(x1)andd2= dim(x2).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 645, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 946}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0646_fee9c502", "text": "7.4 Eigenvalue decomposition (EVD)\nIn this section, we review some standard material on the eigenvalue decomposition orEVDof\nsquare (real-valued) matrices. 7.4.1 Basics\nGiven a square matrix A2Rn\u0002n, we say that \u00152Ris aneigenvalue ofAandu2Rnis the\ncorresponding eigenvector if\nAu=\u0015u;u6= 0: (7.148)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n246 Chapter 7. Linear Algebra\nIntuitively, this deﬁnition means that multiplying Aby the vector uresults in a new vector that\npoints in the same direction as u, but is scaled by a factor \u0015. For example, if Ais a rotation matrix,\nthenuis the axis of rotation and \u0015= 1. Note that for any eigenvector u2Rn, and scalar c2R,\nA(cu) =cAu=c\u0015u=\u0015(cu) (7.149)\nHencecuis also an eigenvector. For this reason when we talk about “the” eigenvector associated\nwith\u0015, we usually assume that the eigenvector is normalized to have length 1 (this still creates some\nambiguity, since uand\u0000uwill both be eigenvectors, but we will have to live with this).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 646, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 989}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0647_552f9f0f", "text": "We can rewrite the equation above to state that (\u0015;x)is an eigenvalue-eigenvector pair of Aif\n(\u0015I\u0000A)u=0;u6= 0: (7.150)\nNow (\u0015I\u0000A)u=0has a non-zero solution to uif and only if (\u0015I\u0000A)has a non-empty nullspace,\nwhich is only the case if (\u0015I\u0000A)is singular, i.e.,\ndet(\u0015I\u0000A) = 0: (7.151)\nThis is called the characteristic equation ofA. (See Exercise 7.2.) The nsolutions of this equation\nare then(possibly complex-valued) eigenvalues \u0015i, anduiare the corresponding eigenvectors. It is\nstandard to sort the eigenvectors in order of their eigenvalues, with the largest magnitude ones ﬁrst. The following are properties of eigenvalues and eigenvectors. •The trace of a matrix is equal to the sum of its eigenvalues,\ntr(A) =nX\ni=1\u0015i: (7.152)\n•The determinant of Ais equal to the product of its eigenvalues,\ndet(A) =nY\ni=1\u0015i: (7.153)\n•The rank of Ais equal to the number of non-zero eigenvalues of A. •IfAis non-singular then 1=\u0015iis an eigenvalue of A\u00001with associated eigenvector ui, i.e.,\nA\u00001ui= (1=\u0015i)ui.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 647, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 996}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0648_7fffdae4", "text": "•IfAis non-singular then 1=\u0015iis an eigenvalue of A\u00001with associated eigenvector ui, i.e.,\nA\u00001ui= (1=\u0015i)ui. •The eigenvalues of a diagonal or triangular matrix are just the diagonal entries. 7.4.2 Diagonalization\nWe can write all the eigenvector equations simultaneously as\nAU=U\u0003 (7.154)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.4. Eigenvalue decomposition (EVD) 247\nwhere the columns of U2Rn\u0002nare the eigenvectors of Aand\u0003is a diagonal matrix whose entries\nare the eigenvalues of A, i.e.,\nU2Rn\u0002n=2\n4j j j\nu1u2\u0001\u0001\u0001un\nj j j3\n5;\u0003= diag(\u00151;:::;\u0015n): (7.155)\nIf the eigenvectors of Aare linearly independent, then the matrix Uwill be invertible, so\nA=U\u0003U\u00001: (7.156)\nA matrix that can be written in this form is called diagonalizable . 7.4.3 Eigenvalues and eigenvectors of symmetric matrices\nWhen Ais real and symmetric, it can be shown that all the eigenvalues are real, and the eigenvectors\nareorthonormal , i.e.,uT\niuj= 0ifi6=j, anduT\niui= 1, whereuiare the eigenvectors.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 648, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1004}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0649_ade1c693", "text": "In matrix\nform, this becomes UTU=UUT=I; hence we see that Uis an orthogonal matrix. We can therefore represent Aas\nA=U\u0003UT=0\n@j j j\nu1u2\u0001\u0001\u0001un\nj j j1\nA0\nBBB@\u00151\n\u00152\n... \u0015n1\nCCCA0\nBBB@\u0000uT\n1\u0000\n\u0000uT\n2\u0000\n... \u0000uT\nn\u00001\nCCCA(7.157)\n=\u001510\n@j\nu1\nj1\nA\u0000\n\u0000uT\n1\u0000\u0001\n+\u0001\u0001\u0001+\u0015n0\n@j\nun\nj1\nA\u0000\n\u0000uT\nn\u0000\u0001\n=nX\ni=1\u0015iuiuT\ni (7.158)\nThus multiplying by any symmetric matrix Acan be interpreted as multiplying by a rotation matrix\nUT, a scaling matrix \u0003, followed by an inverse rotation U. Once we have diagonalized a matrix, it is easy to invert. Since A=U\u0003UT, where UT=U\u00001, we\nhave\nA\u00001=U\u0003\u00001UT=dX\ni=11\n\u0015iuiuT\ni (7.159)\nThis corresponds to rotating, unscaling, and then rotating back. 7.4.3.1 Checking for positive deﬁniteness\nWe can also use the diagonalization property to show that a symmetric matrix is positive deﬁnite iﬀ\nall its eigenvalues are positive. To see this, note that\nxTAx=xTU\u0003UTx=yT\u0003y=nX\ni=1\u0015iy2\ni (7.160)\nwherey=UTx. Becausey2\niis always nonnegative, the sign of this expression depends entirely on\nthe\u0015i’s.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 649, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0650_5b968cb0", "text": "To see this, note that\nxTAx=xTU\u0003UTx=yT\u0003y=nX\ni=1\u0015iy2\ni (7.160)\nwherey=UTx. Becausey2\niis always nonnegative, the sign of this expression depends entirely on\nthe\u0015i’s. If all\u0015i>0, then the matrix is positive deﬁnite; if all \u0015i\u00150, it is positive semideﬁnite. Likewise, if all \u0015i<0or\u0015i\u00140, then Ais negative deﬁnite or negative semideﬁnite respectively. Finally, if Ahas both positive and negative eigenvalues, it is indeﬁnite. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n248 Chapter 7. Linear Algebra\nµu1u2\nλ11/2\nλ21/2\nx1x2\nFigure 7.6: Visualization of a level set of the quadratic form (x\u0000\u0016)TA(x\u0000\u0016)in 2d. The major and minor\naxes of the ellipse are deﬁned by the ﬁrst two eigenvectors of A, namelyu1andu2. Adapted from Figure 2.7\nof [Bis06]. Generated by code at ﬁgures.probml.ai/book1/7.6. 7.4.4 Geometry of quadratic forms\nAquadratic form is a function that can be written as\nf(x) =xTAx (7.161)\nwherex2RnandAis a positive deﬁnite, symmetric n-by-nmatrix.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 650, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 968}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0651_4bf958fd", "text": "7.4.4 Geometry of quadratic forms\nAquadratic form is a function that can be written as\nf(x) =xTAx (7.161)\nwherex2RnandAis a positive deﬁnite, symmetric n-by-nmatrix. Let A=U\u0003UTbe a\ndiagonalization of A(see Section 7.4.3). Hence we can write\nf(x) =xTAx=xTU\u0003UTx=yT\u0003y=nX\ni=1\u0015iy2\ni (7.162)\nwhereyi=xTuiand\u0015i>0(since Ais positive deﬁnite). The level sets of f(x)deﬁne hyper-ellipsoids. For example, in 2d, we have\n\u00151y2\n1+\u00152y2\n2=r (7.163)\nwhich is the equation of a 2d ellipse. This is illustrated in Figure 7.6. The eigenvectors determine\nthe orientation of the ellipse, and the eigenvalues determine how elongated it is. 7.4.5 Standardizing and whitening data\nSuppose we have a dataset X2RN\u0002D. It is common to preprocess the data so that each column has\nzero mean and unit variance. This is called standardizing the data, as we discussed in Section 10.2.8. Although standardizing forces the variance to be 1, it does not remove correlation between the\ncolumns. To do that, we must whitenthe data.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 651, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0652_02936bab", "text": "Although standardizing forces the variance to be 1, it does not remove correlation between the\ncolumns. To do that, we must whitenthe data. To deﬁne this, let the empirical covariance matrix\nbe\u0006=1\nNXTX, and let \u0006=EDETbe its diagonalization. Equivalently, let [U;S;V]be the SVD of\nX(soE=VandD=S2, as we discuss in Section 20.1.3.3.) Now deﬁne\nWpca=D\u00001\n2ET(7.164)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.4. Eigenvalue decomposition (EVD) 249\n66 68 70 72 74 76 78120140160180200220240260280\n12\n34Raw\n(a)\n−0.10 −0.05 0.00 0.05 0.10−1.0−0.50.00.51.01.5\n12\n34Standardized (b)\n−3−2−1 0123−10123\n12\n34PCA-whitened\n(c)\n−3−2−1 01234−10123\n12\n34ZCA-whitened (d)\nFigure 7.7: (a) Height/weight data. (b) Standardized. (c) PCA Whitening. (d) ZCA whitening. Numbers refer\nto the ﬁrst 4 datapoints, but there are 73 datapoints in total. Generated by code at ﬁgures.probml.ai/book1/7.7. This is called the PCA whitening matrix.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 652, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 948}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0653_23ad9a56", "text": "(d) ZCA whitening. Numbers refer\nto the ﬁrst 4 datapoints, but there are 73 datapoints in total. Generated by code at ﬁgures.probml.ai/book1/7.7. This is called the PCA whitening matrix. (We discuss PCA in Section 20.1.) Let y=Wpcaxbe a\ntransformed vector. We can check that its covariance is white as follows:\nCov [y] =WE\u0002\nxxT\u0003\nWT=W\u0006WT= (D\u00001\n2ET)(EDET)(ED\u00001\n2) =I (7.165)\nThe whitening matrix is not unique, since any rotation of it, W=RWpca, will still maintain the\nwhitening property, i.e., WTW=\u0006\u00001. For example, if we take R=E, we get\nWzca=ED\u00001\n2ET=\u0006\u00001\n2=VS\u00001VT(7.166)\nThis is called Mahalanobis whitening orZCA. (ZCA stands for “zero-phase component analysis”,\nand was introduced in [BS97].) The advantage of ZCA whitening over PCA whitening is that the\nresulting transformed data is as close as possible to the original data (in the least squares sense)\n[Amo17]. This is illustrated in Figure 7.7. When applied to images, the ZCA transformed data\nAuthor: Kevin P. Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 653, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0654_b5eea18c", "text": "This is illustrated in Figure 7.7. When applied to images, the ZCA transformed data\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n250 Chapter 7. Linear Algebra\nvectors still look like images. This is useful when the method is used inside a deep learning system\n[KH09]. 7.4.6 Power method\nWe now describe a simple iterative method for computing the eigenvector corresponding to the largest\neigenvalue of a real, symmetric matrix; this is called the power method . This can be useful when\nthe matrix is very large but sparse. For example, it used by Google’s PageRank to compute the\nstationary distribution of the transition matrix of the world wide web (a matrix of size about 3\nbillion by 3 billion!). In Section 7.4.7, we will see how to use this method to compute subsequent\neigenvectors and values. LetAbe a matrix with orthonormal eigenvectors uiand eigenvaluesj\u00151j>j\u00152j\u0015\u0001\u0001\u0001\u0015j\u0015mj\u00150,\nsoA=U\u0003UT. Letv(0)be an arbitrary vector in the range of A, soAx=v(0)for somex.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 654, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 979}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0655_bf6ffed0", "text": "LetAbe a matrix with orthonormal eigenvectors uiand eigenvaluesj\u00151j>j\u00152j\u0015\u0001\u0001\u0001\u0015j\u0015mj\u00150,\nsoA=U\u0003UT. Letv(0)be an arbitrary vector in the range of A, soAx=v(0)for somex. Hence\nwe can write v(0)as\nv0=U(\u0003UTx) =a1u1+\u0001\u0001\u0001+amum (7.167)\nfor some constants ai. We can now repeatedly multiply vbyAand renormalize:\nvt/Avt\u00001 (7.168)\n(We normalize at each iteration for numerical stability.)\nSincevtis a multiple of Atv0, we have\nvt/a1\u0015t\n1u1+a2\u0015t\n2u2+\u0001\u0001\u0001+am\u0015t\nmum (7.169)\n=\u0015t\n1\u0000\na1u1+a1(\u00152=\u00151)tu2+\u0001\u0001\u0001+am(\u0015m=\u00151)tum\u0001\n(7.170)\n!\u0015t\n1a1u1 (7.171)\nsincej\u0015kj\nj\u00151j<1fork>1(assuming the eigenvalues are sorted in descending order). So we see that\nthis converges to u1, although not very quickly (the error is reduced by approximately j\u00152=\u00151jat\neach iteration). The only requirement is that the initial guess satisfy vT\n0u16= 0, which will be true\nfor a random v0with high probability. We now discuss how to compute the corresponding eigenvalue, \u00151.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 655, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 920}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0656_0a1c6b86", "text": "The only requirement is that the initial guess satisfy vT\n0u16= 0, which will be true\nfor a random v0with high probability. We now discuss how to compute the corresponding eigenvalue, \u00151. Deﬁne the Rayleigh quotient\nto be\nR(A;x),xTAx\nxTx(7.172)\nHence\nR(A;ui) =uT\niAui\nuT\niui=\u0015iuT\niui\nuT\niui=\u0015i (7.173)\nThus we can easily compute \u00151fromu1andA. See code.probml.ai/book1/power_method_demo\nfor a demo. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.5. Singular value decomposition (SVD) 251\n7.4.7 Deﬂation\nSuppose we have computed the ﬁrst eigenvector and value u1;\u00151by the power method. We now\ndescribe how to compute subsequent eigenvectors and values. Since the eigenvectors are orthonormal,\nand the eigenvalues are real, we can project out the u1component from the matrix as follows:\nA(2)= (I\u0000u1uT\n1)A(1)=A(1)\u0000u1uT\n1A(1)=A(1)\u0000\u00151u1uT\n1 (7.174)\nThis is called matrix deﬂation .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 656, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 906}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0657_336f33a1", "text": "We can then apply the power method to A(2), which will ﬁnd the\nlargest eigenvector/value in the subspace orthogonal to u1. In Section 20.1.2, we show that the optimal estimate ^Wfor the PCA model (described in\nSection 20.1) is given by the ﬁrst Keigenvectors of the empirical covariance matrix. Hence deﬂation\ncan be used to implement PCA. It can also be modiﬁed to implement sparse PCA [Mac09]. 7.4.8 Eigenvectors optimize quadratic forms\nWe can use matrix calculus to solve an optimization problem in a way that leads directly to\neigenvalue/eigenvector analysis. Consider the following, equality constrained optimization problem:\nmaxx2RnxTAxsubject tokxk2\n2= 1 (7.175)\nfor a symmetric matrix A2Sn. A standard way of solving optimization problems with equality\nconstraints is by forming the Lagrangian, an objective function that includes the equality constraints\n(see Section 8.5.1).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 657, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 885}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0658_0b1982fd", "text": "A standard way of solving optimization problems with equality\nconstraints is by forming the Lagrangian, an objective function that includes the equality constraints\n(see Section 8.5.1). The Lagrangian in this case can be given by\nL(x;\u0015) =xTAx+\u0015(1\u0000xTx) (7.176)\nwhere\u0015is called the Lagrange multiplier associated with the equality constraint. It can be established\nthat forx\u0003to be a optimal point to the problem, the gradient of the Lagrangian has to be zero at x\u0003\n(this is not the only condition, but it is required). That is,\nrxL(x;\u0015) = 2ATx\u00002\u0015x=0: (7.177)\nNotice that this is just the linear equation Ax=\u0015x. This shows that the only points which can\npossibly maximize (or minimize) xTAxassumingxTx= 1are the eigenvectors of A. 7.5 Singular value decomposition (SVD)\nWe now discuss the SVD, which generalizes EVD to rectangular matrices. 7.5.1 Basics\nAny (real)m\u0002nmatrix Acan be decomposed as\nA=USVT=\u001b10\n@j\nu1\nj1\nA\u0000\n\u0000vT\n1\u0000\u0001\n+\u0001\u0001\u0001+\u001br0\n@j\nur\nj1\nA\u0000\n\u0000vT\nr\u0000\u0001\n(7.178)\nAuthor: Kevin P. Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 658, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1000}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0659_2e9e718e", "text": "7.5.1 Basics\nAny (real)m\u0002nmatrix Acan be decomposed as\nA=USVT=\u001b10\n@j\nu1\nj1\nA\u0000\n\u0000vT\n1\u0000\u0001\n+\u0001\u0001\u0001+\u001br0\n@j\nur\nj1\nA\u0000\n\u0000vT\nr\u0000\u0001\n(7.178)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n252 Chapter 7. Linear Algebra\n(a)\n (b)\nFigure 7.8: SVD decomposition of a matrix, A=USVT. The shaded parts of each matrix are not computed\nin the economy-sized version. (a) Tall skinny matrix. (b) Short wide matrix. where Uis anm\u0002mwhose columns are orthornormal (so UTU=Im),Visn\u0002nmatrix whose\nrows and columns are orthonormal (so VTV=VVT=In), and Sis am\u0002nmatrix containing the\nr=min(m;n)singular values \u001bi\u00150on the main diagonal, with 0s ﬁlling the rest of the matrix. The columns of Uare the left singular vectors , and the columns of Vare the right singular vectors. This is called the singular value decomposition orSVDof the matrix. See Figure 7.8 for an\nexample. As is apparent from Figure 7.8a, if m>n, there are at most nsingular values, so the last m\u0000n\ncolumns of Uare irrelevant (since they will be multiplied by 0).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 659, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1007}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0660_c54fca72", "text": "See Figure 7.8 for an\nexample. As is apparent from Figure 7.8a, if m>n, there are at most nsingular values, so the last m\u0000n\ncolumns of Uare irrelevant (since they will be multiplied by 0). The economy sized SVD , also\ncalled athin SVD , avoids computing these unnecessary elements. In other words, if we write the U\nmatrix as U= [U1;U2], we only compute U1. Figure 7.8b shows the opposite case, where m<n,\nwhere we represent V= [V1;V2], and only compute V1. The cost of computing the SVD is O(min(mn2;m2n)). Details on how it works can be found in\nstandard linear algebra textbooks. 7.5.2 Connection between SVD and EVD\nIfAis real, symmetric and positive deﬁnite, then the singular values are equal to the eigenvalues,\nand the left and right singular vectors are equal to the eigenvectors (up to a sign change):\nA=USVT=USUT=USU\u00001(7.179)\nNote, however, that NumPy always returns the singular values in decreasing order, whereas the\neigenvalues need not necessarily be sorted.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 660, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 974}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0661_bdcad791", "text": "In general, for an arbitrary real matrix A, ifA=USVT, we have\nATA=VSTUTUSVT=V(STS)VT(7.180)\nHence\n(ATA)V=VDn (7.181)\nso the eigenvectors of ATAare equal to V, the right singular vectors of A, and the eigenvalues of\nATAare equal to Dn=STS, which is an n\u0002ndiagonal matrix containing the squared singular\nvalues. Similarly\nAAT=USVTVSTUT=U(SST)UT(7.182)\n(AAT)U=UDm (7.183)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.5. Singular value decomposition (SVD) 253\nso the eigenvectors of AATare equal to U, the left singular vectors of A, and the eigenvalues of\nAATare equal to Dm=SST, which is an m\u0002mdiagonal matrix containing the squared singular\nvalues. In summary,\nU=evec(AAT);V=evec(ATA);Dm=eval(AAT);Dn=eval(ATA) (7.184)\nIf we just use the computed (non-zero) parts in the economy-sized SVD, then we can deﬁne\nD=S2=STS=SST(7.185)\nNote also that an EVD does not always exist, even for square A, whereas an SVD always exists.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 661, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 953}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0662_8f3cdc9a", "text": "7.5.3 Pseudo inverse\nTheMoore-Penrose pseudo-inverse ofA, pseudo inverse denoted Ay, is deﬁned as the unique\nmatrix that satisﬁes the following 4 properties:\nAAyA=A;AyAAy=Ay;(AAy)T=AAy;(AyA)T=AyA (7.186)\nIfAis square and non-singular, then Ay=A\u00001. Ifm>n(tall, skinny) and the columns of Aare linearly independent (so Ais full rank), then\nAy= (ATA)\u00001AT(7.187)\nwhich is the same expression as arises in the normal equations (see Section 11.2.2.1). In this case,\nAyis a left inverse of Abecause\nAyA= (ATA)\u00001ATA=I (7.188)\nbut is not a right inverse because\nAAy=A(ATA)\u00001AT(7.189)\nonly has rank n, and so cannot be the m\u0002midentity matrix. Ifm < n(short, fat) and the rows of Aare linearly independent (so Ais full rank), then the\npseudo inverse is\nAy=AT(AAT)\u00001(7.190)\nIn this case, Ayis a right inverse of A. We can compute the pseudo inverse using the SVD decomposition A=USVT. In particular, one\ncan show that\nAy=V[diag(1=\u001b1;\u0001\u0001\u0001;1=\u001br;0;\u0001\u0001\u0001;0)]UT(7.191)\nwhereris the rank of the matrix.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 662, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 981}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0663_eaf9e329", "text": "We can compute the pseudo inverse using the SVD decomposition A=USVT. In particular, one\ncan show that\nAy=V[diag(1=\u001b1;\u0001\u0001\u0001;1=\u001br;0;\u0001\u0001\u0001;0)]UT(7.191)\nwhereris the rank of the matrix. Thus Ayacts just like a matrix inverse for non-square matrices:\nAy=A\u00001= (USVT)\u00001=VS\u00001UT(7.192)\nwhere we deﬁne S\u00001= diag(\u001b\u00001\n1;:::;\u001b\u00001\nr;0;:::; 0). Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n254 Chapter 7. Linear Algebra\n7.5.4 SVD and the range and null space of a matrix *\nIn this section, we show that the left and right singular vectors form an orthonormal basis for the\nrange and null space. From Equation (7.178) we have\nAx=X\nj:\u001bj>0\u001bj(vT\njx)uj=rX\nj=1\u001bj(vT\njx)uj (7.193)\nwhereris the rank of A. Thus any Axcan be written as a linear combination of the left singular\nvectorsu1;:::;ur, so the range of Ais given by\nrange( A) =span (fuj:\u001bj>0g) (7.194)\nwith dimension r.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 663, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 865}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0664_31ae33f4", "text": "Thus any Axcan be written as a linear combination of the left singular\nvectorsu1;:::;ur, so the range of Ais given by\nrange( A) =span (fuj:\u001bj>0g) (7.194)\nwith dimension r. To ﬁnd a basis for the null space, let us now deﬁne a second vector y2Rnthat is a linear\ncombination solely of the right singular vectors for the zero singular values,\ny=X\nj:\u001bj=0cjvj=nX\nj=r+1cjvj (7.195)\nSince thevj’s are orthonormal, we have\nAy=U0\nBBBBBBBB@\u001b1vT\n1y\n... \u001brvT\nry\n\u001br+1vT\nr+1y\n... \u001bnvT\nny1\nCCCCCCCCA=U0\nBBBBBBBB@\u001b10\n... \u001br0\n0vT\nr+1y\n... 0vT\nny1\nCCCCCCCCA=U0=0 (7.196)\nHence the right singular vectors form an orthonormal basis for the null space:\nnullspace( A) =span (fvj:\u001bj= 0g) (7.197)\nwith dimension n\u0000r. We see that\ndim(range( A)) + dim(nullspace( A)) =r+ (n\u0000r) =n (7.198)\nIn words, this is often written as\nrank +nullity =n (7.199)\nThis is called the rank-nullity theorem . It follows from this that the rank of a matrix is the\nnumber of nonzero singular values.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 664, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 952}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0665_3e413610", "text": "It follows from this that the rank of a matrix is the\nnumber of nonzero singular values. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.5. Singular value decomposition (SVD) 255\nrank 200\n(a)\nrank 2 (b)\nrank 5\n(c)\nrank 20 (d)\nFigure 7.9: Low rank approximations to an image. Top left: The original image is of size 200\u0002320, so has\nrank 200. Subsequent images have ranks 2, 5, and 20. Generated by code at ﬁgures.probml.ai/book1/7.9. 0 20 40 60 80 100\ni01234log(σi)Original\nRandomized\nFigure 7.10: First 100 log singular values for the clown image (solid red line), and for a data matrix obtained\nby randomly shuﬄing the pixels (dotted green line). Generated by code at ﬁgures.probml.ai/book1/7.10. Adapted from Figure 14.24 of [HTF09]. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n256 Chapter 7. Linear Algebra\n7.5.5 Truncated SVD\nLetA=USVTbe the SVD of A, and let ^AK=UKSKVT\nK, where we use the ﬁrst Kcolumns of U\nandV.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 665, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 965}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0666_db87bd37", "text": "Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n256 Chapter 7. Linear Algebra\n7.5.5 Truncated SVD\nLetA=USVTbe the SVD of A, and let ^AK=UKSKVT\nK, where we use the ﬁrst Kcolumns of U\nandV. This can be shown to be the optimal rank Kapproximation, in the sense that it minimizes\njjA\u0000^AKjj2\nF. IfK=r=rank(A), there is no error introduced by this decomposition. But if K <r, we incur\nsome error. This is called a truncated SVD . If the singular values die oﬀ quickly, as is typical in\nnatural data (see e.g., Figure 7.10), the error will be small. The total number of parameters needed\nto represent an N\u0002Dmatrix using a rank Kapproximation is\nNK+KD+K=K(N+D+ 1) (7.200)\nAs an example, consider the 200\u0002320pixel image in Figure 7.9(top left). This has 64,000 numbers\nin it. We see that a rank 20 approximation, with only (200 + 320 + 1)\u000220 = 10;420numbers is a\nvery good approximation.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 666, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 890}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0667_bcdeb45c", "text": "This has 64,000 numbers\nin it. We see that a rank 20 approximation, with only (200 + 320 + 1)\u000220 = 10;420numbers is a\nvery good approximation. One can show that the error in this approximation is given by\njjA\u0000^AjjF=rX\nk=K+1\u001bk (7.201)\nwhere\u001bkis thek’th singular value of A. Furthermore, one can show that the SVD oﬀers the best\nrankKapproximation to a matrix (best in the sense of minimizing the above Frobenius norm). 7.6 Other matrix decompositions *\nIn this section, we brieﬂy review some other useful matrix decompositions. 7.6.1 LU factorization\nWe can factorize any square matrix Ainto a product of a lower triangular matrix Land an upper\ntriangular matrix U. For example,\n2\n4a11a12a13\na21a22a23\na31a32a333\n5=2\n4l11 0 0\nl21l22 0\nl31l32l333\n52\n4u11u12u13\n0u22u23\n0 0u333\n5: (7.202)\nIn general we may need to permute the entries in the matrix before creating this decomposition. To see this, suppose a11= 0.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 667, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 910}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0668_4c229b65", "text": "To see this, suppose a11= 0. Sincea11=l11u11, this means either l11oru11or both must be zero,\nbut that would imply LorUare singular. To avoid this, the ﬁrst step of the algorithm can simply\nreorder the rows so that the ﬁrst element is nonzero. This is repeated for subsequent steps. We can\ndenote this process by\nPA=LU (7.203)\nwhere Pis a permutation matrix, i.e., a square binary matrix where Pij= 1if rowjgets permuted\nto rowi. This is called partial pivoting . Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.6. Other matrix decompositions * 257\n(a)\n(b)\nFigure 7.11: Illustration of QR decomposition, A=QR, where QTQ=IandRis upper triangular. (a)\nTall, skinny matrix. The shaded parts are not computed in the economy-sized version, since they are not\nneeded. (b) Short, wide matrix.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 668, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 815}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0669_3e6d29dc", "text": "(a)\nTall, skinny matrix. The shaded parts are not computed in the economy-sized version, since they are not\nneeded. (b) Short, wide matrix. 7.6.2 QR decomposition\nSuppose we have A2Rm\u0002nrepresenting a set of linearly independent basis vectors (so m\u0015n),\nand we want to ﬁnd a series of orthonormal vectors q1;q2;:::that span the successive subspaces of\nspan(a1),span(a1;a2), etc. In other words, we want to ﬁnd vectors qjand coeﬃcients rijsuch that\n0\n@j j j\na1a2\u0001\u0001\u0001an\nj j j1\nA=0\n@j j j\nq1q2\u0001\u0001\u0001qn\nj j j1\nA0\nBBB@r11r12\u0001\u0001\u0001r1n\nr22\u0001\u0001\u0001r2n\n... rnn1\nCCCA(7.204)\nWe can write this\na1=r11q1 (7.205)\na2=r12q1+r22q2 (7.206)\n... an=r1nq1+\u0001\u0001\u0001+rnnqn (7.207)\nso we seeq1spans the space of a1, andq1andq2span the space of fa1;a2g, etc. In matrix notation, we have\nA=^Q^R (7.208)\nwhere ^Qism\u0002nwith orthonormal columns and ^Risn\u0002nand upper triangular. This is called a\nreduced QR oreconomy sized QR factorization of A; see Figure 7.11.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 669, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 913}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0670_46e9af30", "text": "In matrix notation, we have\nA=^Q^R (7.208)\nwhere ^Qism\u0002nwith orthonormal columns and ^Risn\u0002nand upper triangular. This is called a\nreduced QR oreconomy sized QR factorization of A; see Figure 7.11. A full QR factorization appends an additional m\u0000northonormal columns to ^Qso it becomes a\nsquare, orthogonal matrix Q, which satisﬁes QQT=QTQ=I. Also, we append rows made of zero\nto^Rso it becomes an m\u0002nmatrix that is still upper triangular, called R: see Figure 7.11. The\nzero entries in R“kill oﬀ” the new columns in Q, so the result is the same as ^Q^R. QR decomposition is commonly used to solve systems of linear equations, as we discuss in\nSection 11.2.2.3. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n258 Chapter 7. Linear Algebra\n7.6.3 Cholesky decomposition\nAny symmetric positive deﬁnite matrix can be factorized as A=RTR, where Ris upper triangular\nwith real, positive diagonal elements.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 670, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 912}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0671_7c88b22a", "text": "Linear Algebra\n7.6.3 Cholesky decomposition\nAny symmetric positive deﬁnite matrix can be factorized as A=RTR, where Ris upper triangular\nwith real, positive diagonal elements. (This can also be written as A=LLT, where L=RTis\nlower triangular.) This is called a Cholesky factorization ormatrix square root . In NumPy,\nthis is implemented by np.linalg.cholesky . The computational complexity of this operation is\nO(V3), whereVis the number of variables, but can be less for sparse matrices. Below we give some\napplications of this factorization. 7.6.3.1 Application: Sampling from an MVN\nThe Cholesky decomposition of a covariance matrix can be used to sample from a multivariate\nGaussian. Let y\u0018N(\u0016;\u0006)and\u0006=LLT. We ﬁrst sample x\u0018N(0;I), which is easy because it\njust requires sampling from dseparate 1d Gaussians. We then set y=Lx+\u0016. This is valid since\nCov [y] =LCov [x]LT=L I LT=\u0006 (7.209)\nSee code.probml.ai/book1/cholesky_demo for some code.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 671, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 942}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0672_613e7597", "text": "We then set y=Lx+\u0016. This is valid since\nCov [y] =LCov [x]LT=L I LT=\u0006 (7.209)\nSee code.probml.ai/book1/cholesky_demo for some code. 7.7 Solving systems of linear equations *\nAn important application of linear algebra is the study of systems of linear equations. For example,\nconsider the following set of 3 equations:\n3x1+ 2x2\u0000x3= 1 (7.210)\n2x1\u00002x2+ 4x3=\u00002 (7.211)\n\u0000x1+1\n2x2\u0000x3= 0 (7.212)\nWe can represent this in matrix-vector form as follows:\nAx=b (7.213)\nwhere\nA=0\n@3 2\u00001\n2\u00002 4\n\u000011\n2\u000011\nA;b=0\n@1\n\u00002\n01\nA (7.214)\nThe solution is x= [1;\u00002;\u00002]. In general, if we have mequations and nunknowns, then Awill be am\u0002nmatrix, andbwill be\nam\u00021vector. Ifm=n(andAis full rank), there is a single unique solution. If m<n, the system\nisunderdetermined , so there is not a unique solution. If m>n, the system is overdetermined ,\nsince there are more constraints than unknowns, and not all the lines intersect at the same point. See Figure 7.12 for an illustration.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 672, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 951}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0673_95b86df1", "text": "If m>n, the system is overdetermined ,\nsince there are more constraints than unknowns, and not all the lines intersect at the same point. See Figure 7.12 for an illustration. We discuss how to compute solutions in each of these cases below. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.7. Solving systems of linear equations * 259\nFigure 7.12: Solution of a set of mlinear equations in n= 2variables. (a) m= 1< nso the system is\nunderdetermined. We show the minimal norm solution as a blue circle. (The dotted red line is orthogonal\nto the line, and its length is the distance to the origin.) (b) m=n= 2, so there is a unique solution. (c)\nm= 3>n, so there is no unique solution. We show the least squares solution.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 673, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 748}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0674_92b638f9", "text": "(c)\nm= 3>n, so there is no unique solution. We show the least squares solution. 7.7.1 Solving square systems\nIn the case where m=n, we can solve for xby computing an LU decomposition, A=LU, and then\nproceeding as follows:\nAx=b (7.215)\nLUx=b (7.216)\nUx=L\u00001b,y (7.217)\nx=U\u00001y (7.218)\nThe crucial point is that LandUare both triangular matrices, so we can avoid taking matrix\ninverses, and use a method known as backsubstitution instead. In particular, we can solve y=L\u00001bwithout taking inverses as follows. First we write\n0\nBBB@L11\nL21L22\n... Ln1Ln2\u0001\u0001\u0001Lnn1\nCCCA0\nB@y1\n... yn1\nCA=0\nB@b1\n... bn1\nCA (7.219)\nWe start by solving L11y1=b1to ﬁndy1and then substitute this in to solve\nL21y1+L22y2=b2 (7.220)\nfory2. We repeat this recursively. This process is often denoted by the backslash operator ,\ny=Lnb. Once we have y, we can solve x=U\u00001yusing backsubstitution in a similar manner.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 674, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 877}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0675_09061ce7", "text": "We repeat this recursively. This process is often denoted by the backslash operator ,\ny=Lnb. Once we have y, we can solve x=U\u00001yusing backsubstitution in a similar manner. 7.7.2 Solving underconstrained systems (least norm estimation)\nIn this section, we consider the underconstrained setting, where m<n.3We assume the rows are\nlinearly independent, so Ais full rank. 3. Our presentation is based in part on lecture notes by Stephen Boyd at http://ee263.stanford.edu/lectures/\nmin-norm.pdf . Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n260 Chapter 7. Linear Algebra\nWhenm<n, there are multiple possible solutions, which have the form\nfx:Ax=bg=fxp+z:z2nullspace( A)g (7.221)\nwherexpis any particular solution. It is standard to pick the particular solution with minimal `2\nnorm, i.e.,\n^x= argmin\nxjjxjj2\n2s:t:Ax=b (7.222)\nWe can compute the minimal norm solution using the rightpseudo inverse:\nxpinv=AT(AAT)\u00001b (7.223)\nTo see this, suppose xis some other solution, so Ax=b, and A(x\u0000xpinv) =0.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 675, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1007}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0676_9fe37ce6", "text": "Thus\n(x\u0000xpinv)Txpinv= (x\u0000xpinv)TAT(AAT)\u00001b= (A(x\u0000xpinv))T(AAT)\u00001b= 0 (7.224)\nand hence (x\u0000xpinv)?xpinv. ByPythagoras’s theorem , the norm of xis\njjxjj2=jjxpinv+x\u0000xpinvjj2=jjxpinvjj2+jjx\u0000xpinvjj2\u0015jjxpinvjj2(7.225)\nThus any solution apart from xpinvhas larger norm. We can also solve the constrained optimization problem in Equation (7.222) by minimizing the\nfollowing unconstrained objective\nL(x;\u0015) =xTx+\u0015T(Ax\u0000b) (7.226)\nFrom Section 8.5.1, the optimality conditions are\nrxL= 2x+AT\u0015=0;r\u0015L=Ax\u0000b=0 (7.227)\nFrom the ﬁrst condition we have x=\u0000AT\u0015=2. Subsituting into the second we get\nAx=\u00001\n2AAT\u0015=b (7.228)\nwhich implies \u0015=\u00002(AAT)\u00001b. Hencex=AT(AAT)\u00001b, which is the right pseudo inverse\nsolution. There is an interesting connection to regularized least squares. Let J1=jjAx\u0000bjj2\n2andJ2=jjxjj2\n2. The least-norm solution minimizes J2withJ1= 0.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 676, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 838}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0677_10f991cf", "text": "There is an interesting connection to regularized least squares. Let J1=jjAx\u0000bjj2\n2andJ2=jjxjj2\n2. The least-norm solution minimizes J2withJ1= 0. Now consider the following weighted sum objective\nL(x) =J2+\u0015J1=jjxjj2\n2+\u0015jjb\u0000Axjj2\n2 (7.229)\nSince this is a convex problem, the minimum of the Lagrangian occurs when the gradient is zero, so\nrxL(x) = 2x\u00002\u0015AT(b\u0000Ax) =0=)x(I+\u0015ATA) =\u0015ATb (7.230)\nAs\u0015!1, we get\n^x= (ATA)\u00001ATb (7.231)\nwhere (ATA)\u00001ATis theleftpseudoinverse of A. Alternatively (but equivalently) we can put weight\n\u0016on the`2regularizerjjxjj2\n2and let that go to zero. From the above we have\n(\u0016I+ATA)\u00001AT!AT(AAT)\u00001(7.232)\nSo the regularized least squares solution, in the limit of no regularization, converges to the minimum\nnorm solution. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.8.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 677, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 826}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0678_8e21e19c", "text": "Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.8. Matrix calculus 261\n7.7.3 Solving overconstrained systems (least squares estimation)\nIfm > n, we have an overdetermined solution, which typically does not have an exact solution,\nbut we will try to ﬁnd the solution that gets as close as possible to satisfying all of the constraints\nspeciﬁed by Ax=b. We can do this by minimizing the following cost function, known as the least\nsquares objective :\nf(x) =1\n2jjAx\u0000bjj2\n2 (7.233)\nUsing matrix calculus results from Section 7.8 we have that the gradient is given by\ng(x) =@\n@xf(x) =ATAx\u0000ATb (7.234)\nThe optimum can be found by solving g(x) =0. This gives\nATAx=ATb (7.235)\nThese are known as the normal equations , since, at the optimal solution, b\u0000Axis normal\n(orthogonal) to the range of A, as we explain in Section 11.2.2.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 678, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 853}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0679_4b002cc5", "text": "This gives\nATAx=ATb (7.235)\nThese are known as the normal equations , since, at the optimal solution, b\u0000Axis normal\n(orthogonal) to the range of A, as we explain in Section 11.2.2.2. The corresponding solution ^xis\ntheordinary least squares (OLS) solution, which is given by\n^x= (ATA)\u00001ATb (7.236)\nThe quantity Ay= (ATA)\u00001ATis the (left) pseudo inverse of the (non-square) matrix A(see\nSection 7.5.3 for more details). We can check that the solution is unique by showing that the Hessian is positive deﬁnite. In this\ncase, the Hessian is given by\nH(x) =@2\n@x2f(x) =ATA (7.237)\nIfAis full rank (so the columns of Aare linearly independent), then His positive deﬁnite, since for\nanyv>0, we have\nvT(ATA)v= (Av)T(Av) =jjAvjj2>0 (7.238)\nHence in the full rank case, the least squares objective has a unique global minimum. 7.8 Matrix calculus\nThe topic of calculus concerns computing “rates of change” of functions as we vary their inputs.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 679, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 934}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0680_a8ba10c3", "text": "7.8 Matrix calculus\nThe topic of calculus concerns computing “rates of change” of functions as we vary their inputs. It\nis of vital importance to machine learning, as well as almost every other numerical discipline. In this\nsection, we review some standard results. In some cases, we use some concepts and notation from\nmatrix algebra, which we cover in Chapter 7. For more details on these results from a deep learning\nperspective, see [PH18]. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n262 Chapter 7. Linear Algebra\n7.8.1 Derivatives\nConsider a scalar-argument function f:R!R. We deﬁne its derivative at a pointato be the\nquantity\nf0(x),lim\nh!0f(x+h)\u0000f(x)\nh(7.239)\nassuming the limit exists. This measures how quickly the output changes when we move a small\ndistance in input space away from x(i.e., the “rate of change” of the function). We can interpret\nf0(x)as the slope of the tangent line at f(x), and hence\nf(x+h)\u0019f(x) +f0(x)h (7.240)\nfor smallh.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 680, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 971}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0681_e2812288", "text": "We can interpret\nf0(x)as the slope of the tangent line at f(x), and hence\nf(x+h)\u0019f(x) +f0(x)h (7.240)\nfor smallh. We can compute a ﬁnite diﬀerence approximation to the derivative by using a ﬁnite step size h,\nas follows:\nf0(x)\u0011lim\nh!0f(x+h)\u0000f(x)\nh|{z}\nforward diﬀerence= lim\nh!0f(x+h=2)\u0000f(x\u0000h=2)\nh| {z }\ncentral diﬀerence= lim\nh!0f(x)\u0000f(x\u0000h)\nh|{z}\nbackward diﬀerence(7.241)\nThe smaller the step size h, the better the estimate, although if his too small, there can be errors\ndue to numerical cancellation. We can think of diﬀerentiation as an operator that maps functions to functions, D(f) =f0, where\nf0(x)computes the derivative at x(assuming the derivative exists at that point). The use of the\nprime symbol f0to denote the derivative is called Lagrange notation . The second derivative\nfunction, which measures how quickly the gradient is changing, is denoted by f00. Then’th derivative\nfunction is denoted f(n).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 681, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 916}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0682_f3f0be78", "text": "The second derivative\nfunction, which measures how quickly the gradient is changing, is denoted by f00. Then’th derivative\nfunction is denoted f(n). Alternatively, we can use Leibniz notation , in which we denote the function by y=f(x), and its\nderivative bydy\ndxord\ndxf(x). To denote the evaluation of the derivative at a point a, we writedf\ndx\f\f\f\f\nx=a:\n7.8.2 Gradients\nWe can extend the notion of derivatives to handle vector-argument functions, f:Rn!R, by deﬁning\nthepartial derivative offwith respect to xito be\n@f\n@xi= lim\nh!0f(x+hei)\u0000f(x)\nh(7.242)\nwhereeiis thei’th unit vector. Thegradient of a function at a point xis the vector of its partial derivatives:\ng=@f\n@x=rf=0\nB@@f\n@x1... @f\n@xn1\nCA (7.243)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.8. Matrix calculus 263\nTo emphasize the point at which the gradient is evaluated, we can write\ng(x\u0003),@f\n@x\f\f\f\f\nx\u0003(7.244)\nWe see that the operator r(pronounced “nabla”) maps a function f:Rn!Rto another function\ng:Rn!Rn.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 682, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1005}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0683_18666ef9", "text": "Sinceg()is a vector-valued function, it is known as a vector ﬁeld . By contrast, the\nderivative function f0is ascalar ﬁeld . 7.8.3 Directional derivative\nThedirectional derivative measures how much the function f:Rn!Rchanges along a direction\nvin space. It is deﬁned as follows\nDvf(x) = lim\nh!0f(x+hv)\u0000f(x)\nh(7.245)\nWe can approximate this numerically using 2 function calls to f, regardless of n. By contrast, a\nnumerical approximation to the standard gradient vector takes n+ 1calls (or 2nif using central\ndiﬀerences). Note that the directional derivative along vis the scalar product of the gradient gand the vector\nv:\nDvf(x) =rf(x)\u0001v (7.246)\n7.8.4 Total derivative *\nSuppose that some of the arguments to the function depend on each other. Concretely, suppose the\nfunction has the form f(t;x(t);y(t)).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 683, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 805}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0684_fe435519", "text": "Concretely, suppose the\nfunction has the form f(t;x(t);y(t)). We deﬁne the total derivative offwrttas follows:\ndf\ndt=@f\n@t+@f\n@xdx\ndt+@f\n@ydy\ndt(7.247)\nIf we multiply both sides by the diﬀerential dt, we get the total diﬀerential\ndf=@f\n@tdt+@f\n@xdx+@f\n@ydy (7.248)\nThis measures how much fchanges when we change t, both via the direct eﬀect of tonf, but also\nindirectly, via the eﬀects of tonxandy. 7.8.5 Jacobian\nConsider a function that maps a vector to another vector, f:Rn!Rm. TheJacobian matrix of\nthis function is an m\u0002nmatrix of partial derivatives:\nJf(x) =@f\n@xT,0\nB@@f1\n@x1\u0001\u0001\u0001@f1\n@xn......... @fm\n@x1\u0001\u0001\u0001@fm\n@xn1\nCA=0\nB@rf1(x)T\n... rfm(x)T1\nCA (7.249)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n264 Chapter 7.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 684, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 734}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0685_2dfcae48", "text": "@fm\n@x1\u0001\u0001\u0001@fm\n@xn1\nCA=0\nB@rf1(x)T\n... rfm(x)T1\nCA (7.249)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n264 Chapter 7. Linear Algebra\nNote that we lay out the results in the same orientation as the output f; this is sometimes called\nnumerator layout or the Jacobian formulation.4\n7.8.5.1 Multiplying Jacobians and vectors\nTheJacobian vector product orJVPis deﬁned to be the operation that corresponds to right-\nmultiplying the Jacobian matrix J2Rm\u0002nby a vectorv2Rn:\nJf(x)v=0\nB@rf1(x)T\n... rfm(x)T1\nCAv=0\nB@rf1(x)Tv\n... rfm(x)Tv1\nCA (7.250)\nSo we can see that we can approximate this numerically using just 2 calls to f. Thevector Jacobian product orVJPis deﬁned to be the operation that corresponds to\nleft-multiplying the Jacobian matrix J2Rm\u0002nby a vectoru2Rm:\nuTJf(x) =uT\u0010\n@f\n@x1;\u0001\u0001\u0001;@f\n@xn\u0011\n=\u0010\nu\u0001@f\n@x1;\u0001\u0001\u0001;u\u0001@f\n@xn\u0011\n(7.251)\nThe JVP is more eﬃcient if m\u0015n, and the VJP is more eﬃcient if m\u0014n.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 685, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 909}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0686_625c4409", "text": "See Section 13.3 for\ndetails on how this can be used to perform automatic diﬀerentiation in a computation graph such as\na DNN. 7.8.5.2 Jacobian of a composition\nSometimes it is useful to take the Jacobian of the composition of two functions. Let h(x) =g(f(x)). By the chain rule of calculus, we have\nJh(x) =Jg(f(x))Jf(x) (7.252)\nFor example, suppose f:R!R2andg:R2!R2. We have\n@g\n@x=\u0012@\n@xg1(f1(x);f2(x))\n@\n@xg2(f1(x);f2(x))\u0013\n= @g1\n@f1@f1\n@x+@g1\n@f2@f2\n@x\n@g2\n@f1@f1\n@x+@g2\n@f2@f2\n@x! (7.253)\n=@g\n@fT@f\n@x= @g1\n@f1@g1\n@f2@g2\n@f1@g2\n@f2!\u0012@f1\n@x@f2\n@x\u0013\n(7.254)\n7.8.6 Hessian\nFor a function f:Rn!Rthat is twice diﬀerentiable, we deﬁne the Hessian matrix as the\n(symmetric) n\u0002nmatrix of second partial derivatives:\nHf=@2f\n@x2=r2f=0\nBB@@2f\n@x2\n1\u0001\u0001\u0001@2f\n@x1@xn\n... @2f\n@xn@x1\u0001\u0001\u0001@2f\n@x2n1\nCCA(7.255)\nWe see that the Hessian is the Jacobian of the gradient. 4. For a much more detailed discussion of notation, see https://en.wikipedia.org/wiki/Matrix_calculus .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 686, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 950}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0687_7b8b1c51", "text": "@2f\n@xn@x1\u0001\u0001\u0001@2f\n@x2n1\nCCA(7.255)\nWe see that the Hessian is the Jacobian of the gradient. 4. For a much more detailed discussion of notation, see https://en.wikipedia.org/wiki/Matrix_calculus . Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.8. Matrix calculus 265\n7.8.7 Gradients of commonly used functions\nIn this section, we list without proof the gradients of certain widely used functions. 7.8.7.1 Functions that map scalars to scalars\nConsider a diﬀerentiable function f:R!R. Here are some useful identities from scalar calculus,\nwhich you should already be familiar with. d\ndxcxn=cnxn\u00001(7.256)\nd\ndxlog(x) = 1=x (7.257)\nd\ndxexp(x) = exp(x) (7.258)\nd\ndx[f(x) +g(x)] =df(x)\ndx+dg(x)\ndx(7.259)\nd\ndx[f(x)g(x)] =f(x)dg(x)\ndx+g(x)df(x)\ndx(7.260)\nd\ndxf(u(x)) =du\ndxdf(u)\ndu(7.261)\nEquation (7.261) is known as the chain rule of calculus . 7.8.7.2 Functions that map vectors to scalars\nConsider a diﬀerentiable function f:Rn!R.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 687, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 956}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0688_1686e33d", "text": "7.8.7.2 Functions that map vectors to scalars\nConsider a diﬀerentiable function f:Rn!R. Here are some useful identities:5\n@(aTx)\n@x=a (7.262)\n@(bTAx)\n@x=ATb (7.263)\n@(xTAx)\n@x= (A+AT)x (7.264)\nIt is fairly easy to prove these identities by expanding out the quadratic form, and applying scalar\ncalculus. 7.8.7.3 Functions that map matrices to scalars\nConsider a function f:Rm\u0002n!Rwhich maps a matrix to a scalar. We are using the following\nnatural layout for the derivative matrix:\n@f\n@X=0\nB@@f\n@x11\u0001\u0001\u0001@f\n@x1n... @f\n@xm1\u0001\u0001\u0001@f\n@xmn1\nCA (7.265)\n5. Some of the identities are taken from the list at http://www.cs.nyu.edu/~roweis/notes/matrixid.pdf . Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n266 Chapter 7. Linear Algebra\nBelow are some useful identities. Identities involving quadratic forms\nOne can show the following results. @\n@X(aTXb) =abT(7.266)\n@\n@X(aTXTb) =baT(7.267)\nIdentities involving matrix trace\nOne can show the following results.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 688, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 959}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0689_ce072dcd", "text": "Identities involving quadratic forms\nOne can show the following results. @\n@X(aTXb) =abT(7.266)\n@\n@X(aTXTb) =baT(7.267)\nIdentities involving matrix trace\nOne can show the following results. @\n@Xtr(AXB ) =ATBT(7.268)\n@\n@Xtr(XTA) =A (7.269)\n@\n@Xtr(X\u00001A) =\u0000X\u0000TATX\u0000T(7.270)\n@\n@Xtr(XTAX) = (A+AT)X (7.271)\nIdentities involving matrix determinant\nOne can show the following results. @\n@Xdet(AXB ) = det( AXB )X\u0000T(7.272)\n@\n@Xln(det( X)) =X\u0000T(7.273)\n7.9 Exercises\nExercise 7.1 [Orthogonal matrices]\na. A rotation in 3d by angle \u000babout thezaxis is given by the following matrix:\nR(\u000b) =0\n@cos(\u000b)\u0000sin(\u000b) 0\nsin(\u000b) cos(\u000b) 0\n0 0 11\nA (7.274)\nProve that Ris an orthogonal matrix, i.e., RTR=I, for any\u000b. b.What is the only eigenvector vofRwith an eigenvalue of 1.0 and of unit norm (i.e., jjvjj2= 1)? (Your\nanswer should be the same for any \u000b.) Hint: think about the geometrical interpretation of eigenvectors.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 689, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 894}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0690_a218c923", "text": "(Your\nanswer should be the same for any \u000b.) Hint: think about the geometrical interpretation of eigenvectors. Exercise 7.2 [Eigenvectors by hand *]\nFind the eigenvalues and eigenvectors of the following matrix\nA=\u00122 0\n0 3\u0013\n(7.275)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n7.9. Exercises 267\nCompute your result by hand and check it with Python. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n\n8Optimization\nParts of this chapter were written by Frederik Kunstner, Si Yi Meng, Aaron Mishkin, Sharan Vaswani,\nand Mark Schmidt. 8.1 Introduction\nWe saw in Chapter 4 that the core problem in machine learning is parameter estimation (aka model\nﬁtting). This requires solving an optimization problem , where we try to ﬁnd the values for a set\nof variables\u00122\u0002, that minimize a scalar-valued loss function orcost functionL: \u0002!R:\n\u0012\u00032argmin\n\u00122\u0002L(\u0012) (8.1)\nWe will assume that the parameter space is given by \u0002\u0012RD, whereDis the number of variables\nbeing optimized over.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 690, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1004}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0691_7b337451", "text": "Thus we are focusing on continuous optimization , rather than discrete\noptimization . If we want to maximize ascore function orreward function R(\u0012), we can equivalently minimize\nL(\u0012) =\u0000R(\u0012). We will use the term objective function to refer generically to a function we want\nto maximize or minimize. An algorithm that can ﬁnd an optimum of an objective function is often\ncalled asolver. In the rest of this chapter, we discuss diﬀerent kinds of solvers for diﬀerent kinds of objective\nfunctions, with a focus on methods used in the machine learning community. For more details on\noptimization, please consult some of the many excellent textbooks, such as [KW19b; BV04; NW06;\nBer15; Ber16] as well as various review articles, such as [BCN18; Sun+19b; PPS18; Pey20]. 8.1.1 Local vs global optimization\nA point that satisﬁes Equation (8.1) is called a global optimum . Finding such a point is called\nglobal optimization . In general, ﬁnding global optima is computationally intractable [Neu04].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 691, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 990}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0692_1efebc33", "text": "Finding such a point is called\nglobal optimization . In general, ﬁnding global optima is computationally intractable [Neu04]. In such cases, we will\njust try to ﬁnd a local optimum . For continuous problems, this is deﬁned to be a point \u0012\u0003which\nhas lower (or equal) cost than “nearby” points. Formally, we say \u0012\u0003is alocal minimum if\n9\u000e>0;8\u00122\u0002 s:t:jj\u0012\u0000\u0012\u0003jj<\u000e;L(\u0012\u0003)\u0014L(\u0012) (8.2)\n270 Chapter 8. Optimization\n1.0\n 0.5\n 0.0 0.5 1.0 1.5 2.0 2.5 3.02.5\n2.0\n1.5\n1.0\n0.5\n0.00.51.01.5\nlocal minimum\nGlobal minimum\n(a)\nX4\n2\n0\n2\n4Y\n4\n2\n024Z\n20\n10\n01020\n*Saddle Point (b)\nFigure 8.1: (a) Illustration of local and global minimum in 1d. Generated by code at ﬁgures.probml.ai/book1/8.1. (b) Illustration of a saddle point in 2d. Generated by code at ﬁgures.probml.ai/book1/8.1. A local minimum could be surrounded by other local minima with the same objective value; this\nis known as a ﬂat local minimum .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 692, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 888}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0693_e36715bc", "text": "Generated by code at ﬁgures.probml.ai/book1/8.1. A local minimum could be surrounded by other local minima with the same objective value; this\nis known as a ﬂat local minimum . A point is said to be a strict local minimum if its cost is\nstrictly lower than those of neighboring points:\n9\u000e>0;8\u00122\u0002;\u00126=\u0012\u0003:jj\u0012\u0000\u0012\u0003jj<\u000e;L(\u0012\u0003)<L(\u0012) (8.3)\nWe can deﬁne a (strict) local maximum analogously. See Figure 8.1a for an illustration. A ﬁnal note on terminology; if an algorithm is guaranteed to converge to a stationary point\nfrom any starting point, it is called globally convergent . However, this does not mean (rather\nconfusingly) that it will converge to a global optimum; instead, it just means it will converge to some\nstationary point. 8.1.1.1 Optimality conditions for local vs global optima\nFor continuous, twice diﬀerentiable functions, we can precisely characterize the points which cor-\nrespond to local minima. Let g(\u0012) =rL(\u0012)be the gradient vector, and H(\u0012) =r2L(\u0012)be the\nHessian matrix.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 693, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0694_95f67c72", "text": "Let g(\u0012) =rL(\u0012)be the gradient vector, and H(\u0012) =r2L(\u0012)be the\nHessian matrix. (See Section 7.8 for a refresher on these concepts, if necessary.) Consider a point\n\u0012\u00032RD, and letg\u0003=g(\u0012)j\u0012\u0003be the gradient at that point, and H\u0003=H(\u0012)j\u0012\u0003be the corresponding\nHessian. One can show that the following conditions characterize every local minimum:\n•Necessary condition: If \u0012\u0003is a local minimum, then we must have g\u0003=0(i.e.,\u0012\u0003must be a\nstationary point ), and H\u0003must be positive semi-deﬁnite. •Suﬃcient condition: If g\u0003=0andH\u0003is positive deﬁnite, then \u0012\u0003is a local optimum. To see why the ﬁrst condition is necessary, suppose we were at a point \u0012\u0003at which the gradient is\nnon-zero: at such a point, we could decrease the function by following the negative gradient a small\ndistance, so this would not be optimal. So the gradient must be zero. (In the case of nonsmooth\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 694, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 938}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0695_0afd9b85", "text": "So the gradient must be zero. (In the case of nonsmooth\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.1. Introduction 271\nfunctions, the necessary condition is that the zero is a local subgradient at the minimum.) To see why\na zero gradient is not suﬃcient, note that the stationary point could be a local minimum, maximum\norsaddle point , which is a point where some directions point downhill, and some uphill (see\nFigure 8.1b). More precisely, at a saddle point, the eigenvalues of the Hessian will be both positive\nand negative. However, if the Hessian at a point is positive semi-deﬁnite, then some directions may\npoint uphill, while others are ﬂat. This means that the objective function can not be decreased in\nthe neighbourhood of this point, implying that it is necessarily a local minimum. Moreover, if the\nHessian is strictly positive deﬁnite, then we are at the bottom of a “bowl”, and all directions point\nuphill, which is suﬃcient for this to be a minimum.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 695, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1001}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0696_6f351581", "text": "Moreover, if the\nHessian is strictly positive deﬁnite, then we are at the bottom of a “bowl”, and all directions point\nuphill, which is suﬃcient for this to be a minimum. 8.1.2 Constrained vs unconstrained optimization\nInunconstrained optimization , we deﬁne the optimization task as ﬁnding any value in the\nparameter space \u0002that minimizes the loss. However, we often have a set of constraints on the\nallowable values. It is standard to partition the set of constraints Cintoinequality constraints ,\ngj(\u0012)\u00140forj2Iandequality constraints ,hk(\u0012) = 0fork2E. For example, we can represent a\nsum-to-one constraint as an equality constraint h(\u0012) = (1\u0000PD\ni=1\u0012i) = 0, and we can represent a non-\nnegativity constraint on the parameters by using Dinequality constraints of the form gi(\u0012) =\u0000\u0012i\u00140\n.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 696, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 787}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0697_5020423b", "text": "We deﬁne the feasible set as the subset of the parameter space that satisﬁes the constraints:\nC=f\u0012:gj(\u0012)\u00140 :j2I;hk(\u0012) = 0 :k2Eg\u0012 RD(8.4)\nOurconstrained optimization problem now becomes\n\u0012\u00032argmin\n\u00122CL(\u0012) (8.5)\nIfC=RD, it is called unconstrained optimization . The addition of constraints can change the number of optima of a function. For example, a function\nthat was previously unbounded (and hence had no well-deﬁned global maximum or minimum) can\n“acquire” multiple maxima or minima when we add constraints, as illustrated in Figure 8.2. However,\nif we add too many constraints, we may ﬁnd that the feasible set becomes empty. The task of ﬁnding\nany point (regardless of its cost) in the feasible set is called a feasibility problem ; this can be a\nhard subproblem in itself. A common strategy for solving constrained problems is to create penalty terms that measure\nhow much we violate each constraint. We then add these terms to the objective and solve an\nunconstrained optimization problem.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 697, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 995}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0698_b1e5a74c", "text": "We then add these terms to the objective and solve an\nunconstrained optimization problem. The Lagrangian is a special case of such a combined objective\n(see Section 8.5 for details). 8.1.3 Convex vs nonconvex optimization\nInconvex optimization , we require the objective to be a convex function deﬁned over a convex\nset (see Section 8.1.3 for deﬁnitions of these terms). In such problems, every local minimum is also a\nglobal minimum. Thus many models are designed so that their training objectives are convex. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n272 Chapter 8. Optimization\nFigure 8.2: Illustration of constrained maximization of a nonconvex 1d function. The area between the dotted\nvertical lines represents the feasible set. (a) There is a unique global maximum since the function is concave\nwithin the support of the feasible set. (b) There are two global maxima, both occuring at the boundary of the\nfeasible set.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 698, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 942}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0699_9a5f0595", "text": "(a) There is a unique global maximum since the function is concave\nwithin the support of the feasible set. (b) There are two global maxima, both occuring at the boundary of the\nfeasible set. (c) In the unconstrained case, this function has no global maximum, since it is unbounded. Figure 8.3: Illustration of some convex and non-convex sets. 8.1.3.1 Convex sets\nWe saySis aconvex set if, for anyx;x02S, we have\n\u0015x+ (1\u0000\u0015)x02S;8\u00152[0;1] (8.6)\nThat is, if we draw a line from xtox0, all points on the line lie inside the set. See Figure 8.3 for\nsome illustrations of convex and non-convex sets. 8.1.3.2 Convex functions\nWe sayfis aconvex function if itsepigraph (the set of points above the function, illustrated in\nFigure 8.4a) deﬁnes a convex set. Equivalently, a function f(x)is called convex if it is deﬁned on a\nconvex set and if, for any x;y2S, and for any 0\u0014\u0015\u00141, we have\nf(\u0015x+ (1\u0000\u0015)y)\u0014\u0015f(x) + (1\u0000\u0015)f(y) (8.7)\nSee Figure 8.5(a) for a 1d example of a convex function.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 699, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 969}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0700_7d8a6c29", "text": "A function is called strictly convex if the\ninequality is strict. A function f(x)isconcave if\u0000f(x)is convex, and strictly concave if\u0000f(x)\nis strictly convex. See Figure 8.5(b) for a 1d example of a function that is neither convex nor concave. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.1. Introduction 273\nepi f (x)f (x)x1x2\n(a)\n (b)\nFigure 8.4: (a) Illustration of the epigraph of a function. (b) For a convex function f(x), its epipgraph can\nbe represented as the intersection of half-spaces deﬁned by linear lower bounds derived from the conjugate\nfunctionf\u0003(\u0015) = maxx\u0015x\u0000f(x). Xy1-ll\n(a)\nAB (b)\nFigure 8.5: (a) Illustration of a convex function. We see that the chord joining (x;f(x))to(y;f(y))lies\nabove the function. (b) A function that is neither convex nor concave. Ais a local minimum, Bis a global\nminimum.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 700, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 850}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0701_ad46e0c5", "text": "We see that the chord joining (x;f(x))to(y;f(y))lies\nabove the function. (b) A function that is neither convex nor concave. Ais a local minimum, Bis a global\nminimum. Here are some examples of 1d convex functions:\nx2\neax\n\u0000logx\nxa; a> 1;x> 0\njxja; a\u00151\nxlogx; x> 0\n8.1.3.3 Characterization of convex functions\nIntuitively, a convex function is shaped like a bowl. Formally, one can prove the following important\nresult:\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n274 Chapter 8. Optimization\nFigure 8.6: The quadratic form f(x) =xTAxin 2d. (a) Ais positive deﬁnite, so fis convex. (b) Ais\nnegative deﬁnite, so fis concave. (c) Ais positive semideﬁnite but singular, so fis convex, but not strictly. Notice the valley of constant height in the middle. (d) Ais indeﬁnite, so fis neither convex nor concave. The stationary point in the middle of the surface is a saddle point. From Figure 5 of [She94]. Theorem 8.1.1. Supposef:Rn!Ris twice diﬀerentiable over its domain.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 701, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 981}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0702_c70421bf", "text": "The stationary point in the middle of the surface is a saddle point. From Figure 5 of [She94]. Theorem 8.1.1. Supposef:Rn!Ris twice diﬀerentiable over its domain. Then fis convex iﬀ\nH=r2f(x)is positive semi deﬁnite (Section 7.1.5.3) for all x2dom(f). Furthermore, fis strictly\nconvex if His positive deﬁnite. For example, consider the quadratic form\nf(x) =xTAx (8.8)\nThis is convex if Ais positive semi deﬁnite, and is strictly convex if Ais positive deﬁnite. It is\nneither convex nor concave if Ahas eigenvalues of mixed sign. See Figure 8.6. 8.1.3.4 Strongly convex functions\nWe say a function fisstrongly convex with parameter m> 0if the following holds for all x,yin\nf’s domain:\n(rf(x)\u0000rf(y))T(x\u0000y)\u0015mjjx\u0000yjj2\n2 (8.9)\nA strongly convex function is also strictly convex, but not vice versa.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 702, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 792}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0703_4b46de36", "text": "If the function fis twice continuously diﬀerentiable, then it is strongly convex with parameter m\nif and only ifr2f(x)\u0017mIfor allxin the domain, where Iis the identity and r2fis the Hessian\nmatrix, and the inequality \u0017means thatr2f(x)\u0000mIis positive semi-deﬁnite. This is equivalent\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.1. Introduction 275\n−1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.000.00.20.40.60.81.0Smooth function\n(a)\n−1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.000.00.20.40.60.81.0Non-smooth function (b)\nFigure 8.7: (a) Smooth 1d function. (b) Non-smooth 1d function. (There is a discontinuity at the origin.)\nGenerated by code at ﬁgures.probml.ai/book1/8.7. to requiring that the minimum eigenvalue of r2f(x)be at least mfor allx. If the domain is just\nthe real line, then r2f(x)is just the second derivative f00(x), so the condition becomes f00(x)\u0015m.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 703, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 903}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0704_dee7862e", "text": "to requiring that the minimum eigenvalue of r2f(x)be at least mfor allx. If the domain is just\nthe real line, then r2f(x)is just the second derivative f00(x), so the condition becomes f00(x)\u0015m. Ifm= 0, then this means the Hessian is positive semideﬁnite (or if the domain is the real line, it\nmeans that f00(x)\u00150), which implies the function is convex, and perhaps strictly convex, but not\nstrongly convex. The distinction between convex, strictly convex, and strongly convex is rather subtle. To better\nunderstand this, consider the case where fis twice continuously diﬀerentiable and the domain is the\nreal line. Then we can characterize the diﬀerences as follows:\n•fis convex if and only if f00(x)\u00150for allx. •fis strictly convex if f00(x)>0for allx(note: this is suﬃcient, but not necessary). •fis strongly convex if and only if f00(x)\u0015m> 0for allx. Note that it can be shown that a function fis strongly convex with parameter miﬀ the function\nJ(x) =f(x)\u0000m\n2jjxjj2(8.10)\nis convex.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 704, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 985}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0705_c2bfbcd7", "text": "•fis strongly convex if and only if f00(x)\u0015m> 0for allx. Note that it can be shown that a function fis strongly convex with parameter miﬀ the function\nJ(x) =f(x)\u0000m\n2jjxjj2(8.10)\nis convex. 8.1.4 Smooth vs nonsmooth optimization\nInsmooth optimization , the objective and constraints are continuously diﬀerentiable functions. For smooth functions, we can quantify the degree of smoothness using the Lipschitz constant . In\nthe 1d case, this is deﬁned as any constant L\u00150such that, for all real x1andx2, we have\njf(x1)\u0000f(x2)j\u0014Ljx1\u0000x2j (8.11)\nThis is illustrated in Figure 8.8: for a given constant L, the function output cannot change by more\nthanLif we change the function input by 1 unit. This can be generalized to vector inputs using a\nsuitable norm. Innonsmooth optimization , there are at least some points where the gradient of the objective\nfunction or the constraints is not well-deﬁned. See Figure 8.7 for an example. In some optimization\nAuthor: Kevin P. Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 705, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 985}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0706_e32c08e5", "text": "See Figure 8.7 for an example. In some optimization\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n276 Chapter 8. Optimization\nFigure 8.8: For a Lipschitz continuous function f, there exists a double cone (white) whose origin can be\nmoved along the graph of fso that the whole graph always stays outside the double cone. From https: // en. wikipedia. org/ wiki/ Lipschitz_ continuity . Used with kind permission of Wikipedia author Taschee. problems, we can partition the objective into a part that only contains smooth terms, and a part\nthat contains the nonsmooth terms:\nL(\u0012) =Ls(\u0012) +Lr(\u0012) (8.12)\nwhereLsis smooth (diﬀerentiable), and Lris nonsmooth (“rough”). This is often referred to as a\ncomposite objective . In machine learning applications, Lsis usually the training set loss, and\nLris a regularizer, such as the `1norm of\u0012. This composite structure can be exploited by various\nalgorithms.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 706, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 911}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0707_dd81e384", "text": "In machine learning applications, Lsis usually the training set loss, and\nLris a regularizer, such as the `1norm of\u0012. This composite structure can be exploited by various\nalgorithms. 8.1.4.1 Subgradients\nIn this section, we generalize the notion of a derivative to work with functions which have local\ndiscontinuities. In particular, for a convex function of several variables, f:Rn!R, we say that\ng2Rnis asubgradient offatx2dom(f)if for all vectors z2dom(f),\nf(z)\u0015f(x) +gT(z\u0000x) (8.13)\nNote that a subgradient can exist even when fis not diﬀerentiable at a point, as shown in Figure 8.9. A function fis calledsubdiﬀerentiable atxif there is at least one subgradient at x. The set of\nsuch subgradients is called the subdiﬀerential offatx, and is denoted @f(x). For example, consider the absolute value function f(x) =jxj. Its subdiﬀerential is given by\n@f(x) =8\n<\n:f\u00001gifx<0\n[\u00001;1]ifx= 0\nf+1gifx>0(8.14)\nwhere the notation [\u00001;1]means any value between -1 and 1 inclusive. See Figure 8.10 for an\nillustration.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 707, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1008}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0708_f21fd558", "text": "Its subdiﬀerential is given by\n@f(x) =8\n<\n:f\u00001gifx<0\n[\u00001;1]ifx= 0\nf+1gifx>0(8.14)\nwhere the notation [\u00001;1]means any value between -1 and 1 inclusive. See Figure 8.10 for an\nillustration. 8.2 First-order methods\nIn this section, we consider iterative optimization methods that leverage ﬁrst order derivatives of\nthe objective function, i.e., they compute which directions point “downhill”, but they ignore curvature\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.2. First-order methods 277\nSubgradient of a function\ngis a subgradient off(not necessarily convex) at xif\nf(y)≥f(x)+gT(y−x)for all y\nx1 x2f(x1)+gT\n1(x−x1)\nf(x2)+gT\n2(x−x2)\nf(x2)+gT\n3(x−x2)f(x)\ng2,g3are subgradients at x2;g1is a subgradient at x1\nEE364b, Stanford University 2\nFigure 8.9: Illustration of subgradients. At x1, the convex function fis diﬀerentiable, and g1(which is\nthe derivative of fatx1) is the unique subgradient at x1. At the point x2,fis not diﬀerentiable, because\nof the “kink”.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 708, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 993}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0709_3a963669", "text": "At x1, the convex function fis diﬀerentiable, and g1(which is\nthe derivative of fatx1) is the unique subgradient at x1. At the point x2,fis not diﬀerentiable, because\nof the “kink”. However, there are many subgradients at this point, of which two are shown. From https:\n// web. stanford. edu/ class/ ee364b/ lectures/ subgradients_ slides. pdf . Used with kind permission\nof Stephen Boyd. Example\nf(x)=|x|\nf(x)=|x| ∂f(x)\nxx1\n−1\nrighthand plot shows/uniontext{(x, g)|x∈R,g∈∂f(x)}\nEE364b, Stanford University 6\nFigure 8.10: The absolute value function (left) and its subdiﬀerential (right). From https: // web. stanford. edu/ class/ ee364b/ lectures/ subgradients_ slides. pdf . Used with kind permission of Stephen Boyd. information. All of these algorithms require that the user specify a starting point \u00120.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 709, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 807}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0710_0ef88728", "text": "stanford. edu/ class/ ee364b/ lectures/ subgradients_ slides. pdf . Used with kind permission of Stephen Boyd. information. All of these algorithms require that the user specify a starting point \u00120. Then at each\niterationt, they perform an update of the following form:\n\u0012t+1=\u0012t+\u001atdt (8.15)\nwhere\u001atis known as the step size orlearning rate , anddtis adescent direction , such as the\nnegative of the gradient , given bygt=r\u0012L(\u0012)j\u0012t. These update steps are continued until the\nmethod reaches a stationary point, where the gradient is zero. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n278 Chapter 8. Optimization\n8.2.1 Descent direction\nWe say that a direction dis adescent direction if there is a small enough (but nonzero) amount \u001a\nwe can move in direction dand be guaranteed to decrease the function value. Formally, we require\nthat there exists an \u001amax>0such that\nL(\u0012+\u001ad)<L(\u0012) (8.16)\nfor all 0<\u001a<\u001a max.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 710, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 918}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0711_c46b8f19", "text": "Formally, we require\nthat there exists an \u001amax>0such that\nL(\u0012+\u001ad)<L(\u0012) (8.16)\nfor all 0<\u001a<\u001a max. The gradient at the current iterate,\ngt,rL(\u0012)j\u0012t=rL(\u0012t) =g(\u0012t) (8.17)\npoints in the direction of maximal increase in f, so the negative gradient is a descent direction. It\ncan be shown that any direction dis also a descent direction if the angle \u0012betweendand\u0000gtis less\nthan 90 degrees and satisﬁes\ndTgt=jjdjjjjgtjjcos(\u0012)<0 (8.18)\nIt seems that the best choice would be to pick dt=\u0000gt. This is known as the direction of steepest\ndescent. However, this can be quite slow. We consider faster versions later. 8.2.2 Step size (learning rate)\nIn machine learning, the sequence of step sizes f\u001atgis called the learning rate schedule . There are\nseveral widely used methods for picking this, some of which we discuss below. (See also Section 8.4.3,\nwhere we discuss schedules for stochastic optimization.)\n8.2.2.1 Constant step size\nThe simplest method is to use a constant step size, \u001at=\u001a.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 711, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 979}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0712_e95fabef", "text": "(See also Section 8.4.3,\nwhere we discuss schedules for stochastic optimization.)\n8.2.2.1 Constant step size\nThe simplest method is to use a constant step size, \u001at=\u001a. However, if it is too large, the method\nmay fail to converge, and if it is too small, the method will converge but very slowly. For example, consider the convex function\nL(\u0012) = 0:5(\u00122\n1\u0000\u00122)2+ 0:5(\u00121\u00001)2(8.19)\nLet us pick as our descent direction dt=\u0000gt. Figure 8.11 shows what happens if we use this descent\ndirection with a ﬁxed step size, starting from (0;0). In Figure 8.11(a), we use a small step size of\n\u001a= 0:1; we see that the iterates move slowly along the valley. In Figure 8.11(b), we use a larger step\nsize\u001a= 0:6; we see that the iterates start oscillating up and down the sides of the valley and never\nconverge to the optimum, even though this is a convex problem. In some cases, we can derive a theoretical upper bound on the maximum step size we can use. For\nexample, consider a quadratic objective, L(\u0012) =1\n2\u0012TA\u0012+bT\u0012+cwithA\u00170.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 712, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1007}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0713_4e753f83", "text": "In some cases, we can derive a theoretical upper bound on the maximum step size we can use. For\nexample, consider a quadratic objective, L(\u0012) =1\n2\u0012TA\u0012+bT\u0012+cwithA\u00170. One can show that\nsteepest descent will have global convergence iﬀ the step size satisﬁes\n\u001a<2\n\u0015max(A)(8.20)\nwhere\u0015max(A)is the largest eigenvalue of A. The intuitive reason for this can be understood by\nthinking of a ball rolling down a valley. We want to make sure it doesn’t take a step that is larger than\nthe slope of the steepest direction, which is what the largest eigenvalue measures (see Section 3.2.2). Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.2. First-order methods 279\n0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.750.5\n0.00.51.01.52.02.5step size 0.100\n(a)\n0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.750.5\n0.00.51.01.52.02.5step size 0.600 (b)\nFigure 8.11: Steepest descent on a simple convex function, starting from (0;0), for 20 steps, using a\nﬁxed step size. The global minimum is at (1;1). (a)\u001a= 0:1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 713, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1005}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0714_0e1377d0", "text": "The global minimum is at (1;1). (a)\u001a= 0:1. (b)\u001a= 0:6. Generated by code at\nﬁgures.probml.ai/book1/8.11. More generally, setting \u001a<2=L, whereLis the Lipschitz constant of the gradient (Section 8.1.4),\nensures convergence. Since this constant is generally unknown, we usually need to adapt the step\nsize, as we discuss below. 8.2.2.2 Line search\nThe optimal step size can be found by ﬁnding the value that maximally decreases the objective along\nthe chosen direction by solving the 1d minimization problem\n\u001at= argmin\n\u001a>0\u001et(\u001a) = argmin\n\u001a>0L(\u0012t+\u001adt) (8.21)\nThis is known as line search , since we are searching along the line deﬁned by dt. If the loss is convex, this subproblem is also convex, because \u001et(\u001a) =L(\u0012t+\u001adt)is a convex\nfunction of an aﬃne function of \u001a, for ﬁxed\u0012tanddt.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 714, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 778}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0715_0a4a9668", "text": "If the loss is convex, this subproblem is also convex, because \u001et(\u001a) =L(\u0012t+\u001adt)is a convex\nfunction of an aﬃne function of \u001a, for ﬁxed\u0012tanddt. For example, consider the quadratic loss\nL(\u0012) =1\n2\u0012TA\u0012+bT\u0012+c (8.22)\nComputing the derivative of \u001egives\nd\u001e(\u001a)\nd\u001a=d\nd\u001a\u00141\n2(\u0012+\u001ad)TA(\u0012+\u001ad) +bT(\u0012+\u001ad) +c\u0015\n(8.23)\n=dTA(\u0012+\u001ad) +dTb (8.24)\n=dT(A\u0012+b) +\u001adTAd (8.25)\nSolving ford\u001e(\u001a)\nd\u001a= 0gives\n\u001a=\u0000dT(A\u0012+b)\ndTAd(8.26)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n280 Chapter 8. Optimization\nUsing the optimal step size is known as exact line search . However, it is not usually necessary\nto be so precise. There are several methods, such as the Armijo backtracking method , that try\nto ensure suﬃcient reduction in the objective function without spending too much time trying to\nsolve Equation (8.21).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 715, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 795}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0716_71e919bc", "text": "There are several methods, such as the Armijo backtracking method , that try\nto ensure suﬃcient reduction in the objective function without spending too much time trying to\nsolve Equation (8.21). In particular, we can start with the current stepsize (or some maximum value),\nand then reduce it by a factor 0<\f < 1at each step until we satisfy the following condition, known\nas theArmijo-Goldstein test:\nL(\u0012t+\u001adt)\u0014L(\u0012t) +c\u001adT\ntrL(\u0012t) (8.27)\nwherec2[0;1]is a constant, typically c= 10\u00004. In practice, the initialization of the line-search and\nhow to backtrack can signiﬁcantly aﬀect performance. See [NW06, Sec 3.1] for details. 8.2.3 Convergence rates\nWe want to ﬁnd optimization algorithms that converge quickly to a (local) optimum. For certain\nconvex problems, with a gradient with bounded Lipschitz constant, one can show that gradient\ndescent converges at a linear rate .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 716, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 875}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0717_85ed5bc8", "text": "For certain\nconvex problems, with a gradient with bounded Lipschitz constant, one can show that gradient\ndescent converges at a linear rate . This means that there exists a number 0<\u0016< 1such that\njL(\u0012t+1)\u0000L(\u0012\u0003)j\u0014\u0016jL(\u0012t)\u0000L(\u0012\u0003)j (8.28)\nHere\u0016is called the rate of convergence . For some simple problems, we can derive the convergence rate explicitly, For example, consider a\nquadratic objective L(\u0012) =1\n2\u0012TA\u0012+bT\u0012+cwithA\u001f0. Suppose we use steepest descent with\nexact line search. One can show (see e.g., [Ber15]) that the convergence rate is given by\n\u0016=\u0012\u0015max\u0000\u0015min\n\u0015max+\u0015min\u00132\n(8.29)\nwhere\u0015maxis the largest eigenvalue of Aand\u0015minis the smallest eigenvalue. We can rewrite this\nas\u0016=\u0000\u0014\u00001\n\u0014+1\u00012, where\u0014=\u0015max\n\u0015minis the condition number of A. Intuitively, the condition number\nmeasures how “skewed” the space is, in the sense of being far from a symmetrical “bowl”. (See\nSection 7.1.4.4 for more information on condition numbers.)\nFigure 8.12 illustrates the eﬀect of the condition number on the convergence rate.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 717, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1005}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0718_3c56130b", "text": "(See\nSection 7.1.4.4 for more information on condition numbers.)\nFigure 8.12 illustrates the eﬀect of the condition number on the convergence rate. On the left\nwe show an example where A= [20;5; 5;2],b= [\u000014;\u00006]andc= 10, so\u0014(A) = 30:234. On the\nright we show an example where A= [20;5; 5;16],b= [\u000014;\u00006]andc= 10, so\u0014(A) = 1:8541. We\nsee that steepest descent converges much more quickly for the problem with the smaller condition\nnumber. In the more general case of non-quadratic functions, the objective will often be locally quadratic\naround a local optimum. Hence the convergence rate depends on the condition number of the Hessian,\n\u0014(H), at that point. We can often improve the convergence speed by optimizing a surrogate objective\n(or model) at each step which has a Hessian that is close to the Hessian of the objective function as\nwe discuss in Section 8.3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 718, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 864}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0719_2b9c2f49", "text": "Although line search works well, we see from Figure 8.12 that the path of steepest descent with an\nexact line-search exhibits a characteristic zig-zag behavior, which is ineﬃcient. This problem can be\novercome using a method called conjugate gradient descent (see e.g., [She94]). Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.2. First-order methods 281\n2.0\n 1.5\n 1.0\n 0.5\n 0.0 0.5 1.0 1.52.0\n1.5\n1.0\n0.5\n0.00.51.01.5condition number of A=30.234\n(a)\n2.0\n 1.5\n 1.0\n 0.5\n 0.0 0.5 1.0 1.52.0\n1.5\n1.0\n0.5\n0.00.51.01.5condition number of A=1.854 (b)\nFigure 8.12: Illustration of the eﬀect of condition number \u0014on the convergence speed of steepest descent with\nexact line searches. (a) Large \u0014. (b) Small \u0014. Generated by code at ﬁgures.probml.ai/book1/8.12. 8.2.4 Momentum methods\nGradient descent can move very slowly along ﬂat regions of the loss landscape, as we illustrated in\nFigure 8.11. We discuss some solutions to this below.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 719, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 959}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0720_3818544d", "text": "8.2.4 Momentum methods\nGradient descent can move very slowly along ﬂat regions of the loss landscape, as we illustrated in\nFigure 8.11. We discuss some solutions to this below. 8.2.4.1 Momentum\nOne simple heuristic, known as the heavy ball ormomentum method [Ber99], is to move faster\nalong directions that were previously good, and to slow down along directions where the gradient has\nsuddenly changed, just like a ball rolling downhill. This can be implemented as follows:\nmt=\fmt\u00001+gt\u00001 (8.30)\n\u0012t=\u0012t\u00001\u0000\u001atmt (8.31)\nwheremtis the momentum (mass times velocity) and 0<\f < 1. A typical value of \fis 0.9. For\n\f= 0, the method reduces to gradient descent.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 720, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 651}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0721_2bb73205", "text": "A typical value of \fis 0.9. For\n\f= 0, the method reduces to gradient descent. We see that mtis like an exponentially weighted moving average of the past gradients (see\nSection 4.4.2.2):\nmt=\fmt\u00001+gt\u00001=\f2mt\u00002+\fgt\u00002+gt\u00001=\u0001\u0001\u0001=t\u00001X\n\u001c=0\f\u001cgt\u0000\u001c\u00001 (8.32)\nIf all the past gradients are a constant, say g, this simpliﬁes to\nmt=gt\u00001X\n\u001c=0\f\u001c(8.33)\nThe scaling factor is a geometric series, whose inﬁnite sum is given by\n1 +\f+\f2+\u0001\u0001\u0001=1X\ni=0\fi=1\n1\u0000\f(8.34)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n282 Chapter 8. Optimization\nCost Regular momentum UpdateNesterov UpdateStarting Pointbmhq1q2\nh12h1\nFigure 8.13: Illustration of the Nesterov update. Adapted from Figure 11.6 of [Gér19]. Thus in the limit, we multiply the gradient by 1=(1\u0000\f). For example, if \f= 0:9, we scale the\ngradient up by 10. Since we update the parameters using the gradient average mt\u00001, rather than just the most recent\ngradient,gt\u00001, we see that past gradients can exhibit some inﬂuence on the present.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 721, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 977}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0722_759cf0ba", "text": "Since we update the parameters using the gradient average mt\u00001, rather than just the most recent\ngradient,gt\u00001, we see that past gradients can exhibit some inﬂuence on the present. Furthermore,\nwhen momentum is combined with SGD, discussed in Section 8.4, we will see that it can simulate\nthe eﬀects of a larger minibatch, without the computational cost. 8.2.4.2 Nesterov momentum\nOne problem with the standard momentum method is that it may not slow down enough at the\nbottom of a valley, causing oscillation. The Nesterov accelerated gradient method of [Nes04]\ninstead modiﬁes the gradient descent to include an extrapolation step, as follows:\n~\u0012t+1=\u0012t+\ft(\u0012t\u0000\u0012t\u00001) (8.35)\n\u0012t+1=~\u0012t+1\u0000\u001atrL(~\u0012t+1) (8.36)\nThis is essentially a form of one-step “look ahead”, that can reduce the amount of oscillation, as\nillustrated in Figure 8.13. Nesterov accelerated gradient can also be rewritten in the same format as standard momentum.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 722, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 923}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0723_3044085b", "text": "Nesterov accelerated gradient can also be rewritten in the same format as standard momentum. In\nthis case, the momentum term is updated using the gradient at the predicted new location,\nmt+1=\fmt\u0000\u001atrL(\u0012t+\fmt) (8.37)\n\u0012t+1=\u0012t+mt+1 (8.38)\nThis explains why the Nesterov accelerated gradient method is sometimes called Nesterov momentum. It also shows how this method can be faster than standard momentum: the momentum vector\nis already roughly pointing in the right direction, so measuring the gradient at the new location,\n\u0012t+\fmt, rather than the current location, \u0012t, can be more accurate. The Nesterov accelerated gradient method is provably faster than steepest descent for convex\nfunctions when \fand\u001atare chosen appropriately. It is called “accelerated” because of this improved\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 723, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 860}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0724_9d077819", "text": "It is called “accelerated” because of this improved\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.3. Second-order methods 283\nconvergence rate, which is optimal for gradient-based methods using only ﬁrst-order information\nwhen the objective function is convex and has Lipschitz-continuous gradients. In practice, however,\nusing Nesterov momentum can be slower than steepest descent, and can even unstable if \for\u001atare\nmisspeciﬁed. 8.3 Second-order methods\nOptimization algorithms that only use the gradient are called ﬁrst-order methods. They have the\nadvantage that the gradient is cheap to compute and to store, but they do not model the curvature\nof the space, and hence they can be slow to converge, as we have seen in Figure 8.12. Second-order\noptimization methods incorporate curvature in various ways (e.g., via the Hessian), which may yield\nfaster convergence. We discuss some of these methods below.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 724, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 939}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0725_e7074ea0", "text": "Second-order\noptimization methods incorporate curvature in various ways (e.g., via the Hessian), which may yield\nfaster convergence. We discuss some of these methods below. 8.3.1 Newton’s method\nThe classic second-order method is Newton’s method . This consists of updates of the form\n\u0012t+1=\u0012t\u0000\u001atH\u00001\ntgt (8.39)\nwhere\nHt,r2L(\u0012)j\u0012t=r2L(\u0012t) =H(\u0012t) (8.40)\nis assumed to be positive-deﬁnite to ensure the update is well-deﬁned. The pseudo-code for Newton’s\nmethod is given in Algorithm 1. The intuition for why this is faster than gradient descent is that the\nmatrix inverse H\u00001“undoes” any skew in the local curvature, converting a topology like Figure 8.12a\nto one like Figure 8.12b. Algorithm 1: Newton’s method for minimizing a function\n1Initialize\u00120;\n2fort= 1;2;:::until convergence do\n3Evaluategt=rL(\u0012t);\n4Evaluate Ht=r2L(\u0012t);\n5Solve Htdt=\u0000gtfordt;\n6Use line search to ﬁnd stepsize \u001atalongdt;\n7\u0012t+1=\u0012t+\u001atdt;\nThis algorithm can be derived as follows.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 725, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 949}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0726_61249b1f", "text": "Consider making a second-order Taylor series approxi-\nmation ofL(\u0012)around\u0012t:\nLquad(\u0012) =L(\u0012t) +gT\nt(\u0012\u0000\u0012t) +1\n2(\u0012\u0000\u0012t)THt(\u0012\u0000\u0012t) (8.41)\nThe minimum of Lquadis at\n\u0012=\u0012t\u0000H\u00001\ntgt (8.42)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n284 Chapter 8. Optimization\n(a)\n (b)\nFigure 8.14: Illustration of Newton’s method for minimizing a 1d function. (a) The solid curve is the function\nL(x). The dotted line Lquad(\u0012)is its second order approximation at \u0012t. The Newton step dtis what must\nbe added to \u0012tto get to the minimum of Lquad(\u0012). Adapted from Figure 13.4 of [Van06]. Generated by\ncode at ﬁgures.probml.ai/book1/8.14. (b) Illustration of Newton’s method applied to a nonconvex function. We ﬁt a quadratic function around the current point \u0012tand move to its stationary point, \u0012t+1=\u0012t+dt. Unfortunately, this takes us near a local maximum of f, not minimum. This means we need to be careful\nabout the extent of our quadratic approximation. Adapted from Figure 13.11 of [Van06].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 726, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 981}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0727_c271b441", "text": "Unfortunately, this takes us near a local maximum of f, not minimum. This means we need to be careful\nabout the extent of our quadratic approximation. Adapted from Figure 13.11 of [Van06]. Generated by code at\nﬁgures.probml.ai/book1/8.14. So if the quadratic approximation is a good one, we should pick dt=\u0000H\u00001\ntgtas our descent direction. See Figure 8.14(a) for an illustration. Note that, in a “pure” Newton method, we use \u001at= 1as our\nstepsize. However, we can also use linesearch to ﬁnd the best stepsize; this tends to be more robust\nas using\u001at= 1may not always converge globally. If we apply this method to linear regression, we get to the optimum in one step, since (as we shown\nin Section 11.2.2.1) we have H=XTXandg=XTXw\u0000XTy, so the Newton update becomes\nw1=w0\u0000H\u00001g=w0\u0000(XTX)\u00001(XTXw0\u0000XTy) =w0\u0000w0+ (XTX)\u00001XTy (8.43)\nwhich is the OLS estimate. However, when we apply this method to logistic regression, it may take\nmultiple iterations to converge to the global optimum, as we discuss in Section 10.2.6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 727, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1007}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0728_9176774e", "text": "However, when we apply this method to logistic regression, it may take\nmultiple iterations to converge to the global optimum, as we discuss in Section 10.2.6. 8.3.2 BFGS and other quasi-Newton methods\nQuasi-Newton methods, sometimes called variable metric methods, iteratively build up an\napproximation to the Hessian using information gleaned from the gradient vector at each step. The\nmost common method is called BFGS(named after its simultaneous inventors, Broyden, Fletcher,\nGoldfarb and Shanno), which updates the approximation to the Hessian Bt\u0019Htas follows:\nBt+1=Bt+ytyT\nt\nyT\ntst\u0000(Btst)(Btst)T\nsT\ntBtst(8.44)\nst=\u0012t\u0000\u0012t\u00001 (8.45)\nyt=gt\u0000gt\u00001 (8.46)\nThis is a rank-two update to the matrix. If B0is positive-deﬁnite, and the step size \u001ais chosen\nvia line search satisfying both the Armijo condition in Equation (8.27) and the following curvature\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 728, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 929}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0729_bb2a2d73", "text": "August 27, 2021\n8.3. Second-order methods 285\nFigure 8.15: Illustration of the trust region approach. The dashed lines represents contours of the original\nnonconvex objective. The circles represent successive quadratic approximations. From Figure 4.2 of [Pas14]. Used with kind permission of Razvan Pascanu. condition\nrL(\u0012t+\u001adt)\u0015c2\u001adT\ntrL(\u0012t) (8.47)\nthen Bt+1will remain positive deﬁnite. The constant c2is chosen within (c;1)wherecis the\ntunable parameter in Equation (8.27). The two step size conditions are together known as the Wolfe\nconditions . We typically start with a diagonal approximation, B0=I. Thus BFGS can be thought\nof as a “diagonal plus low-rank” approximation to the Hessian.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 729, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 694}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0730_be0cc51a", "text": "We typically start with a diagonal approximation, B0=I. Thus BFGS can be thought\nof as a “diagonal plus low-rank” approximation to the Hessian. Alternatively, BFGS can iteratively update an approximation to the inverse Hessian, Ct\u0019H\u00001\nt,\nas follows:\nCt+1=\u0012\nI\u0000styT\nt\nyT\ntst\u0013\nCt\u0012\nI\u0000ytsT\nt\nyT\ntst\u0013\n+stsT\nt\nyT\ntst(8.48)\nSince storing the Hessian approximation still takes O(D2)space, for very large problems, one\ncan uselimited memory BFGS , orL-BFGS , where we control the rank of the approximation by\nonly using the Mmost recent (st;yt)pairs while ignoring older information. Rather than storing\nBtexplicitly, we just store these vectors in memory, and then approximate H\u00001\ntgtby performing a\nsequence of inner products with the stored standytvectors. The storage requirements are therefore\nO(MD). Typically choosing Mto be between 5–20 suﬃces for good performance [NW06, p177].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 730, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 876}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0731_9dc1576c", "text": "The storage requirements are therefore\nO(MD). Typically choosing Mto be between 5–20 suﬃces for good performance [NW06, p177]. Note that sklearn uses LBFGS as its default solver for logistic regression.1\n8.3.3 Trust region methods\nIf the objective function is nonconvex, then the Hessian Htmay not be positive deﬁnite, so dt=\n\u0000H\u00001\ntgtmay not be a descent direction. This is illustrated in 1d in Figure 8.14(b), which shows\nthat Newton’s method can end up in a local maximum rather than a local minimum. In general, any time the quadratic approximation made by Newton’s method becomes invalid, we\nare in trouble. However, there is usually a local region around the current iterate where we can safely\n1. See https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html . Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n286 Chapter 8. Optimization\napproximate the objective by a quadratic.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 731, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 933}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0732_f7e7e374", "text": "Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n286 Chapter 8. Optimization\napproximate the objective by a quadratic. Let us call this region Rt, and let us call M(\u000e)the model\n(or approximation) to the objective, where \u000e=\u0012\u0000\u0012t. Then at each step we can solve\n\u000e\u0003= argmin\n\u000e2RtMt(\u000e) (8.49)\nThis is called trust-region optimization . (This can be seen as the “opposite” of line search, in the\nsense that we pick a distance we want to travel, determined by Rt, and then solve for the optimal\ndirection, rather than picking the direction and then solving for the optimal distance.)\nWe usually assume that Mt(\u000e)is a quadratic approximation:\nMt(\u000e) =L(\u0012t) +gT\nt\u000e+1\n2\u000eTHt\u000e (8.50)\nwheregt=r\u0012L(\u0012)j\u0012tis the gradient, and Ht=r2\n\u0012L(\u0012)j\u0012tis the Hessian. Furthermore, it is common\nto assume thatRtis a ball of radius r, i.e.,Rt=f\u000e:jj\u000ejj2\u0014rg.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 732, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 835}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0733_5558ac5b", "text": "Furthermore, it is common\nto assume thatRtis a ball of radius r, i.e.,Rt=f\u000e:jj\u000ejj2\u0014rg. Using this, we can convert the\nconstrained problem into an unconstrained one as follows:\n\u000e\u0003= argmin\n\u000eM(\u000e) +\u0015jj\u000ejj2\n2= argmin\n\u000egT\u000e+1\n2\u000eT(H+\u0015I)\u000e (8.51)\nfor some Lagrange multiplier \u0015>0which depends on the radius r(see Section 8.5.1 for a discussion\nof Lagrange multipliers). We can solve this using\n\u000e=\u0000(H+\u0015I)\u00001g (8.52)\nThis is called Tikhonov damping orTikhonov regularization . See Figure 8.15 for an illustration. Note that adding a suﬃciently large \u0015ItoHensures the resulting matrix is always positive deﬁnite. As\u0015!0, this trust method reduces to Newton’s method, but for \u0015large enough, it will make all\nthe negative eigenvalues positive (and all the 0 eigenvalues become equal to \u0015). 8.4 Stochastic gradient descent\nIn this section, we consider stochastic optimization , where the goal is to minimize the average\nvalue of a function:\nL(\u0012) =Eq(z)[L(\u0012;z)] (8.53)\nwherezisarandominputtotheobjective.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 733, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 985}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0734_fddeb7f9", "text": "Thiscouldbea“noise” term, comingfromtheenvironment,\nor it could be a training example drawn randomly from the training set, as we explain below. At each iteration, we assume we observe Lt(\u0012) =L(\u0012;zt), wherezt\u0018q. We also assume a way to\ncompute an unbiased estimate of the gradient of L. If the distribution q(z)is independent of the\nparameters we are optimizing, we can use gt=r\u0012Lt(\u0012t). In this case, The resulting algorithm can\nbe written as follows:\n\u0012t+1=\u0012t\u0000\u001atrL(\u0012t;zt) =\u0012t\u0000\u001atgt (8.54)\nThis method is known as stochastic gradient descent orSGD. As long as the gradient estimate\nis unbiased, then this method will converge to a stationary point, providing we decay the step size \u001at\nat a certain rate, as we discuss in Section 8.4.3. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.4. Stochastic gradient descent 287\n8.4.1 Application to ﬁnite sum problems\nSGD is very widely used in machine learning.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 734, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 931}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0735_9e5c8fec", "text": "Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.4. Stochastic gradient descent 287\n8.4.1 Application to ﬁnite sum problems\nSGD is very widely used in machine learning. To see why, recall from Section 4.3 that many model\nﬁtting procedures are based on empirical risk minimization, which involve minimizing the following\nloss:\nL(\u0012t) =1\nNNX\nn=1`(yn;f(xn;\u0012t)) =1\nNNX\nn=1Ln(\u0012t) (8.55)\nThis is called a ﬁnite sum problem . The gradient of this objective has the form\ngt=1\nNNX\nn=1r\u0012Ln(\u0012t) =1\nNNX\nn=1r\u0012`(yn;f(xn;\u0012t)) (8.56)\nThis requires summing over all Ntraining examples, and thus can be slow if Nis large. Fortunately\nwe can approximate this by sampling a minibatch ofB\u001cNsamples to get\ngt\u00191\njBtjX\nn2Btr\u0012Ln(\u0012t) =1\njBtjX\nn2Btr\u0012`(yn;f(xn;\u0012t)) (8.57)\nwhereBtis a set of randomly chosen examples to use at iteration t.2This is an unbiased approximation\nto the empirical average in Equation (8.56). Hence we can safely use this with SGD.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 735, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 957}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0736_468f36a9", "text": "Hence we can safely use this with SGD. Although the theoretical rate of convergence of SGD is slower than batch GD (in particular, SGD\nhas a sublinear convergence rate), in practice SGD is often faster, since the per-step time is much\nlower [BB08; BB11]. To see why SGD can make faster progress than full batch GD, suppose we have\na dataset consisting of a single example duplicated Ktimes. Batch training will be (at least) Ktimes\nslower than SGD, since it will waste time computing the gradient for the repeated examples. Even if\nthere are no duplicates, batch training can be wasteful, since early on in training the parameters are\nnot well estimated, so it is not worth carefully evaluating the gradient. 8.4.2 Example: SGD for ﬁtting linear regression\nIn this section, we show how to use SGD to ﬁt a linear regression model. Recall from Section 4.2.7\nthat the objective has the form\nL(\u0012) =1\n2NNX\nn=1(xT\nn\u0012\u0000yn)2=1\n2NjjX\u0012\u0000yjj2\n2 (8.58)\nThe gradient is\ngt=1\nNNX\nn=1(\u0012T\ntxn\u0000yn)xn (8.59)\n2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 736, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 990}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0737_d38aa57f", "text": "Recall from Section 4.2.7\nthat the objective has the form\nL(\u0012) =1\n2NNX\nn=1(xT\nn\u0012\u0000yn)2=1\n2NjjX\u0012\u0000yjj2\n2 (8.58)\nThe gradient is\ngt=1\nNNX\nn=1(\u0012T\ntxn\u0000yn)xn (8.59)\n2. In practice we usually sample Btwithout replacement. However, once we reach the end of the dataset (i.e., after a\nsingle training epoch), we can perform a random shuﬄing of the examples, to ensure that each minibatch on the next\nepoch is diﬀerent from the last. This version of SGD is analyzed in [HS19]. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n288 Chapter 8. Optimization\n−1 0 1 2−1.0−0.50.00.51.01.52.02.5\nLMS trajectory\n(a)\n0 5 10 15 20 25 300.00.51.01.52.02.5\nRSS vs iteration (b)\nFigure 8.16: Illustration of the LMS algorithm. Left: we start from \u0012= (\u00000:5;2)and slowly converging to\nthe least squares solution of ^\u0012= (1:45;0:93)(red cross). Right: plot of objective function over time. Note\nthat it does not decrease monotonically. Generated by code at ﬁgures.probml.ai/book1/8.16.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 737, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 968}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0738_7e2ce654", "text": "Right: plot of objective function over time. Note\nthat it does not decrease monotonically. Generated by code at ﬁgures.probml.ai/book1/8.16. Now consider using SGD with a minibatch size of B= 1. The update becomes\n\u0012t+1=\u0012t\u0000\u001at(\u0012T\ntxn\u0000yn)xn (8.60)\nwheren=n(t)is the index of the example chosen at iteration t. The overall algorithm is called the\nleast mean squares (LMS) algorithm, and is also known as the delta rule , or theWidrow-Hoﬀ\nrule. Figure 8.16 shows the results of applying this algorithm to the data shown in Figure 11.2. We\nstart at\u0012= (\u00000:5;2)and converge (in the sense that jj\u0012t\u0000\u0012t\u00001jj2\n2drops below a threshold of 10\u00002)\nin about 26 iterations. Note that SGD (and hence LMS) may require multiple passes through the\ndata to ﬁnd the optimum. 8.4.3 Choosing the step size (learning rate)\nWhen using SGD, we need to be careful in how we choose the learning rate in order to achieve\nconvergence.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 738, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 901}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0739_67740110", "text": "8.4.3 Choosing the step size (learning rate)\nWhen using SGD, we need to be careful in how we choose the learning rate in order to achieve\nconvergence. For example, in Figure 8.17 we plot the loss vs the learning rate when we apply SGD\nto a deep neural network classiﬁer (see Chapter 13 for details). We see a U-shaped curve, where an\noverly small learning rate results in underﬁtting, and overly large learning rate results in instability\nof the model (c.f., Figure 8.11(b)); in both cases, we fail to converge to a local optimum. One heuristic for choosing a good learning rate, proposed in [Smi18], is to start with a small\nlearning rate and gradually increase it, evaluating performance using a small number of minibatches. We then make a plot like the one in Figure 8.17, and pick the learning rate with the lowest loss.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 739, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 824}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0740_5c732ebb", "text": "We then make a plot like the one in Figure 8.17, and pick the learning rate with the lowest loss. (In\npractice, it is better to pick a rate that is slightly smaller than (i.e., to the left of) the one with the\nlowest loss, to ensure stability.)\nRather than choosing a single constant learning rate, we can use a learning rate schedule , in\nwhich we adjust the step size over time. Theoretically, a suﬃcient condition for SGD to achieve\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.4. Stochastic gradient descent 289\n105\n104\n103\n102\n101\n100101\nLearning rate02468101214Loss\nFigure 8.17: Loss vs learning rate (horizontal axis). Training loss vs learning rate for a small MLP ﬁt to\nFashionMNIST using vanilla SGD. (Raw loss in blue, EWMA smoothed version in orange). Generated by\ncode at ﬁgures.probml.ai/book1/8.17.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 740, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 846}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0741_bfc76959", "text": "Training loss vs learning rate for a small MLP ﬁt to\nFashionMNIST using vanilla SGD. (Raw loss in blue, EWMA smoothed version in orange). Generated by\ncode at ﬁgures.probml.ai/book1/8.17. 0 20 40 60 80 1000.750.800.850.900.951.00piecewise constant\n(a)\n0 20 40 60 80 1000.00.20.40.60.81.0exponential decay (b)\n0 20 40 60 80 1000.20.40.60.81.0polynomial decay (c)\nFigure 8.18: Illustration of some common learning rate schedules. (a) Piecewise constant. (b) Exponential\ndecay. (c) Polynomial decay. Generated by code at ﬁgures.probml.ai/book1/8.18. convergence is if the learning rate schedule satisﬁes the Robbins-Monro conditions :\n\u001at!0;P1\nt=1\u001a2\ntP1\nt=1\u001at!0 (8.61)\nSome common examples of learning rate schedules are listed below:\n\u001at=\u001aiifti\u0014t\u0014ti+1piecewise constant (8.62)\n\u001at=\u001a0e\u0000\u0015texponential decay (8.63)\n\u001at=\u001a0(\ft+ 1)\u0000\u000bpolynomial decay (8.64)\nIn the piecewise constant schedule, tiare a set of time points at which we adjust the learning rate\nto a speciﬁed value.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 741, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 965}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0742_448f72fc", "text": "For example, we may set \u001ai=\u001a0\ri, which reduces the initial learning rate by a\nfactor of\rfor each threshold (or milestone) that we pass. Figure 8.18a illustrates this for \u001a0= 1\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n290 Chapter 8. Optimization\n(a)\n (b)\nFigure 8.19: (a) Linear warm-up followed by cosine cool-down. (b) Cyclical learning rate schedule. and\r= 0:9. This is called step decay . Sometimes the thresholds times are computed adaptively,\nby estimating when the train or validation loss has plateaued; this is called reduce-on-plateau . Exponential decay is typically too fast, as illustrated in Figure 8.18b. A common choice is polynomial\ndecay, with \u000b= 0:5and\f= 1, as illustrated in Figure 8.18c; this corresponds to a square-root\nschedule ,\u001at=\u001a01pt+1. In the deep learning community, another common schedule is to quickly increase the learning rate\nand then gradually decrease it again, as shown in Figure 8.19a.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 742, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 943}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0743_1e09c5a0", "text": "In the deep learning community, another common schedule is to quickly increase the learning rate\nand then gradually decrease it again, as shown in Figure 8.19a. This is called learning rate warmup ,\nor theone-cycle learning rate schedule [Smi18]. The motivation for this is the following: initially\nthe parameters may be in a part of the loss landscape that is poorly conditioned, so a large step size\nwill “bounce around” too much (c.f., Figure 8.11(b)) and fail to make progress downhill. However,\nwith a slow learning rate, the algorithm can discover ﬂatter regions of space, where a larger step size\ncan be used. Once there, fast progress can be made. However, to ensure convergence to a point, we\nmust reduce the learning rate to 0. See [Got+19] for more details. It is also possible to increase and decrease the learning rate multiple times, in a cyclical fashion. This is called a cyclical learning rate [Smi18], and was popularized by the fast.ai course.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 743, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 962}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0744_c6f05ede", "text": "It is also possible to increase and decrease the learning rate multiple times, in a cyclical fashion. This is called a cyclical learning rate [Smi18], and was popularized by the fast.ai course. See\nFigure 8.19b for an illustration using triangular shapes. The motivation behind this approach is to\nescape local minima. The minimum and maximum learning rates can be found based on the initial\n“dry run” described above, and the half-cycle can be chosen based on how many restarts you want to\ndo with your training budget. A related approach, known as stochastic gradient descent with\nwarm restarts , was proposed in [LH17]; they proposed storing all the checkpoints visited after each\ncool down, and using all of them as members of a model ensemble. (See Section 18.2 for a discussion\nof ensemble learning.)\nAn alternative to using heuristics for estimating the learning rate is to use line search (Sec-\ntion 8.2.2.2).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 744, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 917}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0745_02ea5c21", "text": "(See Section 18.2 for a discussion\nof ensemble learning.)\nAn alternative to using heuristics for estimating the learning rate is to use line search (Sec-\ntion 8.2.2.2). This is tricky when using SGD, because the noisy gradients make the computation of\nthe Armijo condition diﬃcult [CS20]. However, [Vas+19] show that it can be made to work if the\nvariance of the gradient noise goes to zero over time. This can happen if the model is suﬃciently\nﬂexible that it can perfectly interpolate the training set. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.4. Stochastic gradient descent 291\n8.4.4 Iterate averaging\nThe parameter estimates produced by SGD can be very unstable over time. To reduce the variance\nof the estimate, we can compute the average using\n\u0012t=1\nttX\ni=1\u0012i=1\nt\u0012t+t\u00001\nt\u0012t\u00001 (8.65)\nwhere\u0012tare the usual SGD iterates. This is called iterate averaging orPolyak-Ruppert\naveraging [Rup88].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 745, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 928}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0746_8951680a", "text": "This is called iterate averaging orPolyak-Ruppert\naveraging [Rup88]. In [PJ92], they prove that the estimate \u0012tachieves the best possible asymptotic convergence rate\namong SGD algorithms, matching that of variants using second-order information, such as Hessians. This averaging can also have statistical beneﬁts. For example, in [NR18], they prove that, in the\ncase of linear regression, this method is equivalent to `2regularization (i.e., ridge regression). Rather than an exponential moving average of SGD iterates, Stochastic Weight Averaging\n(SWA) [Izm+18] uses an equalaverage in conjunction with a modiﬁed learning rate schedule. In\ncontrast to standard Polyak-Ruppert averaging, which was motivated for faster convergence rates,\nSWA exploits the ﬂatness in objectives used to train deep neural networks, to ﬁnd solutions which\nprovide better generalization. 8.4.5 Variance reduction *\nIn this section, we discuss various ways to reduce the variance in SGD.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 746, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 965}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0747_2d7cb615", "text": "8.4.5 Variance reduction *\nIn this section, we discuss various ways to reduce the variance in SGD. In some cases, this can\nimprove the theoretical convergence rate from sublinear to linear (i.e., the same as full-batch gradient\ndescent) [SLRB17; JZ13; DBLJ14]. These methods reduce the variance of the gradients, rather than\nthe parameters themselves and are designed to work for ﬁnite sum problems. 8.4.5.1 SVRG\nThe basic idea of stochastic variance reduced gradient (SVRG) [JZ13] is to use a control\nvariate, in which we estimate a baseline value of the gradient based on the full batch, which we then\nuse to compare the stochastic gradients to. More precisely, ever so often (e.g., once per epoch), we compute the full gradient at a “snapshot”\nof the model parameters ~\u0012; the corresponding “exact” gradient is therefore rL(~\u0012). At stept, we\ncompute the usual stochastic gradient at the current parameters, rLt(\u0012t), but also at the snapshot\nparameters,rLt(~\u0012), which we use as a baseline.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 747, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 990}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0748_bdbad521", "text": "At stept, we\ncompute the usual stochastic gradient at the current parameters, rLt(\u0012t), but also at the snapshot\nparameters,rLt(~\u0012), which we use as a baseline. We can then use the following improved gradient\nestimate\ngt=rLt(\u0012t)\u0000rLt(~\u0012) +rL(~\u0012) (8.66)\nto compute\u0012t+1. This is unbiased because Eh\nrLt(~\u0012)i\n=rL(~\u0012). Furthermore, the update only\ninvolves two gradient computations, since we can compute rL(~\u0012)once per epoch. At the end of the\nepoch, we update the snapshot parameters, ~\u0012, based on the most recent value of \u0012t, or a running\naverage of the iterates, and update the expected baseline. (We can compute snapshots less often, but\nthen the baseline will not be correlated with the objective and can hurt performance, as shown in\n[DB18].)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n292 Chapter 8. Optimization\nIterations of SVRG are computationally faster than those of full-batch GD, but SVRG can still\nmatch the theoretical convergence rate of GD.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 748, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 971}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0749_d50bec8b", "text": "(C) MIT Press. CC-BY-NC-ND license\n292 Chapter 8. Optimization\nIterations of SVRG are computationally faster than those of full-batch GD, but SVRG can still\nmatch the theoretical convergence rate of GD. 8.4.5.2 SAGA\nIn this section, we describe the stochastic averaged gradient accelerated (SAGA) algorithm\nof [DBLJ14]. Unlike SVRG, it only requires one full batch gradient computation, at the start of\nthe algorithm. However, it “pays” for this saving in time by using more memory. In particular, it\nmust storeNgradient vectors. This enables the method to maintain an approximation of the global\ngradient by removing the old local gradient from the overall sum and replacing it with the new local\ngradient. This is called an aggregated gradient method. More precisely, we ﬁrst initialize by computing glocal\nn =rLn(\u00120)for alln, and the average,\ngavg=1\nNPN\nn=1glocal\nn.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 749, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 869}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0750_b08639cf", "text": "This is called an aggregated gradient method. More precisely, we ﬁrst initialize by computing glocal\nn =rLn(\u00120)for alln, and the average,\ngavg=1\nNPN\nn=1glocal\nn. Then, at iteration t, we use the gradient estimate\ngt=rLn(\u0012t)\u0000glocal\nn+gavg(8.67)\nwheren\u0018Uniff1;:::;Ngis the example index sampled at iteration t. We then update glocal\nn=\nrLn(\u0012t)andgavgby replacing the old glocal\nnby its new value. This has an advantage over SVRG since it only has to do one full batch sweep at the start. (In\nfact, the initial sweep is not necessary, since we can compute gavg“lazily”, by only incorporating\ngradients we have seen so far.) The downside is the large extra memory cost. However, if the features\n(and hence gradients) are sparse, the memory cost can be reasonable.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 750, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 759}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0751_2b0046d2", "text": "However, if the features\n(and hence gradients) are sparse, the memory cost can be reasonable. Indeed, the SAGA algorithm is\nrecommended for use in the sklearn logistic regression code when Nis large andxis sparse.3\n8.4.5.3 Application to deep learning\nVariance reduction methods are widely used for ﬁtting ML models with convex objectives, such as\nlinear models. However, there are various diﬃculties associated with using SVRG with conventional\ndeep learning training practices. For example, the use of batch normalization (Section 14.2.4.1), data\naugmentation (Section 19.1) and dropout (Section 13.5.4) all break the assumptions of the method,\nsince the loss will diﬀer randomly in ways that depend not just on the parameters and the data index\nn. For more details, see e.g., [DB18; Arn+19].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 751, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 794}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0752_12cdd054", "text": "For more details, see e.g., [DB18; Arn+19]. 8.4.6 Preconditioned SGD\nIn this section, we consider preconditioned SGD , which involves the following update:\n\u0012t+1=\u0012t\u0000\u001atM\u00001\ntgt; (8.68)\nwhere Mtis apreconditioning matrix , or simply the preconditioner , typically chosen to be\npositive-deﬁnite. Unfortunately the noise in the gradient estimates make it diﬃcult to reliably\nestimate the Hessian, which makes it diﬃcult to use the methods from Section 8.3. In addition,\nit is expensive to solve for the update direction with a full preconditioning matrix. Therefore\nmost practitioners use a diagonal preconditioner Mt. Such preconditioners do not necessarily use\nsecond-order information, but often result in speedups compared to vanilla SGD. (See also [Roo+21]\nfor a probabilitic interpretation of these heuristics.)\n3. See https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 752, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 979}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0753_ff285391", "text": "See https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression . Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.4. Stochastic gradient descent 293\n8.4.6.1 AdaGrad\nTheAdaGrad (short for “adaptive gradient”) method of [DHS11] was originally designed for\noptimizing convex objectives where many elements of the gradient vector are zero; these might\ncorrespond to features that are rarely present in the input, such as rare words. The update has the\nfollowing form\n\u0012t+1;d=\u0012t;d\u0000\u001at1pst;d+\u000fgt;d (8.69)\nwhere\nst;d=tX\nt=1g2\nt;d (8.70)\nis the sum of the squared gradients and \u000f>0is a small term to avoid dividing by zero. Equivalently\nwe can write the update in vector form as follows:\n\u0001\u0012t=\u0000\u001at1pst+\u000fgt (8.71)\nwhere the square root and division is performed elementwise. Viewed as preconditioned SGD, this is\nequivalent to taking Mt=diag(st+\u000f)1=2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 753, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 886}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0754_1b565656", "text": "Viewed as preconditioned SGD, this is\nequivalent to taking Mt=diag(st+\u000f)1=2. This is an example of an adaptive learning rate ; the\noverall stepsize \u001atstill needs to be chosen, but the results are less sensitive to it compared to vanilla\nGD. In particular, we usually ﬁx \u001at=\u001a0. 8.4.6.2 RMSProp andAdaDelta\nA deﬁning feature of AdaGrad is that the term in the denominator gets larger over time, so the\neﬀective learning rate drops. While it is necessary to ensure convergence, it might hurt performance\nas the denominator gets large too fast. An alternative is to use an exponentially weighted moving average (EWMA, Section 4.4.2.2) of the\npast squared gradients, rather than their sum:\nst+1;d=\fst;d+ (1\u0000\f)g2\nt;d (8.72)\nIn practice we usually use \f\u00180:9, which puts more weight on recent examples. In this case,\npst;d\u0019RMS(g1:t;d) =vuut1\nttX\n\u001c=1g2\n\u001c;d(8.73)\nwhere RMS stands for “root mean squared”. Hence this method, (which is based on the earlier\nRPROP method of [RB93]) is known as RMSProp [Hin14].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 754, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 998}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0755_32fbc205", "text": "Hence this method, (which is based on the earlier\nRPROP method of [RB93]) is known as RMSProp [Hin14]. The overall update of RMSProp\nis\n\u0001\u0012t=\u0000\u001at1pst+\u000fgt: (8.74)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n294 Chapter 8. Optimization\nTheAdaDelta method was independently introduced in [Zei12], and is similar to RMSprop. However, in addition to accumulating an EWMA of the gradients in ^s, it also keeps an EWMA of\nthe updates\u000etto obtain an update of the form\n\u0001\u0012t=\u0000\u001atp\n\u000et\u00001+\u000fpst+\u000fgt (8.75)\nwhere\n\u000et=\f\u000et\u00001+ (1\u0000\f)(\u0001\u0012t)2(8.76)\nandstis the same as in RMSProp . This has the advantage that the “units” of the numerator and\ndenominator cancel, so we are just elementwise-multiplying the gradient by a scalar. This eliminates\nthe need to tune the learning rate \u001at, which means one can simply set \u001at= 1, although popular\nimplementations of AdaDelta still keep\u001atas a tunable hyperparameter.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 755, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 895}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0756_d132f08e", "text": "This eliminates\nthe need to tune the learning rate \u001at, which means one can simply set \u001at= 1, although popular\nimplementations of AdaDelta still keep\u001atas a tunable hyperparameter. However, since these\nadaptive learning rates need not decrease with time (unless we choose \u001atto explicitly do so), these\nmethods are not guaranteed to converge to a solution. 8.4.6.3 Adam\nIt is possible to combine RMSProp with momentum. In particular, let us compute an EWMA of\nthe gradients (as in momentum) and squared gradients (as in RMSProp )\nmt=\f1mt\u00001+ (1\u0000\f1)gt (8.77)\nst=\f2st\u00001+ (1\u0000\f2)g2\nt (8.78)\nWe then perform the following update:\n\u0001\u0012t=\u0000\u001at1pst+\u000fmt (8.79)\nThe resulting method is known as Adam, which stands for “adaptive moment estimation” [KB15]. The standard values for the various constants are \f1= 0:9,\f2= 0:999and\u000f= 10\u00006. (If we set\n\f1= 0and no bias-correction, we recover RMSProp , which does not use momentum.) For the\noverall learning rate, it is common to use a ﬁxed value such as \u001at= 0:001.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 756, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 989}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0757_d0397fbe", "text": "(If we set\n\f1= 0and no bias-correction, we recover RMSProp , which does not use momentum.) For the\noverall learning rate, it is common to use a ﬁxed value such as \u001at= 0:001. Again, as the adaptive\nlearning rate may not decrease over time, convergence is not guaranteed (see Section 8.4.6.4). If we initialize with m0=s0=0, then initial estimates will be biased towards small values. The\nauthors therefore recommend using the bias-corrected moments, which increase the values early in\nthe optimization process. These estimates are given by\n^mt=mt=(1\u0000\ft\n1) (8.80)\n^st=st=(1\u0000\ft\n2) (8.81)\nThe advantage of bias-correction is shown in Figure 4.3. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.5. Constrained optimization 295\n8.4.6.4 Issues with adaptive learning rates\nWhen using diagonal scaling methods, the overall learning rate is determined by \u001a0M\u00001\nt, which\nchanges with time. Hence these methods are often called adaptive learning rate methods.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 757, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 978}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0758_2ae3ae69", "text": "Hence these methods are often called adaptive learning rate methods. However,\nthey still require setting the base learning rate \u001a0. Since the EWMA methods are typically used in the stochastic setting where the gradient estimates\nare noisy, their learning rate adaptation can result in non-convergence even on convex problems\n[RKK18]. Various solutions to this problem have been proposed, including AMSGrad [RKK18],\nPadam[CG18; Zho+18], and Yogi[Zah+18]. For example, the Yogiupdate modiﬁes Adamby\nreplacing\nst=\f2st\u00001+ (1\u0000\f2)g2\nt=st\u00001+ (1\u0000\f2)(g2\nt\u0000st\u00001) (8.82)\nwith\nst=st\u00001+ (1\u0000\f2)g2\nt\fsgn(g2\nt\u0000st\u00001) (8.83)\n8.4.6.5 Non-diagonal preconditioning matrices\nAlthough the methods we have discussed above can adapt the learning rate of each parameter, they\ndo not solve the more fundamental problem of ill-conditioning due to correlation of the parameters,\nand hence do not always provide as much of a speed boost over vanilla SGD as one may hope.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 758, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 940}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0759_5b1e400b", "text": "One way to get faster convergence is to use the following preconditioning matrix, known as\nfull-matrix Adagrad [DHS11]:\nMt= [(GtGT\nt)1\n2+\u000fID]\u00001(8.84)\nwhere\nGt= [gt;:::;g1] (8.85)\nHeregi=r\u0017c(\u0017i)is theD-dimensional gradient vector computed at step i. Unfortunately, Mtis a\nD\u0002Dmatrix, which is expensive to store and invert. TheShampoo algorithm [GKS18] makes a block diagonal approximation to M, one per layer\nof the model, and then exploits kronecker product structure to eﬃciently invert it. (It is called\n“shampoo” because it uses a conditioner.) Recently, [Ani+20] scaled this method up to ﬁt very large\ndeep models in record time. 8.5 Constrained optimization\nIn this section, we consider the following constrained optimization problem :\n\u0012\u0003= arg min\n\u00122CL(\u0012) (8.86)\nwhere the feasible set, or constraint set, is\nC=f\u00122RD:hi(\u0012) = 0;i2E; gj(\u0012)\u00140;j2Ig (8.87)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n296 Chapter 8.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 759, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 931}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0760_fda55ed6", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n296 Chapter 8. Optimization\n𝜃*= (1/2 , 1/2) 𝜃₁+ 𝜃₂=1𝜃₂𝜃₁f(𝜃)=𝜃₁²+𝜃₂²\n0𝜃₂−𝜃₁≤1−𝜃₂−𝜃₁≤1𝜃₁−𝜃₂≤1𝜃₁+𝜃₂≤1𝜃₂\n𝜃₁(3/2 , 1/8)\nFigure 8.20: Illustration of some constrained optimization problems. Red contours are the level sets of the\nobjective function L(\u0012). Optimal constrained solution is the black dot, (a) Blue line is the equality constraint\nh(\u0012) = 0. (b) Blue lines denote the inequality constraints j\u00121j+j\u00122j\u00141. (Compare to Figure 11.8 (left).)\nwhereEis the set of equality constraints , andIis the set of inequality constraints . For example, suppose we have a quadratic objective, L(\u0012) =\u00122\n1+\u00122\n2, subject to a linear equality\nconstraint, h(\u0012) = 1\u0000\u00121\u0000\u00122= 0. Figure 8.20(a) plots the level sets of f, as well as the constraint\nsurface. What we are trying to do is ﬁnd the point \u0012\u0003that lives on the line, but which is closest to\nthe origin. It is clear from the geometry that the optimal solution is \u0012= (0:5;0:5), indicated by the\nsolid black dot.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 760, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 987}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0761_84da8a86", "text": "It is clear from the geometry that the optimal solution is \u0012= (0:5;0:5), indicated by the\nsolid black dot. In the following sections, we brieﬂy describe some of the theory and algorithms underlying\nconstrained optimization. More details can be found in other books, such as [BV04; NW06; Ber15;\nBer16]. 8.5.1 Lagrange multipliers\nIn this section, we discuss how to solve equality contrained optimization problems. We initially\nassume that we have just one equality constraint, h(\u0012) = 0. First note that for any point on the constraint surface, rh(\u0012)will be orthogonal to the constraint\nsurface. To see why, consider another point nearby, \u0012+\u000f, that also lies on the surface. If we make a\nﬁrst-order Taylor expansion around \u0012we have\nh(\u0012+\u000f)\u0019h(\u0012) +\u000fTrh(\u0012) (8.88)\nSince both\u0012and\u0012+\u000fare on the constraint surface, we must have h(\u0012) =h(\u0012+\u000f)and hence\n\u000fTrh(\u0012)\u00190. Since\u000fis parallel to the constraint surface, rh(\u0012)must be perpendicular to it. We seek a point \u0012\u0003on the constraint surface such that L(\u0012)is minimized.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 761, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1002}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0762_0fb43291", "text": "Since\u000fis parallel to the constraint surface, rh(\u0012)must be perpendicular to it. We seek a point \u0012\u0003on the constraint surface such that L(\u0012)is minimized. We just showed that it\nmust satisfy the condition that rh(\u0012\u0003)is orthogonal to the constraint surface. In addition, such a\npoint must have the property that rL(\u0012)is also orthogonal to the constraint surface, as otherwise\nwe could decrease L(\u0012)by moving a short distance along the constraint surface. Since both rh(\u0012)\nandrL(\u0012)are orthogonal to the constraint surface at \u0012\u0003, they must be parallel (or anti-parallel) to\neach other. Hence there must exist a constant \u0015\u00032Rsuch that\nrL(\u0012\u0003) =\u0015\u0003rh(\u0012\u0003) (8.89)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.5. Constrained optimization 297\n(We cannot just equate the gradient vectors, since they may have diﬀerent magnitudes.) The constant\n\u0015\u0003is called a Lagrange multiplier , and can be positive, negative, or zero. This latter case occurs\nwhenrf(\u0012\u0003) = 0.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 762, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 976}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0763_dc462d1a", "text": "This latter case occurs\nwhenrf(\u0012\u0003) = 0. We can convert Equation (8.89) into an objective, known as the Lagrangian , that we should ﬁnd\na stationary point of the following:\nL(\u0012;\u0015),L(\u0012) +\u0015h(\u0012) (8.90)\nAt a stationary point of the Lagrangian, we have\nr\u0012;\u0015L(\u0012;\u0015) =0()\u0015r\u0012h(\u0012) =rL(\u0012); h(\u0012) = 0 (8.91)\nThis is called a critical point , and satisﬁes the original constraint h(\u0012) = 0and Equation (8.89). If we havem> 1constraints, we can form a new constraint function by addition, as follows:\nL(\u0012;\u0015) =L(\u0012) +mX\nj=1\u0015jhj(\u0012) (8.92)\nWe now have D+mequations in D+munknowns and we can use standard unconstrained optimization\nmethods to ﬁnd a stationary point. We give some examples below. 8.5.1.1 Example: 2d Quadratic objective with one linear equality constraint\nConsider minimizing L(\u0012) =\u00122\n1+\u00122\n2subject to the constraint that \u00121+\u00122= 1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 763, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 825}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0764_67334223", "text": "We give some examples below. 8.5.1.1 Example: 2d Quadratic objective with one linear equality constraint\nConsider minimizing L(\u0012) =\u00122\n1+\u00122\n2subject to the constraint that \u00121+\u00122= 1. (This is the problem illustrated in Figure 8.20(a).) The Lagrangian is\nL(\u00121;\u00122;\u0015) =\u00122\n1+\u00122\n2+\u0015(\u00121+\u00122\u00001) (8.93)\nWe have the following conditions for a stationary point:\n@\n@\u00121L(\u00121;\u00122;\u0015) = 2\u00121+\u0015= 0 (8.94)\n@\n@\u00122L(\u00121;\u00122;\u0015) = 2\u00122+\u0015= 0 (8.95)\n@\n@\u0015L(\u00121;\u00122;\u0015) =\u00121+\u00122\u00001 = 0 (8.96)\nFrom Equations 8.94 and 8.95 we ﬁnd 2\u00121=\u0000\u0015= 2\u00122, so\u00121=\u00122. Also, from Equation (8.96), we\nﬁnd2\u00121= 1. So\u0012\u0003= (0:5;0:5), as we claimed earlier. Furthermore, this is the global minimum since\nthe objective is convex and the constraint is aﬃne. 8.5.2 The KKT conditions\nIn this section, we generalize the concept of Lagrange multipliers to additionally handle inequality\nconstraints. First consider the case where we have a single inequality constraint g(\u0012)\u00140.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 764, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 905}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0765_e91c6044", "text": "First consider the case where we have a single inequality constraint g(\u0012)\u00140. To ﬁnd the optimum,\none approach would be to consider an unconstrained problem where we add the penalty as an inﬁnite\nstep function:\n^L(\u0012) =L(\u0012) +1I(g(\u0012)>0) (8.97)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n298 Chapter 8. Optimization\nHowever, this is a discontinuous function that is hard to optimize. Instead, we create a lower bound of the form \u0016g(\u0012), where\u0016\u00150. This gives us the following\nLagrangian:\nL(\u0012;\u0016) =L(\u0012) +\u0016g(\u0012) (8.98)\nNote that the step function can be recovered using\n^L(\u0012) = max\n\u0016\u00150L(\u0012;\u0016) =(\n1ifg(\u0012)>0;\nL(\u0012)otherwise(8.99)\nThus our optimization problem becomes\nmin\n\u0012max\n\u0016\u00150L(\u0012;\u0016) (8.100)\nNow consider the general case where we have multiple inequality constraints, g(\u0012)\u00140, and\nmultiple equality constraints, h(\u0012) =0.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 765, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 826}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0766_aefe7f95", "text": "Thegeneralized Lagrangian becomes\nL(\u0012;\u0016;\u0015) =L(\u0012) +X\ni\u0016igi(\u0012) +X\nj\u0015jhj(\u0012) (8.101)\n(We are free to change \u0000\u0015jhjto+\u0015jhjsince the sign is arbitrary.) Our optimization problem\nbecomes\nmin\n\u0012max\n\u0016\u00150;\u0015L(\u0012;\u0016;\u0015) (8.102)\nWhenLaregare convex, then all critical points of this problem must satisfy the following criteria\n(under some conditions [BV04, Sec.5.2.3]):\n•All constraints are satisﬁed (this is called feasibility ):\ng(\u0012)\u00140;h(\u0012) =0 (8.103)\n•The solution is a stationary point:\nrL(\u0012\u0003) +X\ni\u0016irgi(\u0012\u0003) +X\nj\u0015irhj(\u0012\u0003) =0 (8.104)\n•The penalty for the inequality constraint points in the right direction (this is called dual feasi-\nbility):\n\u0016\u00150 (8.105)\n•The Lagrange multipliers pick up any slack in the inactive constraints, i.e., either \u0016i= 0or\ngi(\u0012\u0003) = 0, so\n\u0016\fg=0 (8.106)\nThis is called complementary slackness . Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 766, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 884}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0767_28622288", "text": "Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.5. Constrained optimization 299\nxx\n(a)(b)\nFigure 8.21: (a) A convex polytope in 2d deﬁned by the intersection of linear constraints. (b) Depiction of\nthe feasible set as well as the linear objective function. The red line is a level set of the objective, and the\narrow indicates the direction in which it is improving. We see that the optimal solution lies at a vertex of the\npolytope. To see why the last condition holds, consider (for simplicity) the case of a single inequality\nconstraint, g(\u0012)\u00140. Either it is active, meaningg(\u0012) = 0, or it is inactive, meaning g(\u0012)<0. In the active case, the solution lies on the constraint boundary, and g(\u0012) = 0becomes an equality\nconstraint; then we have rL=\u0016rgfor some constant \u00166= 0, because of Equation (8.89). In the\ninactive case, the solution is not on the constraint boundary; we still have rL=\u0016rg, but now\n\u0016= 0. These are called called the Karush-Kuhn-Tucker (KKT) conditions.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 767, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1004}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0768_d378eac4", "text": "In the\ninactive case, the solution is not on the constraint boundary; we still have rL=\u0016rg, but now\n\u0016= 0. These are called called the Karush-Kuhn-Tucker (KKT) conditions. IfLis a convex function,\nand the constraints deﬁne a convex set, the KKT conditions are suﬃcient for (global) optimality, as\nwell as necessary. 8.5.3 Linear programming\nConsider optimizing a linear function subject to linear constraints. When written in standard form ,\nthis can be represented as\nmin\n\u0012cT\u0012 s:t:A\u0012\u0014b;\u0012\u00150 (8.107)\nThe feasible set deﬁnes a convex polytope , which is a convex set deﬁned as the intersection of\nhalf spaces. See Figure 8.21(a) for a 2d example. Figure 8.21(b) shows a linear cost function that\ndecreases as we move to the bottom right. We see that the lowest point that is in the feasible set is a\nvertex. In fact, it can be proved that the optimum point always occurs at a vertex of the polytope,\nassuming the solution is unique. If there are multiple solutions, the line will be parallel to a face.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 768, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 999}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0769_19d7064a", "text": "In fact, it can be proved that the optimum point always occurs at a vertex of the polytope,\nassuming the solution is unique. If there are multiple solutions, the line will be parallel to a face. There may also be no optima inside the feasible set; in this case, the problem is said to be infeasible. 8.5.3.1 The simplex algorithm\nIt can be shown that the optima of an LP occur at vertices of the polytope deﬁning the feasible set\n(see Figure 8.21(b) for an example). The simplex algorithm solves LPs by moving from vertex to\nvertex, each time seeking the edge which most improves the objective. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n300 Chapter 8. Optimization\nIn the worst-case scenario, the simplex algorithm can take time exponential in D, although in\npractice it is usually very eﬃcient. There are also various polynomial-time algorithms, such as the\ninterior point method, although these are often slower in practice.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 769, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 944}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0770_e8ae7436", "text": "There are also various polynomial-time algorithms, such as the\ninterior point method, although these are often slower in practice. 8.5.3.2 Applications\nThere are many applications of linear programming in science, engineering and business. It is also\nuseful in some machine learning problems. For example, Section 11.6.1.1 shows how to use it to solve\nrobust linear regression. It is also useful for state estimation in graphical models (see e.g., [SGJ11]). 8.5.4 Quadratic programming\nConsider minimizing a quadratic objective subject to linear equality and inequality constraints. This\nkind of problem is known as a quadratic program orQP, and can be written as follows:\nmin\n\u00121\n2\u0012TH\u0012+cT\u0012 s:t:A\u0012\u0014b;Aeq\u0012=beq (8.108)\nIfHis positive semideﬁnite, then this is a convex optimization problem.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 770, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 787}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0771_9c49fe70", "text": "8.5.4.1 Example: 2d quadratic objective with linear inequality constraints\nAs a concrete example, suppose we want to minimize\nL(\u0012) = (\u00121\u00003\n2)2+ (\u00122\u00001\n8)2=1\n2\u0012TH\u0012+cT\u0012+ const (8.109)\nwhere H= 2Iandc=\u0000(3;1=4), subject to\nj\u00121j+j\u00122j\u00141 (8.110)\nSee Figure 8.20(b) for an illustration. We can rewrite the constraints as\n\u00121+\u00122\u00141; \u00121\u0000\u00122\u00141;\u0000\u00121+\u00122\u00141;\u0000\u00121\u0000\u00122\u00141 (8.111)\nwhich we can write more compactly as\nA\u0012\u0000b\u00140 (8.112)\nwhereb=1and\nA=0\nBB@1 1\n1\u00001\n\u00001 1\n\u00001\u000011\nCCA(8.113)\nThis is now in the standard QP form. From the geometry of the problem, shown in Figure 8.20(b), we see that the constraints corre-\nsponding to the two left faces of the diamond) are inactive (since we are trying to get as close to the\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.6. Proximal gradient method * 301\ncenter of the circle as possible, which is outside of, and to the right of, the constrained feasible region).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 771, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 912}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0772_ce447c59", "text": "August 27, 2021\n8.6. Proximal gradient method * 301\ncenter of the circle as possible, which is outside of, and to the right of, the constrained feasible region). Denotinggi(\u0012)as the inequality constraint corresponding to row iofA, this means g3(\u0012\u0003)>0and\ng4(\u0012\u0003)>0, and hence, by complementarity, \u0016\u0003\n3=\u0016\u0003\n4= 0. We can therefore remove these inactive\nconstraints. From the KKT conditions we know that\nH\u0012+c+AT\u0016=0 (8.114)\nUsing these for the actively constrained subproblem, we get\n0\nBB@2 0 1 1\n0 2 1\u00001\n1 1 0 0\n1\u00001 0 01\nCCA0\nBB@\u00121\n\u00122\n\u00161\n\u001621\nCCA=0\nBB@3\n1=4\n1\n11\nCCA(8.115)\nHence the solution is\n\u0012\u0003= (1;0)T;\u0016\u0003= (0:625;0:375;0;0)T(8.116)\nNotice that the optimal value of \u0012occurs at one of the vertices of the `1“ball” (the diamond shape). 8.5.4.2 Applications\nThere are several applications of quadratic programming in ML. In Section 11.4, we show how to use\nit for sparse linear regression. And in Section 17.3, we show how to use it for SVMs (support vector\nmachines).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 772, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 962}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0773_33e37783", "text": "In Section 11.4, we show how to use\nit for sparse linear regression. And in Section 17.3, we show how to use it for SVMs (support vector\nmachines). 8.5.5 Mixed integer linear programming *\nInteger linear programming orILPcorresponds to minimizing a linear objective, subject to\nlinear constraints, where the optimization variables are discrete integers instead of reals. In standard\nform, the problem is as follows:\nmin\n\u0012cT\u0012 s:t:A\u0012\u0014b;\u0012\u00150;\u00122ZD(8.117)\nwhereZis the set of integers. If some of the optimization variables are real-valued, it is called a\nmixed ILP , often called a MIPfor short. (If all of the variables are real-valued, it becomes a\nstandard LP.)\nMIPs have a large number of applications, such as in vehicle routing, scheduling and packing. They are also useful for some ML applications, such as formally verifying the behavior of certain\nkinds of deep neural networks [And+18], and proving robustness properties of DNNs to adversarial\n(worst-case) perturbations [TXT19].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 773, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 984}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0774_a372ccb8", "text": "8.6 Proximal gradient method *\nWe are often interested in optimizing an objective of the form\nL(\u0012) =Ls(\u0012) +Lr(\u0012) (8.118)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n302 Chapter 8. Optimization\nwhereLsis diﬀerentiable (smooth), and Lris convex but not necessarily diﬀerentiable (i.e., it may be\nnon-smooth or “rough”). For example, Lsmight be the negative log likelihood (NLL), and Lrmight\nbe an indicator function that is inﬁnite if a constraint is violated (see Section 8.6.1), or Lrmight be\nthe`1norm of some parameters (see Section 8.6.2), or Lrmight measure how far the parameters are\nfrom a set of allowed quantized values (see Section 8.6.3). One way to tackle such problems is to use the proximal gradient method (see e.g., [PB+14;\nPSW15]). Roughly speaking, this takes a step of size \u001ain the direction of the gradient, and then\nprojects the resulting parameter update into a space that respects Lr.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 774, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 922}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0775_3bfa7ca1", "text": "Roughly speaking, this takes a step of size \u001ain the direction of the gradient, and then\nprojects the resulting parameter update into a space that respects Lr. More precisely, the update is\nas follows\n\u0012t+1= prox\u001atLr(\u0012t\u0000\u001atrLs(\u0012t)) (8.119)\nwhere prox\u001af(\u0012)is theproximal operator ofLr(scaled by\u0011) evaluated at \u0012:\nprox\u0011Lr(\u0012),argmin\nz\u0012\nLr(z) +1\n2\u0011jjz\u0000\u0012jj2\n2\u0013\n(8.120)\n(The factor of1\n2is an arbitrary convention.) We can rewrite the proximal operator as solving a\nconstrained opimtization problem, as follows:\nprox\u0011Lr(\u0012) = argmin\nzLr(z) s:t:jjz\u0000\u0012jj2\u0014\u001a (8.121)\nwhere the bound \u001adepends on the scaling factor \u0011. Thus we see that the proximal projection\nminimizes the function while staying close to (i.e., proximal to) the current iterate. We give some\nexamples below. 8.6.1 Projected gradient descent\nSuppose we want to solve the problem\nargmin\n\u0012Ls(\u0012) s:t:\u00122C (8.122)\nwhereCis a convex set. For example, we may have the box constraints C=f\u0012:l\u0014\u0012\u0014ug,\nwhere we specify lower and upper bounds on each element.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 775, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 997}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0776_a88ffab6", "text": "For example, we may have the box constraints C=f\u0012:l\u0014\u0012\u0014ug,\nwhere we specify lower and upper bounds on each element. These bounds can be inﬁnite for certain\nelements if we don’t want to constrain values along that dimension. For example, if we just want to\nensure the parameters are non-negative, we set ld= 0andud=1for each dimension d. We can convert the constrained optimization problem into an unconstrained one by adding a\npenalty term to the original objective:\nL(\u0012) =Ls(\u0012) +Lr(\u0012) (8.123)\nwhereLr(\u0012)is the indicator function for the convex set C, i.e.,\nLr(\u0012) =IC(\u0012) =(\n0if\u00122C\n1if\u001262C(8.124)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.6. Proximal gradient method * 303\nC⇢RDww0PC(w0)\u0000rL(w)\nFigure 8.22: Illustration of projected gradient descent. wis the current parameter estimate, w0is the update\nafter a gradient step, and PC(w0)projects this onto the constraint set C. From https: // bit. ly/ 3eJ3BhZ\nUsed with kind permission of Martin Jaggi.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 776, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 984}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0777_648138a0", "text": "wis the current parameter estimate, w0is the update\nafter a gradient step, and PC(w0)projects this onto the constraint set C. From https: // bit. ly/ 3eJ3BhZ\nUsed with kind permission of Martin Jaggi. We can use proximal gradient descent to solve Equation (8.123). The proximal operator for the\nindicator function is equivalent to projection onto the set C:\nprojC(\u0012) = argmin\n\u001202Cjj\u00120\u0000\u0012jj2 (8.125)\nThis method is known as projected gradient descent . See Figure 8.22 for an illustration. For example, consider the box constraints C=f\u0012:l\u0014\u0012\u0014ug. The projection operator in this\ncase can be computed elementwise by simply thresholding at the boundaries:\nprojC(\u0012)d=8\n><\n>:ldif\u0012d\u0014ld\nxdifld\u0014\u0012d\u0014ud\nudif\u0012d\u0015ud(8.126)\nFor example, if we want to ensure all elements are non-negative, we can use\nprojC(\u0012) =\u0012+= [max(\u00121;0);:::; max(\u0012D;0)] (8.127)\nSee Section 11.4.9.2 for an application of this method to sparse linear regression.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 777, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 915}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0778_3c3bbaf5", "text": "8.6.2 Proximal operator for `1-norm regularizer\nConsider a linear predictor of the form f(x;\u0012) =PD\nd=1\u0012dxd. If we have \u0012d= 0for any dimension\nd, we ignore the corresponding feature xd. This is a form of feature selection , which can be\nuseful both as a way to reduce overﬁtting as well as way to improve model interpretability. We can\nencourage weights to be zero (and not just small) by penalizing the `1norm,\njj\u0012jj1=DX\nd=1j\u0012dj (8.128)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n304 Chapter 8. Optimization\nThis is called a sparsity inducing regularizer . To see why this induces sparsity, consider two possible parameter vectors, one which is sparse,\n\u0012= (1;0), and one which is non-sparse, \u00120= (1=p\n2;1=p\n2). Both have the same `2norm\njj(1;0)jj2\n2=jj(1=p\n2;1=p\n2)jj2\n2= 1 (8.129)\nHence`2regularization (Section 4.5.3) will not favor the sparse solution over the dense solution.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 778, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 896}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0779_f3b71878", "text": "Both have the same `2norm\njj(1;0)jj2\n2=jj(1=p\n2;1=p\n2)jj2\n2= 1 (8.129)\nHence`2regularization (Section 4.5.3) will not favor the sparse solution over the dense solution. However, when using `1regularization, the sparse solution is cheaper, since\njj(1;0)jj1= 1<jj(1=p\n2;1=p\n2)jj1=p\n2 (8.130)\nSee Section 11.4 for more details on sparse regression. If we combine this regularizer with our smooth loss, we get\nL(\u0012) = NLL(\u0012) +\u0015jj\u0012jj1 (8.131)\nWe can optimize this objective using proximal gradient descent. The key question is how to compute\nthe prox operator for the function f(\u0012) =jj\u0012jj1. Since this function decomposes over dimensions d,\nthe proximal projection can be computed componentwise.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 779, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 689}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0780_f14eaba7", "text": "The key question is how to compute\nthe prox operator for the function f(\u0012) =jj\u0012jj1. Since this function decomposes over dimensions d,\nthe proximal projection can be computed componentwise. From Equation (8.120), with \u001a= 1, we\nhave\nprox\u0015f(\u0012) = argmin\n\u00120j\u00120j+1\n2\u0015(z\u0000\u0012)2= argmin\n\u00120\u0015j\u00120j+1\n2(z\u0000\u0012)2(8.132)\nIn Section 11.4.3, we show that the solution to this is given by\nprox\u0015f(\u0012) =8\n><\n>:\u0012\u0000\u0015if\u0012\u0015\u0015\n0ifj\u0012j\u0014\u0015\n\u0012+\u0015if\u0012\u0014\u0000\u0015(8.133)\nThis is known as the soft thresholding operator , since values less than \u0015in absolute value are\nset to 0 (thresholded), but in a continuous way. Note that soft thresholding can be written more\ncompactly as\nSoftThreshold( \u0012;\u0015) = sign(\u0012) (j\u0012j\u0000\u0015)+(8.134)\nwhere\u0012+= max(\u0012;0)is the positive part of \u0012. In the vector case, we perform this elementwise:\nSoftThreshold( \u0012;\u0015) = sign(\u0012)\f(j\u0012j\u0000\u0015)+(8.135)\nSee Section 11.4.9.3 for an application of this method to sparse linear regression.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 780, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 893}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0781_ee702401", "text": "In the vector case, we perform this elementwise:\nSoftThreshold( \u0012;\u0015) = sign(\u0012)\f(j\u0012j\u0000\u0015)+(8.135)\nSee Section 11.4.9.3 for an application of this method to sparse linear regression. 8.6.3 Proximal operator for quantization\nIn some applications (e.g., when training deep neural networks to run on memory-limited edge\ndevices, such as mobile phones) we want to ensure that the parameters are quantized . For\nexample, in the extreme case where each parameter can only be -1 or +1, the state space becomes\nC=f\u00001;+1gD. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 781, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 591}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0782_b2ce3db2", "text": "For\nexample, in the extreme case where each parameter can only be -1 or +1, the state space becomes\nC=f\u00001;+1gD. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.6. Proximal gradient method * 305\nLet us deﬁne a regularizer that measures distance to the nearest quantized version of the parameter\nvector:\nLr(\u0012) = inf\n\u001202Cjj\u0012\u0000\u00120jj1 (8.136)\n(We could also use the `2norm.) In the case of C=f\u00001;+1gD, this becomes\nLr(\u0012) =DX\nd=1inf\n[\u00120]d2f\u00061gj\u0012d\u0000[\u00120]dj=DX\nd=1minfj\u0012d\u00001j;j\u0012d+ 1jg=jj\u0012\u0000sign(\u0012)jj1 (8.137)\nLet us deﬁne the corresponding quantization operator to be\nq(\u0012) = projC(\u0012) = argminLr(\u0012) = sign(\u0012) (8.138)\nThe core diﬃculty with quantized learning is that quantization is not a diﬀerentiable operation. A\npopular solution to this is to use the straight-through estimator , which uses the approximation\n@L\n@q(\u0012)\u0019@L\n@\u0012(see e.g., [Yin+19]).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 782, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 863}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0783_509fcf2a", "text": "A\npopular solution to this is to use the straight-through estimator , which uses the approximation\n@L\n@q(\u0012)\u0019@L\n@\u0012(see e.g., [Yin+19]). The corresponding update can be done in two steps: ﬁrst compute the\ngradient vector at the quantized version of the current parameters, and then update the unconstrained\nparameters using this approximate gradient:\n~\u0012t= projC(\u0012t) =q(\u0012t) (8.139)\n\u0012t+1=\u0012t\u0000\u001atrLs(~\u0012t) (8.140)\nWhen applied to C=f\u00001;+1gD, this is known as the binary connect method [CBD15]. We can get better results using proximal gradient descent, in which we treat quantization as a\nregularizer, rather than a hard constraint; this is known as ProxQuant [BWL19].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 783, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 660}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0784_8b25d91b", "text": "We can get better results using proximal gradient descent, in which we treat quantization as a\nregularizer, rather than a hard constraint; this is known as ProxQuant [BWL19]. The update\nbecomes\n~\u0012t= prox\u0015Lr(\u0012t\u0000\u001atrLs(\u0012t)) (8.141)\nIn the case thatC=f\u00001;+1gD, one can show that the proximal operator is a generalization of the\nsoft thresholding operator in Equation (8.135):\nprox\u0015Lr(\u0012) = SoftThreshold( \u0012;\u0015;sign(\u0012)) (8.142)\n= sign(\u0012) + sign(\u0012\u0000sign(\u0012))\f(j\u0012\u0000sign(\u0012)j\u0000\u0015)+ (8.143)\nThis can be generalized to other forms of quantization; see [Yin+19] for details. 8.6.4 Incremental (online) proximal methods\nMany ML problems have an objective function which is a sum of losses, one per example. Such\nproblems can be solved incrementally; this is a special case of online learning . It is possible to\nextend proximal methods to this setting. For a probabilistic perspective on such methods (in terms\nof Kalman ﬁltering), see [AEM18; Aky+19]. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n306 Chapter 8.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 784, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1007}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0785_494542fd", "text": "For a probabilistic perspective on such methods (in terms\nof Kalman ﬁltering), see [AEM18; Aky+19]. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n306 Chapter 8. Optimization\n8.7 Bound optimization *\nIn this section, we consider a class of algorithms known as bound optimization orMMalgorithms. In the context of minimization, MM stands for majorize-minimize . In the context of maximization,\nMM stands for minorize-maximize . We will discuss a special case of MM, known as expectation\nmaximization orEM, in Section 8.7.2. 8.7.1 The general algorithm\nIn this section, we give a brief outline of MM methods. (More details can be found in e.g., [HL04;\nMai15; SBP17; Nad+19].) To be consistent with the literature, we assume our goal is to maximize\nsome function LL(\u0012), such as the log likelihood, wrt its parameters \u0012. The basic approach in MM\nalgorithms is to construct a surrogate function Q(\u0012;\u0012t)which is a tight lowerbound to LL(\u0012)\nsuch thatQ(\u0012;\u0012t)\u0014LL(\u0012)andQ(\u0012t;\u0012t) =LL(\u0012t).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 785, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 989}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0786_484a8cfd", "text": "The basic approach in MM\nalgorithms is to construct a surrogate function Q(\u0012;\u0012t)which is a tight lowerbound to LL(\u0012)\nsuch thatQ(\u0012;\u0012t)\u0014LL(\u0012)andQ(\u0012t;\u0012t) =LL(\u0012t). If these conditions are met, we say that Q\nminorizesLL. We then perform the following update at each step:\n\u0012t+1= argmax\n\u0012Q(\u0012;\u0012t) (8.144)\nThis guarantees us monotonic increases in the original objective:\nLL(\u0012t+1)\u0015Q(\u0012t+1;\u0012t)\u0015Q(\u0012t;\u0012t) =LL(\u0012t) (8.145)\nwhere the ﬁrst inequality follows since Q(\u0012t+1;\u00120)is a lower bound on LL(\u0012t+1)for any\u00120; the\nsecond inequality follows from Equation (8.144); and the ﬁnal equality follows the tightness property. As a consequence of this result, if you do not observe monotonic increase of the objective, you must\nhave an error in your math and/or code. This is a surprisingly powerful debugging tool. This process is sketched in Figure 8.23. The dashed red curve is the original function (e.g., the\nlog-likelihood of the observed data).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 786, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 928}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0787_93202aaa", "text": "This is a surprisingly powerful debugging tool. This process is sketched in Figure 8.23. The dashed red curve is the original function (e.g., the\nlog-likelihood of the observed data). The solid blue curve is the lower bound, evaluated at \u0012t; this\ntouches the objective function at \u0012t. We then set \u0012t+1to the maximum of the lower bound (blue\ncurve), and ﬁt a new bound at that point (dotted green curve). The maximum of this new bound\nbecomes\u0012t+2, etc. IfQis a quadratic lower bound, the overall method is similar to Newton’s method, which repeatedly\nﬁts and then optimizes a quadratic approximation, as shown in Figure 8.14(a). The diﬀerence is that\noptimizingQis guaranteed to lead to an improvement in the objective, even if it is not convex, whereas\nNewton’s method may overshoot or lead to a decrease in the objective, as shown in Figure 8.24, since\nit is a quadratic approximation and not a bound.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 787, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 902}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0788_7b76de07", "text": "8.7.2 The EM algorithm\nIn this section, we discuss the expectation maximization (EM) algorithm [DLR77; MK97], which\nis a bound optimization algorithm designed to compute the MLE or MAP parameter estimate for\nprobability models that have missing data and/orhidden variables . We letynbe the visible\ndata for example n, andznbe the hidden data. The basic idea behind EM is to alternate between estimating the hidden variables (or missing\nvalues) during the E step(expectation step), and then using the fully observed data to compute the\nMLE during the M step (maximization step). Of course, we need to iterate this process, since the\nexpected values depend on the parameters, but the parameters depend on the expected values. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.7. Bound optimization * 307\n \nQ(θ,θt)\nQ(θ,θt+1)\nl(θ)\nθtθt+1θt+2\nFigure 8.23: Illustration of a bound optimization algorithm. Adapted from Figure 9.14 of [Bis06].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 788, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 963}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0789_fcdb77c8", "text": "August 27, 2021\n8.7. Bound optimization * 307\n \nQ(θ,θt)\nQ(θ,θt+1)\nl(θ)\nθtθt+1θt+2\nFigure 8.23: Illustration of a bound optimization algorithm. Adapted from Figure 9.14 of [Bis06]. Generated\nby code at ﬁgures.probml.ai/book1/8.23. Figure 8.24: The quadratic lower bound of an MM algorithm (solid) and the quadratic approximation of\nNewton’s method (dashed) superimposed on an empirical density esitmate (dotted). The starting point of\nboth algorithms is the circle. The square denotes the outcome of one MM update. The diamond denotes\nthe outcome of one Newton update. (a) Newton’s method overshoots the-se, global maximum. (b) Newton’s\nmethod results in a reduction of the objective. From Figure 4 of [FT05]. Used with kind permission of Carlo\nTomasi. In Section 8.7.2.1, we show that EM is an MM algorithm, which implies that this iterative procedure\nwill converge to a local maximum of the log likelihood.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 789, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 907}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0790_eef798c5", "text": "Used with kind permission of Carlo\nTomasi. In Section 8.7.2.1, we show that EM is an MM algorithm, which implies that this iterative procedure\nwill converge to a local maximum of the log likelihood. The speed of convergence depends on the\namount of missing data, which aﬀects the tightness of the bound [XJ96; MD97; SRG03; KKS20]. 8.7.2.1 Lower bound\nThe goal of EM is to maximize the log likelihood of the observed data:\nLL(\u0012) =NX\nn=1logp(ynj\u0012) =NX\nn=1log\"X\nznp(yn;znj\u0012)#\n(8.146)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n308 Chapter 8. Optimization\nwhereynare the visible variables and znare the hidden variables. Unfortunately this is hard to\noptimize, since the log cannot be pushed inside the sum. EM gets around this problem as follows. First, consider a set of arbitrary distributions qn(zn)over\neach hidden variable zn.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 790, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 845}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0791_406556a0", "text": "EM gets around this problem as follows. First, consider a set of arbitrary distributions qn(zn)over\neach hidden variable zn. The observed data log likelihood can be written as follows:\nLL(\u0012) =NX\nn=1log\"X\nznqn(zn)p(yn;znj\u0012)\nqn(zn)#\n(8.147)\nUsing Jensen’s inequality (Equation (6.34)), we can push the log (which is a concave function)\ninside the expectation to get the following lower bound on the log likelihood:\nLL(\u0012)\u0015X\nnX\nznqn(zn) logp(yn;znj\u0012)\nqn(zn)(8.148)\n=X\nnEqn[logp(yn;znj\u0012)] +H(qn)|{z }\nE(\u0012;qnjyn)(8.149)\n=X\nnE(\u0012;qnjyn),E(\u0012;fqngjD) (8.150)\nwhereH(q)is the entropy of probability distribution q, andE(\u0012;fqngjD)is called the evidence\nlower bound orELBO, since it is a lower bound on the log marginal likelihood, logp(y1:Nj\u0012), also\ncalled the evidence. Optimizing this bound is the basis of variational inference, which we discuss in\nSection 4.6.8.3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 791, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 856}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0792_13c61a9a", "text": "Optimizing this bound is the basis of variational inference, which we discuss in\nSection 4.6.8.3. 8.7.2.2 E step\nWe see that the lower bound is a sum of Nterms, each of which has the following form:\nE(\u0012;qnjyn) =X\nznqn(zn) logp(yn;znj\u0012)\nqn(zn)(8.151)\n=X\nznqn(zn) logp(znjyn;\u0012)p(ynj\u0012)\nqn(zn)(8.152)\n=X\nznqn(zn) logp(znjyn;\u0012)\nqn(zn)+X\nznqn(zn) logp(ynj\u0012) (8.153)\n=\u0000KL(qn(zn)kp(znjyn;\u0012)) + logp(ynj\u0012) (8.154)\nwhereKL(qkp),P\nzq(z)logq(z)\np(z)is the Kullback-Leibler divergence (or KL divergence for short)\nbetween probability distributions qandp. We discuss this in more detail in Section 6.2, but the key\nproperty we need here is that KL(qkp)\u00150andKL(qkp)= 0iﬀq=p. Hence we can maximize the\nlower boundE(\u0012;fqngjD)wrtfqngby setting each one to q\u0003\nn=p(znjyn;\u0012). This is called the E\nstep. This ensures the ELBO is a tight lower bound:\nE(\u0012;fq\u0003\nngjD) =X\nnlogp(ynj\u0012) =LL(\u0012jD) (8.155)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.7.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 792, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 954}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0793_be52db33", "text": "This is called the E\nstep. This ensures the ELBO is a tight lower bound:\nE(\u0012;fq\u0003\nngjD) =X\nnlogp(ynj\u0012) =LL(\u0012jD) (8.155)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.7. Bound optimization * 309\nTo see how this connects to bound optimization, let us deﬁne\nQ(\u0012;\u0012t) =E(\u0012;fp(znjyn;\u0012t)g) (8.156)\nThen we have Q(\u0012;\u0012t)\u0014LL(\u0012)andQ(\u0012t;\u0012t) =LL(\u0012t), as required. However, if we cannot compute the posteriors p(znjyn;\u0012t)exactly, we can still use an approximate\ndistribution q(znjyn;\u0012t); thiswillyieldanon-tightlower-boundonthelog-likelihood. Thisgeneralized\nversion of EM is known as variational EM [NH98]. See the sequel to this book, [Mur22], for details. 8.7.2.3 M step\nIn the M step, we need to maximize E(\u0012;fqt\nng)wrt\u0012, where the qt\nnare the distributions computed\nin the E step at iteration t. Since the entropy terms H(qn)are constant wrt \u0012, so we can drop them\nin the M step.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 793, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 901}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0794_3cd597ad", "text": "Since the entropy terms H(qn)are constant wrt \u0012, so we can drop them\nin the M step. We are left with\nLLt(\u0012) =X\nnEqt\nn(zn)[logp(yn;znj\u0012)] (8.157)\nThis is called the expected complete data log likelihood . If the joint probability is in the\nexponential family (Section 3.4), we can rewrite this as\nLLt(\u0012) =X\nnE\u0002\nT(yn;zn)T\u0012\u0000A(\u0012)\u0003\n=X\nn(E[T(yn;zn)]T\u0012\u0000A(\u0012)) (8.158)\nwhereE[T(yn;zn)]are called the expected suﬃcient statistics . In the M step, we maximize the expected complete data log likelihood to get\n\u0012t+1= arg max\n\u0012X\nnEqtn[logp(yn;znj\u0012)] (8.159)\nIn the case of the exponential family, the maximization can be solved in closed-form by matching the\nmoments of the expected suﬃcient statistics. We see from the above that the E step does not in fact need to return the full set of posterior\ndistributionsfq(zn)g, but can instead just return the sum of the expected suﬃcient statistics,P\nnEq(zn)[T(yn;zn)]. This will become clearer in the examples below.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 794, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 948}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0795_c787c4de", "text": "This will become clearer in the examples below. 8.7.3 Example: EM for a GMM\nIn this section, we show how to use the EM algorithm to compute MLE and MAP estimates of the\nparameters for a Gaussian mixture model (GMM). 8.7.3.1 E step\nThe E step simply computes the responsibility of clusterkfor generating data point n, as estimated\nusing the current parameter estimates \u0012(t):\nr(t)\nnk=p\u0003(zn=kj\u0012(t)) =\u0019(t)\nkp(ynj\u0012(t)\nk)\nP\nk0\u0019(t)\nk0p(ynj\u0012(t)\nk0)(8.160)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n310 Chapter 8. Optimization\n8.7.3.2 M step\nThe M step maximizes the expected complete data log likelihood, given by\nLLt(\u0012) =E\"X\nnlogp(znj\u0019) + logp(ynjzn;\u0012)#\n(8.161)\n=E\"X\nnlog Y\nk\u0019znk\nk! + log Y\nkN(ynj\u0016k;\u0006k)znk!#\n(8.162)\n=X\nnX\nkE[znk] log\u0019k+X\nnX\nkE[znk] logN(ynj\u0016k;\u0006k) (8.163)\n=X\nnX\nkr(t)\nnklog(\u0019k)\u00001\n2X\nnX\nkr(t)\nnk\u0002\nlogj\u0006kj+ (yn\u0000\u0016k)T\u0006\u00001\nk(yn\u0000\u0016k)\u0003\n+ const\n(8.164)\nwhereznk=I(zn=k)is a one-hot encoding of the categorical value zn.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 795, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 937}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0796_090af82b", "text": "This objective is just a\nweighted version of the standard problem of computing the MLEs of an MVN (see Section 4.2.6). One can show that the new parameter estimates are given by\n\u0016(t+1)\nk=P\nnr(t)\nnkyn\nr(t)\nk(8.165)\n\u0006(t+1)\nk=P\nnr(t)\nnk(yn\u0000\u0016(t+1)\nk)(yn\u0000\u0016(t+1)\nk)T\nr(t)\nk\n=P\nnr(t)\nnkynyT\nn\nr(t)\nk\u0000\u0016(t+1)\nk(\u0016(t+1)\nk)T(8.166)\nwherer(t)\nk,P\nnr(t)\nnkis the weighted number of points assigned to cluster k. The mean of cluster kis\njust the weighted average of all points assigned to cluster k, and the covariance is proportional to the\nweighted empirical scatter matrix. The M step for the mixture weights is simply a weighted form of the usual MLE:\n\u0019(t+1)\nk=1\nNX\nnr(t)\nnk=r(t)\nk\nN(8.167)\n8.7.3.3 Example\nAn example of the algorithm in action is shown in Figure 8.25 where we ﬁt some 2d data with a\n2 component GMM. The data set, from [Bis06], is derived from measurements of the Old Faithful\ngeyser in Yellowstone National Park.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 796, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 920}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0797_3db4fdb5", "text": "The data set, from [Bis06], is derived from measurements of the Old Faithful\ngeyser in Yellowstone National Park. In particular, we plot the time to next eruption in minutes\nversus the duration of the eruption in minutes. The data was standardized, by removing the mean\nand dividing by the standard deviation, before processing; this often helps convergence. We start\nwith\u00161= (\u00001;1),\u00061=I,\u00162= (1;\u00001),\u00062=I. We then show the cluster assignments, and\ncorresponding mixture components, at various iterations. For more details on applying GMMs for clustering, see Section 21.4.1. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.7. Bound optimization * 311\n1\n 0 12\n1\n01Iteration 0\n1\n 0 12\n1\n01Iteration 10\n1\n 0 12\n1\n01Iteration 25\n1\n 0 12\n1\n01Iteration 30\n1\n 0 12\n1\n01Iteration 35\n1\n 0 12\n1\n01Iteration 40\nFigure 8.25: Illustration of the EM for a GMM applied to the Old Faithful data.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 797, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 908}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0798_d92d6a95", "text": "The degree of redness\nindicates the degree to which the point belongs to the red cluster, and similarly for blue; thus purple points have\na roughly 50/50 split in their responsibilities to the two clusters. Adapted from [Bis06] Figure 9.8. Generated\nby code at ﬁgures.probml.ai/book1/8.25. 8.7.3.4 MAP estimation\nComputing the MLE of a GMM often suﬀers from numerical problems and overﬁtting. To see why,\nsuppose for simplicity that \u0006k=\u001b2\nkIfor allk. It is possible to get an inﬁnite likelihood by assigning\none of the centers, say \u0016k, to a single data point, say yn, since then the likelihood of that data point\nis given by\nN(ynj\u0016k=yn;\u001b2\nkI) =1p\n2\u0019\u001b2\nke0(8.168)\nHence we can drive this term to inﬁnity by letting \u001bk!0, as shown in Figure 8.26(a). We call this\nthe “collapsing variance problem”. An easy solution to this is to perform MAP estimation. Fortunately, we can still use EM to ﬁnd\nthis MAP estimate.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 798, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 909}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0799_1d7e29ab", "text": "We call this\nthe “collapsing variance problem”. An easy solution to this is to perform MAP estimation. Fortunately, we can still use EM to ﬁnd\nthis MAP estimate. Our goal is now to maximize the expected complete data log-likelihood plus the\nlog prior:\nLLt(\u0012) =\"X\nnX\nkr(t)\nnklog\u0019nk+X\nnX\nkr(t)\nnklogp(ynj\u0012k)#\n+ logp(\u0019) +X\nklogp(\u0012k)(8.169)\nNote that the E step remains unchanged, but the M step needs to be modiﬁed, as we now explain. For the prior on the mixture weights, it is natural to use a Dirichlet prior (Section 4.6.3.2),\n\u0019\u0018Dir(\u000b), since this is conjugate to the categorical distribution. The MAP estimate is given by\n~\u0019(t+1)\nk=r(t)\nk+\u000bk\u00001\nN+P\nk\u000bk\u0000K(8.170)\nIf we use a uniform prior, \u000bk= 1, this reduces to the MLE. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n312 Chapter 8.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 799, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 796}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0800_46bffa1c", "text": "The MAP estimate is given by\n~\u0019(t+1)\nk=r(t)\nk+\u000bk\u00001\nN+P\nk\u000bk\u0000K(8.170)\nIf we use a uniform prior, \u000bk= 1, this reduces to the MLE. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n312 Chapter 8. Optimization\nxp(x)\n(a)\n20 40 60 80 100\ndimensionality0.00.20.40.60.81.0fraction of times EM for GMM fails\nMLE\nMAP (b)\nFigure 8.26: (a) Illustration of how singularities can arise in the likelihood function of GMMs. Here K= 2,\nbut the ﬁrst mixture component is a narrow spike (with \u001b1\u00190) centered on a single data point x1. Adapted\nfrom Figure 9.7 of [Bis06]. Generated by code at ﬁgures.probml.ai/book1/8.26. (b) Illustration of the beneﬁt\nof MAP estimation vs ML estimation when ﬁtting a Gaussian mixture model. We plot the fraction of times\n(out of 5 random trials) each method encounters numerical problems vs the dimensionality of the problem, for\nN= 100samples. Solid red (upper curve): MLE. Dotted black (lower curve): MAP. Generated by code at\nﬁgures.probml.ai/book1/8.26.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 800, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 981}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0801_c54a7930", "text": "Solid red (upper curve): MLE. Dotted black (lower curve): MAP. Generated by code at\nﬁgures.probml.ai/book1/8.26. For the prior on the mixture components, let us consider a conjugate prior of the form\np(\u0016k;\u0006k) = NIW(\u0016k;\u0006kj`m;`\u0014;`\u0017;`S) (8.171)\nThis is called the Normal-Inverse-Wishart distribution (see the sequel to this book, [Mur22],\nfor details.) Suppose we set the hyper-parameters for \u0016to be`\u0014= 0, so that the\u0016kare unregularized;\nthus the prior will only inﬂuence our estimate of \u0006k. In this case, the MAP estimates are given by\n~\u0016(t+1)\nk=^\u0016(t+1)\nk(8.172)\n~\u0006(t+1)\nk=`S+^\u0006(t+1)\nk\n`\u0017+r(t)\nk+D+ 2(8.173)\nwhere ^\u0016kis the MLE for \u0016kfrom Equation (8.165), and ^\u0006kis the MLE for \u0006kfrom Equation (8.166). Now we discuss how to set the prior covariance,`S. One possibility (suggested in [FR07, p163]) is\nto use\n`S=1\nK1=Ddiag(s2\n1;:::;s2\nD) (8.174)\nwheresd= (1=N)PN\nn=1(xnd\u0000xd)2is the pooled variance for dimension d. The parameter`\u0017controls\nhow strongly we believe this prior.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 801, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 972}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0802_a9eab6cf", "text": "The parameter`\u0017controls\nhow strongly we believe this prior. The weakest prior we can use, while still being proper, is to set\n`\u0017=D+ 2, so this is a common choice. We now illustrate the beneﬁts of using MAP estimation instead of ML estimation in the context of\nGMMs. We apply EM to some synthetic data with N= 100samples in Ddimensions, using either\nML or MAP estimation. We count the trial as a “failure” if there are numerical issues involving\nsingular matrices. For each dimensionality, we conduct 5 random trials. The results are illustrated in\nFigure 8.26(b). We see that as soon as Dbecomes even moderately large, ML estimation crashes and\nburns, whereas MAP with an appropriate prior estimation rarely encounters numerical problems. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.8.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 802, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 819}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0803_eb8af4de", "text": "Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n8.8. Blackbox and derivative free optimization 313\n−25 −20 −15 −10 −5 0 5 10 15 20 2505101520253035\n(a)\nµ1µ2\n−15.5 −10.5 −5.5 −0.5 4.5 9.5 14.5 19.5−15.5−10.5−5.5−0.54.59.514.519.5 (b)\nFigure 8.27: Left: N= 200data points sampled from a mixture of 2 Gaussians in 1d, with \u0019k= 0:5,\u001bk= 5,\n\u00161=\u000010and\u00162= 10. Right: Likelihood surface p(Dj\u00161;\u00162), with all other parameters set to their true\nvalues. We see the two symmetric modes, reﬂecting the unidentiﬁability of the parameters. Generated by code\nat ﬁgures.probml.ai/book1/8.27. 8.7.3.5 Nonconvexity of the NLL\nThe likelihood for a mixture model is given by\nLL(\u0012) =NX\nn=1log\"KX\nzn=1p(yn;znj\u0012)#\n(8.175)\nIn general, this will have multiple modes, and hence there will not be a unique global optimum. Figure 8.27 illustrates this for a mixture of 2 Gaussians in 1d.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 803, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 885}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0804_39fc3a70", "text": "Figure 8.27 illustrates this for a mixture of 2 Gaussians in 1d. We see that there are two equally\ngood global optima, corresponding to two diﬀerent labelings of the clusters, one in which the left\npeak corresponds to z= 1, and one in which the left peak corresponds to z= 2. This is called the\nlabel switching problem ; see Section 21.4.1.2 for more details. The question of how many modes there are in the likelihood function is hard to answer. There are\nK!possible labelings, but some of the peaks might get merged, depending on how far apart the \u0016k\nare. Nevertheless, there can be an exponential number of modes. Consequently, ﬁnding any global\noptimum is NP-hard [Alo+09; Dri+04]. We will therefore have to be satisﬁed with ﬁnding a local\noptimum. To ﬁnd a good local optimum, we can use Kmeans++ (Section 21.3.4) to initialize EM. 8.8 Blackbox and derivative free optimization\nIn some optimization problems, the objective function is a blackbox , meaning that its functional\nform is unknown.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 804, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 997}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0805_910fec8e", "text": "8.8 Blackbox and derivative free optimization\nIn some optimization problems, the objective function is a blackbox , meaning that its functional\nform is unknown. This means we cannot use gradient-based methods to optimize it. Instead,\nsolving such problems require blackbox optimization (BBO) methods, also called derivative\nfree optimization (DFO). In ML, this kind of problem often arises when performing model selection. For example, suppose\nwe have some hyper-parameters, \u00152\u0003, which control the type or complexity of a model. We often\ndeﬁne the objective function L(\u0015)to be the loss on a validation set (see Section 4.5.4). Since the\nvalidation loss depends on the optimal model parameters, which are computed using a complex\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n314 Chapter 8.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 805, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 803}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0806_ed838b50", "text": "Since the\nvalidation loss depends on the optimal model parameters, which are computed using a complex\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n314 Chapter 8. Optimization\nalgorithm, this objective function is eﬀectively a blackbox.4A simple approach to such problems\nis to usegrid search , where we evaluate each point in the parameter space, and pick the one\nwith the lowest loss. Unfortunately, this does not scale to high dimensions, because of the curse of\ndimensionality. In addition, even in low dimensions this can be expensive if evaluate the blackbox\nobjective is expensive (e.g., if it ﬁrst requires training the model before computing the validation\nloss). Various solutions to this problem have been proposed. See the sequel to this book, [Mur22],\nfor details. 8.9 Exercises\nExercise 8.1 [Subderivative of the hinge loss function *]\nLetf(x) = (1\u0000x)+be the hinge loss function, where (z)+=max(0;z). What are @f(0),@f(1), and@f(2)?", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 806, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 960}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0807_bf4c81cb", "text": "8.9 Exercises\nExercise 8.1 [Subderivative of the hinge loss function *]\nLetf(x) = (1\u0000x)+be the hinge loss function, where (z)+=max(0;z). What are @f(0),@f(1), and@f(2)? Exercise 8.2 [EM for the Student distribution]\nDerive the EM equations for computing the MLE for a multivariate Student distribution. Consider the case\nwhere the dof parameter is known and unknown separately. Hint: write the Student distribution as a scale\nmixture of Gaussians. 4. If the optimal parameters are computed using a gradient-based optimizer, we can “unroll” the gradient steps, to\ncreate a deep circuit that maps from the training data to the optimal parameters and hence to the validation loss. We\ncan then optimize through the optimizer (see e.g., [Fra+17]. However, this technique can only be applied in limited\nsettings. Draft of “Probabilistic Machine Learning: An Introduction”.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 807, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 866}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0808_55ff59fb", "text": "We\ncan then optimize through the optimizer (see e.g., [Fra+17]. However, this technique can only be applied in limited\nsettings. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\nPart II\nLinear Models\n\n9Linear Discriminant Analysis\n9.1 Introduction\nIn this chapter, we consider classiﬁcation models of the following form:\np(y=cjx;\u0012) =p(xjy=c;\u0012)p(y=c;\u0012)P\nc0p(xjy=c0;\u0012)p(y=c;\u0012)(9.1)\nThe termp(y=c;\u0012)is the prior over class labels, and the term p(xjy=c;\u0012)is called the class\nconditional density for classc. The overall model is called a generative classiﬁer , since it speciﬁes a way to generate the features\nxfor each class c, by sampling from p(xjy=c;\u0012). By contrast, a discriminative classiﬁer directly\nmodels the class posterior p(yjx;\u0012). We discuss the pros and cons of these two approaches to\nclassiﬁcation in Section 9.4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 808, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 851}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0809_9d75a93b", "text": "By contrast, a discriminative classiﬁer directly\nmodels the class posterior p(yjx;\u0012). We discuss the pros and cons of these two approaches to\nclassiﬁcation in Section 9.4. If we choose the class conditional densities in a special way, we will see that the resulting posterior\nover classes is a linear function of x, i.e., logp(y=cjx;\u0012) =wTx+const, wherewis derived from\n\u0012. Thus the overall method is called linear discriminant analysis orLDA.1\n9.2 Gaussian discriminant analysis\nIn this section, we consider a generative classiﬁer where the class conditional densities are multivariate\nGaussians:\np(xjy=c;\u0012) =N(xj\u0016c;\u0006c) (9.2)\nThe corresponding class posterior therefore has the form\np(y=cjx;\u0012)/\u0019cN(xj\u0016c;\u0006c) (9.3)\nwhere\u0019c=p(y=c)is the prior probability of label c. (Note that we can ignore the normalization\nconstant in the denominator of the posterior, since it is independent of c.) We call this model\nGaussian discriminant analysis orGDA. 1. This term is rather confusing for two reaons.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 809, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 989}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0810_e71e4126", "text": "1. This term is rather confusing for two reaons. First, LDA is a generative, not discriminative, classiﬁer. Second,\nLDA also stands for “latent Dirichlet allocation”, which is a popular unsupervised generative model for bags of words\n[BNJ03]. 318 Chapter 9. Linear Discriminant Analysis\n4\n 2\n 0 2 4 62\n0246\n(a)\n4\n 2\n 0 2 4 64\n2\n0246\n (b)\nFigure 9.1: (a) Some 2d data from 3 diﬀerent classes. (b) Fitting 2d Gaussians to each class. Generated by\ncode at ﬁgures.probml.ai/book1/9.1. 4\n 2\n 0 2 4 64\n2\n0246\nQDA\n(a)\n4\n 2\n 0 2 4 64\n2\n0246\nLDA (b)\nFigure 9.2: Gaussian discriminant analysis ﬁt to data in Figure 9.1. (a) Unconstrained covariances induce\nquadratic decision boundaries. (b) Tied covariances induce linear decision boundaries. Generated by code at\nﬁgures.probml.ai/book1/9.2. 9.2.1 Quadratic decision boundaries\nFrom Equation (9.3), we see that the log posterior over class labels is given by\nlogp(y=cjx;\u0012) = log\u0019c\u00001\n2logj2\u0019\u0006cj\u00001\n2(x\u0000\u0016c)T\u0006\u00001\nc(x\u0000\u0016c) + const (9.4)\nThisiscalledthe discriminantfunction .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 810, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1009}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0811_1a943b32", "text": "Weseethatthedecisionboundarybetweenanytwoclasses,\nsaycandc0, will be a quadratic function of x. Hence this is known as quadratic discriminant\nanalysis (QDA). For example, consider the 2d data from 3 diﬀerent classes in Figure 9.1a. We ﬁt full covariance\nGaussian class-conditionals (using the method explained in Section 9.2.4), and plot the results in\nFigure 9.1b. We see that the features for the blue class are somewhat correlated, whereas the features\nfor the green class are independent, and the features for the red class are independent and isotropic\n(spherical covariance). In Figure 9.2a, we see that the resulting decision boundaries are quadratic\nfunctions ofx. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n9.2. Gaussian discriminant analysis 319\nFigure 9.3: Geometry of LDA in the 2 class case where \u00061=\u00062=I.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 811, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 851}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0812_12ed2fb6", "text": "Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n9.2. Gaussian discriminant analysis 319\nFigure 9.3: Geometry of LDA in the 2 class case where \u00061=\u00062=I. 9.2.2 Linear decision boundaries\nNow we consider a special case of Gaussian discriminant analysis in which the covariance matrices are\ntiedorsharedacross classes, so \u0006c=\u0006. If\u0006is independent of c, we can simplify Equation (9.4)\nas follows:\nlogp(y=cjx;\u0012) = log\u0019c\u00001\n2(x\u0000\u0016c)T\u0006\u00001(x\u0000\u0016c) + const (9.5)\n= log\u0019c\u00001\n2\u0016T\nc\u0006\u00001\u0016c\n|{z}\n\rc+xT\u0006\u00001\u0016c|{z}\n\fc+const\u00001\n2xT\u0006\u00001x\n|{z}\n\u0014(9.6)\n=\rc+xT\fc+\u0014 (9.7)\nThe ﬁnal term is independent of c, and hence is an irrelevant additive constant that can be dropped. Hence we see that the discriminant function is a linear function of x, so the decision boundaries will\nbe linear. Hence this method is called linear discriminant analysis orLDA. See Figure 9.2b for\nan example.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 812, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 873}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0813_21c0f5b4", "text": "Hence this method is called linear discriminant analysis orLDA. See Figure 9.2b for\nan example. 9.2.3 The connection between LDA and logistic regression\nIn this section, we derive an interesting connection between LDA and logistic regression, which we\nintroduced in Section 2.5.3. From Equation (9.7) we can write\np(y=cjx;\u0012) =e\fT\ncx+\rc\nP\nc0e\fT\nc0x+\rc0=ewT\nc[1;x]\nP\nc0ewT\nc0[1;x](9.8)\nwherewc= [\rc;\fc]. We see that Equation (9.8) has the same form as the multinomial logistic\nregression model. The key diﬀerence is that in LDA, we ﬁrst ﬁt the Gaussians (and class prior) to\nmaximize the joint likelihood p(x;yj\u0012), as discussed in Section 9.2.4, and then we derive wfrom\u0012. By contrast, in logistic regression, we estimate wdirectly to maximize the conditional likelihood\np(yjx;w). In general, these can give diﬀerent results (see Exercise 10.3). To gain further insight into Equation (9.8), let us consider the binary case. In this case, the\nAuthor: Kevin P. Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 813, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 979}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0814_1055e961", "text": "In general, these can give diﬀerent results (see Exercise 10.3). To gain further insight into Equation (9.8), let us consider the binary case. In this case, the\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n320 Chapter 9. Linear Discriminant Analysis\nposterior is given by\np(y= 1jx;\u0012) =e\fT\n1x+\r1\ne\fT\n1x+\r1+e\fT\n0x+\r0=1\n1 +e(\f0\u0000\f1)Tx+(\r0\u0000\r1)(9.9)\n=\u001b\u0000\n(\f1\u0000\f0)Tx+ (\r1\u0000\r0)\u0001\n(9.10)\nwhere\u001b(\u0011)refers to the sigmoid function. Now\n\r1\u0000\r0=\u00001\n2\u0016T\n1\u0006\u00001\u00161+1\n2\u0016T\n0\u0006\u00001\u00160+ log(\u00191=\u00190) (9.11)\n=\u00001\n2(\u00161\u0000\u00160)T\u0006\u00001(\u00161+\u00160) + log(\u00191=\u00190) (9.12)\nSo if we deﬁne\nw=\f1\u0000\f0=\u0006\u00001(\u00161\u0000\u00160) (9.13)\nx0=1\n2(\u00161+\u00160)\u0000(\u00161\u0000\u00160)log(\u00191=\u00190)\n(\u00161\u0000\u00160)T\u0006\u00001(\u00161\u0000\u00160)(9.14)\nthen we have wTx0=\u0000(\r1\u0000\r0), and hence\np(y= 1jx;\u0012) =\u001b(wT(x\u0000x0)) (9.15)\nThis has the same form as binary logistic regression. Hence the MAP decision rule is\n^y(x) = 1iﬀwTx>c (9.16)\nwherec=wTx0. If\u00190=\u00191= 0:5, then the threshold simpliﬁes to c=1\n2wT(\u00161+\u00160). To interpret this equation geometrically, suppose \u0006=\u001b2I.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 814, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 939}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0815_504895ec", "text": "Hence the MAP decision rule is\n^y(x) = 1iﬀwTx>c (9.16)\nwherec=wTx0. If\u00190=\u00191= 0:5, then the threshold simpliﬁes to c=1\n2wT(\u00161+\u00160). To interpret this equation geometrically, suppose \u0006=\u001b2I. In this case, w=\u001b\u00002(\u00161\u0000\u00160),\nwhich is parallel to a line joining the two centroids, \u00160and\u00161. So we can classify a point by\nprojecting it onto this line, and then checking if the projection is closer to \u00160or\u00161, as illustrated in\nFigure 9.3. The question of how close it has to be depends on the prior over classes. If \u00191=\u00190, then\nx0=1\n2(\u00161+\u00160), which is halfway between the means. If we make \u00191>\u00190, we have to be closer to\n\u00160than halfway in order to pick class 0. And vice versa if \u00190>\u00191. Thus we see that the class prior\njust changes the decision threshold, but not the overall shape of the decision boundary. (A similar\nargument applies in the multi-class case.)\n9.2.4 Model ﬁtting\nWe now discuss how to ﬁt a GDA model using maximum likelihood estimation.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 815, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 942}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0816_9fb0a023", "text": "(A similar\nargument applies in the multi-class case.)\n9.2.4 Model ﬁtting\nWe now discuss how to ﬁt a GDA model using maximum likelihood estimation. The likelihood\nfunction is as follows\np(Dj\u0012) =NY\nn=1Cat(ynj\u0019)CY\nc=1N(xnj\u0016c;\u0006c)I(yn=c)(9.17)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n9.2. Gaussian discriminant analysis 321\nHence the log-likelihood is given by\nlogp(Dj\u0012) =\"NX\nn=1CX\nc=1I(yn=c) log\u0019c#\n+CX\nc=1\"X\nn:yn=clogN(xnj\u0016c;\u0006c)#\n(9.18)\nThus we see that we can optimize \u0019and the (\u0016c;\u0006c)terms separately. From Section 4.2.4, we have that the MLE for the class prior is ^\u0019c=Nc\nN. Using the results from\nSection 4.2.6, we can derive the MLEs for the Gaussians as follows:\n^\u0016c=1\nNcX\nn:yn=cxn (9.19)\n^\u0006c=1\nNcX\nn:yn=c(xn\u0000^\u0016c)(xn\u0000^\u0016c)T(9.20)\nUnfortunately the MLE for ^\u0006ccan easily overﬁt (i.e., the estimate may not be well-conditioned) if\nNcis small compared to D, the dimensionality of the input features. We discuss some solutions to\nthis below.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 816, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 975}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0817_b0336860", "text": "We discuss some solutions to\nthis below. 9.2.4.1 Tied covariances\nIf we force \u0006c=\u0006to be tied, we will get linear decision boundaries, as we have seen. This also\nusually results in a more reliable parameter estimate, since we can pool all the samples across classes:\n^\u0006=1\nNCX\nc=1X\nn:yn=c(xn\u0000^\u0016c)(xn\u0000^\u0016c)T(9.21)\n9.2.4.2 Diagonal covariances\nIf we force \u0006cto be diagonal, we reduce the number of parameters from O(CD2)toO(CD), which\navoids the overﬁtting problem. However, this loses the ability to capture correlations between the\nfeatures. (This is known as the naive Bayes assumption, which we discuss further in Section 9.3.)\nDespite this approximation, this approach scales well to high dimensions. We can further restrict the model capacity by using a shared (tied) diagonal covariace matrix. This is called “diagonal LDA” [BL04]. 9.2.4.3 MAP estimation\nForcing the covariance matrix to be diagonal is a rather strong assumption.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 817, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 932}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0818_e59f780a", "text": "This is called “diagonal LDA” [BL04]. 9.2.4.3 MAP estimation\nForcing the covariance matrix to be diagonal is a rather strong assumption. An alternative approach\nis to perform MAP estimation of a (shared) full covariance Gaussian, rather than using the MLE. Based on the results of Section 4.5.2, we ﬁnd that the MAP estimate is\n^\u0006map=\u0015diag( ^\u0006mle) + (1\u0000\u0015)^\u0006mle (9.22)\nwhere\u0015controlstheamountofregularization. Thistechniqueisknownas regularizeddiscriminant\nanalysis or RDA [HTF09, p656]. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n322 Chapter 9. Linear Discriminant Analysis\n9.2.5 Nearest centroid classiﬁer\nIf we assume a uniform prior over classes, we can compute the most probable class label as follows:\n^y(x) = argmax\nclogp(y=cjx;\u0012) = argmin\nc(x\u0000\u0016c)T\u0006\u00001(x\u0000\u0016c) (9.23)\nThis is called the nearest centroid classiﬁer , ornearest class mean classiﬁer (NCM), since\nwe are assigning xto the class with the closest \u0016c, where distance is measured using (squared)\nMahalanobis distance.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 818, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 996}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0819_b7d7fb36", "text": "We can replace this with any other distance metric to get the decision rule\n^y(x) = argmin\ncd2(x;\u0016c) (9.24)\nWe discuss how to learn distance metrics in Section 16.2, but one simple approach is to use\nd2(x;\u0016c) =jjx\u0000\u0016cjj2\nW= (x\u0000\u0016c)T(WWT)(x\u0000\u0016c) =jjW(x\u0000\u0016c)jj2(9.25)\nThe corresponding class posterior becomes\np(y=cjx;\u0016;W) =exp(\u00001\n2jjW(x\u0000\u0016c)jj2\n2)\nPC\nc0=1exp(\u00001\n2jjW(x\u0000\u0016c0)jj2\n2)(9.26)\nWe can optimize Wusing gradient descent applied to the discriminative loss. This is called nearest\nclass mean metric learning [Men+12]. The advantage of this technique is that it can be used for\none-shot learning of new classes, since we just need to see a single labeled prototype \u0016cper class\n(assuming we have learned a good Walready). 9.2.6 Fisher’s linear discriminant analysis *\nDiscriminant analysis is a generative approach to classiﬁcation, which requires ﬁtting an MVN to\nthe features. As we have discussed, this can be problematic in high dimensions.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 819, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 940}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0820_acf3cd72", "text": "As we have discussed, this can be problematic in high dimensions. An alternative\napproach is to reduce the dimensionality of the features x2RDand then ﬁt an MVN to the\nresulting low-dimensional features z2RK. The simplest approach is to use a linear projection\nmatrix,z=Wx, where Wis aK\u0002Dmatrix. One approach to ﬁnding Wwould be to use principal\ncomponents analysis or PCA (Section 20.1). However, PCA is an unsupervised technique that does\nnot take class labels into account. Thus the resulting low dimensional features are not necessarily\noptimal for classiﬁcation, as illustrated in Figure 9.4. An alternative approach is to use gradient based methods to optimize the log likelihood, derived\nfrom the class posterior in the low dimensional space, as we discussed in Section 9.2.5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 820, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 783}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0821_425b64f5", "text": "An alternative approach is to use gradient based methods to optimize the log likelihood, derived\nfrom the class posterior in the low dimensional space, as we discussed in Section 9.2.5. A third approach (which relies on an eigendecomposition, rather than a gradient-based optimizer)\nis to ﬁnd the matrix Wsuch that the low-dimensional data can be classiﬁed as well as possible using\na Gaussian class-conditional density model. The assumption of Gaussianity is reasonable since we\nare computing linear combinations of (potentially non-Gaussian) features. This approach is called\nFisher’s linear discriminant analysis , orFLDA. FLDA is an interesting hybrid of discriminative and generative techniques. The drawback of this\ntechnique is that it is restricted to using K\u0014C\u00001dimensions, regardless of D, for reasons that we\nwill explain below. In the two-class case, this means we are seeking a single vector wonto which we\ncan project the data. Below we derive the optimal win the two-class case.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 821, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 993}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0822_d90b3ecd", "text": "In the two-class case, this means we are seeking a single vector wonto which we\ncan project the data. Below we derive the optimal win the two-class case. We then generalize to\nthe multi-class case, and ﬁnally we give a probabilistic interpretation of this technique. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n9.2. Gaussian discriminant analysis 323\n2\n 0 2 4 6 80.51.01.52.02.53.03.5\nMale\nFemale\nPCA vector\n(a)\n2\n 0 2 4 6 80.51.01.52.02.53.03.5\nMale\nFemale\nFisherLDA vector (b)\n4\n 2\n 0 2 4 6 805101520Projection of points onto PCA vector\n(c)\n80\n 60\n 40\n 20\n 005101520Projection of points onto Fisher vector (d)\nFigure 9.4: Linear disciminant analysis applied to two class dataset in 2d, representing (standardized) height\nand weight for male and female adults (a) PCA direction. (b) FLDA direction. (c) Projection onto PCA\ndirection shows poor class separation. (d) Projection onto FLDA direction shows good class separation. Generated by code at ﬁgures.probml.ai/book1/9.4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 822, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1007}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0823_5e4b65c0", "text": "(b) FLDA direction. (c) Projection onto PCA\ndirection shows poor class separation. (d) Projection onto FLDA direction shows good class separation. Generated by code at ﬁgures.probml.ai/book1/9.4. 9.2.6.1 Derivation of the optimal 1d projection\nWe now derive this optimal direction w, for the two-class case, following the presentation of [Bis06,\nSec 4.1.4]. Deﬁne the class-conditional means as\n\u00161=1\nN1X\nn:yn=1xn;\u00162=1\nN2X\nn:yn=2xn (9.27)\nLetmk=wT\u0016kbe the projection of each mean onto the line w. Also, let zn=wTxnbe the\nprojection of the data onto the line. The variance of the projected points is proportional to\ns2\nk=X\nn:yn=k(zn\u0000mk)2(9.28)\nThe goal is to ﬁnd wsuch that we maximize the distance between the means, m2\u0000m1, while also\nensuring the projected clusters are “tight”, which we can do by minimizing their variance. This\nsuggests the following objective:\nJ(w) =(m2\u0000m1)2\ns2\n1+s2\n2(9.29)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n324 Chapter 9.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 823, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 969}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0824_8977c929", "text": "This\nsuggests the following objective:\nJ(w) =(m2\u0000m1)2\ns2\n1+s2\n2(9.29)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n324 Chapter 9. Linear Discriminant Analysis\nWe can rewrite the right hand side of the above in terms of was follows\nJ(w) =wTSBw\nwTSWw(9.30)\nwhere SBis the between-class scatter matrix given by\nSB= (\u00162\u0000\u00161)(\u00162\u0000\u00161)T(9.31)\nandSWis the within-class scatter matrix, given by\nSW=X\nn:yn=1(xn\u0000\u00161)(xn\u0000\u00161)T+X\nn:yn=2(xn\u0000\u00162)(xn\u0000\u00162)T(9.32)\nTo see this, note that\nwTSBw=wT(\u00162\u0000\u00161)(\u00162\u0000\u00161)Tw= (m2\u0000m1)(m2\u0000m1) (9.33)\nand\nwTSWw=X\nn:yn=1wT(xn\u0000\u00161)(xn\u0000\u00161)Tw+\nX\nn:yn=2wT(xn\u0000\u00162)(xn\u0000\u00162)Tw (9.34)\n=X\nn:yn=1(zn\u0000m1)2+X\nn:yn=2(zn\u0000m2)2(9.35)\nEquation (9.30) is a ratio of two scalars; we can take its derivative with respect to wand equate to\nzero. One can show (Exercise 9.1) that J(w)is maximized when\nSBw=\u0015SWw (9.36)\nwhere\n\u0015=wTSBw\nwTSWw(9.37)\nEquation (9.36) is called a generalized eigenvalue problem.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 824, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 903}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0825_86260436", "text": "One can show (Exercise 9.1) that J(w)is maximized when\nSBw=\u0015SWw (9.36)\nwhere\n\u0015=wTSBw\nwTSWw(9.37)\nEquation (9.36) is called a generalized eigenvalue problem. If SWis invertible, we can convert it\nto a regular eigenvalue problem:\nS\u00001\nWSBw=\u0015w (9.38)\nHowever, in the two class case, there is a simpler solution. In particular, since\nSBw= (\u00162\u0000\u00161)(\u00162\u0000\u00161)Tw= (\u00162\u0000\u00161)(m2\u0000m1) (9.39)\nthen, from Equation (9.38) we have\n\u0015w=S\u00001\nW(\u00162\u0000\u00161)(m2\u0000m1) (9.40)\nw/S\u00001\nW(\u00162\u0000\u00161) (9.41)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n9.2. Gaussian discriminant analysis 325\n4\n 3\n 2\n 1\n 0 1 2 33\n2\n1\n012PCA projection of vowel data to 2d\n(a)\n4.0\n 3.5\n 3.0\n 2.5\n 2.0\n 1.5\n 1.0\n 0.5\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0FLDA projection of vowel data to 2d (b)\nFigure 9.5: (a) PCA projection of vowel data to 2d. (b) FLDA projection of vowel data to 2d. We see there\nis better class separation in the FLDA case. Adapted from Figure 4.11 of [HTF09]. Generated by code at\nﬁgures.probml.ai/book1/9.5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 825, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0826_e72a0ed7", "text": "(b) FLDA projection of vowel data to 2d. We see there\nis better class separation in the FLDA case. Adapted from Figure 4.11 of [HTF09]. Generated by code at\nﬁgures.probml.ai/book1/9.5. Since we only care about the directionality, and not the scale factor, we can just set\nw=S\u00001\nW(\u00162\u0000\u00161) (9.42)\nThis is the optimal solution in the two-class case. If SW/I, meaning the pooled covariance matrix\nis isotropic, then wis proportional to the vector that joins the class means. This is an intuitively\nreasonable direction to project onto, as shown in Figure 9.3. 9.2.6.2 Extension to higher dimensions and multiple classes\nWe can extend the above idea to multiple classes, and to higher dimensional subspaces, by ﬁnding a\nprojection matrix Wwhich maps from DtoK. Letzn=Wxnbe the low dimensional projection\nof then’th data point. Let mc=1\nNcP\nn:yn=cznbe the corresponding mean for the c’th class and\nm=1\nNPC\nc=1Ncmcbe the overall mean, both in the low dimensional space.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 826, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 961}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0827_8227c273", "text": "Let mc=1\nNcP\nn:yn=cznbe the corresponding mean for the c’th class and\nm=1\nNPC\nc=1Ncmcbe the overall mean, both in the low dimensional space. We deﬁne the following\nscatter matrices:\n~SW=CX\nc=1X\nn:yn=c(zn\u0000mc)(zn\u0000mc)T(9.43)\n~SB=CX\nc=1Nc(mc\u0000m)(mc\u0000m)T(9.44)\nFinally, we deﬁne the objective function as maximizing the following:2\nJ(W) =j~SBj\nj~SWj=jWTSBWj\njWTSWWj(9.45)\n2. An alternative criterion that is sometimes used [Fuk90] is J(W) =trn\n~S\u00001\nW~SBo\n=tr\b\n(WSWWT)\u00001(WSBWT) \n. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n326 Chapter 9. Linear Discriminant Analysis\nwhere SWandSBare deﬁned in the original high dimensional space in the obvious way (namely using\nxninstead ofzn,\u0016cinstead ofmc, and\u0016instead ofm). The solution can be shown [DHS01] to be\nW=S\u00001\n2\nWU;where Uare theKleading eigenvectors of S\u00001\n2\nWSBS\u00001\n2\nW, assuming SWis non-singular.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 827, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 857}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0828_3310470e", "text": "The solution can be shown [DHS01] to be\nW=S\u00001\n2\nWU;where Uare theKleading eigenvectors of S\u00001\n2\nWSBS\u00001\n2\nW, assuming SWis non-singular. (If it is singular, we can ﬁrst perform PCA on all the data.)\nFigure 9.5 gives an example of this method applied to some D= 10dimensional speech data,\nrepresenting C= 11diﬀerent vowel sounds. We project to K= 2dimensions in order to visualize\nthe data. We see that FLDA gives better class separation than PCA. Note that FLDA is restricted to ﬁnding at most a K\u0014C\u00001dimensional linear subspace, no\nmatter how large D, because the rank of the between class scatter matrix SBisC\u00001. (The -1 term\narises because of the \u0016term, which is a linear function of the \u0016c.) This is a rather severe restriction\nwhich limits the usefulness of FLDA. 9.3 Naive Bayes classiﬁers\nInthissection, wediscussasimplegenerativeapproachtoclassiﬁcationinwhichweassumethefeatures\nare conditionally independent given the class label. This is called the naive Bayes assumption .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 828, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 982}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0829_385a3ab9", "text": "This is called the naive Bayes assumption . The model is called “naive” since we do not expect the features to be independent, even conditional on\nthe class label. However, even if the naive Bayes assumption is not true, it often results in classiﬁers\nthat work well [DP97]. One reason for this is that the model is quite simple (it only has O(CD)\nparameters, for Cclasses and Dfeatures), and hence it is relatively immune to overﬁtting. More precisely, the naive Bayes assumption corresponds to using a class conditional density of the\nfollowing form:\np(xjy=c;\u0012) =DY\nd=1p(xdjy=c;\u0012dc) (9.46)\nwhere\u0012dcare the parameters for the class conditional density for class cand feature d. Hence the\nposterior over class labels is given by\np(y=cjx;\u0012) =p(y=cj\u0019)QD\nd=1p(xdjy=c;\u0012dc)\nP\nc0p(y=c0j\u0019)QD\nd=1p(xdjy=c0;\u0012dc0)(9.47)\nwhere\u0019cis the prior probability of class c, and\u0012= (\u0019;f\u0012dcgg)are all the parameters. This is known\nas anaive Bayes classiﬁer orNBC.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 829, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 940}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0830_b6102e70", "text": "This is known\nas anaive Bayes classiﬁer orNBC. 9.3.1 Example models\nWe still need to specify the form of the probability distributions in Equation (9.46). This depends on\nwhat type of feature xdis. We give some examples below:\n•In the case of binary features, xd2f0;1g, we can use the Bernoulli distribution: p(xjy=c;\u0012) =QD\nd=1Ber(xdj\u0012dc), where\u0012dcis the probability that xd= 1in classc. This is sometimes called\nthemultivariate Bernoulli naive Bayes model. For example, Figure 9.6 shows the estimated\nparameters for each class when we ﬁt this model to a binarized version of MNIST. This approach\ndoes surprisingly well, and has a test set accuracy of 84.3%. (See Figure 9.7 for some sample\npredictions.)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n9.3. Naive Bayes classiﬁers 327\nFigure 9.6: Visualization of the Bernoulli class conditional densities for a naive Bayes classiﬁer ﬁt to a\nbinarized version of the MNIST dataset. Generated by code at ﬁgures.probml.ai/book1/9.6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 830, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1008}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0831_f1c08f03", "text": "Generated by code at ﬁgures.probml.ai/book1/9.6. 1\n 4\n 9\n 4\n 9\nFigure 9.7: Visualization of the predictions made by the model in Figure 9.6 when applied to some bi-\nnarized MNIST test images. The title shows the most probable predicted class. Generated by code at\nﬁgures.probml.ai/book1/9.7. •In the case of categorical features, xd2f1;:::;Kg, we can use the categorical distribution:\np(xjy=c;\u0012) =QD\nd=1Cat(xdj\u0012dc), where\u0012dckis the probability that xd=kgiven thaty=c. •In the case of real-valued features, xd2R, we can use the univariate Gaussian distribution:\np(xjy=c;\u0012) =QD\nd=1N(xdj\u0016dc;\u001b2\ndc), where\u0016dcis the mean of feature dwhen the class label is\nc, and\u001b2\ndcis its variance. (This is equivalent to Gaussian discriminant analysis using diagonal\ncovariance matrices.)\n9.3.2 Model ﬁtting\nIn this section, we discuss how to ﬁt a naive Bayes classiﬁer using maximum likelihood estimation.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 831, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 888}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0832_fdb03905", "text": "We can write the likelihood as follows:\np(Dj\u0012) =NY\nn=1Cat(ynj\u0019)DY\nd=1p(xndjyn;\u0012d) (9.48)\n=NY\nn=1Cat(ynj\u0019)DY\nd=1CY\nc=1p(xndj\u0012d;c)I(yn=c)(9.49)\nso the log-likelihood is given by\nlogp(Dj\u0012) =\"NX\nn=1CX\nc=1I(yn=c) log\u0019c#\n+CX\nc=1DX\nd=1\"X\nn:yn=clogp(xndj\u0012dc)#\n(9.50)\nWe see that this decomposes into a term for \u0019, andCDterms for each \u0012dc:\nlogp(Dj\u0012) = logp(Dyj\u0019) +X\ncX\ndlogp(Ddcj\u0012dc) (9.51)\nwhereDy=fyn:n= 1 :Ngare all the labels, and Ddc=fxnd:yn=cgare all the values of feature\ndfor examples from class c. Hence we can estimate these parameters separately. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n328 Chapter 9. Linear Discriminant Analysis\nIn Section 4.2.4, we show that the MLE for \u0019is the vector of empirical counts, ^\u0019c=Nc\nN. The\nMLEs for\u0012dcdepend on the choice of the class conditional density for feature d. We discuss some\ncommon choices below. •In the case of discrete features, we can use a categorical distribution.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 832, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 937}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0833_5e295a30", "text": "The\nMLEs for\u0012dcdepend on the choice of the class conditional density for feature d. We discuss some\ncommon choices below. •In the case of discrete features, we can use a categorical distribution. A straightforward extension\nof the results in Section 4.2.4 gives the following expression for the MLE:\n^\u0012dck=NdckPK\nk0=1Ndck0=Ndck\nNc(9.52)\nwhereNdck=PN\nn=1I(xnd=k;yn=c)is the number of times that feature dhad valuekin\nexamples of class c. •In the case of binary features, the categorical distribution becomes the Bernoulli, and the MLE\nbecomes\n^\u0012dc=Ndc\nNc(9.53)\nwhich is the empirical fraction of times that feature dis on in examples of class c. •In the case of real-valued features, we can use a Gaussian distribution. A straightforward extension\nof the results in Section 4.2.5 gives the following expression for the MLE:\n^\u0016dc=1\nNcX\nn:yn=cxnd (9.54)\n^\u001b2\ndc=1\nNcX\nn:yn=c(xnd\u0000^\u0016dc)2(9.55)\nThus we see that ﬁtting a naive Bayes classiﬁer is extremely simple and eﬃcient.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 833, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 968}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0834_aa646d55", "text": "9.3.3 Bayesian naive Bayes\nInthissection, weextendourdiscussionofMLEestimationfornaiveBayesclassiﬁersfromSection9.3.2\nto compute the posterior distribution over the parameters. For simplicity, let us assume we have\ncategorical features, so p(xdj\u0012dc) =Cat(xdj\u0012dc), where\u0012dck=p(xd=kjy=c). In Section 4.6.3.2,\nwe show that the conjugate prior for the categorical likelihood is the Dirichlet distribution, p(\u0012dc) =\nDir(\u0012dcj\fdc), where\fdckcan be interpereted as a set of “ pseudo counts ”, corresponding to counts\nNdckthat come from prior data. Similarly we use a Dirichlet prior for the label frequencies,\np(\u0019) =Dir(\u0019j\u000b). By using a conjugate prior, we can compute the posterior in closed form, as we\nexplain in Section 4.6.3. In particular, we have\np(\u0012jD) = Dir(\u0019ja\u000b)DY\nd=1CY\nc=1Dir(\u0012dcja\fdc) (9.56)\nwherea\u000bc=`\u000bc+Ncanda\fdck=`\fdck+Ndck. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n9.3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 834, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 913}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0835_c5943d2a", "text": "In particular, we have\np(\u0012jD) = Dir(\u0019ja\u000b)DY\nd=1CY\nc=1Dir(\u0012dcja\fdc) (9.56)\nwherea\u000bc=`\u000bc+Ncanda\fdck=`\fdck+Ndck. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n9.3. Naive Bayes classiﬁers 329\nUsing the results from Section 4.6.3.4, we can derive the posterior predictive distribution as follows. The prior over the label is given by p(yjD) =Cat(yj\u0019), where\u0019c=a\u000bc=P\nc0a\u000bc0. For the features,\nwe havep(xd=kjy=c;D) =\u0012dck, where\n\u0012dck=a\fdckP\nk0a\fdck0=`\fdck+NdckP\nk0`\fdck0+Ndck0(9.57)\nis the posterior mean of the parameters. If`\fdck= 0, this reduces to the MLE in Equation (9.52). By contrast, if we set`\fdck= 1, we add\n1 to all the empirical counts before normalizing. This is called add-one smoothing orLaplace\nsmoothing .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 835, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 745}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0836_200fc1ec", "text": "If`\fdck= 0, this reduces to the MLE in Equation (9.52). By contrast, if we set`\fdck= 1, we add\n1 to all the empirical counts before normalizing. This is called add-one smoothing orLaplace\nsmoothing . For example, in the binary case, this gives\n\u0012dc=`\fdc1+Ndc1\n`\fdc0+Ndc0+`\fdc1+Ndc1=1 +Ndc1\n2 +Nc(9.58)\nOnce we have estimated the parameter posterior, we can compute the predicted distribution over\nthe label as follows:\np(y=cjx;D)/p(y=cjD)Y\ndp(xdjy=c;D) =\u0019cY\ndY\nk\u0012I(xd=k)\ndck (9.59)\nThis gives us a fully Bayesian form of naive Bayes, in which we have integrated out all the parameters. (In this case, the predictive distribution can be obtained merely by plugging in the posterior mean\nparameters.)\n9.3.4 The connection between naive Bayes and logistic regression\nIn this section, we show that the class posterior p(yjx;\u0012)for a NBC model has the same form as\nmultinomial logistic regression.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 836, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 890}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0837_227a18c8", "text": "For simplicity, we assume that the features are all discrete, and each\nhasKstates, although the result holds for arbitrary feature distributions in the exponential family. Letxdk=I(xd=k), soxdis a one-hot encoding of feature d. Then the class conditional density\ncan be written as follows:\np(xjy=c;\u0012) =DY\nd=1Cat(xdjy=c;\u0012) =DY\nd=1KY\nk=1\u0012xdk\ndck(9.60)\nHence the posterior over classes is given by\np(y=cjx;\u0012) =\u0019cQ\ndQ\nk\u0012xdk\ndckP\nc0\u0019c0Q\ndQ\nk\u0012xdk\ndc0k=exp[log\u0019c+P\ndP\nkxdklog\u0012dck]P\nc0exp[log\u0019c0+P\ndP\nkxdklog\u0012dc0k](9.61)\nThis can be written as a softmax\np(y=cjx;\u0012) =e\fT\ncx+\rc\nPC\nc0=1e\fT\nc0x+\rc0(9.62)\nby suitably deﬁning \fcand\rc. This has exactly the same form as multinomial logistic regression in\nSection 2.5.3. The diﬀerence is that with naive Bayes we optimize the joint likelihoodQ\nnp(yn;xnj\u0012),\nwhereas with logistic regression, we optimize the conditional likelihoodQ\nnp(ynjxn;\u0012). In general,\nthese can give diﬀerent results (see Exercise 10.3). Author: Kevin P. Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 837, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 983}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0838_d4453094", "text": "In general,\nthese can give diﬀerent results (see Exercise 10.3). Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n330 Chapter 9. Linear Discriminant Analysis\n0.0 0.2 0.4 0.6 0.8 1.0\nx01234class conditional densitiesp(x|y=1)p(x|y=2)\n(a)\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.01.2\np(y=1|x) p(y=2|x) (b)\nFigure 9.8: The class-conditional densities p(xjy=c)(left) may be more complex than the class posteriors\np(y=cjx)(right). Adapted from Figure 1.27 of [Bis06]. Generated by code at ﬁgures.probml.ai/book1/9.8. 9.4 Generative vs discriminative classiﬁers\nA model of the form p(x;y) =p(y)p(xjy)is called a generative classiﬁer , since it can be used\nto generate examples xfrom each class y. By contrast, a model of the form p(yjx)is called a\ndiscriminative classiﬁer , since it can only be used to discriminate between diﬀerent classes. Below\nwe discuss various pros and cons of the generative and discriminative approaches to classiﬁcation.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 838, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 951}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0839_e18f53bb", "text": "Below\nwe discuss various pros and cons of the generative and discriminative approaches to classiﬁcation. 9.4.1 Advantages of discriminative classiﬁers\nThe main advantages of discriminative classiﬁers are as follows:\n•Better predictive accuracy . Discriminative classiﬁers are often much more accurate than\ngenerative classiﬁers [NJ02]. The reason is that the conditional distribution p(yjx)is often much\nsimpler (and therefore easier to learn) than the joint distribution p(y;x), as illustrated in Figure 9.8. In particular, discriminative models do not need to “waste eﬀort” modeling the distribution of the\ninput features. •Can handle feature preprocessing . A big advantage of discriminative methods is that they\nallow us to preprocess the input in arbitrary ways. For example, we can perform a polynomial\nexpansion of the input features, and we can replace a string of words with embedding vectors (see\nSection 20.5).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 839, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 921}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0840_37055856", "text": "For example, we can perform a polynomial\nexpansion of the input features, and we can replace a string of words with embedding vectors (see\nSection 20.5). It is often hard to deﬁne a generative model on such pre-processed data, since the\nnew features can be correlated in complex ways which are hard to model. •Well-calibrated probabilities . Some generative classiﬁers, such as naive Bayes (described in\nSection 9.3), make strong independence assumptions which are often not valid. This can result\nin very extreme posterior class probabilities (very near 0 or 1). Discriminative models, such as\nlogistic regression, are often better calibrated in terms of their probability estimates, although\nthey also sometimes need adjustment (see e.g., [NMC05]). Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n9.4. Generative vs discriminative classiﬁers 331\n9.4.2 Advantages of generative classiﬁers\nThe main advantages of generative classiﬁers are as follows:\n•Easy to ﬁt.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 840, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 991}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0841_831dbeb0", "text": "August 27, 2021\n9.4. Generative vs discriminative classiﬁers 331\n9.4.2 Advantages of generative classiﬁers\nThe main advantages of generative classiﬁers are as follows:\n•Easy to ﬁt. Generative classiﬁers are often very easy to ﬁt. For example, in Section 9.3.2, we\nshow how to ﬁt a naive Bayes classiﬁer by simple counting and averaging. By contrast, logistic\nregression requires solving a convex optimization problem (see Section 10.2.3 for the details), and\nneural nets require solving a non-convex optimization problem, both of which are much slower. •Can easily handle missing input features. Sometimes some of the inputs (components of x)\nare not observed. In a generative classiﬁer, there is a simple method for dealing with this, as we\nshow in Section 1.5.5. However, in a discriminative classiﬁer, there is no principled solution to\nthis problem, since the model assumes that xis always available to be conditioned on. •Can ﬁt classes separately.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 841, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 953}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0842_20b936fc", "text": "However, in a discriminative classiﬁer, there is no principled solution to\nthis problem, since the model assumes that xis always available to be conditioned on. •Can ﬁt classes separately. In a generative classiﬁer, we estimate the parameters of each class\nconditional density independently (as we show in Section 9.3.2), so we do not have to retrain\nthe model when we add more classes. In contrast, in discriminative models, all the parameters\ninteract, so the whole model must be retrained if we add a new class. •Can handle unlabeled training data. It is easy to use generative models for semi-supervised\nlearning, in which we combine labeled data Dxy=f(xn;yn)gand unlabeled data, Dx=fxng. However, this is harder to do with discriminative models, since there is no uniquely optimal way\nto exploitDx. 9.4.3 Handling missing features\nSometimes we are missing parts of the input xduring training and/or testing.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 842, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 912}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0843_83ef1214", "text": "9.4.3 Handling missing features\nSometimes we are missing parts of the input xduring training and/or testing. In a generative\nclassiﬁer, we can handle this situation by marginalizing out the missing values. (We assume that\nthe missingness of a feature is not informative about its potential value.) By contrast, when using\na discriminative model, there is no unique best way to handle missing inputs, as we discuss in\nSection 1.5.5. For example, suppose we are missing the value of x1. We just have to compute\np(y=cjx2:D;\u0012)/p(y=cj\u0019)p(x2:Djy=c;\u0012) (9.63)\n=p(y=cj\u0019)X\nx1p(x1;x2:Djy=c;\u0012) (9.64)\nIn Gaussian discriminant analysis, we can marginalize out x1using the equations from Section 3.2.3. If we make the naive Bayes assumption, things are even easier, since we can just ignore the\nlikelihood term for x1. This follows because\nX\nx1p(x1;x2:Djy=c;\u0012) =\"X\nx1p(x1j\u00121c)#DY\nd=2p(xdj\u0012dc) =DY\nd=2p(xdj\u0012dc) (9.65)\nwhere we exploited the fact thatP\nx1p(x1jy=c;\u00121c) = 1. Author: Kevin P. Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 843, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 997}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0844_5ffc35a2", "text": "This follows because\nX\nx1p(x1;x2:Djy=c;\u0012) =\"X\nx1p(x1j\u00121c)#DY\nd=2p(xdj\u0012dc) =DY\nd=2p(xdj\u0012dc) (9.65)\nwhere we exploited the fact thatP\nx1p(x1jy=c;\u00121c) = 1. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n332 Chapter 9. Linear Discriminant Analysis\n9.5 Exercises\nExercise 9.1 [Derivation of Fisher’s linear discriminant]\nShow that the maximum of J(w) =wTSBw\nwTSWwis given by SBw=\u0015SWw\nwhere\u0015=wTSBw\nwTSWw:Hint: recall that the derivative of a ratio of two scalars is given byd\ndxf(x)\ng(x)=f0g\u0000fg0\ng2;\nwheref0=d\ndxf(x)andg0=d\ndxg(x). Also, recall thatd\ndxxTAx= (A+AT)x:\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n10 Logistic Regression\n10.1 Introduction\nLogistic regression is a widely used discriminative classiﬁcation model p(yjx;\u0012), wherex2RD\nis a ﬁxed-dimensional input vector, y2f1;:::;Cgis the class label, and \u0012are the parameters.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 844, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 875}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0845_9dd266c2", "text": "If\nC= 2, this is known as binary logistic regression , and ifC > 2, it is known as multinomial\nlogistic regression , or alternatively, multiclass logistic regression . We give the details below. 10.2 Binary logistic regression\nBinary logistic regression corresponds to the following model\np(yjx;\u0012) = Ber(yj\u001b(wTx+b)) (10.1)\nwhere\u001bis the sigmoid function deﬁned in Section 2.4.2, ware the weights, bis the bias, and\n\u0012= (w;b)are all the parameters. In other words,\np(y= 1jx;\u0012) =\u001b(a) =1\n1 +e\u0000a(10.2)\nwherea=wTx+bis the log-odds, log(p=1\u0000p), wherep=p(y= 1jx;\u0012), as explained in Section 2.4.2. (In ML, the quantity ais usually called the logitor thepre-activation .)\nSometimes we choose to use the labels ~y2f\u0000 1;+1ginstead ofy2f0;1g. We can compute the\nprobability of these alternative labels using\np(~yjx;\u0012) =\u001b(~ya) (10.3)\nsince\u001b(\u0000a) = 1\u0000\u001b(a). This slightly more compact notation is widely used in the ML literature. 10.2.1 Linear classiﬁers\nThe sigmoid gives the probability that the class label is y= 1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 845, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1001}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0846_d8444a93", "text": "This slightly more compact notation is widely used in the ML literature. 10.2.1 Linear classiﬁers\nThe sigmoid gives the probability that the class label is y= 1. If the loss for misclassifying each class\nis the same, then the optimal decision rule is to predict y= 1iﬀ class 1 is more likely than class 0, as\nwe explained in Section 5.1.2.2. Thus\nf(x) =I(p(y= 1jx)>p(y= 0jx)) =I\u0012\nlogp(y= 1jx)\np(y= 0jx)>0\u0013\n=I(a>0) (10.4)\n334 Chapter 10. Logistic Regression\n(x, y, z)(x , y , z )0 0 0w 1w 2w 3w=[]\nxz\ny\n(a)\n3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0\nPetal length1.001.251.501.752.002.252.50Petal width\nNot Iris-VirginicaIris-Virginica (b)\nFigure 10.1: (a) Visualization of a 2d plane in a 3d space with surface normal wgoing through point\nx0= (x0;y0;z0). See text for details. (b) Visualization of optimal linear decision boundary induced by logistic\nregression on a 2-class, 2-feature version of the iris dataset. Generated by code at ﬁgures.probml.ai/book1/10.1. Adapted from Figure 4.24 of [Gér19]. wherea=wTx+b.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 846, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1007}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0847_6024d453", "text": "Generated by code at ﬁgures.probml.ai/book1/10.1. Adapted from Figure 4.24 of [Gér19]. wherea=wTx+b. Thus we can write the prediction function as follows:\nf(x;\u0012) =b+wTx=b+DX\nd=1wdxd (10.5)\nwherewTx=hw;xiis the inner product between the weight vector wand the feature vector x. This function deﬁnes a linear hyperplane , with normal vector w2RDand an oﬀset b2Rfrom\nthe origin. Equation (10.5) can be understood by looking at Figure 10.1a. Here we show a plane in a 3d\nfeature space going through the point x0with surface normal w. Points on the surface satisfy\nwT(x\u0000x0) = 0. If we deﬁne b=\u0000wTx0, we can rewrite this as wTx+b= 0. This plane separates\n3d space into two half spaces . This linear plane is known as a decision boundary . If we can\nperfectly separate the training examples by such a linear boundary (without making any classiﬁcation\nerrors on the training set), we say the data is linearly separable .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 847, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 912}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0848_e9cb7793", "text": "If we can\nperfectly separate the training examples by such a linear boundary (without making any classiﬁcation\nerrors on the training set), we say the data is linearly separable . From Figure 10.1b, we see that\nthe two-class, two-feature version of the iris dataset is not linearly separable. In general, there will be uncertainty about the correct class label, so we need to predict a probability\ndistribution over labels, and not just decide which side of the decision boundary we are on. In\nFigure 10.2, we plot p(y= 1jx1;x2;w) =\u001b(w1x1+w2x2)for diﬀerent weight vectors w. The vector\nwdeﬁnes the orientation of the decision boundary, and its magnitude, jjwjj=qPD\nd=1w2\nd, controls\nthe steepness of the sigmoid, and hence the conﬁdence of the predictions. 10.2.2 Nonlinear classiﬁers\nWe can often make a problem linearly separable by preprocessing the inputs in a suitable way. In\nparticular, let \u001e(x)be a transformed version of the input feature vector.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 848, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 955}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0849_64a15f0d", "text": "10.2.2 Nonlinear classiﬁers\nWe can often make a problem linearly separable by preprocessing the inputs in a suitable way. In\nparticular, let \u001e(x)be a transformed version of the input feature vector. For example, suppose we\nuse\u001e(x1;x2) = [1;x2\n1;x2\n2], and we letw= [\u0000R2;1;1]. ThenwT\u001e(x) =x2\n1+x2\n2\u0000R2, so the decision\nboundary (where f(x) = 0) deﬁnes a circle with radius R, as shown in Figure 10.3. The resulting\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n10.2. Binary logistic regression 335\n4\n 2\n 0 2 44\n2\n024\n10\n5051010\n505100.00.51.0W=(-3,-3)\n10\n5051010\n505100.00.51.0W=(0,-3)\n10\n5051010\n505100.00.51.0W=(3,-3)10\n5051010\n505100.00.51.0W=(-3,0)\n10\n5051010\n505100.00.51.0W=(0,0.5)\n10\n5051010\n505100.00.51.0W=(3,0)10\n5051010\n505100.00.51.0W=(-3,3)\n10\n5051010\n505100.00.51.0W=(0,3)\n10\n5051010\n505100.00.51.0W=(3,3)\nFigure 10.2: Plots of \u001b(w1x1+w2x2). Herew= (w1;w2)deﬁnes the normal to the decision boundary.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 849, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 942}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0850_03f05f6f", "text": "Herew= (w1;w2)deﬁnes the normal to the decision boundary. Points to the right of this have \u001b(wTx)>0:5, and points to the left have \u001b(wTx)<0:5. Adapted from Figure\n39.3 of [Mac03]. Generated by code at ﬁgures.probml.ai/book1/10.2. 2 Rx1\nx2x1\nx22\nFigure 10.3: Illustration of how we can transform a quadratic decision boundary into a linear one by\ntransforming the features from x= (x1;x2)to\u001e(x) = (x2\n1;x2\n2). Used with kind permission of Jean-Philippe\nVert. functionfis still linear in the parameters w, which is important for simplifying the learning problem,\nas we will see in Section 10.2.3. However, we can gain even more power by learning the parameters\nof the feature extractor \u001e(x)in addition to linear weights w; we discuss how to do this in Part III. In Figure 10.3, we used a quadratic expansion of the features. We can also use a higher order\npolynomial, as in Section 1.2.2.2. In Figure 1.7, we show the eﬀects of using polynomial expansion\nup to degree Kon a 2d logistic regression problem.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 850, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1003}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0851_e212d65b", "text": "We can also use a higher order\npolynomial, as in Section 1.2.2.2. In Figure 1.7, we show the eﬀects of using polynomial expansion\nup to degree Kon a 2d logistic regression problem. As in Figure 1.7, we see that the model becomes\nmore complex as the number of parameters increases, and eventually results in overﬁtting. We discuss\nways to reduce overﬁtting in Section 10.2.7. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n336 Chapter 10. Logistic Regression\n10.2.3 Maximum likelihood estimation\nIn this section, we discuss how to estimate the parameters of a logistic regression model using\nmaximum likelihood estimation.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 851, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 634}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0852_ff548a65", "text": "Logistic Regression\n10.2.3 Maximum likelihood estimation\nIn this section, we discuss how to estimate the parameters of a logistic regression model using\nmaximum likelihood estimation. 10.2.3.1 Objective function\nThe negative log likelihood (scaled by the dataset size N) is given by the following (we assume the\nbias termbis absorbed into the weight vector w):\nNLL(w) =\u00001\nNlogp(Djw) =\u00001\nNlogNY\nn=1Ber(ynj\u0016n) (10.6)\n=\u00001\nNNX\nn=1log[\u0016yn\nn\u0002(1\u0000\u0016n)1\u0000yn] (10.7)\n=\u00001\nNNX\nn=1[ynlog\u0016n+ (1\u0000yn) log(1\u0000\u0016n)] (10.8)\n=1\nNNX\nn=1H(yn;\u0016n) (10.9)\nwhere\u0016n=\u001b(an)is the probability of class 1, an=wTxnis the log odds, and H(yn;\u0016n)is the\nbinary cross entropy deﬁned by\nH(p;q) =\u0000[plogq+ (1\u0000p) log(1\u0000q)] (10.10)\nIf we use ~yn2f\u0000 1;+1ginstead ofyn2f0;1g, then we can rewrite this as follows:\nNLL(w) =\u00001\nNNX\nn=1[I(~yn= 1) log(\u001b(an)) +I(~yn=\u00001) log(\u001b(\u0000an))] (10.11)\n=\u00001\nNNX\nn=1log(\u001b(~ynan)) (10.12)\n=1\nNNX\nn=1log(1 + exp(\u0000~ynan)) (10.13)\nHowever, in this book, we will mostly use the yn2f0;1gnotation, since it is easier to generalize to\nthe multiclass case (Section 10.3), and makes the connection with cross-entropy easier to see.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 852, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1103}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0853_e079c640", "text": "10.2.3.2 Optimizing the objective\nTo ﬁnd the MLE, we must solve\nrwNLL(w) =g(w) =0 (10.14)\nWe can use any gradient-based optimization algorithm to solve this, such as those we discuss in\nChapter 8. We give a speciﬁc example in Section 10.2.4. But ﬁrst we must derive the gradient, as we\nexplain below. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n10.2. Binary logistic regression 337\n(a)\n (b)\n(c)\n (d)\nFigure 10.4: Polynomial feature expansion applied to a two-class, two-dimensional logistic regression problem. (a) Degree K= 1. (b) Degree K= 2. (c) Degree K= 4. (d) Train and test error vs degree. Generated by\ncode at ﬁgures.probml.ai/book1/10.4. 10.2.3.3 Deriving the gradient\nAlthough we can use automatic diﬀerentiation methods (Section 13.3) to compute the gradient of\nthe NLL, it is also easy to do explicitly, as we show below.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 853, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 866}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0854_dc5e7dd1", "text": "10.2.3.3 Deriving the gradient\nAlthough we can use automatic diﬀerentiation methods (Section 13.3) to compute the gradient of\nthe NLL, it is also easy to do explicitly, as we show below. Fortunately the resulting equations will\nturn out to have a simple and intuitive interpretation, which can be used to derive other methods, as\nwe will see. To start, note that\nd\u0016n\ndan=\u001b(an)(1\u0000\u001b(an)) (10.15)\nwherean=wTxnand\u0016n=\u001b(an). Hence by the chain rule (and the rules of vector calculus,\ndiscussed in Section 7.8) we have\n@\n@wd\u0016n=@\n@wd\u001b(wTxn) =@\n@an\u001b(an)@an\n@wd=\u0016n(1\u0000\u0016n)xnd (10.16)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n338 Chapter 10. Logistic Regression\nThe gradient for the bias term can be derived in the same way, by using the input xn0= 1in the\nabove equation. However, we will ignore the bias term for simplicity.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 854, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 832}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0855_159b0bdd", "text": "Logistic Regression\nThe gradient for the bias term can be derived in the same way, by using the input xn0= 1in the\nabove equation. However, we will ignore the bias term for simplicity. Hence\nrwlog(\u0016n) =1\n\u0016nrw\u0016n= (1\u0000\u0016n)xn (10.17)\nSimilarly,\nrwlog(1\u0000\u0016n) =\u0000\u0016n(1\u0000\u0016n)xn\n1\u0000\u0016n=\u0000\u0016nxn (10.18)\nThus the gradient vector of the NLL is given by\nrwNLL(w) =\u00001\nNNX\nn=1[yn(1\u0000\u0016n)xn\u0000(1\u0000yn)\u0016nxn] (10.19)\n=\u00001\nNNX\nn=1[ynxn\u0000ynxn\u0016n\u0000xn\u0016n+ynxn\u0016n)] (10.20)\n=1\nNNX\nn=1(\u0016n\u0000yn)xn (10.21)\nIf we interpret en=\u0016n\u0000ynas an error signal, we can see that the gradient weights each input xn\nby its error, and then averages the result. Note that we can rewrite the gradient in matrix form as\nfollows:\nrwNLL(w) =1\nN(1T\nN(diag(\u0016\u0000y)X))T(10.22)\n10.2.3.4 Deriving the Hessian\nGradient-based optimizers will ﬁnd a stationary point where g(w) =0. This could either be a global\noptimum or a local optimum. To be sure the stationary point is the global optimum, we must show\nthat the objective is convex, for reasons we explain in Section 8.1.1.1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 855, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 999}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0856_dd3a7651", "text": "This could either be a global\noptimum or a local optimum. To be sure the stationary point is the global optimum, we must show\nthat the objective is convex, for reasons we explain in Section 8.1.1.1. Intuitvely this means that\nthe NLL has a bowl shape , with a unique lowest point, which is indeed the case, as illustrated in\nFigure 10.5b. More formally, we must prove that the Hessian is positive semi-deﬁnite, which we now do. (See\nChapter 7 for relevant background information on linear algebra.) One can show that the Hessian is\ngiven by\nH(w) =rwrT\nwNLL(w) =1\nNNX\nn=1(\u0016n(1\u0000\u0016n)xn)xT\nn=1\nNXTSX (10.23)\nwhere\nS,diag(\u00161(1\u0000\u00161);:::;\u0016N(1\u0000\u0016N)) (10.24)\nWe see that His positive deﬁnite, since for any nonzero vector v, we have\nvTXTSXv= (vTXTS1\n2)(S1\n2Xv) =jjvTXTS1\n2jj2\n2>0 (10.25)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n10.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 856, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 857}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0857_868ec056", "text": "August 27, 2021\n10.2. Binary logistic regression 339\n−7.5−5.0−2.5 0.0 2.5 5.0 7.5\nw−10−50510bLoss function surface\n0.02.44.87.29.612.014.416.8\nNLL\n(a)\n−7.5−5.0−2.50.02.55.07.5−15−10−5051015246810121416 (b)\nFigure 10.5: NLL loss surface for binary logistic regression applied to Iris dataset with 1 feature and 1 bias\nterm. The goal is to minimize the function. Generated by code at ﬁgures.probml.ai/book1/10.5. This follows since \u0016n>0for alln, because of the use of the sigmoid function. Consequently the\nNLL is strictly convex. However, in practice, values of \u0016nwhich are close to 0 or 1 might cause\nthe Hessian to be close to singular. We can avoid this by using `2regularization, as we discuss in\nSection 10.2.7.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 857, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 715}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0858_57c4520e", "text": "However, in practice, values of \u0016nwhich are close to 0 or 1 might cause\nthe Hessian to be close to singular. We can avoid this by using `2regularization, as we discuss in\nSection 10.2.7. 10.2.4 Stochastic gradient descent\nOur goal is to solve the following optimization problem\n^w,argmin\nwL(w) (10.26)\nwhereL(w)is the loss function, in this case the negative log likelihood:\nNLL(w) =\u00001\nNNX\nn=1[ynlog\u0016n+ (1\u0000yn) log(1\u0000\u0016n)] (10.27)\nwhere\u0016n=\u001b(an)is the probability of class 1, and an=wTxnis the log odds. There are many algorithms we could use to solve Equation (10.26), as we discuss in Chapter 8. Perhaps the simplest is to use stochastic gradient descent (Section 8.4). If we use a minibatch of size\n1, then we get the following simple update equation:\nwt+1=wt\u0000\u001atrwNLL(wt) =wt\u0000\u001at(\u0016n\u0000yn)xn (10.28)\nwhere we replaced the average over all Nexamples in the gradient of Equation (10.21) with a single\nstochastically chosen sample n.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 858, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 926}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0859_07450c63", "text": "(The index nchanges with t.)\nSince we know the objective is convex (see Section 10.2.3.4), then one can show that this procedure\nwill converge to the global optimum, provided we decay the learning rate at the appropriate rate (see\nSection 8.4.3). We can improve the convergence speed using variance reduction techniques such as\nSAGA (Section 8.4.5.2). Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n340 Chapter 10. Logistic Regression\n10.2.5 Perceptron algorithm\nAperceptron , ﬁrst introduced in [Ros58], is a deterministic binary classiﬁer of the following form:\nf(xn;\u0012) =I\u0000\nwTxn+b>0\u0001\n(10.29)\nThis can be seen to be a limiting case of a binary logistic regression classiﬁer, in which the sigmoid\nfunction\u001b(a)is replaced by the Heaviside step function H(a),I(a>0). See Figure 2.10 for a\ncomparison of these two functions. Since the Heaviside function is not diﬀerentiable, we cannot use gradient-based optimization\nmethods to ﬁt this model.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 859, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 953}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0860_726de3a0", "text": "See Figure 2.10 for a\ncomparison of these two functions. Since the Heaviside function is not diﬀerentiable, we cannot use gradient-based optimization\nmethods to ﬁt this model. However, Rosenblatt proposed the perceptron learning algorithm\ninstead. The basic idea is to start with random weights, and then iteratively update them whenever\nthe model makes a prediction mistake. More precisely, we update the weights using\nwt+1=wt\u0000\u001at(^yn\u0000yn)xn (10.30)\nwhere (xn;yn)is the labeled example sampled at iteration t, and\u001atis the learning rate or step\nsize. (We can set the step size to 1, since the magnitude of the weights does not aﬀect the decision\nboundary.) See code.probml.ai/book1/perceptron_demo_2d for a simple implementation of this\nalgorithm. The perceptron update rule in Equation (10.30) has an intuitive interpretation: if the prediction is\ncorrect, no change is made, otherwise we move the weights in a direction so as to make the correct\nanswer more likely.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 860, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 965}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0861_00f43542", "text": "More precisely, if yn= 1and^yn= 0, we havewt+1=wt+xn, and ifyn= 0\nand^yn= 1, we havewt+1=wt\u0000xn. By comparing Equation (10.30) to Equation (10.28), we see that the perceptron update rule is\nequivalent to the SGD update rule for binary logistic regression using the approximation where we\nreplace the soft probabilities \u0016n=p(yn= 1jxn)with hard labels ^yn=f(xn). The advantage of the\nperceptron method is that we don’t need to compute probabilities, which can be useful when the\nlabel space is very large. The disadvantage is that the method will only converge when the data is\nlinearly separable [Nov62], whereas SGD for minimizing the NLL for logistic regression will always\nconverge to the globally optimal MLE, even if the data is not linearly separable. In Section 13.2, we will generalize perceptrons to nonlinear functions, thus signiﬁcantly enhancing\ntheir usefulness.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 861, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 873}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0862_6d6d9136", "text": "In Section 13.2, we will generalize perceptrons to nonlinear functions, thus signiﬁcantly enhancing\ntheir usefulness. 10.2.6 Iteratively reweighted least squares\nGradient descent is a ﬁrst order optimization method, which means it only uses ﬁrst order gradients\nto navigate through the loss landscape. This can be slow, especially when some directions of space\npoint steeply downhill, whereas other have a shallower gradient, as is the case in Figure 10.5a. In\nsuch problems, it can be much faster to use a second order optimization method, that takes the\ncurvature of the space into account. We discuss such methods in more detail in Section 8.3. Here we just consider a simple second order\nmethod that works well for logistic regression. We focus on the full batch setting (so we assume N\nis small), since it is harder to make second order methods work in the stochastic setting (see e.g.,\n[Byr+16; Liu+18b] for some methods). The classic second-order method is Newton’s method .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 862, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 981}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0863_1c3376e7", "text": "The classic second-order method is Newton’s method . This consists of updates of the form\nwt+1=wt\u0000\u001atH\u00001\ntgt (10.31)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n10.2. Binary logistic regression 341\nwhere\nHt,r2L(w)jwt=r2L(wt) =H(wt) (10.32)\nis assumed to be positive-deﬁnite to ensure the update is well-deﬁned. If the Hessian is exact, we can\nset the step size to \u001at= 1. We now apply this method to logistic regression. Recall from Section 10.2.3.3 that the gradient\nand Hessian are given by\nrwNLL(w) =1\nNNX\nn=1(\u0016n\u0000yn)xn (10.33)\nH=1\nNXTSX (10.34)\nS,diag(\u00161(1\u0000\u00161);:::;\u0016N(1\u0000\u0016N)) (10.35)\nHence the Newton update has the form\nwt+1=wt\u0000H\u00001gt (10.36)\n=wt+ (XTStX)\u00001XT(y\u0000\u0016t) (10.37)\n= (XTStX)\u00001\u0002\n(XTStX)wt+XT(y\u0000\u0016t)\u0003\n(10.38)\n= (XTStX)\u00001XT[StXwt+y\u0000\u0016t] (10.39)\n= (XTStX)\u00001XTStzt (10.40)\nwhere we have deﬁned the working response as\nzt,Xwt+S\u00001\nt(y\u0000\u0016t) (10.41)\nandSt=diag(\u0016t;n(1\u0000\u0016t;n)).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 863, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 904}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0864_5a5a6515", "text": "Since Stis a diagonal matrix, we can rewrite the targets in component\nform as follows:\nzt;n=wT\ntxn+yn\u0000\u0016t;n\n\u0016t;n(1\u0000\u0016t;n)(10.42)\nEquation (10.40) is an example of a weighted least squares problem (Section 11.2.2.4), which is a\nminimizer of\nNX\nn=1St;n(zt;n\u0000wT\ntxn)2(10.43)\nThe overall method is therefore known as the iteratively reweighted least squares (IRLS)\nalgorithm, since at each iteration we solve a weighted least squares problem, where the weight matrix\nStchanges at each iteration. See Algorithm 2 for some pseudocode. Note that Fisher scoring is the same as IRLS except we replace the Hessian of the actual\nlog-likelihood with its expectation, i.e., we use the Fisher information matrix (Section 4.7.2) instead\nofH. Since the Fisher information matrix is independent of the data, it can be precomputed, unlike\nthe Hessian, which must be reevaluated at every iteration. This can be faster for problems with\nmany parameters. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n342 Chapter 10.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 864, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1007}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0865_da183fed", "text": "This can be faster for problems with\nmany parameters. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n342 Chapter 10. Logistic Regression\nAlgorithm 2: Iteratively reweighted least squares (IRLS)\n1w=0;\n2repeat\n3forn= 1 :Ndo\n4an=wTxn;\n5\u0016n=\u001b(an);\n6sn=\u0016n(1\u0000\u0016n);\n7zn=an+yn\u0000\u0016n\nsn;\n8 S= diag(s1:N);\n9w= (XTSX)\u00001XTSz;\n10untilconverged ;\n10.2.7 MAP estimation\nIn Figure 10.4, we saw how logistic regression can overﬁt when there are too many parameters\ncompared to training examples. This is a consequence of the ability of maximum likelihood to ﬁnd\nweights that force the decision boundary to “wiggle” in just the right way so as to curve around the\nexamples. To get this behavior, the weights often need to be set to large values.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 865, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 735}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0866_66ed0275", "text": "To get this behavior, the weights often need to be set to large values. For example, in\nFigure 10.4, when we use degree K= 1, we ﬁnd that the MLE for the two input weights (ignoring\nthe bias) is\n^w= [0:51291712;0:11866937] (10.44)\nWhen we use degree K= 2, we get\n^w= [2:27510513;0:05970325;11:84198867;15:40355969;2:51242311] (10.45)\nAnd whenK= 4, we get\n^w= [\u00003:07813766;\u0001\u0001\u0001;\u000059:03196044;51:77152431;10:25054164] (10.46)\nOne way to reduce such overﬁtting is to prevent the weights from becoming so large. We can do\nthis by using a zero-mean Gaussian prior, p(w) =N(wj0;CI), and then using MAP estimation, as\nwe discussed in Section 4.5.3. The new training objective becomes\nL(w) = NLL(w) +\u0015jjwjj2\n2 (10.47)\nwherejjwjj2\n2=PD\nd=1w2\ndand\u0015= 1=C. This is called `2regularization orweight decay . The\nlarger the value of \u0015, the more the parameters are penalized for being “large” (deviating from the\nzero-mean prior), and thus the less ﬂexible the model. See Figure 10.6 for an illustration.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 866, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0867_12f6ca2d", "text": "The\nlarger the value of \u0015, the more the parameters are penalized for being “large” (deviating from the\nzero-mean prior), and thus the less ﬂexible the model. See Figure 10.6 for an illustration. We can compute the MAP estimate by slightly modifying the input to the above gradient-based\noptimization algorithms. The gradient and Hessian of the penalized negative log likelihood have the\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n10.2. Binary logistic regression 343\n(a)\n (b)\n(c)\n100101102103104105\nInverse regularization0.150.200.250.300.350.40error rate\ntrain\ntest (d)\nFigure 10.6: Weight decay with variance Capplied to two-class, two-dimensional logistic regression problem\nwith a degree 4 polynomial. (a) C= 1. (b)C= 316. (c)C= 100;000. (d) Train and test error vs C. Generated by code at ﬁgures.probml.ai/book1/10.6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 867, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 855}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0868_0718e978", "text": "(a) C= 1. (b)C= 316. (c)C= 100;000. (d) Train and test error vs C. Generated by code at ﬁgures.probml.ai/book1/10.6. following forms:\nPNLL(w) = NLL(w) +\u0015wTw (10.48)\nrwPNLL(w) =g(w) + 2\u0015w (10.49)\nr2\nwPNLL(w) =H(w) + 2\u0015I (10.50)\nwhereg(w)is the gradient and H(w)is the Hessian of the unpenalized NLL. For an interesting exercise related to `2regularized logistic regression, see Exercise 10.2. 10.2.8 Standardization\nIn Section 10.2.7, we use an isotropic prior N(wj0;\u0015\u00001I)to prevent overﬁtting. This implicitly\nencodes the assumption that we expect all weights to be similar in magnitude, which in turn encodes\nthe assumption we expect all input features to be similar in magnitude. However, in many datasets,\ninput features are on diﬀerent scales. In such cases, it is common to standardize the data, to ensure\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n344 Chapter 10. Logistic Regression\neach feature has mean 0 and variance 1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 868, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 946}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0869_d912d3ad", "text": "In such cases, it is common to standardize the data, to ensure\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n344 Chapter 10. Logistic Regression\neach feature has mean 0 and variance 1. We can do this by subtracting the mean and dividing by\nthe standard deviation of each feature, as follows:\nstandardize (xnd) =xnd\u0000^\u0016d\n^\u001bd(10.51)\n^\u0016d=1\nNNX\nn=1xnd (10.52)\n^\u001b2\nd=1\nNNX\nn=1(xnd\u0000^\u0016d)2(10.53)\nAn alternative is to use min-max scaling , in which we rescale the inputs so they lie in the interval\n[0;1]. Both methods ensure the features are comparable in magnitude, which can help with model\nﬁtting and inference, even if we don’t use MAP estimation.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 869, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 657}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0870_2fddb037", "text": "Both methods ensure the features are comparable in magnitude, which can help with model\nﬁtting and inference, even if we don’t use MAP estimation. (See Section 11.7.5 for a discussion of\nthis point.)\n10.3 Multinomial logistic regression\nMultinomial logistic regression is a discriminative classiﬁcation model of the following form:\np(yjx;\u0012) = Cat(yjS(Wx+b)) (10.54)\nwherex2RDis the input vector, y2f1;:::;Cgis the class label, S()is the softmax function\n(Section 2.5.2), Wis aC\u0002Dweight matrix, bisC-dimensional bias vector, \u0012= (W;b)are all the\nparameters. We will henceforth ignore the bias term b; we assume we prepend each xwith a 1, and\naddbto the ﬁrst column of W. Thus\u0012=W. If we leta=Wxbe theC-dimensional vector of logits, then we can rewrite the above as follows:\np(y=cjx;\u0012) =eac\nPC\nc0=1eac0(10.55)\nBecause of the normalization conditionPC\nc=1p(yn=cjxn;\u0012) = 1, we can setwC=0.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 870, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 883}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0871_5ef39085", "text": "(For example,\nin binary logistic regression, where C= 2, we only learn a single weight vector.) Therefore the\nparameters\u0012correspond to a weight matrix Wof size (C\u00001)\u0002D, wherexn2RD. Note that this model assumes the labels are mutually exclusive, i.e., there is only one true label. For\nsome applications (e.g., image tagging ), we want to predict one or more labels for an input; in this\ncase, the output space is the set of subsets of f1;:::;Cg. This is called multi-label classiﬁcation ,\nas opposed to multi-class classiﬁcation . This can be viewed as a bit vector, Y=f0;1gC, where\nthec’th output is set to 1 if the c’th tag is present. We can tackle this using a modiﬁed version of\nbinary logistic regression with multiple outputs:\np(yjx;\u0012) =CY\nc=1Ber(ycj\u001b(wT\ncx)) (10.56)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n10.3. Multinomial logistic regression 345\n(a)\n (b)\nFigure 10.7: Example of 3-class logistic regression with 2d inputs. (a) Original features.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 871, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0872_2e741613", "text": "August 27, 2021\n10.3. Multinomial logistic regression 345\n(a)\n (b)\nFigure 10.7: Example of 3-class logistic regression with 2d inputs. (a) Original features. (b) Quadratic\nfeatures. Generated by code at ﬁgures.probml.ai/book1/10.7. 10.3.1 Linear and nonlinear classiﬁers\nLogistic regression computes linear decision boundaries in the input space, as shown in Figure 10.7(a)\nfor the case where x2R2and we have C= 3classes. However, we can always transform the inputs\nin some way to create nonlinear boundaries. For example, suppose we replace x= (x1;x2)by\n\u001e(x) = [1;x1;x2;x2\n1;x2\n2;x1x2] (10.57)\nThis lets us create quadratic decision boundaries, as illustrated in Figure 10.7(b). 10.3.2 Maximum likelihood estimation\nIn this section, we discuss how to compute the maximum likelihood estimate (MLE) by minimizing\nthe negative log likelihood (NLL).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 872, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 846}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0873_7a8a55ff", "text": "10.3.2 Maximum likelihood estimation\nIn this section, we discuss how to compute the maximum likelihood estimate (MLE) by minimizing\nthe negative log likelihood (NLL). 10.3.2.1 Objective\nThe NLL is given by\nNLL(\u0012) =\u00001\nNlogNY\nn=1CY\nc=1\u0016ync\nnc=\u00001\nNNX\nn=1CX\nc=1ynclog\u0016nc=1\nNNX\nn=1H(yn;\u0016n) (10.58)\nwhere\u0016nc=p(ync= 1jxn;\u0012) =S(f(xn;\u0012))c,ynis the one-hot encoding of the label (so ync=\nI(yn=c)), andH(yn;\u0016n)is the cross-entropy:\nH(p;q) =\u0000CX\nc=1pclogqc (10.59)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n346 Chapter 10. Logistic Regression\n10.3.2.2 Optimizing the objective\nTo ﬁnd the optimum, we need to solve rwNLL(w) =0, wherewis a vectorized version of the weight\nmatrix W, and where we are ignoring the bias term for notational simplicity. We can ﬁnd such a\nstationary point using any gradient-based optimizer; we give some examples below. But ﬁrst we\nderive the gradient and Hessian, and then prove that the objective is convex.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 873, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 942}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0874_91264cf3", "text": "We can ﬁnd such a\nstationary point using any gradient-based optimizer; we give some examples below. But ﬁrst we\nderive the gradient and Hessian, and then prove that the objective is convex. 10.3.2.3 Deriving the gradient\nTo derive the gradient of the NLL, we need to use the Jacobian of the softmax function, which is as\nfollows (see Exercise 10.1 for the proof):\n@\u0016c\n@aj=\u0016c(\u000ecj\u0000\u0016j) (10.60)\nwhere\u000ecj=I(c=j). For example, if we have 3 classes, the Jacobian matrix is given by\n\u0014@\u0016c\n@aj\u0015\ncj=0\n@\u00161(1\u0000\u00161)\u0000\u00161\u00162\u0000\u00161\u00163\n\u0000\u00162\u00161\u00162(1\u0000\u00162)\u0000\u00162\u00163\n\u0000\u00163\u00161\u0000\u00163\u00162\u00163(1\u0000\u00163)1\nA (10.61)\nIn matrix form, this can be written as\n@\u0016\n@a= (\u00161T)\f(I\u00001\u0016T) (10.62)\nwhere\fis elementwise product, \u00161Tcopies\u0016across each column, and 1\u0016Tcopies\u0016across each\nrow. We now derive the gradient of the NLL for a single example, indexed by n. To do this, we ﬂatten\ntheD\u0002Cweight matrix into a vector wof sizeCD(or(C\u00001)Dif we freeze one of the classes to\nhave zero weight) by concatenating the rows, and then transposing into a column vector.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 874, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 989}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0875_1209fc8b", "text": "To do this, we ﬂatten\ntheD\u0002Cweight matrix into a vector wof sizeCD(or(C\u00001)Dif we freeze one of the classes to\nhave zero weight) by concatenating the rows, and then transposing into a column vector. We use wj\nto denote the vector of weights associated with class j. The gradient wrt this vector is giving by the\nfollowing (where we use the Kronecker delta notation, \u000ejc, which equals 1 if j=cand 0 otherwise):\nrwjNLLn=X\nc@NLLn\n@\u0016nc@\u0016nc\n@anj@anj\n@wj(10.63)\n=\u0000X\ncync\n\u0016nc\u0016nc(\u000ejc\u0000\u0016nj)xn (10.64)\n=X\ncync(\u0016nj\u0000\u000ejc)xn (10.65)\n= (X\ncync)\u0016njxn\u0000X\nc\u000ejcynjxn (10.66)\n= (\u0016nj\u0000ynj)xn (10.67)\nWe can repeat this computation for each class, to get the full gradient vector. The gradient of the\noverall NLL is obtained by summing over examples, to give the D\u0002Cmatrix\ng(w) =1\nNNX\nn=1xn(\u0016n\u0000yn)T(10.68)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n10.3. Multinomial logistic regression 347\nThis has the same form as in the binary logistic regression case, namely an error term times the\ninput.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 875, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1001}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0876_3135158d", "text": "August 27, 2021\n10.3. Multinomial logistic regression 347\nThis has the same form as in the binary logistic regression case, namely an error term times the\ninput. 10.3.2.4 Deriving the Hessian\nExercise 10.1 asks you to show that the Hessian of the NLL for multinomial logistic regression is\ngiven by\nH(w) =1\nNNX\nn=1(diag(\u0016n)\u0000\u0016n\u0016T\nn)\n(xnxT\nn) (10.69)\nwhere A\nBis the Kronecker product (Section 7.2.5). In other words, the block c;c0submatrix is\ngiven by\nHc;c0(w) =1\nNX\nn\u0016nc(\u000ec;c0\u0000\u0016n;c0)xnxT\nn (10.70)\nFor example, if we have 3 features and 2 classes, this becomes\nH(w) =1\nNX\nn\u0012\u0016n1\u0000\u00162\nn1\u0000\u0016n1\u0016n2\n\u0000\u0016n1\u0016n2\u0016n2\u0000\u00162\nn2\u0013\n\n0\n@xn1xn1xn1xn2xn1xn3\nxn2xn1xn2xn2xn2xn3\nxn3xn1xn3xn2xn3xn31\nA (10.71)\n=1\nNX\nn\u0012(\u0016n1\u0000\u00162\nn1)Xn\u0000\u0016n1\u0016n2Xn\n\u0000\u0016n1\u0016n2Xn (\u0016n2\u0000\u00162\nn2)Xn\u0013\n(10.72)\nwhere Xn=xnxT\nn. Exercise 10.1 also asks you to show that this is a positive deﬁnite matrix, so the\nobjective is convex. 10.3.3 Gradient-based optimization\nIt is straightforward to use the gradient in Section 10.3.2.3 to derive the SGD algorithm.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 876, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0877_50f31751", "text": "10.3.3 Gradient-based optimization\nIt is straightforward to use the gradient in Section 10.3.2.3 to derive the SGD algorithm. Similarly,\nwe can use the Hessian in Section 10.3.2.4 to derive a second-order optimization method. However,\ncomputing the Hessian can be expensive, so it is common to approximate it using quasi-Newton\nmethods, such as limited memory BFGS. (BFGS stands for Broyden, Fletcher, Goldfarb and Shanno.)\nSee Section 8.3.2 for details. Another approach, which is similar to IRLS, is described in Section 10.3.4. All of these methods rely on computing the gradient of the log-likelihood, which in turn requires\ncomputing normalized probabilities, which can be computed from the logits vector a=Wxusing\np(y=cjx) = exp(ac\u0000lse(a)) (10.73)\nwhere lse is the log-sum-exp function deﬁned in Section 2.5.4. For this reason, many software libraries\ndeﬁne a version of the cross-entropy loss that takes unnormalized logits as input.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 877, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 940}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0878_ab635501", "text": "For this reason, many software libraries\ndeﬁne a version of the cross-entropy loss that takes unnormalized logits as input. 10.3.4 Bound optimization\nIn this section, we consider an approach for ﬁtting logistic regression using a class of algorithms\nknown as bound optimization, which we describe in Section 8.7. The basic idea is to iteratively\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n348 Chapter 10. Logistic Regression\nconstruct a lower bound on the function you want to maximize, and then to update the bound, so it\n“pushes up” on the true function. Optimizing the bound is often easier than updating the function\ndirectly. IfLL(\u0012)is a concave function we want to maximize, then one way to obtain a valid lower bound\nis to use a bound on its Hessian, i.e., to ﬁnd a negative deﬁnite matrix Bsuch that H(\u0012)\u001fB. In\nthis case, one can show that\nLL(\u0012)\u0015LL(\u0012t) + (\u0012\u0000\u0012t)Tg(\u0012t) +1\n2(\u0012\u0000\u0012t)TB(\u0012\u0000\u0012t) (10.74)\nwhereg(\u0012t) =rLL(\u0012t).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 878, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 940}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0879_ea16100e", "text": "In\nthis case, one can show that\nLL(\u0012)\u0015LL(\u0012t) + (\u0012\u0000\u0012t)Tg(\u0012t) +1\n2(\u0012\u0000\u0012t)TB(\u0012\u0000\u0012t) (10.74)\nwhereg(\u0012t) =rLL(\u0012t). DeﬁningQ(\u0012;\u0012t)as the right-hand-side of Equation (10.74), the update\nbecomes\n\u0012t+1=\u0012t\u0000B\u00001g(\u0012t) (10.75)\nThis is similar to a Newton update, except we use B, which is a ﬁxed matrix, rather than H(\u0012t),\nwhich changes at each iteration. This can give us some of the advantages of second order methods at\nlower computational cost. Letusnowapplythistologisticregression,following[Kri+05],Let \u0016n(w) = [p(yn= 1jxn;w);:::;p (yn=\nCjxn;w)]andyn= [I(yn= 1);:::;I(yn=C)]. We want to maximize the log-likelihood, which is\nas follows:\nLL(w) =NX\nn=1\"CX\nc=1yncwT\ncxn\u0000logCX\nc=1exp(wT\ncxn)#\n(10.76)\nThe gradient is given by the following (see Section 10.3.2.3 for details of the derivation):\ng(w) =NX\nn=1(yn\u0000\u0016n(w))\nxn (10.77)\nwhere\ndenotes kronecker product (which, in this case, is just outer product of the two vectors).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 879, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 909}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0880_bfd7a36a", "text": "The Hessian is given by the following (see Section 10.3.2.4 for details of the derivation):\nH(w) =\u0000NX\nn=1(diag(\u0016n(w))\u0000\u0016n(w)\u0016n(w)T)\n(xnxT\nn) (10.78)\nWe can construct a lower bound on the Hessian, as shown in [Boh92]:\nH(w)\u001f\u00001\n2[I\u000011T=C]\n NX\nn=1xnxT\nn! ,B (10.79)\nwhere Iis aC-dimensional identity matrix, and 1is aC-dimensional vector of all 1s.1In the binary\ncase, this becomes\nH(w)\u001f\u00001\n2\u0012\n1\u00001\n2\u0013 NX\nn=1xnxT\nn! =\u00001\n4XTX (10.80)\n1. If we enforce that wC=0, we can use C\u00001dimensions for these vectors / matrices. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n10.3. Multinomial logistic regression 349\nThis follows since \u0016n\u00140:5so\u0000(\u0016n\u0000\u00162\nn)\u0015\u00000:25. We can use this lower bound to construct an MM algorithm to ﬁnd the MLE. The update becomes\nwt+1=wt\u0000B\u00001g(wt) (10.81)\nThis iteration can be faster than IRLS (Section 10.2.6) since we can precompute B\u00001in time\nindependent of N, rather than having to invert the Hessian at each iteration.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 880, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 957}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0881_39de0398", "text": "For example, let us\nconsider the binary case, so gt=rLL(wt) =XT(y\u0000\u0016t), where\u0016t= [pn(wt);(1\u0000pn(wt))]N\nn=1. The update becomes\nwt+1=wt\u00004(XTX)\u00001gt(10.82)\nCompare this to Equation (10.37), which has the following form:\nwt+1=wt\u0000H\u00001g(wt) =wt\u0000(XTStX)\u00001gt(10.83)\nwhere St=diag(\u0016t\f(1\u0000\u0016t)). We see that Equation (10.82) is faster to compute, since we can\nprecompute the constant matrix (XTX)\u00001. 10.3.5 MAP estimation\nIn Section 10.2.7 we discussed the beneﬁts of `2regularization for binary logistic regression. These\nbeneﬁts hold also in the multi-class case. However, there is also an additional, and surprising, beneﬁt\nto do with identiﬁability of the parameters, as pointed out in [HTF09, Ex.18.3].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 881, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 692}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0882_68b4fbae", "text": "These\nbeneﬁts hold also in the multi-class case. However, there is also an additional, and surprising, beneﬁt\nto do with identiﬁability of the parameters, as pointed out in [HTF09, Ex.18.3]. (We say that the\nparameters are identiﬁable if there is a unique value that maximizes the likelihood; equivalently, we\nrequire that the NLL be strictlyconvex.)\nTo see why identiﬁability is an issue, recall that multiclass logistic regression has the form\np(y=cjx;W) =exp(wT\ncx)PC\nk=1exp(wT\nkx)(10.84)\nwhere Wis aC\u0002Dweight matrix. We can arbitrarily deﬁne wc=0for one of the classes, say\nc=C, sincep(y=Cjx;W) = 1\u0000PC\u00001\nc=1p(y=cjx;w). In this case, the model has the form\np(y=cjx;W) =exp(wT\ncx)\n1 +PC\u00001\nk=1exp(wT\nkx)(10.85)\nIf we don’t “clamp” one of the vectors to some constant value, the parameters will be unidentiﬁable.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 882, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 812}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0883_9f8a6572", "text": "In this case, the model has the form\np(y=cjx;W) =exp(wT\ncx)\n1 +PC\u00001\nk=1exp(wT\nkx)(10.85)\nIf we don’t “clamp” one of the vectors to some constant value, the parameters will be unidentiﬁable. However, suppose we don’t clamp wc=0, so we are using Equation 10.84, but we add `2\nregularization by optimizing\nPNLL( W) =\u0000NX\nn=1logp(ynjxn;W) +\u0015CX\nc=1jjwcjj2\n2 (10.86)\nwhere we have absorbed the 1=Nterm into\u0015. At the optimum we havePC\nc=1^wcj= 0forj= 1 :D,\nso the weights automatically satisfy a sum-to-zero constraint, thus making them uniquely identiﬁable. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n350 Chapter 10. Logistic Regression\nTo see why, note that at the optimum we have\nrNLL(w) + 2\u0015w=0 (10.87)\nX\nn(yn\u0000\u0016n)\nxn=\u0015w (10.88)\nHence for any feature dimension jwe have\n\u0015X\ncwcj=X\nnX\nc(ync\u0000\u0016nc)xnj=X\nn(X\ncync\u0000X\nc\u0016nc)xnj=X\nn(1\u00001)xnj= 0 (10.89)\nThus if\u0015>0we haveP\nc^wcj= 0, so the weights will sum to zero across classes for each feature\ndimension.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 883, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 957}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0884_1eba6828", "text": "10.3.6 Maximum entropy classiﬁers\nRecall that the multinomial logistic regression model can be written as\np(y=cjx;W) =exp(wT\ncx)\nZ(w;x)=exp(wT\ncx)PC\nc0=1exp(wT\nc0x)(10.90)\nwhereZ(w;x) =P\ncexp(wT\ncx)is the partition function (normalization constant). This uses the\nsame features, but a diﬀerent weight vector, for every class. There is a slight extension of this model\nthat allows us to use features that are class-dependent. This model can be written as\np(y=cjx;w) =1\nZ(w;x)exp(wT\u001e(x;c)) (10.91)\nwhere\u001e(x;c)is the feature vector for class c. This is called a maximum entropy classifer , or\nmaxent classiﬁer for short. (The origin of this term is explained in Section 3.4.4.)\nMaxent classiﬁers include multinomial logistic regression as a special case. To see this let w=\n[w1;:::;wC], and deﬁne the feature vector as follows:\n\u001e(x;c) = [0;:::;x;:::;0] (10.92)\nwherexis embedded in the c’th block, and the remaining blocks are zero.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 884, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 929}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0885_183dd9eb", "text": "To see this let w=\n[w1;:::;wC], and deﬁne the feature vector as follows:\n\u001e(x;c) = [0;:::;x;:::;0] (10.92)\nwherexis embedded in the c’th block, and the remaining blocks are zero. In this case, wT\u001e(x;c) =\nwT\ncx, so we recover multinomial logistic regression. Maxent classiﬁers are very widely used in the ﬁeld of natural language processing. For example,\nconsider the problem of semantic role labeling , where we classify a word xinto a semantic role y,\nsuch as person, place or thing. We might deﬁne (binary) features such as the following:\n\u001e1(x;y) =I(y=person^xoccurs after “Mr.” or “Mrs” ) (10.93)\n\u001e2(x;y) =I(y=person^xis in whitelist of common names ) (10.94)\n\u001e3(x;y) =I(y=place^xis in Google maps ) (10.95)\n... Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n10.3. Multinomial logistic regression 351\nMammals\nDog\nGolden\nRetrieverGerman\nShepherdTabby SiameseCatBirds ... ... ... ... Figure 10.8: A simple example of a label hierarchy.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 885, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 964}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0886_8a34cf71", "text": "August 27, 2021\n10.3. Multinomial logistic regression 351\nMammals\nDog\nGolden\nRetrieverGerman\nShepherdTabby SiameseCatBirds ... ... ... ... Figure 10.8: A simple example of a label hierarchy. Nodes within the same ellipse have a mutual exclusion\nrelationship between them. We see that the features we use depend on the label. There are two main ways of creating these features. The ﬁrst is to manually specify many possibly\nuseful features using various templates, and then use a feature selection algorithm, such as the group\nlasso method of Section 11.4.7. The second is to incrementally add features to the model, using a\nheuristic feature generation method. 10.3.7 Hierarchical classiﬁcation\nSometimes the set of possible labels can be structured into a hierarchy ortaxonomy . For example,\nwe might want to predict what kind of an animal is in an image: it could be a dog or a cat; if it is a\ndog, it could be a golden retriever or a German shepherd, etc.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 886, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 958}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0887_886399f3", "text": "For example,\nwe might want to predict what kind of an animal is in an image: it could be a dog or a cat; if it is a\ndog, it could be a golden retriever or a German shepherd, etc. Intuitively, it makes sense to try to\npredict the most precise label for which we are conﬁdent [Den+12], that is, the system should “hedge\nits bets”. One simple way to achieve this, proposed in [RF17], is as follows. First, create a model with\na binary output label for every possible node in the tree. Before training the model, we will use\nlabel smearing , so that a label is propagated to all of its parents ( hypernyms ). For example, if\nan image is labeled “golden retriever”, we will also label it “dog”. If we train a multi-label classiﬁer\n(which produces a vector p(yjx)of binary labels) on such smeared data, it will perform hierarchical\nclassiﬁcation, predicting a set of labels at diﬀerent levels of abstraction.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 887, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 902}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0888_df1471f0", "text": "However, this method could predict “golden retriever”, “cat” and “bird” all with probability 1.0,\nsince the model does not capture the fact that some labels are mutually exclusive. To prevent this,\nwe can add a mutual exclusion constraint between all label nodes which are siblings, as shown in\nFigure 10.8. For example, this model enforces that p(mammaljx) +p(birdjx) = 1, since these two\nlabels are children of the root node. We can further partition the mammal probability into dogs and\ncats, so we have p(dogjx) +p(catjx) =p(mammaljx). [Den+14; Din+15] generalize the above method by using a conditional graphical model where\nthe graph structure can be more complex than a tree. In addition, they allow for soft constraints\nbetween labels, in addition to hard constraints. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n352 Chapter 10.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 888, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 852}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0889_e64ef742", "text": "In addition, they allow for soft constraints\nbetween labels, in addition to hard constraints. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n352 Chapter 10. Logistic Regression\n“What”p(w | C )\n“I’m”0.11 0.28 0.03 0.15 0.09 0.17 0.05 0.11\n“Horse” “Why”Context C\n“Huh” “No” “Y es” “Sup”\n(a)\n“What”p(w | C )\n“I’m”0.28 0.72 0.17 0.83 0.370.320.57 0.43\n0.38 0.68 0.62\n0.63 0.69 0.31\n“Horse” “Why”Context C\n“Huh” “No” “Y es” “Sup” (b)\nFigure 10.9: A ﬂat and hierarchical softmax model p(wjC), whereCare the input features (context) and wis\nthe output label (word). Adapted from https: // www. quora. com/ What-is-hierarchical-softmax . 10.3.8 Handling large numbers of classes\nIn this section, we discuss some issues that arise when there are a large number of potential labels,\ne.g., if the labels correspond to words from a language.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 889, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 842}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0890_d2ce8cad", "text": "10.3.8 Handling large numbers of classes\nIn this section, we discuss some issues that arise when there are a large number of potential labels,\ne.g., if the labels correspond to words from a language. 10.3.8.1 Hierarchical softmax\nIn regular softmax classiﬁers, computing the normalization constant, which is needed to compute\nthe gradient of the log likelihood, takes O(C)time, which can become the bottleneck if Cis large. However, if we structure the labels as a tree, we can compute the probability of any label in O(logC)\ntime, by multiplying the probabilities of each edge on the path from the root to the leaf. For example,\nconsider the tree in Figure 10.9. We have\np(y=I’mjC) = 0:57\u00020:68\u00020:72 = 0:28 (10.96)\nThus we replace the “ﬂat” output softmax with a tree-structured sequence of binary classiﬁers. This\nis calledhierarchical softmax [Goo01; MB05].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 890, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 859}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0891_e5bacc75", "text": "We have\np(y=I’mjC) = 0:57\u00020:68\u00020:72 = 0:28 (10.96)\nThus we replace the “ﬂat” output softmax with a tree-structured sequence of binary classiﬁers. This\nis calledhierarchical softmax [Goo01; MB05]. A good way to structure such a tree is to use Huﬀman encoding, where the most frequent labels\nare placed near the top of the tree, as suggested in [Mik+13a]. (For a diﬀerent appproach, based on\nclustering the most common labels together, see [Gra+17]. And for yet another approach, based on\nsampling labels, see [Tit16].)\n10.3.8.2 Class imbalance and the long tail\nAnother issue that often arises when there are a large number of classes is that for most classes, we\nmay have very few examples. More precisely, if Ncis the number of examples of class c, then the\nempirical distribution p(N1;:::;NC)may have a long tail . The result is an extreme form of class\nimbalance (see e.g., [ASR15]).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 891, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 886}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0892_23ca9ffb", "text": "More precisely, if Ncis the number of examples of class c, then the\nempirical distribution p(N1;:::;NC)may have a long tail . The result is an extreme form of class\nimbalance (see e.g., [ASR15]). Since the rare classes will have a smaller eﬀect on the overall loss\nthan the common classes, the model may “focus its attention” on the common classes. One method that can help is to set the bias terms bsuch thatS(b)c=Nc=N; such a model will\nmatch the empirical label prior even when using weights of w=0. As the weights are adjusted, the\nmodel can learn input-dependent deviations from this prior. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n10.4. Robust logistic regression * 353\nAnother common approach is to resample the data to make it more balanced, before (or during)\ntraining.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 892, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 813}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0893_a54ed0fd", "text": "August 27, 2021\n10.4. Robust logistic regression * 353\nAnother common approach is to resample the data to make it more balanced, before (or during)\ntraining. In particular, suppose we sample a datapoint from class cwith probability\npc=Nq\ncPC\niNq\ni(10.97)\nIf we setq= 1, we recover standard instance-balanced sampling , wherepc/Nc; the common\nclasses will be sampled more than rare classes. If we set q= 0, we recover class-balanced sampling ,\nwherepc= 1=C; this can be thought of as ﬁrst sampling a class uniformly at random, and then\nsampling an instance of this class. Finally, we can consider other options, such as q= 0:5, which is\nknown as square-root sampling [Mah+18]. Yet another method that is simple and can easily handle the long tail is to use the nearest class\nmean classiﬁer . This has the form\nf(x) = argmin\ncjjx\u0000\u0016cjj2\n2 (10.98)\nwhere\u0016c=1\nNcP\nn:yn=cxnis the mean of the features belonging to class c. This induces a softmax\nposterior, as we discussed in Section 9.2.5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 893, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 983}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0894_6496c8d9", "text": "This has the form\nf(x) = argmin\ncjjx\u0000\u0016cjj2\n2 (10.98)\nwhere\u0016c=1\nNcP\nn:yn=cxnis the mean of the features belonging to class c. This induces a softmax\nposterior, as we discussed in Section 9.2.5. We can get much better results if we ﬁrst use a neural\nnetwork (see Part III) to learn good features, by training a DNN classiﬁer with cross-entropy loss\non the original unbalanced data. We then replace xwith\u001e(x)in Equation (10.98). This simple\napproach can give very good performance on long-tailed distributions [Kan+20]. 10.4 Robust logistic regression *\nSometimes we have outliers in our data, which are often due to labeling errors, also called label\nnoise. To prevent the model from being adversely aﬀected by such contamination, we will use\nrobust logistic regression . In this section, we discuss some approaches to this problem. (Note\nthat the methods can also be applied to DNNs.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 894, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 882}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0895_1c4d4be4", "text": "In this section, we discuss some approaches to this problem. (Note\nthat the methods can also be applied to DNNs. For a more thorough survey of label noise, and how\nit impacts deep learning, see [Han+20].)\n10.4.1 Mixture model for the likelihood\nOne of the simplest ways to deﬁne a robust logistic regression model is to modify the likelihood so\nthat it predicts that each output label yis generated uniformly at random with probability \u0019, and\notherwise is generated using the usual conditional model. In the binary case, this becomes\np(yjx) =\u0019Ber(yj0:5) + (1\u0000\u0019)Ber(yj\u001b(wTx)) (10.99)\nThis approach, of using a mixture model for the observation model to make it robust, can be applied\nto many diﬀerent models (e.g., DNNs). We can ﬁt this model using standard methods, such as SGD or Bayesian inference methods such\nas MCMC. For example, let us create a “contaminated” version of the 1d, two-class Iris dataset that\nwe discussed in Section 4.6.7.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 895, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 945}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0896_5eafcc67", "text": "For example, let us create a “contaminated” version of the 1d, two-class Iris dataset that\nwe discussed in Section 4.6.7.2. We will add 6 examples of class 1 (Versicolor) with abnormally\nlow sepal length. In Figure 10.10a, we show the results of ﬁtting a standard (Bayesian) logistic\nregression model to this dataset. In Figure 10.10b, we show the results of ﬁtting the above robust\nmodel. In the latter case, we see that the decision boundary is similar to the one we inferred from\nnon-contaminated data, as shown in Figure 4.20b. We also see that the posterior uncertainty about\nthe decision boundary’s location is smaller than when using a non-robust model. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n354 Chapter 10. Logistic Regression\n3.9 4.4 4.9 5.4 5.9 6.4 6.9\nsepal_length0.00.20.40.60.81.0\np(y=1)\n(a)\n3.9 4.4 4.9 5.4 5.9 6.4 6.9\nsepal_length0.00.20.40.60.81.0\np(y=1) (b)\nFigure 10.10: (a) Logistic regression on some data with outliers (denoted by x).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 896, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 977}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0897_0e8ce8f0", "text": "Training points have been\n(vertically) jittered to avoid overlapping too much. Vertical line is the decision boundary, and its posterior\ncredible interval. (b) Same as (a) but using robust model, with a mixture likelihood. Adapted from Figure 4.13\nof [Mar18]. Generated by code at ﬁgures.probml.ai/book1/10.10. 10.4.2 Bi-tempered loss\nIn this section, we present an approach to robust logistic regression proposed in [Ami+19]. The ﬁrst observation is that examples that are far from the decision boundary, but mislabeled, will\nhave undue adverse aﬀect on the model if the loss function is convex [LS10]. This can be overcome by\nreplacing the usual cross entropy loss with a “tempered” version, that uses a temperature parameter\n0\u0014t1<1to ensure the loss from outliers is bounded. In particular, consider the standard relative\nentropy loss function:\nL(y;^y) =H(y;^y) =X\ncyclog ^yc (10.100)\nwhereyis the true label distribution (often one-hot) and ^yis the predicted distribution.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 897, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 977}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0898_10d00ab0", "text": "In particular, consider the standard relative\nentropy loss function:\nL(y;^y) =H(y;^y) =X\ncyclog ^yc (10.100)\nwhereyis the true label distribution (often one-hot) and ^yis the predicted distribution. We deﬁne\nthetempered cross entropy loss as follows:\nL(y;^y) =X\nc\u0014\nyc(logt1yc\u0000logt1^yc)\u00001\n2\u0000t1(y2\u0000t1\nc\u0000^y2\u0000t1\nc)\u0015\n(10.101)\nwhich simplifes to the following when the true distribution yis one-hot, with all its mass on class c:\nL(c;^y) =\u0000logt1^yc\u00001\n2\u0000t1 \n1\u0000CX\nc0=1^y2\u0000t1\nc0! (10.102)\nHere logtis tempered version of the log function:\nlogt(x),1\n1\u0000t(x1\u0000t\u00001) (10.103)\nThis is mononotically increasing and concave, and reduces to the standard (natural) logarithm when\nt= 1. (Similarly, tempered cross entropy reduces to standard cross entropy when t= 1.) However,\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n10.4. Robust logistic regression * 355\n(a)\n (b)\nFigure 10.11: (a) Illustration of logistic and tempered logistic loss with t1= 0:8.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 898, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 963}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0899_c55100a5", "text": "August 27, 2021\n10.4. Robust logistic regression * 355\n(a)\n (b)\nFigure 10.11: (a) Illustration of logistic and tempered logistic loss with t1= 0:8. (b) Illustration of sigmoid\nand tempered sigmoid transfer function with t2= 2:0. From https: // ai. googleblog. com/ 2019/ 08/\nbi-tempered-logistic-loss-for-training. html . Used with kind permission of Ehsan Amid. the tempered log function is bounded from below by \u00001=(1\u0000t)for0\u0014t<1, and hence the cross\nentropy loss is bounded from above (see Figure 10.11). The second observation is that examples that are near the decision boundary, but mislabeled, need\nto use a transfer function (that maps from activations RCto probabilities [0;1]C) that has heavier\ntails than the softmax, which is based on the exponential, so it can “look past” the neighborhood of\nthe immediate examples. In particular, the standard softmax is deﬁned by\n^yc=acPC\nc0=1exp(ac0)= exp\"\nac\u0000logCX\nc0=1exp(ac0)#\n(10.104)\nwhereais the logits vector.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 899, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 965}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0900_e9acecd3", "text": "In particular, the standard softmax is deﬁned by\n^yc=acPC\nc0=1exp(ac0)= exp\"\nac\u0000logCX\nc0=1exp(ac0)#\n(10.104)\nwhereais the logits vector. We can make a heavy tailed version by using the tempered softmax ,\nwhich uses a temperature parameter t2>1>t1as follows:\n^yc= expt2(ac\u0000\u0015t2(a)) (10.105)\nwhere\nexpt(x),[1 + (1\u0000t)x]1=(1\u0000t)\n+ (10.106)\nis a tempered version of the exponential function. (This reduces to the standard exponental function\nast!1.) In Figure 10.11(right), we show that the tempered softmax (in the two-class case) has\nheavier tails, as desired. All that remains is a way to compute \u0015t2(a). This must satisfy the following ﬁxed point equation:\nCX\nc=1expt2(ac\u0000\u0015(a)) = 1 (10.107)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n356 Chapter 10. Logistic Regression\nAlgorithm 3: Iterative algorithm for computing \u0015(a)in Equation (10.107). From [AWS19].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 900, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 870}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0901_3a704d99", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n356 Chapter 10. Logistic Regression\nAlgorithm 3: Iterative algorithm for computing \u0015(a)in Equation (10.107). From [AWS19]. 1Input: logits a, temperature t>1;\n2\u0016:= max(a);\n3a:=a\u0000\u0016;\n4whileanot converged do\n5Z(a) :=PC\nc=1expt(ac);\n6a:=Z(a)1\u0000t(a\u0000\u00161);\n7Return\u0000logt1\nZ(a)+\u0016\nFigure 10.12: Illustration of standard and bi-tempered logistic regression on data with label noise. From\nhttps: // ai. googleblog. com/ 2019/ 08/ bi-tempered-logistic-loss-for-training. html . Used with\nkind permission of Ehsan Amid. We can solve for \u0015using binary search, or by using the iterative procedure in Algorithm 3. Combining the tempered softmax with the tempered cross entropy results in a method called\nbi-tempered logistic regression . In Figure 10.12, we show an example of this in 2d. The top\nrow is standard logistic regression, the bottom row is bi-tempered. The ﬁrst column is clean data. The second column has label noise near the boundary.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 901, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 971}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0902_6b81bb87", "text": "The top\nrow is standard logistic regression, the bottom row is bi-tempered. The ﬁrst column is clean data. The second column has label noise near the boundary. The robust version uses t1= 1(standard\ncross entropy) but t2= 4(tempered softmax with heavy tails). The third column has label noise far\nfrom the boundary. The robust version uses t1= 0:2(tempered cross entropy with bounded loss)\nbutt2= 1(standard softmax). The fourth column has both kinds of noise; in this case, the robust\nversion uses t1= 0:2andt2= 4. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n10.5. Bayesian logistic regression * 357\n10.5 Bayesian logistic regression *\nSo far we have focused on point estimates of the parameters, either the MLE or the MAP estimate. However, in some cases we want to compute the posterior, p(wjD), in order to capture our uncertainty. This can be particularly useful in settings where we have little data, and where choosing the wrong\ndecision may be costly.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 902, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 991}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0903_510abd26", "text": "This can be particularly useful in settings where we have little data, and where choosing the wrong\ndecision may be costly. Unlike with linear regression, it is not possible to compute the posterior exactly for a logistic\nregression model. A wide range of approximate algorithms can be used,. In this section, we use one\nof the simplest, known as the Laplace approximation (Section 4.6.8.2). See the sequel to this book,\n[Mur22] for more advanced approximations. 10.5.1 Laplace approximation\nAs we discuss in Section 4.6.8.2, the Laplace approximation approximates the posterior using a\nGaussian. The mean of the Gaussian is equal to the MAP estimate ^w, and the covariance is equal to\nthe inverse Hessian Hcomputed at the MAP estimate, i.e., p(wjD)\u0019N(wj^w;H\u00001), We can ﬁnd\nthe mode using a standard optimization method (see Section 10.2.7), and then we can use the results\nfrom Section 10.2.3.4 to compute the Hessian at the mode. As an example, consider the data illustrated in Figure 10.13(a).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 903, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 996}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0904_dc3fec81", "text": "As an example, consider the data illustrated in Figure 10.13(a). There are many parameter settings\nthat correspond to lines that perfectly separate the training data; we show 4 example lines. The\nlikelihood surface is shown in Figure 10.13(b). The diagonal line connects the origin to the point in\nthe grid with maximum likelihood, ^wmle= (8:0;3:4). (The unconstrained MLE has jjwjj=1, as we\ndiscussed in Section 10.2.7; this point can be obtained by following the diagonal line inﬁnitely far to\nthe right.)\nFor each decision boundary in Figure 10.13(a), we plot the corresponding parameter vector in\nFigure 10.13(b). These parameters values are w1= (3;1),w2= (4;2),w3= (5;3), andw4= (7;3). These points all approximately satisfy wi(1)=wi(2)\u0019^wmle(1)=^wmle(2), and hence are close to the\norientation of the maximum likelihood decision boundary. The points are ordered by increasing\nweight norm (3.16, 4.47, 5.83, and 7.62).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 904, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 923}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0905_d02adb72", "text": "The points are ordered by increasing\nweight norm (3.16, 4.47, 5.83, and 7.62). To ensure a unique solution, we use a (spherical) Gaussian prior centered at the origin, N(wj0;\u001b2I). The value of \u001b2controls the strength of the prior. If we set \u001b2=1, we force the MAP estimate\nto bew=0; this will result in maximally uncertain predictions, since all points xwill produce a\npredictive distribution of the form p(y= 1jx) = 0:5. If we set\u001b2= 0, the MAP estimate becomes\nthe MLE, resulting in minimally uncertain predictions. (In particular, all positively labeled points\nwill havep(y= 1jx) = 1:0, and all negatively labeled points will have p(y= 1jx) = 0:0, since the\ndata is separable.) As a compromise (to make a nice illustration), we pick the value \u001b2= 100. Multiplying this prior by the likelihood results in the unnormalized posterior shown in Fig-\nure 10.13(c). The MAP estimate is shown by the blue dot. The Laplace approximation to this\nposterior is shown in Figure 10.13(d).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 905, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 977}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0906_c1565eb5", "text": "The MAP estimate is shown by the blue dot. The Laplace approximation to this\nposterior is shown in Figure 10.13(d). We see that it gets the mode correct (by construction), but\nthe shape of the posterior is somewhat distorted. (The southwest-northeast orientation captures\nuncertainty about the magnitude of w, and the southeast-northwest orientation captures uncertainty\nabout the orientation of the decision boundary.)\nIn Figure 10.14, we show contours of the posterior predictive distribution. Figure 10.14(a) shows the\nplugin approximation using the MAP estimate. We see that there is no uncertainty about the decision\nboundary, even though we are generating probabilistic predictions over the labels. Figure 10.14(b)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n358 Chapter 10.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 906, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 796}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0907_b93b075d", "text": "Figure 10.14(b)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n358 Chapter 10. Logistic Regression\n−8 −6 −4 −2 0 2 4 6−8−6−4−20246data\n(a)\n−8−6−4−2 0246−8−6−4−20246\n1234Log-Likelihood (b)\n−8 −6 −4 −2 0 2 4 6−8−6−4−20246Log-Unnormalised Posterior\n(c)\n−8 −6 −4 −2 0 2 4 6−8−6−4−20246Laplace Approximation to Posterior (d)\nFigure 10.13: (a) Illustration of the data. (b) Log-likelihood for a logistic regression model. The line is drawn\nfrom the origin in the direction of the MLE (which is at inﬁnity). The numbers correspond to 4 points in\nparameter space, corresponding to the lines in (a). (c) Unnormalized log posterior (assuming vague spherical\nprior). (d) Laplace approximation to posterior. Adapted from a ﬁgure by Mark Girolami. Generated by code\nat ﬁgures.probml.ai/book1/10.13. shows what happens when we plug in samples from the Gaussian posterior. Now we see that there is\nconsiderable uncertainty about the orientation of the “best” decision boundary.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 907, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 975}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0908_baedb249", "text": "shows what happens when we plug in samples from the Gaussian posterior. Now we see that there is\nconsiderable uncertainty about the orientation of the “best” decision boundary. Figure 10.14(c) shows\nthe average of these samples. By averaging over multiple predictions, we see that the uncertainty in\nthe decision boundary “splays out” as we move further from the training data. Figure 10.14(d) shows\nthat the probit approximation gives very similar results to the Monte Carlo approximation. 10.5.2 Approximating the posterior predictive\nThe posterior p(wjD)tells us everything we know about the parameters of the model given the data. However, in machine learning applications, the main task of interest is usually to predict an output y\ngiven an input x, rather than to try to understand the parameters of our model. Thus we need to\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n10.5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 908, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 915}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0909_275f6c6d", "text": "Thus we need to\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n10.5. Bayesian logistic regression * 359\n−8 −6 −4 −2 0 2 4 6−8−6−4−20246p(y=1|x, wMAP)\n(a)\n−8 −6 −4 −2 0 2 4 6−8−6−4−20246Decision boundary for sampled w (b)\n−8 −6 −4 −2 0 2 4 6−8−6−4−20246MC approx of p(y=1|x)\n(c)\n−8 −6 −4 −2 0 2 4 6−8−6−4−20246Deterministic approx of p(y=1|x) (d)\nFigure 10.14: Posterior predictive distribution for a logistic regression model in 2d. (a): contours of\np(y= 1jx;^wmap). (b): samples from the posterior predictive distribution. (c): Averaging over these samples. (d): moderated output (probit approximation). Adapted from a ﬁgure by Mark Girolami. Generated by code at\nﬁgures.probml.ai/book1/10.14.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 909, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 723}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0910_2044ca44", "text": "(c): Averaging over these samples. (d): moderated output (probit approximation). Adapted from a ﬁgure by Mark Girolami. Generated by code at\nﬁgures.probml.ai/book1/10.14. compute the posterior predictive distribution\np(yjx;D) =Z\np(yjx;w)p(wjD)dw (10.108)\nAs we discussed in Section 4.6.7.1, a simple approach to this is to ﬁrst compute a point estimate ^w\nof the parameters, such as the MLE or MAP estimate, and then to ignore all posterior uncertainty,\nby assuming p(wjD) =\u000e(w\u0000^w). In this case, the above integral reduces to the following plugin\napproximation:\np(yjx;D)\u0019Z\np(yjx;w)\u000e(w\u0000^w)dw=p(yjx;^w) (10.109)\nHowever, if we want to compute uncertainty in our predictions, we should use a non-degenerate\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n360 Chapter 10. Logistic Regression\nposterior. It is common to use a Gaussian posterior, as we will see. But we still need to approximate\nthe integral in Equation (10.108). We discuss some approaches to this below.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 910, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 979}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0911_e667828d", "text": "Logistic Regression\nposterior. It is common to use a Gaussian posterior, as we will see. But we still need to approximate\nthe integral in Equation (10.108). We discuss some approaches to this below. 10.5.2.1 Monte Carlo approximation\nThe simplest approach is to use a Monte Carlo approximation to the integral. This means we\ndrawSsamples from the posterior, ws\u0018p(wjD). and then compute\np(y= 1jx;D)\u00191\nSSX\ns=1\u001b(wT\nsx) (10.110)\n10.5.2.2 Probit approximation\nAlthough the Monte Carlo approximation is simple, it can be slow, since we need to draw Ssamples\nat test time for each input x. Fortunately, if p(wjD) =N(wj\u0016;\u0006), there is a simple yet accurate\ndeterministic approximation, ﬁrst suggested in [SL90]. To explain this approximation, we follow\nthe presentation of [Bis06, p219]. The key observation is that the sigmoid function \u001b(a)is similar\nin shape to the Gaussian cdf (see Section 2.6.1) \b(a). In particular we have \u001b(a)\u0019\b(\u0015a), where\n\u00152=\u0019=8ensures the two functions have the same slope at the origin.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 911, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1004}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0912_42df5f05", "text": "In particular we have \u001b(a)\u0019\b(\u0015a), where\n\u00152=\u0019=8ensures the two functions have the same slope at the origin. This is useful since we can\nintegrate a Gaussian cdf wrt a Gaussian pdf exactly:\nZ\n\b(\u0015a)N(ajm;v)da= \b\u0012m\n(\u0015\u00002+v)1\n2\u0013\n= \b\u0012\u0015m\n(1 +\u00152v)1\n2\u0013\n\u0019\u001b(\u0014(v)m) (10.111)\nwhere we have deﬁned\n\u0014(v),(1 +\u0019v=8)\u00001\n2 (10.112)\nThus if we deﬁne a=xTw, we have\np(y= 1jx;D)\u0019\u001b(\u0014(v)m) (10.113)\nm=E[a] =xT\u0016 (10.114)\nv=V[a] =V\u0002\nxTw\u0003\n=xT\u0006x (10.115)\nwhere we used Equation (2.165) in the last line. Since \bis the inverse of the probit function, we will\ncall this the probit approximation . Using Equation (10.113) results in predictions that are less extreme (in terms of their conﬁdence)\nthan the plug-in estimate. To see this, note that 0<\u0014(v)<1and hence\u0014(v)m<m, so\u001b(\u0014(v)m)is\ncloser to 0.5 than \u001b(m)is. However, the decision boundary itself will not be aﬀected. To see this,\nnote that the decision boundary is the set of points xfor whichp(y= 1jx;D) = 0:5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 912, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 933}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0913_d89dd1ca", "text": "However, the decision boundary itself will not be aﬀected. To see this,\nnote that the decision boundary is the set of points xfor whichp(y= 1jx;D) = 0:5. This implies\n\u0014(v)m= 0, which implies m=wTx= 0; but this is the same as the decision boundary from the\nplugin estimate. Thus “being Bayesian” doesn’t change the misclassiﬁcation rate (in this case), but\nit does change the conﬁdence estimates of the model, which can be important, as we illustrate in\nSection 10.5.1. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n10.6. Exercises 361\nIn the multiclass case we can use the generalized probit approximation [Gib97]:\np(y=cjx;D)\u0019exp(\u0014(vc)mc)P\nc0exp(\u0014(vc0)mc0)(10.116)\nmc=mT\ncx (10.117)\nvc=xTVc;cx (10.118)\nwhere\u0014is deﬁned in Equation (10.112). Unlike the binary case, taking into account posterior\ncovariance gives diﬀerent predictions than the plug-in approach (see Exercise 3.10.3 of [RW06]).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 913, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 921}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0914_9c9922ec", "text": "Unlike the binary case, taking into account posterior\ncovariance gives diﬀerent predictions than the plug-in approach (see Exercise 3.10.3 of [RW06]). For further approximations of Gaussian integrals combined with sigmoid and softmax functions,\nsee [Dau17]. 10.6 Exercises\nExercise 10.1 [Gradient and Hessian of log-likelihood for multinomial logistic regression]\na. Let\u0016ik=S(\u0011i)k, where\u0011i=wTxi. Show that the Jacobian of the softmax is\n@\u0016ik\n@\u0011ij=\u0016ik(\u000ekj\u0000\u0016ij) (10.119)\nwhere\u000ekj=I(k=j). b. Hence show that the gradient of the NLL is given by\nrwc`=X\ni(yic\u0000\u0016ic)xi (10.120)\nHint: use the chain rule and the fact thatP\ncyic= 1. c. Show that the block submatrix of the Hessian for classes candc0is given by\nHc;c0=\u0000X\ni\u0016ic(\u000ec;c0\u0000\u0016i;c0)xixT\ni (10.121)\nHence show that the Hessian of the NLL is positive deﬁnite. Exercise 10.2 [Regularizing separate terms in 2d logistic regression *]\n(Source: Jaakkola.)\na.Consider the data in Figure 10.15a, where we ﬁt the model p(y= 1jx;w) =\u001b(w0+w1x1+w2x2).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 914, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 984}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0915_4e0d083e", "text": "Exercise 10.2 [Regularizing separate terms in 2d logistic regression *]\n(Source: Jaakkola.)\na.Consider the data in Figure 10.15a, where we ﬁt the model p(y= 1jx;w) =\u001b(w0+w1x1+w2x2). Suppose\nwe ﬁt the model by maximum likelihood, i.e., we minimize\nJ(w) =\u0000`(w;Dtrain) (10.122)\nwhere`(w;Dtrain)is the log likelihood on the training set. Sketch a possible decision boundary corre-\nsponding to ^w. (Copy the ﬁgure ﬁrst (a rough sketch is enough), and then superimpose your answer\non your copy, since you will need multiple versions of this ﬁgure). Is your answer (decision boundary)\nunique? How many classiﬁcation errors does your method make on the training set? Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n362 Chapter 10. Logistic Regression\n(a)\n−2 −1.5 −1 −0.5 0 0.5 1 1.5 2−1−0.8−0.6−0.4−0.200.20.40.60.81\nckwk\n \n1\n2\n3 (b)\nFigure 10.15: (a) Data for logistic regression question. (b) Plot of ^wkvs amount of correlation ckfor three\ndiﬀerent estimators. b.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 915, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 970}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0916_da3e1d12", "text": "(b) Plot of ^wkvs amount of correlation ckfor three\ndiﬀerent estimators. b. Now suppose we regularize only the w0parameter, i.e., we minimize\nJ0(w) =\u0000`(w;Dtrain) +\u0015w2\n0 (10.123)\nSuppose\u0015is a very large number, so we regularize w0all the way to 0, but all other parameters are\nunregularized. Sketch a possible decision boundary. How many classiﬁcation errors does your method\nmake on the training set? Hint: consider the behavior of simple linear regression, w0+w1x1+w2x2when\nx1=x2= 0. c. Now suppose we heavily regularize only the w1parameter, i.e., we minimize\nJ1(w) =\u0000`(w;Dtrain) +\u0015w2\n1 (10.124)\nSketch a possible decision boundary. How many classiﬁcation errors does your method make on the\ntraining set? d.Now suppose we heavily regularize only the w2parameter. Sketch a possible decision boundary. How\nmany classiﬁcation errors does your method make on the training set?", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 916, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 875}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0917_c6727b5a", "text": "d.Now suppose we heavily regularize only the w2parameter. Sketch a possible decision boundary. How\nmany classiﬁcation errors does your method make on the training set? Exercise 10.3 [Logistic regression vs LDA/QDA *]\n(Source: Jaakkola.) Suppose we train the following binary classiﬁers via maximum likelihood. a.GaussI: A generative classiﬁer, where the class-conditional densities are Gaussian, with both covariance\nmatrices set to I(identity matrix), i.e., p(xjy=c) =N(xj\u0016c;I). We assume p(y)is uniform. b. GaussX: as for GaussI, but the covariance matrices are unconstrained, i.e., p(xjy=c) =N(xj\u0016c;\u0006c). c. LinLog: A logistic regression model with linear features. d.QuadLog: A logistic regression model, using linear and quadratic features (i.e., polynomial basis function\nexpansion of degree 2).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 917, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 800}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0918_e7bcb076", "text": "c. LinLog: A logistic regression model with linear features. d.QuadLog: A logistic regression model, using linear and quadratic features (i.e., polynomial basis function\nexpansion of degree 2). After training we compute the performance of each model Mon the training set as follows:\nL(M) =1\nnnX\ni=1logp(yijxi;^\u0012;M) (10.125)\n(Note that this is the conditional log-likelihood p(yjx;^\u0012)and not the joint log-likelihood p(y;xj^\u0012).) We now\nwant to compare the performance of each model. We will write L(M)\u0014L(M0)if modelMmusthave lower\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n10.6. Exercises 363\n(or equal) log likelihood (on the training set) than M0, for any training set (in other words, Mis worse than\nM0, at least as far as training set logprob is concerned).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 918, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 794}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0919_a998ec1c", "text": "Exercises 363\n(or equal) log likelihood (on the training set) than M0, for any training set (in other words, Mis worse than\nM0, at least as far as training set logprob is concerned). For each of the following model pairs, state whether\nL(M)\u0014L(M0),L(M)\u0015L(M0), or whether no such statement can be made (i.e., Mmight sometimes be\nbetter than M0and sometimes worse); also, for each question, brieﬂy (1-2 sentences) explain why. a. GaussI, LinLog. b. GaussX, QuadLog. c. LinLog, QuadLog. d. GaussI, QuadLog. e.Now suppose we measure performance in terms of the average misclassiﬁcation rate on the training set:\nR(M) =1\nnnX\ni=1I(yi6= ^y(xi)) (10.126)\nIs it true in general that L(M)>L(M0)implies that R(M)<R(M0)? Explain why or why not. Author: Kevin P. Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 919, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 771}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0920_0a180b8d", "text": "Explain why or why not. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n\n11 Linear Regression\n11.1 Introduction\nIn this chapter, we discuss linear regression , which is a very widely used method for predicting\na real-valued output (also called the dependent variable ortarget)y2R, given a vector of\nreal-valued inputs (also called independent variables ,explanatory variables , orcovariates )\nx2RD. The key property of the model is that the expected value of the output is assumed to be a\nlinear function of the input, E[yjx]=wTx, which makes the model easy to interpret, and easy to ﬁt\nto data. We discuss nonlinear extensions later in this book. 11.2 Least squares linear regression\nIn this section, we discuss the most common form of linear regression model. 11.2.1 Terminology\nThe term “linear regression” usually refers to a model of the following form:\np(yjx;\u0012) =N(yjw0+wTx;\u001b2) (11.1)\nwhere\u0012= (w0;w;\u001b2)are all the parameters of the model.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 920, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 956}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0921_08293439", "text": "11.2.1 Terminology\nThe term “linear regression” usually refers to a model of the following form:\np(yjx;\u0012) =N(yjw0+wTx;\u001b2) (11.1)\nwhere\u0012= (w0;w;\u001b2)are all the parameters of the model. (In statistics, the parameters w0andw\nare usually denoted by \f0and\f.)\nThe vector of parameters w1:Dare known as the weights orregression coeﬃcients . Each\ncoeﬃcientwdspeciﬁes the change in the output we expect if we change the corresponding input\nfeaturexdby one unit. For example, suppose x1is the age of a person, x2is their education level\n(represented as a continuous number), and yis their income. Thus w1corresponds to the increase\nin income we expect as someone becomes one year older (and hence get more experience), and w2\ncorresponds to the increase in income we expect as someone’s education level increases by one level. The termw0is theoﬀsetorbiasterm, and speciﬁes the output value if all the inputs are 0. This\ncaptures the unconditional mean of the response, w0=E[y], and acts as a baseline.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 921, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 990}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0922_196ef63e", "text": "The termw0is theoﬀsetorbiasterm, and speciﬁes the output value if all the inputs are 0. This\ncaptures the unconditional mean of the response, w0=E[y], and acts as a baseline. We will usually\nassume thatxis written as [1;x1;:::;xD], so we can absorb the oﬀset term w0into the weight vector\nw. If the input is one-dimensional (so D= 1), the model has the form f(x;w) =ax+b, whereb=w0\nis the intercept, and a=w1is the slope. This is called simple linear regression . If the input is\nmulti-dimensional, x2RDwhereD> 1, the method is called multiple linear regression . If the\n366 Chapter 11. Linear Regression\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.010\n5\n051015degree 1\n(a)\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.010\n5\n051015degree 2 (b)\nFigure 11.1: Polynomial of degrees 1 and 2 ﬁt to 21 datapoints. Generated by code at ﬁg-\nures.probml.ai/book1/11.1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 922, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 848}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0923_35aee90a", "text": "Generated by code at ﬁg-\nures.probml.ai/book1/11.1. output is also multi-dimensional, y2RJ, whereJ >1, it is called multivariate linear regression ,\np(yjx;W) =JY\nj=1N(yjjwT\njx;\u001b2\nj) (11.2)\nSee Exercise 11.1 for a simple numerical example. In general, a straight line will not provide a good ﬁt to most data sets. However, we can always\napply a nonlinear transformation to the input features, by replacing xwith\u001e(x)to get\np(yjx;\u0012) =N(yjwT\u001e(x);\u001b2) (11.3)\nAs long as the parameters of the feature extractor \u001eare ﬁxed, the model remains linear in the\nparameters , even if it is not linear in the inputs. (We discuss ways to learn the feature extractor, and\nthe ﬁnal linear mapping, in Part III.)\nAs a simple example of a nonlinear transformation, consider the case of polynomial regression ,\nwhich we introduced in Section 1.2.2.2. If the input is 1d, and we use a polynomial expansion of\ndegreed, we get\u001e(x) = [1;x;x2;:::;xd]. See Figure 11.1 for an example.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 923, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 955}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0924_65034d3b", "text": "If the input is 1d, and we use a polynomial expansion of\ndegreed, we get\u001e(x) = [1;x;x2;:::;xd]. See Figure 11.1 for an example. (See also Section 11.5\nwhere we discuss splines.)\n11.2.2 Least squares estimation\nTo ﬁt a linear regression model to data, we will minimize the negative log likelihood on the training\nset. The objective function is given by\nNLL(w;\u001b2) =\u0000NX\nn=1log\"\u00121\n2\u0019\u001b2\u00131\n2\nexp\u0012\n\u00001\n2\u001b2(yn\u0000wTxn)2\u0013#\n(11.4)\n=1\n2\u001b2NX\nn=1(yn\u0000^yn)2+N\n2log(2\u0019\u001b2) (11.5)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.2. Least squares linear regression 367\nwherewehavedeﬁnedthepredictedresponse ^yn,wTxn. TheMLEisthepointwhere rw;\u001bNLL(w;\u001b2) =\n0. We can ﬁrst optimize wrt w, and then solve for the optimal \u001b. In this section, we just focus on estimating the weights w. In this case, the NLL is equal (up to\nirrelevant constants) to the residual sum of squares , which is given by\nRSS(w) =1\n2NX\nn=1(yn\u0000wTxn)2=1\n2jjXw\u0000yjj2\n2=1\n2(Xw\u0000y)T(Xw\u0000y) (11.6)\nWe discuss how to optimize this below.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 924, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1004}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0925_1727e890", "text": "11.2.2.1 Ordinary least squares\nFrom Equation (7.264) we can show that the gradient is given by\nrwRSS(w) =XTXw\u0000XTy (11.7)\nSetting the gradient to zero and solving gives\nXTXw=XTy (11.8)\nThese are known as the normal equations , since, at the optimal solution, y\u0000Xwis normal\n(orthogonal) to the range of X, as we explain in Section 11.2.2.2. The corresponding solution ^wis\ntheordinary least squares (OLS) solution, which is given by\n^w= (XTX)\u00001XTy (11.9)\nThe quantity Xy= (XTX)\u00001XTis the (left) pseudo inverse of the (non-square) matrix X(see\nSection 7.5.3 for more details). We can check that the solution is unique by showing that the Hessian is positive deﬁnite. In this\ncase, the Hessian is given by\nH(w) =@2\n@x2RSS(w) =XTX (11.10)\nIfXis full rank (so the columns of Xare linearly independent), then His positive deﬁnite, since for\nanyv>0, we have\nvT(XTX)v= (Xv)T(Xv) =jjXvjj2>0 (11.11)\nHence in the full rank case, the least squares objective has a unique global minimum.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 925, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 975}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0926_5beaec8d", "text": "See Figure 11.2\nfor an illustration. 11.2.2.2 Geometric interpretation of least squares\nThe normal equations have an elegant geometrical interpretation, deriving from Section 7.7, as we\nnow explain. We will assume N >D, so there are more observations than unknowns. (This is known\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n368 Chapter 11. Linear Regression\n−8−7−6−5−4−3−2−1 0−0.50−0.250.000.250.500.751.001.251.50\n(a)\n−8−7−6−5−4−3−2−10−0.50−0.250.000.250.500.751.001.251.5010002000300040005000 (b)\nFigure 11.2: (a) Contours of the RSS error surface for the example in Figure 11.1a. The blue cross represents\nthe MLE. (b) Corresponding surface plot. Generated by code at ﬁgures.probml.ai/book1/11.2. a2a1bb00-10.51010.20.40.60.81^\nFigure 11.3: Graphical interpretation of least squares for m= 3equations and n= 2unknowns when solving\nthe system Ax=b.a1anda2are the columns of A, which deﬁne a 2d linear subspace embedded in R3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 926, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 944}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0927_e4810435", "text": "The\ntarget vector bis a vector in R3; its orthogonal projection onto the linear subspace is denoted ^b. The line\nfrombto^bis the vector of residual errors, whose norm we want to minimize. as anoverdetermined system .) We seek a vector ^y2RNthat lies in the linear subspace spanned\nbyXand is as close as possible to y, i.e., we want to ﬁnd\nargmin\n^y2span(fx:;1;:::;x:;dg)ky\u0000^yk2: (11.12)\nwherex:;dis thed’th column of X. Since ^y2span(X), there exists some weight vector wsuch that\n^y=w1x:;1+\u0001\u0001\u0001+wnx:;D=Xw (11.13)\nTo minimize the norm of the residual, y\u0000^y, we want the residual vector to be orthogonal to every\ncolumn of X. Hence\nxT\n:;d(y\u0000^y) = 0)XT(y\u0000Xw) =0)w= (XTX)\u00001XTy (11.14)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.2. Least squares linear regression 369\nHence our projected value of yis given by\n^y=Xw=X(XTX)\u00001XTy (11.15)\nThis corresponds to an orthogonal projection ofyonto the column space of X.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 927, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 942}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0928_147b8b9f", "text": "August 27, 2021\n11.2. Least squares linear regression 369\nHence our projected value of yis given by\n^y=Xw=X(XTX)\u00001XTy (11.15)\nThis corresponds to an orthogonal projection ofyonto the column space of X. For example,\nconsider the case where we have N= 3training examples, each of dimensionality D= 2. The\ntraining data deﬁnes a 2d linear subspace, deﬁned by the 2 columns of X, each of which is a point in\n3d. We project y, which is also a point in 3d, onto this 2d subspace, as shown in Figure 11.3. Theprojection matrix\nProj(X),X(XTX)\u00001XT(11.16)\nis sometimes called the hat matrix , since ^y=Proj(X)y. In the special case that X=xis a column\nvector, the orthogonal projection of yonto the line xbecomes\nProj(x)y=xxTy\nxTx(11.17)\n11.2.2.3 Algorithmic issues\nRecall that the OLS solution is\n^w=Xyy= (XTX)\u00001XTy (11.18)\nHowever, even if it is theoretically possible to compute the pseudo-inverse by inverting XTX, we\nshould not do so for numerical reasons, since XTXmay be ill conditioned or singular.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 928, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 996}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0929_d812ec66", "text": "A better (and more general) approach is to compute the pseudo-inverse using the SVD. Indeed, if\nyou look at the source code for the function sklearn.linear_model.ﬁt, you will see that it uses the\nscipy.linalg.lstsq function, which in turns calls DGELSD, which is an SVD-based solver implemented\nby the LAPACK library, written in Fortran.1\nHowever, if Xis tall and skinny (i.e., N\u001dD), it can be quicker to use QR decomposition\n(Section 7.6.2). To do this, let X=QR, where QTQ=I. In Section 7.7, we show that OLS is\nequivalent to solving the system of linear equations Xw=yin a way that minimizes jjXw\u0000yjj2\n2. (IfN=DandXis full rank, the equations have a unique solution, and the error will be 0.) Using\nQR decomposition, we can rewrite this system of equations as follows:\n(QR)w=y (11.19)\nQTQRw=QTy (11.20)\nw=R\u00001(QTy) (11.21)\nSince Ris upper triangular, we can solve this last set of equations using backsubstitution, thus\navoiding matrix inversion. See code.probml.ai/book1/linsys_solve_demo for a demo.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 929, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1003}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0930_ae04ad2b", "text": "See code.probml.ai/book1/linsys_solve_demo for a demo. An alternative to the use of direct methods based on matrix decomposition (such as SVD and QR)\nis to use iterative solvers, such as the conjugate gradient method (which assumes Xis symmetric\n1. Note that a lot of the “Python” scientiﬁc computing stack sits on top of source code that is written in Fortran\nor C++, for reasons of speed. This makes it hard to change the underlying algorithms. By contrast, the scientiﬁc\ncomputing libraries in the Julia language are written in Julia itself, aiding clarity without sacriﬁcing speed. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n370 Chapter 11. Linear Regression\npositive deﬁnite), and the GMRES (generalized minimal residual method), that works for general\nX.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 930, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 777}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0931_341bc680", "text": "Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n370 Chapter 11. Linear Regression\npositive deﬁnite), and the GMRES (generalized minimal residual method), that works for general\nX. (In SciPy, this is implemented by sparse.linalg.gmres .) These methods just require the ability\nto perform matrix-vector multiplications (i.e., an implementation of a linear operator ), and thus\nare well-suited to problems where Xis sparse or structured. For details, see e.g., [TB97]. A ﬁnal important issue is that it is usually essential to standardize the input features before\nﬁtting the model, to ensure that they are zero mean and unit variance. We can do this using\nEquation (10.51). 11.2.2.4 Weighted least squares\nIn some cases, we want to associate a weight with each example.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 931, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 779}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0932_c67e2d10", "text": "We can do this using\nEquation (10.51). 11.2.2.4 Weighted least squares\nIn some cases, we want to associate a weight with each example. For example, in heteroskedastic\nregression , the variance depends on the input, so the model has the form\np(yjx;\u0012) =N(yjwTx;\u001b2(x)) =1p\n2\u0019\u001b2(x)exp\u0012\n\u00001\n2\u001b2(x)(y\u0000wTx)2\u0013\n(11.22)\nThus\np(yjx;\u0012) =N(yjXw;\u0003\u00001) (11.23)\nwhere \u0003=diag(1=\u001b2(xn)). This is known as weighted linear regression . One can show that the\nMLE is given by\n^w= (XT\u0003X)\u00001XT\u0003y (11.24)\nThis is known as the weighted least squares estimate. 11.2.3 Other approaches to computing the MLE\nIn this section, we discuss other approaches for computing the MLE. 11.2.3.1 Solving for oﬀset and slope separately\nTypically we use a model of the form p(yjx;\u0012) =N(yjw0+wTx;\u001b2), wherew0is an oﬀset or “bias”\nterm. We can compute (w0;w)at the same time by adding a column of 1s to X, and the computing\nthe MLE as above. Alternatively, we can solve for wandw0separately.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 932, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 944}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0933_fdf5b4b6", "text": "We can compute (w0;w)at the same time by adding a column of 1s to X, and the computing\nthe MLE as above. Alternatively, we can solve for wandw0separately. (This will be useful later.)\nIn particular, one can show that\n^w= (XT\ncXc)\u00001XT\ncyc=\"NX\ni=1(xn\u0000x)(xn\u0000x)T#\u00001\"NX\ni=1(yn\u0000y)(xn\u0000x)#\n(11.25)\n^w0=1\nNX\nnyn\u00001\nNX\nnxT\nn^w=y\u0000xT^w (11.26)\nwhere Xcis the centered input matrix containing xc\nn=xn\u0000xalong its rows, and yc=y\u0000yis the\ncentered output vector. Thus we can ﬁrst compute ^won centered data, and then estimate w0using\ny\u0000xT^w. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.2. Least squares linear regression 371\n11.2.3.2 Simple linear regression (1d inputs)\nIn the case of 1d (scalar) inputs, the results from Section 11.2.3.1 reduce to the following simple\nform, which may be familiar from basic statistics classes:\n^w1=P\nn(xn\u0000x)(yn\u0000\u0016y)P\nn(xn\u0000\u0016x)2=Cxy\nCxx(11.27)\n^w0= \u0016y\u0000^w1\u0016x=E[y]\u0000w1E[x] (11.28)\nwhereCxy= Cov [X;Y ]andCxx= Cov [X;X ] =V[X]. We will use this result below.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 933, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1004}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0934_68aec8c8", "text": "We will use this result below. 11.2.3.3 Partial regression\nFrom Equation (11.27), we can compute the regression coeﬃcient ofYonXas follows:\nRYX,@\n@xE[YjX=x] =w1=Cxy\nCxx(11.29)\nThis is the slope of the linear prediction for YgivenX. Now consider the case where we have 2 inputs, so Y=w0+w1X1+w2X2+\u000f, where E[\u000f]= 0. One can show that the optimal regression coeﬃcient for w1is given byRYX1\u0001X2, which is the partial\nregression coeﬃcient ofYonX1, keepingX2constant:\nw1=RYX1\u0001X2=@\n@xE[YjX1=x;X 2] (11.30)\nNote that this quantity is invariant to the speciﬁc value of X2we condition on. We can derive w2in a similar manner. Indeed, we can extend this to multiple input variables. In\neach case, we ﬁnd the optimal coeﬃcients are equal to the partial regression coeﬃcients. This means\nthat we can interpret the j’th coeﬃcient ^wjas the change in output ywe expect per unit change in\ninputxj, keeping all the other inputs constant. 11.2.3.4 Recursively computing the MLE\nOLS is a batch method for computing the MLE.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 934, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1003}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0935_1ff69b96", "text": "11.2.3.4 Recursively computing the MLE\nOLS is a batch method for computing the MLE. In some applications, the data arrives in a continual\nstream, so we want to compute the estimate online, or recursively , as we discussed in Section 4.4.2. In this section, we show how to do this for the case of simple (1d) linear regession. Recall from Section 11.2.3.2 that the batch MLE for simple linear regression is given by\n^w1=P\nn(xn\u0000x)(yn\u0000\u0016y)P\nn(xn\u0000\u0016x)2=Cxy\nCxx(11.31)\n^w0= \u0016y\u0000^w1\u0016x (11.32)\nwhereCxy= Cov [X;Y ]andCxx= Cov [X;X ] =V[X]. We now discuss how to compute these results in a recursive fashion. To do this, let us deﬁne the\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n372 Chapter 11.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 935, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 702}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0936_304f073d", "text": "We now discuss how to compute these results in a recursive fashion. To do this, let us deﬁne the\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n372 Chapter 11. Linear Regression\nfollowing suﬃcient statistics:\nx(n)=1\nnnX\ni=1xi;y(n)=1\nnnX\ni=1yi (11.33)\nC(n)\nxx=1\nnnX\ni=1(xi\u0000x)2; C(n)\nxy=1\nnnX\ni=1(xi\u0000x)(yi\u0000y); C(n)\nyy=1\nnnX\ni=1(yi\u0000y)2(11.34)\nWe can update the means online using\nx(n+1)=x(n)+1\nn+ 1(xn+1\u0000x(n));y(n+1)=y(n)+1\nn+ 1(yn+1\u0000y(n)) (11.35)\nTo update the covariance terms, let us ﬁrst rewrite C(n)\nxyas follows:\nC(n)\nxy=1\nn\"\n(nX\ni=1xiyi) + (nX\ni=1x(n)y(n))\u0000x(n)(nX\ni=1yi)\u0000y(n)(nX\ni=1xi)#\n(11.36)\n=1\nn\"\n(nX\ni=1xiyi) +nx(n)y(n)\u0000x(n)ny(n)\u0000y(n)nx(n)#\n(11.37)\n=1\nn\"\n(nX\ni=1xiyi)\u0000nx(n)y(n)#\n(11.38)\nHence\nnX\ni=1xiyi=nC(n)\nxy+nx(n)y(n)(11.39)\nand so\nC(n+1)\nxy =1\nn+ 1h\nxn+1yn+1+nC(n)\nxy+nx(n)y(n)\u0000(n+ 1)x(n+1)y(n+1)i\n(11.40)\nWe can derive the update for C(n+1)\nxxin a similar manner. See Figure 11.4 for a simple illustration of these equations in action for a 1d regression model.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 936, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 991}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0937_8252d680", "text": "See Figure 11.4 for a simple illustration of these equations in action for a 1d regression model. To extend the above analysis to D-dimensional inputs, the easiest approach is to use SGD. The\nresulting algorithm is called the least mean squares algorithm; see Section 8.4.2 for details. 11.2.3.5 Deriving the MLE from a generative perspective\nLinear regression is a discriminative model of the form p(yjx). However, we can also use generative\nmodels for regression, by analogy to how we use generative models for classiﬁcation in Chapter 9,\nThe goal is to compute the conditional expectation\nf(x) =E[yjx] =Z\ny p(yjx)dy=R\ny p(x;y)dyR\np(x;y)dy(11.41)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.2. Least squares linear regression 373\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0\ntime−6−4−2024weights\nlinregOnlineDemo\nw0\nw1\nw0 batch\nw1 batch\nFigure 11.4: Regression coeﬃcients over time for the 1d model in Figure 1.7a(a). Generated by code at\nﬁgures.probml.ai/book1/11.4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 937, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1000}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0938_a52b019a", "text": "Generated by code at\nﬁgures.probml.ai/book1/11.4. Suppose we ﬁt p(x;y)using an MVN. The MLEs for the parameters of the joint distribution are the\nempiricial means and covariances (see Section 4.2.6 for a proof of this result):\n\u0016x=1\nNX\nnxn (11.42)\n\u0016y=1\nNX\nnyn (11.43)\n\u0006xx=1\nNX\nn(xn\u0000x)(xn\u0000x)T=1\nNXT\ncXc (11.44)\n\u0006xy=1\nNX\nn(xn\u0000x)(yn\u0000y) =1\nNXT\ncyc (11.45)\nHence from Equation (3.28), we have\nE[yjx] =\u0016y+\u0006T\nxy\u0006\u00001\nxx(x\u0000\u0016x) (11.46)\nWe can rewrite this as E[yjx] =w0+wTxby deﬁning\nw0=\u0016y\u0000wT\u0016x=y\u0000wTx (11.47)\nw=\u0006\u00001\nxx\u0006xy=\u0000\nXT\ncXc\u0001\u00001XT\ncyc (11.48)\nThis matches the MLEs for the discriminative model as we showed in Section 11.2.3.1. Thus we see\nthat ﬁtting the joint model, and then conditioning it, yields the same result as ﬁtting the conditional\nmodel. However, this is only true for Gaussian models (see Section 9.4 for further discussion of this\npoint). 11.2.3.6 Deriving the MLE for \u001b2\nAfter estimating ^wmleusing one of the above methods, we can estimate the noise variance.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 938, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 968}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0939_bf7f1a9d", "text": "11.2.3.6 Deriving the MLE for \u001b2\nAfter estimating ^wmleusing one of the above methods, we can estimate the noise variance. It is easy\nto show that the MLE is given by\n^\u001b2\nmle= argmin\n\u001b2NLL( ^w;\u001b2) =1\nNNX\nn=1(yn\u0000xT\nn^w)2(11.49)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n374 Chapter 11. Linear Regression\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0\nx6\n4\n2\n0246residual\ndegree 1. Predictions on the training set\n(a)\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0\nx6\n4\n2\n0246residual\ndegree 2. Predictions on the training set (b)\nFigure 11.5: Residual plot for polynomial regression of degree 1 and 2 for the functions in Figure 1.7a(a-b). Generated by code at ﬁgures.probml.ai/book1/11.5. 10\n 5\n 0 5 10 15 20\ntrue y4\n2\n0246predicted ydegree 1. R2 on Test = 0.473\n(a)\n10\n 5\n 0 5 10 15 20\ntrue y5.0\n2.5\n0.02.55.07.510.012.5predicted ydegree 2. R2 on Test = 0.813 (b)\nFigure 11.6: Fit vs actual plots for polynomial regression of degree 1 and 2 for the functions in Figure 1.7a(a-b).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 939, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0940_4150fd7e", "text": "R2 on Test = 0.813 (b)\nFigure 11.6: Fit vs actual plots for polynomial regression of degree 1 and 2 for the functions in Figure 1.7a(a-b). Generated by code at ﬁgures.probml.ai/book1/11.6. This is just the MSE of the residuals, which is an intuitive result. 11.2.4 Measuring goodness of ﬁt\nIn this section, we discuss some simple ways to assess how well a regression model ﬁts the data\n(which is known as goodness of ﬁt ). 11.2.4.1 Residual plots\nFor 1d inputs, we can check the reasonableness of the model by plotting the residuals, rn=yn\u0000^yn,\nvs the input xn. This is called a residual plot . The model assumes that the residuals have a\nN(0;\u001b2)distribution, so the residual plot should be a cloud of points more or less equally above and\nbelow the horizontal line at 0, without any obvious trends. As an example, in Figure 11.5(a), we plot the residuals for the linear model in Figure 1.7a(a). We\nsee that there is some curved structure to the residuals, indicating a lack of ﬁt.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 940, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 981}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0941_f4edd219", "text": "As an example, in Figure 11.5(a), we plot the residuals for the linear model in Figure 1.7a(a). We\nsee that there is some curved structure to the residuals, indicating a lack of ﬁt. In Figure 11.5(b), we\nplot the residuals for the quadratic model in Figure 1.7a(b). We see a much better ﬁt. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.3. Ridge regression 375\nTo extend this approach to multi-dimensional inputs, we can plot predictions ^ynvs the true output\nyn, rather than plotting vs xn. A good model will have points that lie on a diagonal line. See\nFigure 11.6 for some examples. 11.2.4.2 Prediction accuracy and R2\nWe can assess the ﬁt quantitatively by computing the RSS (residual sum of squares) on the dataset:\nRSS(w) =PN\nn=1(yn\u0000wTxn)2. A model with lower RSS ﬁts the data better.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 941, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 823}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0942_dcfec5e3", "text": "A model with lower RSS ﬁts the data better. Another measure that\nis used isroot mean squared error orRMSE:\nRMSE(w),r\n1\nNRSS(w) (11.50)\nA more interpretable measure can be computed using the coeﬃcient of determination , denoted\nbyR2:\nR2,1\u0000PN\nn=1(^yn\u0000yn)2\nPN\nn=1(y\u0000yn)2= 1\u0000RSS\nTSS(11.51)\nwherey=1\nNPN\nn=1ynis the empirical mean of the response, RSS =PN\nn=1(yn\u0000^yn)2is the residual\nsum of squares, and TSS =PN\nn=1(yn\u0000y)2is the total sum of squares. Thus we see tht R2measures\nthe variance in the predictions relative to a simple constant prediction of ^yn=y. One can show that\n0\u0014R2\u00141, where larger values imply a greater reduction in variance (better ﬁt). This is illustrated\nin Figure 11.6. 11.3 Ridge regression\nMaximum likelihood estimation can result in overﬁtting, as we discussed in Section 1.2.2.2. A\nsimple solution to this is to use MAP estimation with a zero-mean Gaussian prior on the weights,\np(w) =N(wj0;\u0015\u00001I), as we discused in Section 4.5.3. This is called ridge regression .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 942, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 987}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0943_a0fb6fd3", "text": "A\nsimple solution to this is to use MAP estimation with a zero-mean Gaussian prior on the weights,\np(w) =N(wj0;\u0015\u00001I), as we discused in Section 4.5.3. This is called ridge regression . In more detail, we compute the MAP estimate as follows:\n^wmap= argmin1\n2\u001b2(y\u0000Xw)T(y\u0000Xw) +1\n2\u001c2wTw (11.52)\n= argmin RSS( w) +\u0015jjwjj2\n2 (11.53)\nwhere\u0015,\u001b2\n\u001c2is proportional to the strength of the prior, and\njjwjj2,vuutDX\nd=1jwdj2=p\nwTw (11.54)\nis the`2norm of the vector w. Thus we are penalizing weights that become too large in magnitude. In general, this technique is called `2regularization orweight decay , and is very widely used. See Figure 4.5 for an illustration. Note that we do not penalize the oﬀset term w0, since that only aﬀects the global mean of the\noutput, and does not contribute to overﬁtting. See Exercise 11.2. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n376 Chapter 11.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 943, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 890}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0944_0a659674", "text": "See Exercise 11.2. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n376 Chapter 11. Linear Regression\n11.3.1 Computing the MAP estimate\nIn this section, we discuss algorithms for computing the MAP estimate. The MAP estimate corresponds to minimizing the following penalized objective:\nJ(w) = (y\u0000Xw)T(y\u0000Xw) +\u0015jjwjj2\n2 (11.55)\nwhere\u0015=\u001b2=\u001c2is the strength of the regularizer. The derivative is given by\nrwJ(w) = 2\u0000\nXTXw\u0000XTy+\u0015w\u0001\n(11.56)\nand hence\n^wmap= (XTX+\u0015ID)\u00001XTy= (X\nnxnxT\nn+\u0015ID)\u00001(X\nnynxn) (11.57)\n11.3.1.1 Solving using QR\nNaively computing the primal estimate w= (XTX+\u0015I)\u00001XTyusing matrix inversion is a bad idea,\nsince it can be slow and numerically unstable. In this section, we describe a way to convert the\nproblem to a standard least squares problem, to which we can apply QR decomposition, as discussed\nin Section 11.2.2.3. We assume the prior has the form p(w) =N(0;\u0003\u00001), where \u0003is the precision matrix. In the case\nof ridge regression, \u0003= (1=\u001c2)I.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 944, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 971}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0945_2c61c901", "text": "We assume the prior has the form p(w) =N(0;\u0003\u00001), where \u0003is the precision matrix. In the case\nof ridge regression, \u0003= (1=\u001c2)I. We can emulalate this prior by adding “virtual data” to the training\nset to get\n~X=\u0012X=\u001bp\n\u0003\u0013\n;~y=\u0012y=\u001b\n0D\u00021\u0013\n(11.58)\nwhere \u0003=p\n\u0003p\n\u0003Tis a Cholesky decomposition of \u0003. We see that ~Xis(N+D)\u0002D, where the\nextra rows represent pseudo-data from the prior. We now show that the RSS on this expanded data is equivalent to penalized RSS on the original\ndata:\nf(w) = ( ~y\u0000~Xw)T(~y\u0000~Xw) (11.59)\n=\u0012\u0012\ny=\u001b\n0\u0013\n\u0000\u0012X=\u001bp\n\u0003\u0013\nw\u0013T\u0012\u0012y=\u001b\n0\u0013\n\u0000\u0012X=\u001bp\n\u0003\u0013\nw\u0013\n(11.60)\n=\u00121\n\u001b(y\u0000Xw)\n\u0000p\n\u0003w\u0013T\u00121\n\u001b(y\u0000Xw)\n\u0000p\n\u0003w\u0013\n(11.61)\n=1\n\u001b2(y\u0000Xw)T(y\u0000Xw) + (p\n\u0003w)T(p\n\u0003w) (11.62)\n=1\n\u001b2(y\u0000Xw)T(y\u0000Xw) +wT\u0003w (11.63)\nHence the MAP estimate is given by\n^wmap= (~XT~X)\u00001~XT~y (11.64)\nwhich can be solved using standard OLS methods. In particular, we can compute the QR decomposi-\ntion of ~X, and then proceed as in Section 11.2.2.3. This takes O((N+D)D2)time. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 945, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1005}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0946_aab479c0", "text": "This takes O((N+D)D2)time. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.3. Ridge regression 377\n11.3.1.2 Solving using SVD\nIn this section, we assume D>N, which is the usual case when using ridge regression. In this case,\nit is faster to use SVD than QR. To see how this works, let X=USVTbe the SVD of X, where\nVTV=IN,UUT=UTU=IN, and Sis a diagonal N\u0002Nmatrix. Now let R=USbe anN\u0002N\nmatrix. One can show (see Exercise 18.4 of [HTF09]) that\n^wmap=V(RTR+\u0015IN)\u00001RTy (11.65)\nIn other words, we can replace the D-dimensional vectors xiwith theN-dimensional vectors riand\nperform our penalized ﬁt as before. The overall time is now O(DN2)operations, which is less than\nO(D3)ifD>N. 11.3.2 Connection between ridge regression and PCA\nIn this section, we discuss an interesting connection between ridge regression and PCA (which we\ndescribe in Section 20.1), in order to gain further insight into why ridge regression works well. Our\ndiscussion is based on [HTF09, p66].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 946, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0947_f673cea4", "text": "Our\ndiscussion is based on [HTF09, p66]. LetX=USVTbe the SVD of X, where VTV=IN,UUT=UTU=IN, and Sis a diagonal\nN\u0002Nmatrix. Using Equation (11.65) we can see that the ridge predictions on the training set are\ngiven by\n^y=X^wmap=USVTV(S2+\u0015I)\u00001SUTy (11.66)\n=U~SUTy=DX\nj=1uj~SjjuT\njy (11.67)\nwhere\n~Sjj,[S(S2+\u0015I)\u00001S]jj=\u001b2\nj\n\u001b2\nj+\u0015(11.68)\nand\u001bjare the singular values of X. Hence\n^y=X^wmap=DX\nj=1uj\u001b2\nj\n\u001b2\nj+\u0015uT\njy (11.69)\nIn contrast, the least squares prediction is\n^y=X^wmle= (USVT)(VS\u00001UTy) =UUTy=DX\nj=1ujuT\njy (11.70)\nIf\u001b2\njis small compared to \u0015, then direction ujwill not have much eﬀect on the prediction. In view\nof this, we deﬁne the eﬀective number of degrees of freedom of the model as follows:\ndof(\u0015) =DX\nj=1\u001b2\nj\n\u001b2\nj+\u0015(11.71)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n378 Chapter 11. Linear Regression\nprior meanMAP EstimateML Estimatew1w2\nprior meanMAP EstimateML Estimatew1w2\nFigure 11.7: Geometry of ridge regression.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 947, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 946}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0948_e8a6ca37", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n378 Chapter 11. Linear Regression\nprior meanMAP EstimateML Estimatew1w2\nprior meanMAP EstimateML Estimatew1w2\nFigure 11.7: Geometry of ridge regression. The likelihood is shown as an ellipse, and the prior is\nshown as a circle centered on the origin. Adapted from Figure 3.15 of [Bis06]. Generated by code at\nﬁgures.probml.ai/book1/11.7. When\u0015= 0,dof(\u0015) =D, and as\u0015!1,dof(\u0015)!0. Let us try to understand why this behavior is desirable. In Section 11.7, we show that Cov [wjD]/\n(XTX)\u00001, if we use a uniform prior for w. Thus the directions in which we are most uncertain about\nware determined by the eigenvectors of (XTX)\u00001with the largest eigenvalues, as shown in Figure 7.6;\nthese correspond to the eigenvectors of XTXwith the smallest eigenvalues. In Section 7.5.2, we show\nthat the squared singular values \u001b2\njare equal to the eigenvalues of XTX. Hence small singular values\n\u001bjcorrespond to directions with high posterior variance.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 948, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 976}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0949_7b9392f2", "text": "In Section 7.5.2, we show\nthat the squared singular values \u001b2\njare equal to the eigenvalues of XTX. Hence small singular values\n\u001bjcorrespond to directions with high posterior variance. It is these directions which ridge shrinks\nthe most. This process is illustrated in Figure 11.7. The horizontal w1parameter is not-well determined\nby the data (has high posterior variance), but the vertical w2parameter is well-determined. Hence\nwmap(2)is close towmle(2), butwmap(1)is shifted strongly towards the prior mean, which is 0. In\nthis way, ill-determined parameters are reduced in size towards 0. This is called shrinkage . There is a related, but diﬀerent, technique called principal components regression , which is\na supervised version of PCA, which we explain in Section 20.1. The idea is this: ﬁrst use PCA to\nreduce the dimensionality to Kdimensions, and then use these low dimensional features as input to\nregression.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 949, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 920}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0950_ed86536d", "text": "The idea is this: ﬁrst use PCA to\nreduce the dimensionality to Kdimensions, and then use these low dimensional features as input to\nregression. However, this technique does not work as well as ridge regression in terms of predictive\naccuracy [HTF01, p70]. The reason is that in PC regression, only the ﬁrst K(derived) dimensions\nare retained, and the remaining D\u0000Kdimensions are entirely ignored. By contrast, ridge regression\nuses a “soft” weighting of all the dimensions. 11.3.3 Choosing the strength of the regularizer\nTo ﬁnd the optimal value of \u0015, we can try a ﬁnite number of distinct values, and use cross validation\nto estimate their expected loss, as discussed in Section 4.5.5.2. See Figure 4.5d for an example. This approach can be quite expensive if we have many values to choose from.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 950, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 797}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0951_320ab2aa", "text": "See Figure 4.5d for an example. This approach can be quite expensive if we have many values to choose from. Fortunately, we can\noftenwarm start the optimization procedure, using the value of ^w(\u0015k)as an initializer for ^w(\u0015k+1),\nwhere\u0015k+1<\u0015k; in other words, we start with a highly constrained model (strong regularizer), and\nthen gradually relax the constraints (decrease the amount of regularization). The set of parameters\n^wkthat we sweep out in this way is known as the regularization path . See Figure 11.10(a) for an\nexample. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.4. Lasso regression 379\nWe can also use an empirical Bayes approach to choose \u0015. In particular, we choose the hyperpa-\nrameter by computing ^\u0015=argmax\u0015logp(Dj\u0015), wherep(Dj\u0015)is the marginal likelihood or evidence. Figure 4.7b shows that this gives essentially the same result as the CV estimate.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 951, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 905}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0952_61e4485e", "text": "Figure 4.7b shows that this gives essentially the same result as the CV estimate. However, the\nBayesian approach has several advantages: computing p(Dj\u0015)can be done by ﬁtting a single model,\nwhereas CV has to ﬁt the same model Ktimes; andp(Dj\u0015)is a smooth function of \u0015, so we can use\ngradient-based optimization instead of discrete search. 11.4 Lasso regression\nIn Section 11.3, we assumed a Gaussian prior for the regression coeﬃcients when ﬁtting linear\nregression models. This is often a good choice, since it encourages the parameters to be small, and\nhence prevents overﬁtting. However, sometimes we want the parameters to not just be small, but to\nbe exactly zero, i.e., we want ^wto besparse, so that we minimize the L0-norm :\njjwjj0=DX\nd=1I(jwdj>0) (11.72)\nThis is useful because it can be used to perform feature selection . To see this, note that the\nprediction has the form f(x;w) =PD\nd=1wdxd, so if anywd= 0, we ignore the corresponding feature\nxd.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 952, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 961}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0953_23e31802", "text": "To see this, note that the\nprediction has the form f(x;w) =PD\nd=1wdxd, so if anywd= 0, we ignore the corresponding feature\nxd. (The same idea can be applied to nonlinear models, such as DNNs, by encouraging the ﬁrst layer\nweights to be sparse.)\n11.4.1 MAP estimation with a Laplace prior ( `1regularization)\nThere are many ways to compute such sparse estimates (see e.g., [Bha+19]). In this section we focus\non MAP estimation using the Laplace distribution (which we discussed in Section 11.6.1) as the\nprior:\np(wj\u0015) =DY\nd=1Lap(wdj0;1=\u0015)/DY\nd=1e\u0000\u0015jwdj(11.73)\nwhere\u0015is the sparsity parameter, and\nLap(wj\u0016;b),1\n2bexp\u0012\n\u0000jw\u0000\u0016j\nb\u0013\n(11.74)\nHere\u0016is a location parameter and b>0is a scale parameter. Figure 2.15 shows that Lap(wj0;b)\nputs more density on 0 than N(wj0;\u001b2), even when we ﬁx the variance to be the same.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 953, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 809}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0954_2400aa99", "text": "Figure 2.15 shows that Lap(wj0;b)\nputs more density on 0 than N(wj0;\u001b2), even when we ﬁx the variance to be the same. To perform MAP estimation of a linear regression model with this prior, we just have to minimize\nthe following objective:\nPNLL(w) =\u0000logp(Djw)\u0000logp(wj\u0015) =jjXw\u0000yjj2\n2+\u0015jjwjj1 (11.75)\nwherejjwjj1,PD\nd=1jwdjis the`1norm ofw. This method is called lasso, which stands for “least\nabsolute shrinkage and selection operator” [Tib96]. (We explain the reason for this name below.)\nMore generally, MAP estimation with a Laplace prior is called `1-regularization . Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n380 Chapter 11. Linear Regression\nFigure 11.8: Illustration of `1(left) vs`2(right) regularization of a least squares problem. Adapted from\nFigure 3.12 of [HTF01]. Note also that we could use other norms for the weight vector. In general, the q-norm is deﬁned as\nfollows:\nkwkq= DX\nd=1jwdjq!1=q\n(11.76)\nForq<1, we can get even sparser solutions.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 954, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 975}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0955_f1b6bc5f", "text": "Note also that we could use other norms for the weight vector. In general, the q-norm is deﬁned as\nfollows:\nkwkq= DX\nd=1jwdjq!1=q\n(11.76)\nForq<1, we can get even sparser solutions. In the limit where q= 0, we get the `0-norm:\nkwk0=DX\nd=1I(jwdj>0) (11.77)\nHowever, one can show that for any q <1, the problem becomes non-convex (see e.g., [HTW15]). Thus`1-norm is the tightest convex relaxation of the`0-norm. 11.4.2 Why does `1regularization yield sparse solutions? We now explain why `1regularization results in sparse solutions, whereas `2regularization does not. We focus on the case of linear regression, although similar arguments hold for other models.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 955, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 658}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0956_39c3ca24", "text": "We now explain why `1regularization results in sparse solutions, whereas `2regularization does not. We focus on the case of linear regression, although similar arguments hold for other models. The lasso objective is the following non-smooth objective (see Section 8.1.4 for a discussion of\nsmoothness):\nmin\nwNLL(w) +\u0015jjwjj1 (11.78)\nThis is the Lagrangian for the following quadratic program (see Section 8.5.4):\nmin\nwNLL(w) s:t:jjwjj1\u0014B (11.79)\nwhereBis an upper bound on the `1-norm of the weights: a small (tight) bound Bcorresponds to a\nlarge penalty \u0015, and vice versa. Similarly, we can write the ridge regression objective minwNLL(w) +\u0015jjwjj2\n2in bound constrained\nform:\nmin\nwNLL(w) s:t:jjwjj2\n2\u0014B (11.80)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.4. Lasso regression 381\nIn Figure 11.8, we plot the contours of the NLL objective function, as well as the contours of the\n`2and`1constraint surfaces.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 956, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 940}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0957_45211566", "text": "August 27, 2021\n11.4. Lasso regression 381\nIn Figure 11.8, we plot the contours of the NLL objective function, as well as the contours of the\n`2and`1constraint surfaces. From the theory of constrained optimization (Section 8.5) we know\nthat the optimal solution occurs at the point where the lowest level set of the objective function\nintersects the constraint surface (assuming the constraint is active). It should be geometrically clear\nthat as we relax the constraint B, we “grow” the `1“ball” until it meets the objective; the corners of\nthe ball are more likely to intersect the ellipse than one of the sides, especially in high dimensions,\nbecause the corners “stick out” more. The corners correspond to sparse solutions, which lie on the\ncoordinate axes. By contrast, when we grow the `2ball, it can intersect the objective at any point;\nthere are no “corners”, so there is no preference for sparsity. 11.4.3 Hard vs soft thresholding\nThe lasso objective has the form L(w) =NLL(w) +\u0015jjwjj1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 957, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 997}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0958_0b788dfa", "text": "11.4.3 Hard vs soft thresholding\nThe lasso objective has the form L(w) =NLL(w) +\u0015jjwjj1. One can show (Exercise 11.3) that the\ngradient for the smooth NLL part is given by\n@\n@wdNLL(w) =adwd\u0000cd (11.81)\nad=NX\nn=1x2\nnd (11.82)\ncd=NX\nn=1xnd(yn\u0000wT\n\u0000dxn;\u0000d) (11.83)\nwherew\u0000diswwithout component d, and similarly xn;\u0000dis feature vector xnwithout component\nd. We see that cdis proportional to the correlation between d’th column of features, x:;d, and the\nresidual error obtained by predicting using all the other features, r\u0000d=y\u0000X:;\u0000dw\u0000d. Hence the\nmagnitude of cdis an indication of how relevant feature dis for predicting y, relative to the other\nfeatures and the current parameters. Setting the gradient to 0 gives the optimal update for wd,\nkeeping all other weights ﬁxed:\nwd=cd=ad=xT\n:;dr\u0000d\njjx:;djj2\n2(11.84)\nThe corresponding new prediction for r\u0000dbecomes ^r\u0000d=wdx:d, which is the orthogonal projection\nof the residual onto the column vector x:;d, consistent with Equation (11.15).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 958, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 981}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0959_a0b33ce3", "text": "Now we add in the `1term. Unfortunately, the jjwjj1term is not diﬀerentiable whenever wd= 0. Fortunately, we can still compute a subgradient at this point. Using Equation (8.14) we ﬁnd that\n@wdL(w) = (adwd\u0000cd) +\u0015@wdjjwjj1 (11.85)\n=8\n<\n:fadwd\u0000cd\u0000\u0015gifwd<0\n[\u0000cd\u0000\u0015;\u0000cd+\u0015]ifwd= 0\nfadwd\u0000cd+\u0015gifwd>0(11.86)\nDepending on the value of cd, the solution to @wdL(w) = 0can occur at 3 diﬀerent values of wd,\nas follows:\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n382 Chapter 11. Linear Regression\n(a)\n (b)\nFigure 11.9: Left: soft thresholding. Right: hard thresholding. In both cases, the horizontal axis is the residual\nerror incurred by making predictions using all the coeﬃcients except for wk, and the vertical axis is the\nestimated coeﬃcient ^wkthat minimizes this penalized residual. The ﬂat region in the middle is the interval\n[\u0000\u0015;+\u0015].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 959, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 847}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0960_70591626", "text": "The ﬂat region in the middle is the interval\n[\u0000\u0015;+\u0015]. 1.Ifcd<\u0000\u0015, so the feature is strongly negatively correlated with the residual, then the subgradient\nis zero at ^wd=cd+\u0015\nad<0:\n2.Ifcd2[\u0000\u0015;\u0015], so the feature is only weakly correlated with the residual, then the subgradient is\nzero at ^wd= 0:\n3.Ifcd>\u0015, so the feature is strongly positively correlated with the residual, then the subgradient is\nzero at ^wd=cd\u0000\u0015\nad>0:\nIn summary, we have\n^wd(cd) =8\n<\n:(cd+\u0015)=adifcd<\u0000\u0015\n0ifcd2[\u0000\u0015;\u0015]\n(cd\u0000\u0015)=adifcd>\u0015(11.87)\nWe can write this as follows:\n^wd= SoftThreshold(cd\nad;\u0015=ad) (11.88)\nwhere\nSoftThreshold( x;\u000e),sign(x) (jxj\u0000\u000e)+(11.89)\nandx+=max(x;0)is the positive part of x. This is called soft thresholding (see also Section 8.6.2). This is illustrated in Figure 11.9(a), where we plot ^wdvscd. The dotted black line is the line\nwd=cd=adcorresponding to the least squares ﬁt.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 960, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 868}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0961_2f0b99ff", "text": "This is called soft thresholding (see also Section 8.6.2). This is illustrated in Figure 11.9(a), where we plot ^wdvscd. The dotted black line is the line\nwd=cd=adcorresponding to the least squares ﬁt. The solid red line, which represents the regularized\nestimate ^wd, shifts the dotted line down (or up) by \u0015, except when\u0000\u0015\u0014cd\u0014\u0015, in which case it\nsetswd= 0. By contrast, in Figure 11.9(b), we illustrate hard thresholding . This sets values of wdto 0 if\n\u0000\u0015\u0014cd\u0014\u0015, but it does not shrink the values of wdoutside of this interval. The slope of the soft\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.4. Lasso regression 383\n0 5 10 15 20 25 30−0.2−0.100.10.20.30.40.50.6\n \nlcavol\nlweight\nage\nlbph\nsvi\nlcp\ngleason\npgg45\n(a)\n0 0.5 1 1.5 2−0.2−0.100.10.20.30.40.50.60.7\nτ \nlcavol\nlweight\nage\nlbph\nsvi\nlcp\ngleason\npgg45 (b)\nFigure 11.10: (a) Proﬁles of ridge coeﬃcients for the prostate cancer example vs bound Bon`2norm ofw, so\nsmallB(large\u0015) is on the left.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 961, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 984}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0962_f02c023d", "text": "The vertical line is the value chosen by 5-fold CV using the 1 standard error\nrule. Adapted from Figure 3.8 of [HTF09]. Generated by code at ﬁgures.probml.ai/book1/11.10. (b) Same as\n(a) but using `1norm ofw. The x-axis shows the critical values of \u0015= 1=B, where the regularization path is\ndiscontinuous. Adapted from Figure 3.10 of [HTF09]. Generated by code at ﬁgures.probml.ai/book1/11.10. thresholding line does not coincide with the diagonal, which means that even large coeﬃcients are\nshrunk towards zero. This is why lasso stands for “least absolute selection and shrinkage operator”. Consequently, lasso is a biased estimator (see Section 4.7.6.1). A simple solution to the biased estimate problem, known as debiasing , is to use a two-stage\nestimation process: we ﬁrst estimate the support of the weight vector (i.e., identify which elements\nare non-zero) using lasso; we then re-estimate the chosen coeﬃcients using least squares. For an\nexample of this in action, see Figure 11.13.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 962, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0963_b8ce7703", "text": "For an\nexample of this in action, see Figure 11.13. 11.4.4 Regularization path\nIf\u0015= 0, we get the OLS solution. which will be dense. As we increase \u0015, the solution vector ^w(\u0015)\nwill tend to get sparser. If \u0015is bigger than some critical value, we get ^w=0. This critical value is\nobtained when the gradient of the NLL cancels out with the gradient of the penalty:\n\u0015max= max\ndjrwdNLL(0)j= max\ndcd(w= 0) = max\ndjyTx:;dj=jjXTyjj1 (11.90)\nAlternatively, we can work with the bound Bon the`1norm. When B= 0, we get ^w=0. As we\nincreaseB, the solution becomes denser. The largest value of Bfor which any component is zero is\ngiven byBmax=jj^wmlejj1. As we increase \u0015, the solution vector ^wgets sparser, although not necessarily monotonically. We can\nplot the values ^wdvs\u0015(or vs the bound B) for each feature d; this is known as the regularization\npath. This is illustrated in Figure 11.10(b), where we apply lasso to the prostate cancer regression\ndataset from [HTF09].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 963, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 964}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0964_82f08658", "text": "This is illustrated in Figure 11.10(b), where we apply lasso to the prostate cancer regression\ndataset from [HTF09]. (We treat features gleason andsvias numeric, not categorical.) On the left,\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n384 Chapter 11. Linear Regression\n0 0 0 0 0 0 0 0\n0.4279 0 0 0 0 0 0 0\n0.5015 0.0735 0 0 0 0 0 0\n0.5610 0.1878 0 0 0.0930 0 0 0\n0.5622 0.1890 0 0.0036 0.0963 0 0 0\n0.5797 0.2456 0 0.1435 0.2003 0 0 0.0901\n0.5864 0.2572 -0.0321 0.1639 0.2082 0 0 0.1066\n0.6994 0.2910 -0.1337 0.2062 0.3003 -0.2565 0 0.2452\n0.7164 0.2926 -0.1425 0.2120 0.3096 -0.2890 -0.0209 0.2773\nTable 11.1: Values of the coeﬃcients for linear regression model ﬁt to prostate cancer dataset as we vary the\nstrength of the `1regularizer. These numbers are plotted in Figure 11.10(b). whenB= 0, all the coeﬃcients are zero. As we increase B, the coeﬃcients gradually “turn on”.2The\nanalogous result for ridge regression is shown in Figure 11.10(a).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 964, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 967}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0965_df1f5392", "text": "whenB= 0, all the coeﬃcients are zero. As we increase B, the coeﬃcients gradually “turn on”.2The\nanalogous result for ridge regression is shown in Figure 11.10(a). For ridge, we see all coeﬃcients are\nnon-zero (assuming \u0015>0), so the solution is not sparse. Remarkably, it can be shown that the lasso solution path is a piecewise linear function of \u0015[Efr+04;\nGL15]. That is, there are a set of critical values of \u0015where the active set of non-zero coeﬃcients\nchanges. For values of \u0015between these critical values, each non-zero coeﬃcient increases or decreases\nin a linear fashion. This is illustrated in Figure 11.10(b). Furthermore, one can solve for these critical\nvalues analytically [Efr+04]. In Table 11.1. we display the actual coeﬃcient values at each of these\ncritical steps along the regularization path (the last line is the least squares solution). By changing \u0015from\u0015maxto0, we can go from a solution in which all the weights are zero to a\nsolution in which all weights are non-zero.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 965, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 993}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0966_da937b49", "text": "By changing \u0015from\u0015maxto0, we can go from a solution in which all the weights are zero to a\nsolution in which all weights are non-zero. Unfortunately, not all subset sizes are achievable using\nlasso. In particular, one can show that, if D>N, the optimal solution can have at most Nvariables\nin it, before reaching the complete set corresponding to the OLS solution of minimal `1norm. In\nSection 11.4.8, we will see that by using an `2regularizer as well as an `1regularizer (a method\nknown as the elastic net), we can achieve sparse solutions which contain more variables than training\ncases. This lets us explore model sizes between NandD. 11.4.5 Comparison of least squares, lasso, ridge and subset selection\nIn this section, we compare least squares, lasso, ridge and subset selection. For simplicity, we assume\nall the features of Xare orthonormal, so XTX=I.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 966, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 861}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0967_984f6180", "text": "For simplicity, we assume\nall the features of Xare orthonormal, so XTX=I. In this case, the NLL is given by\nNLL(w) =jjy\u0000Xwjj2=yTy+wTXTXw\u00002wTXTy (11.91)\n= const +X\ndw2\nd\u00002X\ndX\nnwdxndyn (11.92)\nso we see this factorizes into a sum of terms, one per dimension. Hence we can write down the MAP\nand ML estimates analytically for each wdseparately, as given below. •MLEFrom Equation (11.85), the OLS solution is given by\n^wmle\nd=cd=ad=xT\n:dy (11.93)\nwherex:dis thed’th column of X. 2. It is common to plot the solution versus the shrinkage factor , deﬁned as s(B) =B=B max, rather than against B. This merely aﬀects the scale of the horizontal axis, not the shape of the curves. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 967, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 754}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0968_3adb6b96", "text": "This merely aﬀects the scale of the horizontal axis, not the shape of the curves. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.4. Lasso regression 385\nTerm OLS Best Subset Ridge Lasso\nintercept 2.465 2.477 2.467 2.465\nlcalvol 0.676 0.736 0.522 0.548\nlweight 0.262 0.315 0.255 0.224\nage -0.141 0.000 -0.089 0.000\nlbph 0.209 0.000 0.186 0.129\nsvi 0.304 0.000 0.259 0.186\nlcp -0.287 0.000 -0.095 0.000\ngleason -0.021 0.000 0.025 0.000\npgg45 0.266 0.000 0.169 0.083\nTest error 0.521 0.492 0.487 0.457\nStd error 0.176 0.141 0.157 0.146\nFigure 11.11: Results of diﬀerent methods on the prostate cancer data, which has 8 features and 67 training\ncases. Methods are: OLS = ordinary least squares, Subset = best subset regression, Ridge, Lasso. Rows\nrepresent the coeﬃcients; we see that subset regression and lasso give sparse solutions. Bottom row is the\nmean squared error on the test set (30 cases). Adapted from Table 3.3. of [HTF09].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 968, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 964}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0969_23cd9c64", "text": "Rows\nrepresent the coeﬃcients; we see that subset regression and lasso give sparse solutions. Bottom row is the\nmean squared error on the test set (30 cases). Adapted from Table 3.3. of [HTF09]. Generated by code at\nﬁgures.probml.ai/book1/11.11. •RidgeOne can show that the ridge estimate is given by\n^wridge\nd=^wmle\nd\n1 +\u0015(11.94)\n•LassoFrom Equation (11.88), and using the fact that ^wmle\nd=cd=ad, we have\n^wlasso\nd= sign( ^wmle\nd)\u0000\nj^wmle\ndj\u0000\u0015\u0001\n+(11.95)\nThis corresponds to soft thresholding, shown in Figure 11.9(a). •Subset selection If we pick the best Kfeatures using subset selection, the parameter estimate\nis as follows\n^wss\nd=\u001a\n^wmle\ndif rank (j^wmle\ndj)\u0014K\n0 otherwise(11.96)\nwhere rank refers to the location in the sorted list of weight magnitudes. This corresponds to\nhard thresholding, shown in Figure 11.9(b). We now experimentally compare the prediction performance of these methods on the prostate cancer\nregression dataset from [HTF09].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 969, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 954}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0970_f096186b", "text": "This corresponds to\nhard thresholding, shown in Figure 11.9(b). We now experimentally compare the prediction performance of these methods on the prostate cancer\nregression dataset from [HTF09]. (We treat features gleason andsvias numeric, not categorical.)\nFigure 11.11 shows the estimated coeﬃcients at the value of \u0015(orK) chosen by cross-validation;\nwe see that the subset method is the sparsest, then lasso. In terms of predictive performance, all\nmethods are very similar, as can be seen from Figure 11.12. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n386 Chapter 11. Linear Regression\nLS Best Subset Ridge Lasso0.000.250.500.751.001.251.501.752.00\nFigure 11.12: Boxplot displaying (absolute value of) prediction errors on the prostate cancer test set for\ndiﬀerent regression methods. Generated by code at ﬁgures.probml.ai/book1/11.12.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 970, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 854}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0971_f89b9e85", "text": "Generated by code at ﬁgures.probml.ai/book1/11.12. 11.4.6 Variable selection consistency\nIt is common to use `1regularization to estimate the set of relevant variables, a process known as\nvariable selection . A method that can recover the true set of relevant variables (i.e., the support\nofw\u0003) in theN!1limit is called model selection consistent . (This is a theoretical notion\nthat assumes the data comes from the model.)\nLet us give an example. We ﬁrst generate a sparse signal w\u0003of sizeD= 4096, consisting of 160\nrandomly placed \u00061spikes. Next we generate a random design matrix Xof sizeN\u0002D, where\nN= 1024. Finally we generate a noisy observation y=Xw\u0003+\u000f, where\u000fn\u0018N(0;0:012). We then\nestimatewfromyandX. The original w\u0003is shown in the ﬁrst row of Figure 11.13. The second\nrow is the`1estimate ^wL1using\u0015= 0:1\u0015max. We see that this has “spikes” in the right places, so\nit has correctly identiﬁed the relevant variables.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 971, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 922}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0972_894cde04", "text": "The second\nrow is the`1estimate ^wL1using\u0015= 0:1\u0015max. We see that this has “spikes” in the right places, so\nit has correctly identiﬁed the relevant variables. However, although we see that ^wL1has correctly\nidentiﬁed the non-zero components, but they are too small, due to shrinkage. In the third row, we\nshow the results of using the debiasing technique discussed in Section 11.4.3. This shows that we\ncan recover the original weight vector. By contrast, the ﬁnal row shows the OLS estimate, which is\ndense. Furthermore, it is visually clear that there is no single threshold value we can apply to ^wmle\nto recover the correct sparse weight vector. To use lasso to perform variable selection, we have to pick \u0015. It is common to use cross validation\nto pick the optimal value on the regularization path. However, it is important to note that cross\nvalidation is picking a value of \u0015that results in good predictive accuracy.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 972, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 922}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0973_e42a9d15", "text": "However, it is important to note that cross\nvalidation is picking a value of \u0015that results in good predictive accuracy. This is not usually the same\nvalue as the one that is likely to recover the “true” model. To see why, recall that `1regularization\nperforms selection andshrinkage, that is, the chosen coeﬃcients are brought closer to 0. In order to\nprevent relevant coeﬃcients from being shrunk in this way, cross validation will tend to pick a value\nof\u0015that is not too large. Of course, this will result in a less sparse model which contains irrelevant\nvariables (false positives). Indeed, it was proved in [MB06] that the prediction-optimal value of \u0015\ndoes not result in model selection consistency. However, various extensions to the basic method have\nbeen devised that are model selection consistent (see e.g., [BG11; HTW15]). Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 973, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 915}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0974_3e0f2319", "text": "Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.4. Lasso regression 387\n0 1000 2000 3000 4000−101Original (D = 4096, number of nonzeros = 160)\n0 1000 2000 3000 4000−101L1 reconstruction (K0 = 1024, lambda = 0.0516, MSE = 0.0027)\n0 1000 2000 3000 4000−101Debiased (MSE = 3.26e−005)\n0 1000 2000 3000 4000−0.500.5Minimum norm solution (MSE = 0.0292)\nFigure 11.13: Example of recovering a sparse signal using lasso. See text for details. Adapted from Figure 1\nof [FNW07]. Generated by code at ﬁgures.probml.ai/book1/11.13. 11.4.7 Group lasso\nIn standard `1regularization, we assume that there is a 1:1 correspondence between parameters\nand variables, so that if ^wd= 0, we interpret this to mean that variable dis excluded. But in more\ncomplex models, there may be many parameters associated with a given variable. In particular,\neach variable dmay have a vector of weights wdassociated with it, so the overall weight vector has\nblock structure, w= [w1;w2;:::;wD].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 974, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 991}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0975_1ca44076", "text": "In particular,\neach variable dmay have a vector of weights wdassociated with it, so the overall weight vector has\nblock structure, w= [w1;w2;:::;wD]. If we want to exclude variable d, we have to force the whole\nsubvectorwdto go to zero. This is called group sparsity . 11.4.7.1 Applications\nHere are some examples where group sparsity is useful:\n•Linear regression with categorical inputs: If the d’th variable is categorical with Kpossible levels,\nthen it will be represented as a one-hot vector of length K(Section 1.5.3.1), so to exclude variable\nd, we have to set the whole vector of incoming weights to 0. •Multinomial logistic regression: The d’th variable will be associated with Cdiﬀerent weights, one\nper class (Section 10.3), so to exclude variable d, we have to set the whole vector of outgoing\nweights to 0. •Neural networks: the k’th neuron will have multiple inputs, so if we want to “turn the neuron\noﬀ”, we have to set all the incoming weights to zero.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 975, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 968}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0976_207df6ff", "text": "•Neural networks: the k’th neuron will have multiple inputs, so if we want to “turn the neuron\noﬀ”, we have to set all the incoming weights to zero. This allows us to use group sparsity to learn\nneural network structure (for details, see e.g., [GEH19]). Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n388 Chapter 11. Linear Regression\n•Multi-task learning: each input feature is associated with Cdiﬀerent weights, one per output task. If we want to use a feature for all of the tasks or none of the tasks, we should select weights at\nthe group level [OTJ07]. 11.4.7.2 Penalizing the two-norm\nTo encourage group sparsity, we partition the parameter vector into Ggroups,w= [w1;:::;wG]. Then we minimize the following objective\nPNLL(w) = NLL(w) +\u0015GX\ng=1jjwgjj2 (11.97)\nwherejjwgjj2=qP\nd2gw2\ndis the 2-norm of the group weight vector. If the NLL is least squares,\nthis method is called group lasso [YL06; Kyu+10].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 976, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 922}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0977_da06cc2c", "text": "If the NLL is least squares,\nthis method is called group lasso [YL06; Kyu+10]. Note that if we had used the sum of the squared 2-norms in Equation (11.97), then the model\nwould become equivalent to ridge regression, since\nGX\ng=1jjwgjj2\n2=X\ngX\nd2gw2\nd=jjwjj2\n2 (11.98)\nBy using the square root, we are penalizing the radius of a ball containing the group’s weight vector:\nthe only way for the radius to be small is if all elements are small. Another way to see why the square root version enforces sparsity at the group level is to consider\nthe gradient of the objective. Suppose there is only one group of two variables, so the penalty has\nthe formp\nw2\n1+w2\n2. The derivative wrt w1is\n@\n@w1(w2\n1+w2\n2)1\n2=w1p\nw2\n1+w2\n2(11.99)\nIfw2is close to zero, then the derivative approaches 1, and w1is driven to zero as well, with force\nproportional to \u0015. If, however, w2is large, the derivative approaches 0, and w1is free to stay large\nas well. So all the coeﬃcients in the group will have similar size.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 977, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 994}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0978_f9d03c1b", "text": "If, however, w2is large, the derivative approaches 0, and w1is free to stay large\nas well. So all the coeﬃcients in the group will have similar size. 11.4.7.3 Penalizing the inﬁnity norm\nA variant of this technique replaces the 2-norm with the inﬁnity-norm [TVW05; ZRY05]:\njjwgjj1= max\nd2gjwdj (11.100)\nIt is clear that this will also result in group sparsity, since if the largest element in the group is forced\nto zero, all the smaller ones will be as well. 11.4.7.4 Example\nAn illustration of these techniques is shown in Figure 11.14 and Figure 11.15. We have a true signal\nwof sizeD= 212= 4096, divided into 64 groups each of size 64. We randomly choose 8 groups\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 978, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 749}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0979_f433c2da", "text": "We have a true signal\nwof sizeD= 212= 4096, divided into 64 groups each of size 64. We randomly choose 8 groups\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.4. Lasso regression 389\n05001000 1500 2000 2500 3000 3500 4000-3-2-10123Original (D = 4096, number groups = 64, active groups = 8)\n(a)\n05001000 1500 2000 2500 3000 3500 4000-3-2-10123Standard L1 (debiased 1, tau = 0.427, MSE = 0.08415) (b)\n05001000 1500 2000 2500 3000 3500 4000-3-2-10123Block-L2 (debiased 1, tau = 0.427, MSE = 0.000378)\n(c)\n05001000 1500 2000 2500 3000 3500 4000-3-2-10123Block-Linf (debiased 1, tau = 0.427, MSE = 0.0613) (d)\nFigure 11.14: Illustration of group lasso where the original signal is piecewise Gaussian. (a) Original signal. (b) Vanilla lasso estimate. (c) Group lasso estimate using an `2norm on the blocks. (d) Group lasso\nestimate using an `1norm on the blocks. Adapted from Figures 3-4 of [WNF09]. Generated by code at\nﬁgures.probml.ai/book1/11.14.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 979, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 976}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0980_ad35bfc6", "text": "(c) Group lasso estimate using an `2norm on the blocks. (d) Group lasso\nestimate using an `1norm on the blocks. Adapted from Figures 3-4 of [WNF09]. Generated by code at\nﬁgures.probml.ai/book1/11.14. ofwand assign them non-zero values. In Figure 11.14 the values are drawn from a N(0;1); in\nFigure 11.15, the values are all set to 1. We then sample a random design matrix Xof sizeN\u0002D,\nwhereN= 210= 1024. Finally, we generate y=Xw+\u000f, where\u000f\u0018N(0;10\u00004IN). Given this data,\nwe estimate the support of wusing`1or group`1, and then estimate the non-zero values using least\nsquares (debiased estimate). We see from the ﬁgures that group lasso does a much better job than vanilla lasso, since it respects\nthe known group structure. We also see that the `1norm has a tendency to make all the elements\nwithin a block to have similar magnitude. This is appropriate in the second example, but not the\nﬁrst. (The value of \u0015was the same in all examples, and was chosen by hand.)\nAuthor: Kevin P. Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 980, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1004}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0981_1afe1028", "text": "This is appropriate in the second example, but not the\nﬁrst. (The value of \u0015was the same in all examples, and was chosen by hand.)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n390 Chapter 11. Linear Regression\n05001000 1500 2000 2500 3000 3500 400000.10.20.30.40.50.60.70.80.91Original (D = 4096, number groups = 64, active groups = 8)\n(a)\n05001000 1500 2000 2500 3000 3500 400000.20.40.60.81Standard L1 (debiased 1, tau = 0.361, MSE = 0.1232) (b)\n05001000 1500 2000 2500 3000 3500 400000.20.40.60.81Block-L2 (debiased 1, tau = 0.361, MSE = 0.000386)\n(c)\n05001000 1500 2000 2500 3000 3500 400000.20.40.60.81Block-Linf (debiased 1, tau = 0.361, MSE = 0.000635) (d)\nFigure 11.15: Same as Figure 11.14, except the original signal is piecewise constant. Generated by code at\nﬁgures.probml.ai/book1/11.15. 11.4.8 Elastic net (ridge and lasso combined)\nIn group lasso, we need to specify the group structure ahead of time.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 981, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 931}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0982_cec8db36", "text": "Generated by code at\nﬁgures.probml.ai/book1/11.15. 11.4.8 Elastic net (ridge and lasso combined)\nIn group lasso, we need to specify the group structure ahead of time. For some problems, we don’t\nknow the group structure, and yet we would still like highly correlated coeﬃcients to be treated as an\nimplicit group. One way to achieve this eﬀect, proposed in [ZH05], is to use the elastic net , which is\na hybrid between lasso and ridge regression.3This corresponds to minimizing the following objective:\nL(w;\u00151;\u00152) =jjy\u0000Xwjj2+\u00152jjwjj2\n2+\u00151jjwjj1 (11.101)\nThis penalty function is strictly convex (assuming\u00152>0) so there is a unique global minimum, even\nifXis not full rank. It can be shown [ZH05] that any strictly convex penalty on wwill exhibit a\ngrouping eﬀect , which means that the regression coeﬃcients of highly correlated variables tend to\n3. It is apparently called the “elastic net” because it is “like a stretchable ﬁshing net that retains all the big ﬁsh” [ZH05].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 982, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 974}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0983_a9179997", "text": "It is apparently called the “elastic net” because it is “like a stretchable ﬁshing net that retains all the big ﬁsh” [ZH05]. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.4. Lasso regression 391\nbe equal. In particular, if two features are identically equal, so X:j=X:k, one can show that their\nestimates are also equal, ^wj=^wk. By contrast, with lasso, we may have that ^wj= 0and ^wk6= 0or\nvice versa, resulting in less stable estimates. In addition to its soft grouping behavior, elastic net has other advantages. In particular, if D>N,\nthe maximum number of non-zero elements that can be selected (excluding the MLE, which has D\nnon-zero elements) is N. By contrast, elastic net can select more than Nnon-zero variables on its\npath to the dense estimate, thus exploring more possible subsets of variables. 11.4.9 Optimization algorithms\nA large variety of algorithms have been proposed to solve the lasso problem, and other `1-regularized\nconvex objectives.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 983, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 994}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0984_eb3a4dff", "text": "11.4.9 Optimization algorithms\nA large variety of algorithms have been proposed to solve the lasso problem, and other `1-regularized\nconvex objectives. In this section, we brieﬂy mention some of the most popular methods. 11.4.9.1 Coordinate descent\nSometimes it is hard to optimize all the variables simultaneously, but it easy to optimize them one\nby one. In particular, we can solve for the j’th coeﬃcient with all the others held ﬁxed as follows:\nw\u0003\nj= argmin\n\u001aL(w+\u001aej) (11.102)\nwhereejis thej’th unit vector. This is called coordinate descent . We can either cycle through\nthe coordinates in a deterministic fashion, or we can sample them at random, or we can choose to\nupdate the coordinate for which the gradient is steepest. This method is particularly appealing if each one-dimensional optimization problem can be solved\nanalytically, as is the case for lasso (see Equation (11.87)). This is known as the shooting algorithm\n[Fu98; WL08].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 984, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 945}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0985_223a00e4", "text": "This is known as the shooting algorithm\n[Fu98; WL08]. (The term “shooting” is a reference to cowboy theme inspired by the term “lasso”.)\nSee Algorithm 4 for details. This coordinate descent method has been generalized to the GLM case in [FHT10], and is the\nbasis of the popular glmnet software library. Algorithm 4: Coordinate descent for lasso (aka shooting algorithm)\n1Initializew= (XTX+\u0015I)\u00001XTy;\n2repeat\n3ford= 1;:::;Ddo\n4ad=PN\nn=1x2\nnd;\n5cd=PN\nn=1xnd(yn\u0000wTxn+wdxnd);\n6wd= SoftThreshold(cd\nad;\u0015=ad);\n7untilconverged ;\n11.4.9.2 Projected gradient descent\nIn this section, we convert the non-diﬀerentiable `1penalty into a smooth regularizer. To do\nthis, we ﬁrst use the split variable trick to deﬁnew=w+\u0000w\u0000, wherew+=maxfw;0gand\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n392 Chapter 11. Linear Regression\nw\u0000=\u0000minfw;0g. Now we can replace jjwjj1withP\nd(w+\nd+w\u0000\nd). We also have to replace NLL(w)\nwith NLL(w++w\u0000).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 985, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 930}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0986_59d2d84c", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n392 Chapter 11. Linear Regression\nw\u0000=\u0000minfw;0g. Now we can replace jjwjj1withP\nd(w+\nd+w\u0000\nd). We also have to replace NLL(w)\nwith NLL(w++w\u0000). Thus we get the following smooth, but constrained, optimization problem:\nmin\nw+\u00150;w\u0000\u00150NLL(w+\u0000w\u0000) +\u0015DX\nd=1(w+\nd+w\u0000\nd) (11.103)\nWe can use projected gradient descent (Section 8.6.1) to solve this problem. Speciﬁcally, we can\nenforcetheconstraintbyprojectingontothepositiveorthant, whichwecandousing wd:=max(wd;0);\nthis operation is denoted by P+. Thus the projected gradient update takes the following form:\n\u0012w+\nt+1\nw\u0000\nt+1\u0013\n=P+\u0012\u0014w+\nt\u0000\u001atrNLL(w+\nt\u0000w\u0000\nt)\u0000\u001at\u0015e\nw\u0000\nt+\u001atrNLL(w+\nt\u0000w\u0000\nt)\u0000\u001at\u0015e\u0015\u0013\n(11.104)\nwhereeis the unit vector of all ones. 11.4.9.3 Proximal gradient descent\nIn Section 8.6, we introduced proximal gradient descent, which can be used to optimize smooth\nfunctions with non-smooth penalties, such as `1. In Section 8.6.2, we showed that the proximal\noperator for the `1penalty corresponds to soft thresholding.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 986, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1000}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0987_a4049801", "text": "In Section 8.6.2, we showed that the proximal\noperator for the `1penalty corresponds to soft thresholding. Thus the proximal gradient descent\nupdate can be written as\nwt+1= SoftThreshold( wt\u0000\u001atrNLL(wt);\u001at\u0015) (11.105)\nwhere the soft thresholding operator (Equation (8.134)) is applied elementwise. This is called the\niterative soft thresholding algorithm orISTA[DDDM04; Don95]. If we combine this with\nNesterov acceleration, we get the method known as “fast ISTA” or FISTA[BT09], which is widely\nused to ﬁt sparse linear models. 11.4.9.4 LARS\nIn this section, we discuss methods that can generate a set of solutions for diﬀerent values of \u0015,\nstarting with the empty set, i.e., they compute the full regularization path (Section 11.4.4). These\nalgorithms exploit the fact that one can quickly compute ^w(\u0015k)from ^w(\u0015k\u00001)if\u0015k\u0019\u0015k\u00001; this is\nknown aswarm starting .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 987, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 859}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0988_0f45c61c", "text": "These\nalgorithms exploit the fact that one can quickly compute ^w(\u0015k)from ^w(\u0015k\u00001)if\u0015k\u0019\u0015k\u00001; this is\nknown aswarm starting . In fact, even if we only want the solution for a single value of \u0015, call it\u0015\u0003,\nit can sometimes be computationally more eﬃcient to compute a set of solutions, from \u0015maxdown\nto\u0015\u0003, using warm-starting; this is called a continuation method orhomotopy method. This is\noften much faster than directly “cold-starting” at \u0015\u0003; this is particularly true if \u0015\u0003is small. TheLARSalgorithm [Efr+04], which stands for “least angle regression and shrinkage”, is an\nexample of a homotopy method for the lasso problem. This can compute ^w(\u0015)for all possible values\nof\u0015in an eﬃcient manner. (A similar algorithm was independently invented in [OPT00b; OPT00a]). LARS works as follows. It starts with a large value of \u0015, such that only the variable that is most\ncorrelated with the response vector yis chosen.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 988, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 914}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0989_e34ccf58", "text": "LARS works as follows. It starts with a large value of \u0015, such that only the variable that is most\ncorrelated with the response vector yis chosen. Then \u0015is decreased until a second variable is found\nwhich has the same correlation (in terms of magnitude) with the current residual as the ﬁrst variable,\nwhere the residual at step kon the path is deﬁned as rk=y\u0000X:;Fkwk, whereFkis the current\nactive set (cf., Equation (11.83)). Remarkably, one can solve for this new value of \u0015analytically,\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.5. Regression splines * 393\nby using a geometric argument (hence the term “least angle”). This allows the algorithm to quickly\n“jump” to the next point on the regularization path where the active set changes. This repeats until\nall the variables are added.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 989, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 825}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0990_31c3e072", "text": "This allows the algorithm to quickly\n“jump” to the next point on the regularization path where the active set changes. This repeats until\nall the variables are added. It is necessary to allow variables to be removed from the current active set, even as we increase \u0015,\nif we want the sequence of solutions to correspond to the regularization path of lasso. If we disallow\nvariable removal, we get a slightly diﬀerent algorithm called least angle regression orLAR. LAR\nis very similar to greedy forward selection , and a method known as least squares boosting\n(see e.g., [HTW15]). 11.5 Regression splines *\nWe have seen how we can use polynomial basis functions to create nonlinear mappings from input to\noutput, even though the model remains linear in the parameters. One problem with polynomials is\nthat they are a global approximation to the function. We can achieve more ﬂexibility by using a\nseries of local approximations.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 990, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 926}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0991_bc84f82e", "text": "One problem with polynomials is\nthat they are a global approximation to the function. We can achieve more ﬂexibility by using a\nseries of local approximations. To do this, we just need to deﬁne a set of basis functions that have\nlocal support. The notion of “locality” is hard to deﬁne in high-dimensional input spaces, so in this\nsection, we restrict ourselves to 1d inputs. We can then approximate the function using\nf(x;\u0012) =mX\ni=1wiBi(x) (11.106)\nwhereBiis thei’th basis function. A common way to deﬁne such basis functions is to use B-splines . (“B” stands for “basis”, and the\nterm “spline” refers to a ﬂexible piece of material used by artists to draw curves.) We discuss this in\nmore detail in Section 11.5.1. 11.5.1 B-spline basis functions\nA spline is a piecewise polynomial of degree D, where the locations of the pieces are deﬁned by a set\nofknots,t1<\u0001\u0001\u0001<tm. More precisely, the polynomial is deﬁned on each of the intervals (\u00001;t1),\n[t1;t2],\u0001\u0001\u0001,[tm;1).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 991, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 964}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0992_85954c72", "text": "More precisely, the polynomial is deﬁned on each of the intervals (\u00001;t1),\n[t1;t2],\u0001\u0001\u0001,[tm;1). The function is continuous and has continuous derivatives of orders 1;:::;D\u00001\nat its knot points. It is common to use cubic splines , in whichD= 3. This ensures the function is\ncontinuous, and has continuous ﬁrst and second derivatives at each knot. We will skip the details on how B-splines are computed, since it is not relevant to our purposes. Suﬃce it to say that we can call the patsy.bs function to convert the N\u00021data matrix Xinto an\nN\u0002(K+D+1)design matrix B, whereKis the number of knots and Dis the degree. (Alternatively,\nyou can specify the desired number of basis functions, and let patsy work out the number and locations\nof the knots.)\nFigure 11.16 illustrates this approach, where we use B-splines of degree 0, 1 and 3, with 3 knots. By taking a weighted combination of these basis functions, we can get increasingly smooth functions,\nas shown in the bottom row.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 992, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 973}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0993_2417e055", "text": "By taking a weighted combination of these basis functions, we can get increasingly smooth functions,\nas shown in the bottom row. We see from Figure 11.16 that each individual basis function has local support. At any given input\npointx, onlyD+ 1basis functions will be “active”. This is more obvious if we plot the design matrix\nBitself. Let us ﬁrst consider the piecewise constant spline, shown in Figure 11.17(a). The ﬁrst\nB-spline (column 1) is 1 for the ﬁrst 5 observations, and otherwise 0. The second B-spline (column\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n394 Chapter 11. Linear Regression\n0.000.250.500.751.00\nPiecewise constant\n Piecewise linear\n Cubic spline\n0.0 0.5 1.00.00.51.01.5\n0.0 0.5 1.0\n 0.0 0.5 1.0\nFigure 11.16: Illustration of B-splines of degree 0, 1 and 3. Top row: unweighted basis functions. Dots\nmark the locations of the 3 internal knots at [0:25;0:5;0:75]. Bottom row: weighted combination of basis\nfunctions using random weights.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 993, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 978}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0994_bf38ae78", "text": "Top row: unweighted basis functions. Dots\nmark the locations of the 3 internal knots at [0:25;0:5;0:75]. Bottom row: weighted combination of basis\nfunctions using random weights. Generated by code at ﬁgures.probml.ai/book1/11.16. Adapted from Figure\n5.4 of [MKL11]. Used with kind permission of Osvaldo Martin. 0 1 2 30.00.10.10.20.20.30.30.40.40.50.50.60.60.70.70.80.80.90.91.0Piecewise constant\n(a)\n0 1 2 3 40.00.10.10.20.20.30.30.40.40.50.50.60.60.70.70.80.80.90.91.0Piecewise linear (b)\n0 1 2 3 4 5 60.00.10.10.20.20.30.30.40.40.50.50.60.60.70.70.80.80.90.91.0Cubic spline (c)\nFigure 11.17: Design matrix for B-splines of degree (a) 0, (b) 1 and (c) 3. We evaluate the splines on 20\ninputs ranging from 0 to 1. Generated by code at ﬁgures.probml.ai/book1/11.17. Adapted from Figure 5.6 of\n[MKL11]. Used with kind permission of Osvaldo Martin. 0) is 0 for the ﬁrst 5 observations, 1 for the second 5, and then 0 again. And so on. Now consider\nthe linear spline, shown in Figure 11.17(b).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 994, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 990}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0995_03229e0f", "text": "Used with kind permission of Osvaldo Martin. 0) is 0 for the ﬁrst 5 observations, 1 for the second 5, and then 0 again. And so on. Now consider\nthe linear spline, shown in Figure 11.17(b). The ﬁrst B-spline (column 0) goes from 1 to 0, the next\nthree splines go from 0 to 1 and back to 0; and the last spline (column 4) goes from 0 to 1; this\nreﬂects the triangular shapes shown in the top middle panel of Figure 11.16. Finally consider the\ncubic spline, shown in Figure 11.17(c). Here the pattern of activations is smoother, and the resulting\nmodel ﬁts will be smoother too. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.5. Regression splines * 395\n800 1000 1200 1400 1600 1800 2000\nyear5678temp\nFigure 11.18: Fitting a cubic spline regression model with 15 knots to a 1d dataset. Generated by code at\nﬁgures.probml.ai/book1/11.18. Adapted from Figure 5.3 of [McE20].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 995, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 901}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0996_99cbcb06", "text": "Generated by code at\nﬁgures.probml.ai/book1/11.18. Adapted from Figure 5.3 of [McE20]. 11.5.2 Fitting a linear model using a spline basis\nOnce we have computed the design matrix B, we can use it to ﬁt a linear model using least squares\nor ridge regression. (It is usually best to use some regularization.) As an example, we consder a\ndataset from [McE20, Sec 4.5], which records the the ﬁrst day of the year, and the corresponding\ntemperature, that marks the start of the cherry blossom season in Japan. (We use this dataset since\nit has interesting semi-periodic structure.) We ﬁt the data using a cubic spline. We pick 15 knots,\nspaced according to quantiles of the data. The results are shown in Figure 11.18. We see that the ﬁt\nis reasonable. Using more knots would improve the quality of the ﬁt, but would eventually result in\noverﬁtting. We can select the number of knots using a model selection method, such as grid search\nplus cross validation.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 996, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 952}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0997_0a03e7bb", "text": "We can select the number of knots using a model selection method, such as grid search\nplus cross validation. 11.5.3 Smoothing splines\nSmoothing splines are related to regression splines, but use Nknots, where Nis the number of\ndatapoints. That is, they are non-parametric models, since the number of parameters grows with the\nsize of the data, rather than being ﬁxed a priori. To avoid overﬁtting, smoothing splines rely on `2\nregularization. This technique is closely related to Gaussian process regression, which we discuss in\nSection 17.2. 11.5.4 Generalized additive models\nAgeneralized additive model orGAMextends spline regression to the case of multidimensional\ninputs [HT90]. It does this by ignoring interactions between the inputs, and assuming the function\nhas the following additive form:\nf(x;\u0012) =\u000b+DX\nd=1fd(xd) (11.107)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n396 Chapter 11.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 997, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 908}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0998_605ee859", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n396 Chapter 11. Linear Regression\n0 0.2 0.4 0.6 0.8 1-6-5-4-3-2-101234\nleast squares\nlaplace\nstudent, 8=0.6\nHuber, /=1.0\n(a)\n−3 −2 −1 0 1 2 3−0.500.511.522.533.544.55\n \nL2\nL1\nhuber (b)\nFigure 11.19: (a) Illustration of robust linear regression. Generated by code at ﬁgures.probml.ai/book1/11.19. (b) Illustration of `2,`1, and Huber loss functions with \u000e= 1:5. Generated by code at ﬁg-\nures.probml.ai/book1/11.19. where each fdis a regression or smoothing spline. This model can be ﬁt using backﬁtting , which\niteratively ﬁts each fdto the partial residuals generated by the other terms. We can extend GAMs\nbeyond the regression case (e.g., to classiﬁcation) by using a link function, as in generalized linear\nmodels (Chapter 12). 11.6 Robust linear regression *\nIt is very common to model the noise in regression models using a Gaussian distribution with zero\nmean and constant variance, rn\u0018N (0;\u001b2), wherern=yn\u0000wTxn.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 998, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 961}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_0999_3402bbf9", "text": "11.6 Robust linear regression *\nIt is very common to model the noise in regression models using a Gaussian distribution with zero\nmean and constant variance, rn\u0018N (0;\u001b2), wherern=yn\u0000wTxn. In this case, maximizing\nlikelihood is equivalent to minimizing the sum of squared residuals, as we have seen. However, if\nwe haveoutliers in our data, this can result in a poor ﬁt, as illustrated in Figure 11.19(a). (The\noutliers are the points on the bottom of the ﬁgure.) This is because squared error penalizes deviations\nquadratically, so points far from the line have more eﬀect on the ﬁt than points near to the line. One way to achieve robustness to outliers is to replace the Gaussian distribution for the response\nvariable with a distribution that has heavy tails . Such a distribution will assign higher likelihood\nto outliers, without having to perturb the straight line to “explain” them.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 999, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 889}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1000_15c7d47c", "text": "Such a distribution will assign higher likelihood\nto outliers, without having to perturb the straight line to “explain” them. We discuss several possible\nalternative probability distributions for the response variable below; see Table 11.2 for a summary. 11.6.1 Laplace likelihood\nIn Section 2.7.3, we noted that the Laplace distribution is also robust to outliers. If we use this as\nour observation model for regression, we get the following likelihood:\np(yjx;w;b) = Lap(yjwTx;b)/exp(\u00001\nbjy\u0000wTxj) (11.108)\nThe robustness arises from the use of jy\u0000wTxjinstead of (y\u0000wTx)2. Figure 11.19(a) gives an\nexample of the method in action. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1000, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 712}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1001_e22357c0", "text": "Figure 11.19(a) gives an\nexample of the method in action. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.6. Robust linear regression * 397\nLikelihood Prior Posterior Name Section\nGaussian Uniform Point Least squares 11.2.2\nStudent Uniform Point Robust regression 11.6.2\nLaplace Uniform Point Robust regression 11.6.1\nGaussian Gaussian Point Ridge 11.3\nGaussian Laplace Point Lasso 11.4\nGaussian Gauss-Gamma Gauss-Gamma Bayesian lin. reg 11.7\nTable 11.2: Summary of various likelihoods, priors and posteriors used for linear regression. The likelihood\nrefers to the distributional form of p(yjx;w;\u001b2), and the prior refers to the distributional form of p(w). The\nposterior refers to the distributional form of p(wjD). “Point” stands for the degenerate distribution \u000e(w\u0000^w),\nwhere ^wis the MAP estimate. MLE is equivalent to using a point posterior and a uniform prior.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1001, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 899}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1002_a08e578c", "text": "“Point” stands for the degenerate distribution \u000e(w\u0000^w),\nwhere ^wis the MAP estimate. MLE is equivalent to using a point posterior and a uniform prior. 11.6.1.1 Computing the MLE using linear programming\nWe can compute the MLE for this model using linear programming. As we explain in Section 8.5.3,\nthis is a way to solve a constrained optimization problems of the form\nargmin\nvcTvs:t:Av\u0014b (11.109)\nwherev2Rnis the set of nunknown parameters, cTvis the linear objective function we want to\nminimize, and aT\niv\u0014biis a set ofmlinear constraints we must satisfy. To apply this to our problem,\nlet us deﬁne v= (w1;:::;wD;e1;:::;eN)2RD+N, whereei=jyi\u0000^yijis the residual error for\nexamplei. Wewanttominimizethesumoftheresiduals, sowedeﬁne c= (0;\u0001\u0001\u0001;0;1;\u0001\u0001\u0001;1)2RD+N,\nwhere the ﬁrst Delements are 0, and the last Nelements are 1. We need to enforce the constraint that ei=j^yi\u0000yij.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1002, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 874}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1003_0464879d", "text": "Wewanttominimizethesumoftheresiduals, sowedeﬁne c= (0;\u0001\u0001\u0001;0;1;\u0001\u0001\u0001;1)2RD+N,\nwhere the ﬁrst Delements are 0, and the last Nelements are 1. We need to enforce the constraint that ei=j^yi\u0000yij. In fact it is suﬃcient to enforce the constraint\nthatjwTxi\u0000yij\u0014ei, since minimizing the sum of the ei’s will “push down” on this constraint\nand make it tight. Since jaj\u0014b=) \u0000b\u0014a\u0014b, we can encode jwTxi\u0000yij\u0014eias two linear\nconstraints:\nei\u0015wTxi\u0000yi (11.110)\nei\u0015\u0000(wTxi\u0000yi) (11.111)\nWe can write Equation (11.110) as\n\u0000xi;0;\u0001\u0001\u0001;0;\u00001;0;\u0001\u0001\u0001;0\u0001Tv\u0014yi (11.112)\nwhere the ﬁrst Dentries are ﬁlled with xi, and the\u00001is in the (D+i)’th entry of the vector. Similarly we can write Equation (11.111) as\n\u0000\u0000xi;0;\u0001\u0001\u0001;0;\u00001;0;\u0001\u0001\u0001;0\u0001Tv\u0014\u0000yi (11.113)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n398 Chapter 11.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1003, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 789}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1004_0bf20ba2", "text": "Similarly we can write Equation (11.111) as\n\u0000\u0000xi;0;\u0001\u0001\u0001;0;\u00001;0;\u0001\u0001\u0001;0\u0001Tv\u0014\u0000yi (11.113)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n398 Chapter 11. Linear Regression\nWe can write these constraints in the form Av\u0014bby deﬁning A2R2N\u0002(N+D)as follows:\nA=0\nBBBBB@x1\u00001 0 0\u0001\u0001\u0001 0\n\u0000x1\u00001 0 0\u0001\u0001\u0001 0\nx2 0\u00001 0\u0001\u0001\u0001 0\n\u0000x20\u00001 0\u0001\u0001\u0001 0\n...1\nCCCCCA(11.114)\nand deﬁning b2R2Nas\nb=\u0000y1;\u0000y1;y2;\u0000y2;\u0001\u0001\u0001;yN;\u0000yN\u0001\n(11.115)\n11.6.2 Student- tlikelihood\nIn Section 2.7.1, we discussed the robustness properties of the Student distribution. To use this in a\nregression context, we can just make the mean be a linear function of the inputs, as proposed in\n[Zel76]:\np(yjx;w;\u001b2;\u0017) =T(yjwTx;\u001b2;\u0017) (11.116)\nWe can ﬁt this model using SGD or EM (see [Mur22] for details).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1004, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 742}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1005_cf44502c", "text": "11.6.3 Huber loss\nAn alternative to minimizing the NLL using a Laplace or Student likelihood is to use the Huber\nloss, which is deﬁned as follows:\n`huber(r;\u000e) =\u001a\nr2=2ifjrj\u0014\u000e\n\u000ejrj\u0000\u000e2=2ifjrj>\u000e(11.117)\nThis is equivalent to `2for errors that are smaller than \u000e, and is equivalent to `1for larger errors. See Figure 5.3 for a plot. The advantage of this loss function is that it is everywhere diﬀerentiable. Consequently optimizing\nthe Huber loss is much faster than using the Laplace likelihood, since we can use standard smooth\noptimization methods (such as SGD) instead of linear programming. Figure 11.19 gives an illustration\nof the Huber loss function in action. The results are qualitatively similiar to the Laplace and Student\nmethods. The parameter \u000e, which controls the degree of robustness, is usually set by hand, or by cross-\nvalidation. However, [Bar19] shows how to approximate the Huber loss such that we can optimize \u000e\nby gradient methods.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1005, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 952}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1006_5ab0211b", "text": "However, [Bar19] shows how to approximate the Huber loss such that we can optimize \u000e\nby gradient methods. 11.6.4 RANSAC\nIn the computer vision community, a common approach to robust regression is to use RANSAC ,\nwhich stands for “random sample consensus” [FB81]. This works as follows: we sample a small initial\nset of points, ﬁt the model to them, identify outliers wrt this model (based on large residuals), remove\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.7. Bayesian linear regression * 399\nthe outliers, and then reﬁt the model to the inliers. We repeat this for many random initial sets and\npick the best model. A deterministic alternative to RANSAC is the following iterative scheme: intially we assume that\nall datapoints are inliers, and we ﬁt the model to compute ^w0; then, for each iteration t, we identify\nthe outlier points as those with large residual under the model ^wt, remove them, and reﬁt the model\nto the remaining points to get ^wt+1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1006, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 994}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1007_8d54b211", "text": "Even though this hard thresholding scheme makes the problem\nnonconvex, this simple scheme can be proved to rapidly converge to the optimal estimate under some\nreasonable assumptions [Muk+19; Sug+19]. 11.7 Bayesian linear regression *\nWe have seen how to compute the MLE and MAP estimate for linear regression models under various\npriors. In this section, we discuss how to compute the posterior over the parameters, p(\u0012jD). For\nsimplicity, we assume the variance is known, so we just want to compute p(wjD;\u001b2). See the sequel\nto this book, [Mur22], for the general case. 11.7.1 Priors\nFor simplicity, we will use a Gaussian prior:\np(w) =N(wj`w;`\u0006) (11.118)\nThis is a small generalization of the prior that we use in ridge regression (Section 11.3). See the\nsequel to this book, [Mur22], for a discussion of other priors. 11.7.2 Posteriors\nWe can rewrite the likelihood in terms of an MVN as follows:\np(Djw;\u001b2) =NY\nn=1p(ynjwTx;\u001b2) =N(yjXw;\u001b2IN) (11.119)\nwhere INis theN\u0002Nidentity matrix.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1007, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1008_7904c7bf", "text": "11.7.2 Posteriors\nWe can rewrite the likelihood in terms of an MVN as follows:\np(Djw;\u001b2) =NY\nn=1p(ynjwTx;\u001b2) =N(yjXw;\u001b2IN) (11.119)\nwhere INis theN\u0002Nidentity matrix. We can then use Bayes rule for Gaussians (Equation (3.37))\nto derive the posterior, which is as follows:\np(wjX;y;\u001b2)/N(wj`w;`\u0006)N(yjXw;\u001b2IN) =N(wjaw;a\u0006) (11.120)\naw,a\u0006(`\u0006\u00001`w+1\n\u001b2XTy) (11.121)\na\u0006,(`\u0006\u00001+1\n\u001b2XTX)\u00001(11.122)\nwhereawis the posterior mean, anda\u0006is the posterior covariance. If`w=0and`\u0006=\u001c2I, then the posterior mean becomesaw=1\n\u001b2a\u0006 XTy. If we deﬁne \u0015=\u001b2\n\u001c2, we\nrecover the ridge regression estimate,aw= (\u0015I+XTX)\u00001XTy, which matches Equation (11.57). Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n400 Chapter 11. Linear Regression\n11.7.3 Example\nSuppose we have a 1d regression model of the form f(x;w) =w0+w1x1, where the true parameters\narew0=\u00000:3andw1= 0:5. We now perform inference p(wjD)and visualize the 2d prior and\nposterior as the size of the training set Nincreases.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1008, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 965}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1009_11206263", "text": "We now perform inference p(wjD)and visualize the 2d prior and\nposterior as the size of the training set Nincreases. In particular, in Figure 11.20 (which inspired the front cover of this book), we plot the likelihood,\nthe posterior, and an approximation to the posterior predictive distribution.4Each row plots these\ndistributions as we increase the amount of training data, N. We now explain each row:\n•In the ﬁrst row, N= 0, so the posterior is the same as the prior. In this case, our predictions are\n“all over the place”, since our prior is essentially uniform. •In the second row, N= 1, so we have seen one data point (the blue circle in the plot in the third\ncolumn). Our posterior becomes constrained by the corresponding likelihood, and our predictions\npass close to the observed data. However, we see that the posterior has a ridge-like shape, reﬂecting\nthe fact that there are many possible solutions, with diﬀerent slopes/intercepts.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1009, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 944}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1010_a1667340", "text": "However, we see that the posterior has a ridge-like shape, reﬂecting\nthe fact that there are many possible solutions, with diﬀerent slopes/intercepts. This makes sense\nsince we cannot uniquely infer two parameters ( w0andw1) from one observation. •In the third row, N= 2. In this case, the posterior becomes much narrower since we have two\nconstraints from the likelihood. Our predictions about the future are all now closer to the training\ndata. •In the fourth (last) row, N= 100. Now the posterior is essentially a delta function, centered on\nthe true value of w\u0003= (\u00000:3;0:5), indicated by a white cross in the plots in the ﬁrst and second\ncolumns. The variation in our predictions is due to the inherent Gaussian noise with magnitude\n\u001b2. This example illustrates that, as the amount of data increases, the posterior mean estimate,\na\u0016=E[wjD], converges to the true value w\u0003that generated the data. We thus say that the Bayesian\nestimate is a consistent estimator (see Section 5.3.2 for more details).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1010, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1002}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1011_bd8cd87f", "text": "We thus say that the Bayesian\nestimate is a consistent estimator (see Section 5.3.2 for more details). We also see that our posterior\nuncertainty decreases over time. This is what we mean when we say we are “learning” about the\nparameters as we see more data. 11.7.4 Computing the posterior predictive\nWe have discussed how to compute our uncertainty about the parameters of the model, p(wjD). But what about the uncertainty associated with our predictions about future outputs? Using\nEquation (3.38), we can show that the posterior predictive distribution at a test point xis also\nGaussian:\np(yjx;D;\u001b2) =Z\nN(yjxTw;\u001b2)N(wja\u0016;a\u0006)dw (11.123)\n=N(yja\u0016Tx;a\u001b2(x)) (11.124)\n4. To approximate this, we draw some samples from the posterior, ws\u0018N(\u0016;\u0006), and then plot the line E[yjx;ws],\nwherexranges over [\u00001;1], for each sampled parameter value. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.7.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1011, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 918}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1012_57ce7d1b", "text": "Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.7. Bayesian linear regression * 401\nlikelihood\n1\n 0 1\nw01\n01w1prior/posterior\n1\n 0 1\nx1\n01ydata space\n1\n 0 1\nw01\n01w1\n1\n 0 1\nw01\n01w1\n1\n 0 1\nx1\n01y\n1\n 0 1\nw01\n01w1\n1\n 0 1\nw01\n01w1\n1\n 0 1\nx1\n01y\n1\n 0 1\nw01\n01w1\n1\n 0 1\nw01\n01w1\n1\n 0 1\nx1\n01y\nFigure 11.20: Sequential Bayesian inference of the parameters of a linear regression model p(yjx) =N(yjw0+\nw1x1;\u001b2). Left column: likelihood function for current data point. Middle column: posterior given ﬁrst N\ndata points, p(w0;w1jx1:N;y1:N;\u001b2). Right column: samples from the current posterior predictive distribution. Row 1: prior distribution ( N= 0). Row 2: after 1 data point. Row 3: after 2 data points. Row 4: after 100\ndata points. The white cross in columns 1 and 2 represents the true parameter value; we see that the mode\nof the posterior rapidly converges to this point. The blue circles in column 3 are the observed data points. Adapted from Figure 3.7 of [Bis06].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1012, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 998}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1013_7ff53e50", "text": "The blue circles in column 3 are the observed data points. Adapted from Figure 3.7 of [Bis06]. Generated by code at ﬁgures.probml.ai/book1/11.20. wherea\u001b2(x),\u001b2+xTa\u0006xis the variance of the posterior predictive distribution at point x\nafter seeing the Ntraining examples. The predicted variance depends on two terms: the variance\nof the observation noise, \u001b2, and the variance in the parameters,a\u0006. The latter translates into\nvariance about observations in a way which depends on how close xis to the training data D. This is\nillustrated in Figure 11.21(b), where we see that the error bars get larger as we move away from the\ntraining points, representing increased uncertainty. This can be important for certain applications,\nsuch as active learning, where we choose where to collect training data (see Section 19.4). In some cases, it is computationally intractable to compute the parameter posterior, p(wjD).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1013, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 911}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1014_c30905e7", "text": "In some cases, it is computationally intractable to compute the parameter posterior, p(wjD). In\nsuch cases, we may choose to use a point estimate, ^w, and then to use the plugin approximation. This gives\np(yjx;D;\u001b2) =Z\nN(yjxTw;\u001b2)\u000e(w\u0000^w)dw=p(yjxT^w;\u001b2): (11.125)\nWe see that the posterior predictive variance is constant, and independent of the data, as illustrated in\nFigure 11.21(a). If we sample a parameter from this posterior, we will always recover a single function,\nas shown in Figure 11.21(c). By contrast, if we sample from the true posterior, ws\u0018p(wjD;\u001b2), we\nwill get a range of diﬀerent functions, as shown in Figure 11.21(d), which more accurately reﬂects\nour uncertainty. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n402 Chapter 11.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1014, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 762}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1015_2ad8a459", "text": "Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n402 Chapter 11. Linear Regression\n6\n 4\n 2\n 0 2 4 620406080100Plugin approximation\nprediction\ntraining data\n(a)\n6\n 4\n 2\n 0 2 4 620406080100120Posterior predictive\nprediction\ntraining data (b)\n6\n 4\n 2\n 0 2 4 620406080100functions sampled from plugin approximation to posterior\n(c)\n6\n 4\n 2\n 0 2 4 6020406080100120140functions sampled from posterior (d)\nFigure 11.21: (a) Plugin approximation to predictive density (we plug in the MLE of the parameters) when\nﬁtting a second degree polynomial to some 1d data. (b) Posterior predictive density, obtained by integrating\nout the parameters. Black curve is posterior mean, error bars are 2 standard deviations of the posterior\npredictive density. (c) 10 samples from the plugin approximation to posterior predictive distribution. (d) 10\nsamples from the true posterior predictive distribution. Generated by code at ﬁgures.probml.ai/book1/11.21.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1015, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 946}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1016_dbd31d06", "text": "(c) 10 samples from the plugin approximation to posterior predictive distribution. (d) 10\nsamples from the true posterior predictive distribution. Generated by code at ﬁgures.probml.ai/book1/11.21. 11.7.5 The advantage of centering\nThe astute reader might notice that the shape of the 2d posterior in Figure 11.20 is an elongated\nellipse (which eventually collapses to a point as N!1). This implies that there is a lot of posterior\ncorrelation between the two parameters, which can cause computational diﬃculties. To understand why this happens, note that each data point induces a likelihood function corre-\nsponding to a line which goes through that data point. When we look at all the data together, we see\nthat predictions with maximum likelihood must correspond to lines that go through the mean of the\ndata, (x;y). There are many such lines, but if we increase the slope, we must decrease the intercept.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1016, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 909}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1017_927ecef7", "text": "There are many such lines, but if we increase the slope, we must decrease the intercept. Thus we can think of the set of high probability lines as spinning around the data mean, like a wheel\nof fortune.5This correlation between w0andw1is why the posterior has the form of a diagonal line. (The Gaussian prior converts this into an elongated ellipse, but the posterior correlation still persists\nuntil the sample size causes the posterior to shrink to a point.)\nIt can be hard to compute such elongated posteriors. One simple solution is to center the input\ndata, i.e., by using x0\nn=xn\u0000x. Now the lines can pivot around the origin, reducing the posterior\n5. This analogy is from [Mar18, p96]. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.7. Bayesian linear regression * 403\n(a)\n (b)\nFigure 11.22: Posterior samples of p(w0;w1jD)for 1d linear regression model p(yjx;\u0012) =N(yjw0+\nw1x;\u001b2)with a Gaussian prior. (a) Original data. (b) Centered data.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1017, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 978}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1018_5ac495fe", "text": "(a) Original data. (b) Centered data. Generated by code at ﬁg-\nures.probml.ai/book1/11.22. correlation between w0andw1. See Figure 11.22 for an illustration. (We may also choose to divide\neachxnby the standard deviation of that feature, as discussed in Section 10.2.8.)\nNote that we can convert the posterior derived from ﬁtting to the centered data back to the original\ncoordinates by noting that\ny0=w0\n0+w0\n1x0=w0\n0+w0\n1(x\u0000x) = (w0\n0\u0000w0\n1x) +w0\n1x (11.126)\nThus the parameters on the uncentered data are w0=w0\n0\u0000w0\n1xandw1=w0\n1. 11.7.6 Dealing with multicollinearity\nIn many datasets, the input variables can be highly correlated with each other. Including all of\nthem does not generally harm predictive accuracy (provided you use a suitable prior or regularizer to\nprevent overﬁtting). However, it can make interpretation of the coeﬃcients more diﬃcult. To illustrate this, we use a toy example from [McE20, Sec 6.1].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1018, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 920}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1019_e0af4da2", "text": "However, it can make interpretation of the coeﬃcients more diﬃcult. To illustrate this, we use a toy example from [McE20, Sec 6.1]. Suppose we have a dataset of N\npeople in which we record their heights hi, as well as the length of their left legs liand right legs ri. Supposehi\u0018N(10;2), so the average height is h= 10(in unspeciﬁed units). Suppose the length of\nthe legs is some fraction \u001ai\u0018Unif(0:4;0:5)of the height, plus a bit of Gaussian noise, speciﬁcally\nli\u0018N(\u001aihi;0:02)andri\u0018N(\u001aihi;0:02). Now suppose we want to predict the height of a person given measurement of their leg lengths. (I did mention this is a toy example!) Since both left and right legs are noisy measurements\nof the unknown quantity, it is useful to use both of them. So we use linear regression to ﬁt\np(hjl;r) =N(hj\u000b+\fll+\frr;\u001b2). We use vague priors, \u000b;\fl;\fr\u0018N(0;100), and\u001b\u0018Expon(1) . Since the average leg length is l= 0:45h= 4:5, we might expect each \fcoeﬃcient to be around\nh=l= 10=4:5 = 2:2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1019, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 971}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1020_ca9faa27", "text": "We use vague priors, \u000b;\fl;\fr\u0018N(0;100), and\u001b\u0018Expon(1) . Since the average leg length is l= 0:45h= 4:5, we might expect each \fcoeﬃcient to be around\nh=l= 10=4:5 = 2:2. However, the posterior marginals shown in Figure 11.23 tell a diﬀerent story: we\nsee that the posterior mean of \flis near 2.6, but \fris near -0.6. Thus it seems like the right leg feature\nis not needed. This is because the regression coeﬃcient for feature jencodes the value of knowing xj\ngiven that all the other features x\u0000jare already known, as we discussed in Section 11.2.2.1. If we\nalready know the left leg, the marginal value of also knowing the right leg is small. However, if we\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n404 Chapter 11. Linear Regression\n4\n 2\n 0 2 4 6sigmabrbla95.0% HDI\n(a)\nFigure 11.23: Posterior marginals for the parameters in the multi-leg example. Generated by code at\nﬁgures.probml.ai/book1/11.23.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1020, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 915}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1021_e2a8243c", "text": "Linear Regression\n4\n 2\n 0 2 4 6sigmabrbla95.0% HDI\n(a)\nFigure 11.23: Posterior marginals for the parameters in the multi-leg example. Generated by code at\nﬁgures.probml.ai/book1/11.23. 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5\nbr5.0\n2.5\n0.02.55.07.510.0bl\n(a)\n1.8 1.9 2.0 2.1 2.2012345sum of bl and br (b)\nFigure 11.24: Posteriors for the multi-leg example. (a) Joint posterior p(\fl;\frjD)(b) Posterior of p(\fl+\n\frjdata). Generated by code at ﬁgures.probml.ai/book1/11.24. rerun this example with slightly diﬀerent data, we may reach the opposite conclusion, and favor the\nright leg over the left. We can gain more insight by looking at the joint distribution p(\fl;\frjD), shown in Figure 11.24a. We see that the parameters are very highly correlated, so if \fris large, then \flis small, and vice\nversa. The marginal distribution for each parameter does not capture this. However, it does show\nthat there is a lot of uncertainty about each parameter, showing that they are non-identiﬁable .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1021, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 976}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1022_82619df8", "text": "The marginal distribution for each parameter does not capture this. However, it does show\nthat there is a lot of uncertainty about each parameter, showing that they are non-identiﬁable . However, their sumis well-determined, as can be seen from Figure 11.24b, where we plot p(\fl+\frjD);\nthis is centered on 2.2, as we might expect. This example goes to show that we must be careful trying to interpret the signiﬁcance of individual\ncoeﬃcient estimates in a model, since they do not mean much in isolation. 11.7.7 Automatic relevancy determination (ARD) *\nConsider a linear regression model with known observation noise but unknown regression weights,\nN(yjXw;\u001b2I). Suppose we use a Gaussian prior for the weights, wj\u0018N(0;1=\u000bj), where\u000bjis the\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.8. Exercises 405\nprecision of the j’th parameter.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1022, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 868}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1023_6a381f29", "text": "August 27, 2021\n11.8. Exercises 405\nprecision of the j’th parameter. Now suppose we estimate the prior precisions as follows:\n^\u000b= argmax\n\u000bp(yjX;\u000b) (11.127)\nwhere\np(yjX;\u000b) =Z\np(yjXw;\u001b2)p(wj0;diag(\u000b)\u00001)dw (11.128)\nis the marginal likelihood. This is an example of empirical Bayes, since we are estimating the prior\nfrom data. We can view this as as a computational shortcut to a fully Bayesian approach. However,\nthere are additional advantages. In particular, suppose, after estimating \u000b, we compute the MAP\nestimate\n^w= argmax\nwN(wj0;^\u000b\u00001) (11.129)\nThis results in a sparse estimate for ^w, which is perhaps surprising given that the Gaussian prior for\nwis not sparsity promoting. The reasons for this are explained in the sequel to this book. This technique is known as sparse Bayesian learning [Tip01] or automatic relevancy deter-\nmination (ARD) [Mac95; Nea96]. It was originally developed for neural networks (where sparsity\nis applied to the ﬁrst layer weights), but here we apply it to linear models.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1023, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1006}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1024_64389d3a", "text": "It was originally developed for neural networks (where sparsity\nis applied to the ﬁrst layer weights), but here we apply it to linear models. See also Section 17.4.1,\nwhere we apply it kernelized linear models. 11.8 Exercises\nExercise 11.1 [Multi-output linear regression *]\n(Source: Jaakkola.)\nConsider a linear regression model with a 2 dimensional response vector yi2R2. Suppose we have some\nbinary input data, xi2f0;1g. The training data is as follows:\nxy\n0(\u00001;\u00001)T\n0(\u00001;\u00002)T\n0(\u00002;\u00001)T\n1(1;1)T\n1(1;2)T\n1(2;1)T\nLet us embed each xiinto 2d using the following basis function:\n\u001e(0) = (1;0)T;\u001e(1) = (0;1)T(11.130)\nThe model becomes\n^y=WT\u001e(x) (11.131)\nwhere Wis a2\u00022matrix. Compute the MLE for Wfrom the above data. Exercise 11.2 [Centering and ridge regression]\nAssume that x= 0, so the input data has been centered. Show that the optimizer of\nJ(w;w0) = (y\u0000Xw\u0000w01)T(y\u0000Xw\u0000w01) +\u0015wTw (11.132)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n406 Chapter 11.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1024, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 966}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1025_4d51974d", "text": "Show that the optimizer of\nJ(w;w0) = (y\u0000Xw\u0000w01)T(y\u0000Xw\u0000w01) +\u0015wTw (11.132)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n406 Chapter 11. Linear Regression\nis\n^w0=y (11.133)\nw= (XTX+\u0015I)\u00001XTy (11.134)\nExercise 11.3 [Partial derivative of the RSS *]\nLetRSS (w) =jjXw\u0000yjj2\n2be the residual sum of squares. a. Show that\n@\n@wkRSS (w) =akwk\u0000ck (11.135)\nak= 2nX\ni=1x2\nik= 2jjx:;kjj2(11.136)\nck= 2nX\ni=1xik(yi\u0000wT\n\u0000kxi;\u0000k) = 2xT\n:;krk (11.137)\nwherew\u0000k=wwithout component k,xi;\u0000kisxiwithout component k, andrk=y\u0000wT\n\u0000kx:;\u0000kis the\nresidual due to using all the features except feature k. Hint: Partition the weights into those involving k\nand those not involving k. b. Show that if@\n@wkRSS (w) = 0, then\n^wk=xT\n:;krk\njjx:;kjj2(11.138)\nHence when we sequentially add features, the optimal weight for feature kis computed by computing\northogonally projecting x:;konto the current residual.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1025, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 888}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1026_839a88dc", "text": "Exercise 11.4 [Reducing elastic net to lasso]\nDeﬁne\nJ1(w) =jy\u0000Xwj2+\u00152jwj2+\u00151jwj1 (11.139)\nand\nJ2(w) =j~y\u0000~X~wj2+c\u00151jwj1 (11.140)\nwherec= (1 +\u00152)\u00001\n2and\n~X=c\u0012Xp\n\u00152Id\u0013\n;~y=\u0012y\n0d\u00021\u0013\n(11.141)\nShow\nargminJ1(w) =c(argminJ2(w)) (11.142)\ni.e. J1(cw) =J2(w) (11.143)\nand hence that one can solve an elastic net problem using a lasso solver on modiﬁed data. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n11.8. Exercises 407\nExercise 11.5 [Shrinkage in linear regression *]\n(Source: Jaakkola.) Consider performing linear regression with an orthonormal design matrix, so jjx:;kjj2\n2= 1\nfor each column (feature) k, andxT\n:;kx:;j= 0, so we can estimate each parameter wkseparately. Figure 10.15b plots ^wkvsck= 2yTx:;k, the correlation of feature kwith the response, for 3 diﬀerent\nestimation methods: ordinary least squares (OLS), ridge regression with parameter \u00152, and lasso with\nparameter\u00151. a.Unfortunately we forgot to label the plots.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1026, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 958}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1027_2edd87c7", "text": "a.Unfortunately we forgot to label the plots. Which method does the solid (1), dotted (2) and dashed (3)\nline correspond to? b. What is the value of \u00151? c. What is the value of \u00152? Exercise 11.6 [EM for mixture of linear regression experts]\nDerive the EM equations for ﬁtting a mixture of linear regression experts. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n\n12 Generalized Linear Models *\n12.1 Introduction\nIn Chapter 10, we discussed logistic regression, which, in the binary case, corresponds to the model\np(yjx;w) =Ber(yj\u001b(wTx)). In Chapter 11, we discussed linear regression, which corresponds to the\nmodelp(yjx;w) =N(yjwTx;\u001b2). These are obviously very similar to each other. In particular, the\nmean of the output, E[yjx;w], is a linear function of the inputs xin both cases. It turns out that there is a broad family of models with this property, known as generalized\nlinear models orGLMs[MN89].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1027, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 920}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1028_9730b886", "text": "It turns out that there is a broad family of models with this property, known as generalized\nlinear models orGLMs[MN89]. A GLM is a conditional version of an exponential family distribution (Section 3.4), in which the\nnatural parameters are a linear function of the input. More precisely, the model has the following\nform:\np(ynjxn;w;\u001b2) = exp\u0014yn\u0011n\u0000A(\u0011n)\n\u001b2+ logh(yn;\u001b2)\u0015\n(12.1)\nwhere\u0011n,wTxnis the (input dependent) natural parameter, A(\u0011n)is the log normalizer, T(y) =y\nis the suﬃcient statistic, and \u001b2is the dispersion term.1\nWe will denote the mapping from the linear inputs to the mean of the output using \u0016n=`\u00001(\u0011n),\nwhere the function `is known as the link function , and`\u00001is known as the mean function . Based on the results in Section 3.4.3, we can show that the mean and variance of the response\nvariable are as follows:\nE\u0002\nynjxn;w;\u001b2\u0003\n=A0(\u0011n),`\u00001(\u0011n) (12.2)\nV\u0002\nynjxn;w;\u001b2\u0003\n=A00(\u0011n)\u001b2(12.3)\n12.2 Examples\nIn this section, we give some examples of widely used GLMs. 1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1028, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 977}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1029_68aa6f6d", "text": "1. Technically speaking, GLMs use a slight extension of the natural exponential family known as the exponential\ndispersion family . For a scalar variable, this has the form p(yj\u0011;\u001b2) =h(y;\u001b2)exph\n\u0011y\u0000A(\u0011)\n\u001b2i\n. Here\u001b2is called\nthedispersion parameter . For ﬁxed \u001b2, this is a natural exponential family. 410 Chapter 12. Generalized Linear Models *\n12.2.1 Linear regression\nRecall that linear regression has the form\np(ynjxn;w;\u001b2) =1p\n2\u0019\u001b2exp(\u00001\n2\u001b2(yn\u0000wTxn)2) (12.4)\nHence\nlogp(ynjxn;w;\u001b2) =\u00001\n2\u001b2(yn\u0000\u0011n)2\u00001\n2log(2\u0019\u001b2) (12.5)\nwhere\u0011n=wTxn. We can write this in GLM form as follows:\nlogp(ynjxn;w;\u001b2) =yn\u0011n\u0000\u00112\nn\n2\n\u001b2\u00001\n2\u0012y2\nn\n\u001b2+ log(2\u0019\u001b2)\u0013\n(12.6)\nWe see that A(\u0011n) =\u00112\nn=2and hence\nE[yn] =\u0011n=wTxn (12.7)\nV[yn] =\u001b2(12.8)\n12.2.2 Binomial regression\nIf the response variable is the number of successes in Nntrials,yn2f0;:::;Nng, we can use\nbinomial regression , which is deﬁned by\np(ynjxn;Nn;w) = Bin(ynj\u001b(wTxn);Nn) (12.9)\nWe see that binary logistic regression is the special case when Nn= 1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1029, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 988}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1030_3e18ee9d", "text": "The log pdf is given by\nlogp(ynjxn;Nn;w) =ynlog\u0016n+ (Nn\u0000yn) log(1\u0000\u0016n) + log\u0012Nn\nyn\u0013\n(12.10)\n=ynlog(\u0016n\n1\u0000\u0016n) +Nnlog(1\u0000\u0016n) + log\u0012Nn\nyn\u0013\n(12.11)\nwhere\u0016n=\u001b(\u0011n). To rewrite this in GLM form, let us deﬁne\n\u0011n,log\u0014\u0016n\n(1\u0000\u0016n)\u0015\n= log\"\n1\n1 +e\u0000wTxn1 +e\u0000wTxn\ne\u0000wTxn#\n= log1\ne\u0000wTxn=wTxn (12.12)\nHence we can write binomial regression in GLM form as follows\nlogp(ynjxn;Nn;w) =yn\u0011n\u0000A(\u0011n) +h(yn) (12.13)\nwhereh(yn) = log\u0012Nn\nyn\u0013\nand\nA(\u0011n) =\u0000Nnlog(1\u0000\u0016n) =Nnlog(1 +e\u0011n) (12.14)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n12.3. GLMs with non-canonical link functions 411\nHence\nE[yn] =dA\nd\u0011n=Nne\u0011n\n1 +e\u0011n=Nn\n1 +e\u0000\u0011n=Nn\u0016n (12.15)\nand\nV[yn] =d2A\nd\u00112n=Nn\u0016n(1\u0000\u0016n) (12.16)\n12.2.3 Poisson regression\nIf the response variable is an integer count, yn2f0;1;:::g, we can use Poisson regression , which\nis deﬁned by\np(ynjxn;w) = Poi(ynjexp(wTxn)) (12.17)\nwhere\nPoi(yj\u0016) =e\u0000\u0016\u0016y\ny!(12.18)\nis the Poisson distribution.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1030, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 912}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1031_cbf76791", "text": "Poisson regression is widely used in bio-statistical applications, where\nynmight represent the number of diseases of a given person or place, or the number of reads at a\ngenomic location in a high-throughput sequencing context (see e.g., [Kua+09]). The log pdf is given by\nlogp(ynjxn;w) =ynlog\u0016n\u0000\u0016n\u0000log(yn!) (12.19)\nwhere\u0016n= exp(wTxn). Hence in GLM form we have\nlogp(ynjxn;w) =yn\u0011n\u0000A(\u0011n) +h(yn) (12.20)\nwhere\u0011n= log(\u0016n) =wTxn,A(\u0011n) =\u0016n=e\u0011n, andh(yn) =\u0000log(yn!). Hence\nE[yn] =dA\nd\u0011n=e\u0011n=\u0016n (12.21)\nand\nV[yn] =d2A\nd\u00112n=e\u0011n=\u0016n (12.22)\n12.3 GLMs with non-canonical link functions\nWe have seen how the mean parameters of the output distribution are given by \u0016=`\u00001(\u0011), where\nthe function `is the link function. There are several choices for this function, as we now discuss. Thecanonical link function `satisﬁes the property that \u0012=`(\u0016), where\u0012are the canonical\n(natural) parameters. Hence\n\u0012=`(\u0016) =`(`\u00001(\u0011)) =\u0011 (12.23)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n412 Chapter 12.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1031, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 988}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1032_250fe6c1", "text": "Hence\n\u0012=`(\u0016) =`(`\u00001(\u0011)) =\u0011 (12.23)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n412 Chapter 12. Generalized Linear Models *\nThis is what we have assumed so far. For example, for the Bernoulli distribution, the canonical\nparameter is the log-odds \u0012= log(\u0016=(1\u0000\u0016)), which is given by the logit transform\n\u0012=`(\u0016) = logit(\u0016) = log\u0012\u0016\n1\u0000\u0016\u0013\n(12.24)\nThe inverse of this is the sigmoid or logistic funciton \u0016=\u001b(\u0012) = 1=(1 +e\u0000\u0012). However, we are free to use other kinds of link function. For example, the probit link function\nhas the form\n\u0011=`(\u0016) = \b\u00001(\u0016) (12.25)\nAnother link function that is sometimes used for binary responses is the complementary log-log\nfunction\n\u0011=`(\u0016) = log(\u0000log(1\u0000\u0016)) (12.26)\nThis is used in applications where we either observe 0 events (denoted by y= 0) or one or more\n(denoted by y= 1), where events are assumed to be governed by a Poisson distribution with rate \u0015. LetEbe the number of events.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1032, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 922}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1033_a38d7ab9", "text": "LetEbe the number of events. The Poisson assumption means p(E= 0) = exp(\u0000\u0015)and hence\np(y= 0) = (1\u0000\u0016) =p(E= 0) = exp(\u0000\u0015) (12.27)\nThus\u0015=\u0000log(1\u0000\u0016). When\u0015is a function of covariates, we need to ensure it is positive, so we use\n\u0015=e\u0011, and hence\n\u0011= log(\u0015) = log(\u0000log(1\u0000\u0016)) (12.28)\n12.4 Maximum likelihood estimation\nGLMs can be ﬁt using similar methods to those that we used to ﬁt logistic regression. In particular,\nthe negative log-likelihood has the following form (ignoring constant terms):\nNLL(w) =\u0000logp(Djw) =\u00001\n\u001b2NX\nn=1`n (12.29)\nwhere\n`n,\u0011nyn\u0000A(\u0011n) (12.30)\nwhere\u0011n=wTxn. For notational simplicity, we will assume \u001b2= 1. We can compute the gradient for a single term as follows:\ngn,@`n\n@w=@`n\n@\u0011n@\u0011n\n@w= (yn\u0000A0(\u0011n))xn= (yn\u0000\u0016n)xn (12.31)\nwhere\u0016n=f(wTx), andfis the inverse link function that maps from canonical parameters to\nmean parameters. For example, in the case of logistic regression, f(\u0011n) =\u001b(\u0011n), so we recover\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n12.5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1033, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1000}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1034_04097654", "text": "For example, in the case of logistic regression, f(\u0011n) =\u001b(\u0011n), so we recover\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n12.5. Worked example: predicting insurance claims 413\n0 5 10 15 20 25 30\ny (observed Frequency)100101102103104105#samplesData\n1\n 0 1 2 3 4\ny_pred (predicted expected Frequency)DummyRegressor\n1\n 0 1 2 3 4\ny_pred (predicted expected Frequency)Ridge\n1\n 0 1 2 3 4\ny_pred (predicted expected Frequency)PoissonRegressor\nFigure 12.1: Predictions of insurance claim rates on the test set. (a) Data. (b) Constant predictor. (c) Linear\nregression. (d) Poisson regression. Generated by code at ﬁgures.probml.ai/book1/12.1. Equation (10.21). This gradient expression can be used inside SGD, or some other gradient method,\nin the obvious way.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1034, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 782}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1035_a6f9086f", "text": "(d) Poisson regression. Generated by code at ﬁgures.probml.ai/book1/12.1. Equation (10.21). This gradient expression can be used inside SGD, or some other gradient method,\nin the obvious way. The Hessian is given by\nH=@2\n@w@wTNLL(w) =\u0000NX\nn=1@gn\n@wT(12.32)\nwhere\n@gn\n@wT=@gn\n@\u0016n@\u0016n\n@wT=\u0000xnf0(wTxn)xT\nn (12.33)\nHence\nH=NX\nn=1f0(\u0011n)xnxT\nn (12.34)\nFor example, in the case of logistic regression, f(\u0011n) =\u001b(\u0011n), andf0(\u0011n) =\u001b(\u0011n)(1\u0000\u001b(\u0011n)), so we\nrecover Equation (10.23). In general, we see that the Hessian is positive deﬁnite, since f0(\u0011n)>0;\nhence the negative log likelihood is convex, so the MLE for a GLM is unique (assuming f(\u0011n)>0\nfor alln). Based on the above results, we can ﬁt GLMs using gradient based solvers in a manner that is very\nsimilar to how we ﬁt logistic regression models. 12.5 Worked example: predicting insurance claims\nIn this section, we give an example of predicting insurance claims using linear and Poisson regression.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1035, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 944}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1036_df1be314", "text": "12.5 Worked example: predicting insurance claims\nIn this section, we give an example of predicting insurance claims using linear and Poisson regression.2. The goal is to predict the expected number of insurance claims per year following car accidents. The\ndataset consists of 678k examples with 9 features, such as driver age, vehicle age, vehicle power,\n2. This example is from https://scikit-learn.org/stable/auto_examples/linear_model/plot_poisson_\nregression_non_normal_loss.html\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n414 Chapter 12. Generalized Linear Models *\nName MSE MAE Deviance\nDummy 0.564 0.189 0.625\nRidge 0.560 0.177 0.601\nPoisson 0.560 0.186 0.594\nTable 12.1: Performance metrics on the test set. MSE = mean squared error. MAE = mean absolute error. Deviance = Poisson deviance. etc. The target is the frequency of claims, which is the number of claims per policy divided by the\nexposure (i.e., the duration of the policy in years). We plot the test set in Figure 12.1(a).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1036, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1008}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1037_668c9a4c", "text": "etc. The target is the frequency of claims, which is the number of claims per policy divided by the\nexposure (i.e., the duration of the policy in years). We plot the test set in Figure 12.1(a). We see that for 94% of the policies, no claims are made, so\nthe data has lots of 0s, as is typical for count and rate data. The average frequency of claims is 10%. This can be converted into a dummy model, which always predicts this constant. This results in the\npredictions shown in Figure 12.1(b). The goal is to do better than this. A simple approach is to use linear regression, combined with some simple feature engineering\n(binning the continuous values, and one-hot encoding the categoricals). (We use a small amount of `2\nregularization, so technically this is ridge regression.) This gives the results shown in Figure 12.1(c). This is better than the baseline, but still not very good. In particular, it can predict negative\noutcomes, and fails to capture the long tail.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1037, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 973}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1038_f0f7f9de", "text": "This is better than the baseline, but still not very good. In particular, it can predict negative\noutcomes, and fails to capture the long tail. We can do better using Poisson regression, using the same features but a log link function. The\nresults are shown in Figure 12.1(d). We see that predictions are much better. An interesting question is how to quantify performance in this kind of problem. If we use mean\nsquared error, or mean absolute error, we may conclude from Table 12.1 that ridge regression is\nbetter than Poisson regression, but this is clearly not true, as shown in Figure 12.1. Instead it is\nmore common to measure performance using the deviance , which is deﬁned as\nD(y;^\u0016) = 2X\ni(logp(yij\u0016\u0003\ni)\u0000logp(yij\u0016i)) (12.35)\nwhere\u0016iis the predicted parameters for the i’th example (based on the input features xiand the\ntraining setD), and\u0016\u0003\niis the optimal parameter estimated by ﬁtting the model just to the true\noutputyi.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1038, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 934}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1039_79b8a83e", "text": "(This is the so-called saturated model, that perfectly ﬁts the test set.) In the case of\nPoisson regression, we have \u0016\u0003\ni=yi. Hence\nD(y;\u0016) = 2X\ni[(yilogyi\u0000yi\u0000log(yi!))\u0000(yilog ^\u0016i\u0000^\u0016i\u0000log(yi!))] (12.36)\n= 2X\ni\u0014\n(yilogyi\n^\u0016i+ ^\u0016i\u0000yi\u0015\n(12.37)\nBy this metric, the Poisson model is clearly better (see last column of Table 12.1). We can also compute a calibration plot , which plots the actual frequency vs the predicted\nfrequency. To compute this, we bin the predictions into intervals, and then count the empirical\nfrequency of claims for all examples whose predicted frequency falls into that bin. The results\nare shown in Figure 12.2. We see that the constant baseline is well calibrated, but of course it is\nnot very accurate. The ridge model is miscalibrated in the low frequency regime. In particular, it\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n12.5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1039, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 888}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1040_3d47e97e", "text": "The ridge model is miscalibrated in the low frequency regime. In particular, it\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n12.5. Worked example: predicting insurance claims 415\n0.0 0.2 0.4 0.6 0.8 1.0\nFraction of samples sorted by y_pred0.00.10.20.30.40.5Mean Frequency (y_pred)\nDummyRegressor()\npredictions\nobservations\n0.0 0.2 0.4 0.6 0.8 1.0\nFraction of samples sorted by y_pred0.00.10.20.30.40.5Mean Frequency (y_pred)\nRidge(alpha=1e-06)\npredictions\nobservations\n0.0 0.2 0.4 0.6 0.8 1.0\nFraction of samples sorted by y_pred0.00.10.20.30.40.5Mean Frequency (y_pred)\nPoissonRegressor(alpha=1e-12, max_iter=300)\npredictions\nobservations\nFigure 12.2: Calibration plot for insurance claims prediction. Generated by code at ﬁg-\nures.probml.ai/book1/12.2. underestimates the total number of claims in the test set to be 10,693, whereas the truth is 11,935.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1040, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 886}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1041_f162e76f", "text": "Generated by code at ﬁg-\nures.probml.ai/book1/12.2. underestimates the total number of claims in the test set to be 10,693, whereas the truth is 11,935. The Poisson model is better calibrated (i.e., when it predicts examples will have a high claim rate,\nthey do in fact have a high claim rate), and it predicts the total number of claims to be 11,930. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n\nPart III\nDeep Neural Networks\n\n13 Neural Networks for Structured Data\n13.1 Introduction\nIn Part II, we discussed linear models for regression and classiﬁcation. In particular, in Chapter 10,\nwe discussed logistic regression, which, in the binary case, corresponds to the model p(yjx;w) =\nBer(yj\u001b(wTx)), and in the multiclass case corresponds to the model p(yjx;W) =Cat(yjS(Wx)). In\nChapter 11, we discussed linear regression, which corresponds to the model p(yjx;w) =N(yjwTx;\u001b2).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1041, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 892}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1042_25f5f475", "text": "In\nChapter 11, we discussed linear regression, which corresponds to the model p(yjx;w) =N(yjwTx;\u001b2). And in Chapter 12, we discussed generalized linear models, which generalizes these models to other\nkinds of output distributions, such as Poisson. However, all these models make the strong assumption\nthat the input-output mapping is linear. A simple way of increasing the ﬂexibility of such models is to perform a feature transformation, by\nreplacingxwith\u001e(x). For example, we can use a polynomial transform, which in 1d is given by\n\u001e(x) = [1;x;x2;x3;:::], as we discussed in Section 1.2.2.2. This is sometimes called basis function\nexpansion . The model now becomes\nf(x;\u0012) =W\u001e(x) +b (13.1)\nThis is still linear in the parameters \u0012= (W;b), which makes model ﬁtting easy (since the negative\nlog-likelihood is convex). However, having to specify the feature transformation by hand is very\nlimiting.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1042, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 897}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1043_6b5d5f1e", "text": "However, having to specify the feature transformation by hand is very\nlimiting. A natural extension is to endow the feature extractor with its own parameters, \u00122, to get\nf(x;\u0012) =W\u001e(x;\u00122) +b (13.2)\nwhere\u0012= (\u00121;\u00122)and\u00121= (W;b). We can obviously repeat this process recursively, to create more\nand more complex functions. If we compose Lfunctions, we get\nf(x;\u0012) =fL(fL\u00001(\u0001\u0001\u0001(f1(x))\u0001\u0001\u0001)) (13.3)\nwheref`(x) =f(x;\u0012`)is the function at layer `. This is the key idea behind deep neural networks\norDNNs. The term “DNN” actually encompases a larger family of models, in which we compose diﬀerentiable\nfunctions into any kind of DAG (directed acyclic graph), mapping input to output. Equation (13.3) is\nthe simplest example where the DAG is a chain. This is known as a feedforward neural network\n(FFNN) ormultilayer perceptron (MLP). An MLP assumes that the input is a ﬁxed-dimensional vector, say x2RD.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1043, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 892}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1044_6827e276", "text": "This is known as a feedforward neural network\n(FFNN) ormultilayer perceptron (MLP). An MLP assumes that the input is a ﬁxed-dimensional vector, say x2RD. It is common to\ncall such data “ structured data ” or “tabular data ”, since the data is often stored in an N\u0002D\n420 Chapter 13. Neural Networks for Structured Data\nx1x2y\n0 0 0\n0 1 1\n1 0 1\n1 1 0\nTable 13.1: Truth table for the XOR (exclusive OR) function, y=x1Yx2. design matrix, where each column (feature) has a speciﬁc meaning, such as height, weight, age,\netc.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1044, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 517}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1045_6cc2a26b", "text": "design matrix, where each column (feature) has a speciﬁc meaning, such as height, weight, age,\netc. In later chapters, we discuss other kinds of DNNs that are more suited to “ unstructured\ndata” such as images and text, where the input data is variable sized, and each individual element\n(e.g., pixel or word) is often meaningless on its own.1In particular, in Chapter 14, we discuss\nconvolutional neural networks (CNN), which are designed to work with images; in Chapter 15,\nwe discuss recurrent neural networks (RNN) andtransformers , which are designed to work\nwith sequences; and in Chapter 23, we discuss graph neural networks (GNN), which are designed\nto work with graphs. Although DNNs can work well, there are often a lot of engineering details that need to be addressed\nto get good performance. Some of these details are discussed in the supplementary material to\nthis book, available at probml.ai.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1045, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 907}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1046_a30c158d", "text": "Some of these details are discussed in the supplementary material to\nthis book, available at probml.ai. There are also various other books that cover this topic in more\ndepth (e.g., [Zha+20; Cho21; Gér19; GBC16]), as well as a multitude of online courses. For a more\ntheoretical treatment, see e.g., [Ber+21; Cal20; al99]. 13.2 Multilayer perceptrons (MLPs)\nIn Section 10.2.5, we explained that a perceptron is a deterministic version of logistic regression. Speciﬁcally, it is a mapping of the following form:\nf(x;\u0012) =I\u0000\nwTx+b\u00150\u0001\n=H(wTx+b) (13.4)\nwhereH(a)is theheaviside step function , also known as a linear threshold function . Since\nthe decision boundaries represented by perceptrons are linear, they are very limited in what they can\nrepresent. In 1969, Marvin Minsky and Seymour Papert published a famous book called Perceptrons\n[MP69] in which they gave numerous examples of pattern recognition problems which perceptrons\ncannot solve.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1046, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 944}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1047_28695df9", "text": "In 1969, Marvin Minsky and Seymour Papert published a famous book called Perceptrons\n[MP69] in which they gave numerous examples of pattern recognition problems which perceptrons\ncannot solve. We give a speciﬁc example below, before discussing how to solve the problem. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.2. Multilayer perceptrons (MLPs) 421\n−0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2−0.20.00.20.40.60.81.01.2Activation function: heaviside\n(a)\n-1.5 -0.5 +1 +1 +1 +1-0.5 -1 +1\n1 1 x1 x21 h1 h2y (b)\nFigure 13.1: (a) Illustration of the fact that the XOR function is not linearly separable, but can be separated\nby the two layer model using Heaviside activation functions. Adapted from Figure 10.6 of [Gér19]. Generated\nby code at ﬁgures.probml.ai/book1/13.1. (b) A neural net with one hidden layer, whose weights have been\nmanually constructed to implement the XOR function. h1is the AND function and h2is the OR function.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1047, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 955}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1048_c6cdad2f", "text": "(b) A neural net with one hidden layer, whose weights have been\nmanually constructed to implement the XOR function. h1is the AND function and h2is the OR function. The bias terms are implemented using weights from constant nodes with the value 1. 13.2.1 The XOR problem\nOne of the most famous examples from the Perceptrons book is the XOR problem . Here the goal\nis to learn a function that computes the exclusive OR of its two binary inputs. The truth table for\nthis function is given in Table 13.1. We visualize this function in Figure 13.1a. It is clear that the\ndata is not linearly separable, so a perceptron cannot represent this mapping. However, we can overcome this problem by stacking multiple perceptrons on top of each other. This is called a multilayer perceptron (MLP). For example, to solve the XOR problem, we can\nuse the MLP shown in Figure 13.1b. This consists of 3 perceptrons, denoted h1,h2andy. The nodes\nmarkedxare inputs, and the nodes marked 1are constant terms.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1048, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 986}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1049_1ebd3f65", "text": "For example, to solve the XOR problem, we can\nuse the MLP shown in Figure 13.1b. This consists of 3 perceptrons, denoted h1,h2andy. The nodes\nmarkedxare inputs, and the nodes marked 1are constant terms. The nodes h1andh2are called\nhidden units , since their values are not observed in the training data. The ﬁrst hidden unit computes h1=x1^x2by using appropriately set weights. (Here ^is the\nAND operation.) In particular, it has inputs from x1andx2, both weighted by 1.0, but has a bias\nterm of -1.5 (this is implemented by a “wire” with weight -1.5 coming from a dummy node whose\nvalue is ﬁxed to 1). Thus h1will ﬁre iﬀx1andx2are both on, since then\nwT\n1x\u0000b1= [1:0;1:0]T[1;1]\u00001:5 = 0:5>0 (13.5)\n1. The term “unstructured data” is a bit misleading, since images and text dohave structure. For example, neighboring\npixels in an image are highly correlated, as are neighboring words in a sentence. Indeed, it is precisely this structure\nthat is exploited (assumed) by CNNs and RNNs.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1049, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 981}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1050_df8ceaad", "text": "For example, neighboring\npixels in an image are highly correlated, as are neighboring words in a sentence. Indeed, it is precisely this structure\nthat is exploited (assumed) by CNNs and RNNs. By contrast, MLPs make no assumptions about their inputs. This is\nuseful for applications such as tabular data, where the structure (dependencies between the columns) is usually not\nobvious, and thus needs to be learned. We can also apply MLPs to images and text, as we will see, but performance\nwill usually be worse compared to specialized models, such as as CNNs and RNNs. (There are some exceptions, such\nas the MLP-mixer model of [Tol+21], which is an unstructured model that can learn to perform well on image and\ntext data, but such models need massive datasets to overcome their lack of inductive bias.)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n422 Chapter 13.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1050, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 879}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1051_6ee0cbb1", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n422 Chapter 13. Neural Networks for Structured Data\nSimilarly, the second hidden unit computes h2=x1_x2, where_is the OR operation, and the third\ncomputes the output y=h1^h2, whereh=:his the NOT (logical negation) operation. Thus y\ncomputes\ny=f(x1;x2) =(x1^x2)^(x1_x2) (13.6)\nThis is equivalent to the XOR function. By generalizing this example, we can show that an MLP can represent any logical function. However, we obviously want to avoid having to specify the weights and biases by hand. In the rest of\nthis chapter, we discuss ways to learn these parameters from data. 13.2.2 Diﬀerentiable MLPs\nThe MLP we discussed in Section 13.2.1 was deﬁned as a stack of perceptrons, each of which involved\nthe non-diﬀerentiable Heaviside function. This makes such models diﬃcult to train, which is why\nthey were never widely used. However, suppose we replace the Heaviside function H:R!f0;1g\nwith a diﬀerentiable activation function ':R!R.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1051, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 976}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1052_9d9a03c2", "text": "This makes such models diﬃcult to train, which is why\nthey were never widely used. However, suppose we replace the Heaviside function H:R!f0;1g\nwith a diﬀerentiable activation function ':R!R. More precisely, we deﬁne the hidden units\nzlat each layer lto be a linear transformation of the hidden units at the previous layer passed\nelementwise through this activation function:\nzl=fl(zl\u00001) ='l(bl+Wlzl\u00001) (13.7)\nor, in scalar form,\nzkl='l0\n@bkl+Kl\u00001X\nj=1wjklzjl\u000011\nA (13.8)\nThe quantity that is passed to the activation function is called the pre-activations :\nal=bl+Wlzl\u00001 (13.9)\nsozl='l(al). If we now compose Lof these functions together, as in Equation (13.3), then we can compute\nthe gradient of the output wrt the parameters in each layer using the chain rule, also known as\nbackpropagation , as we explain in Section 13.3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1052, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 827}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1053_918d9b1b", "text": "(This is true for any kind of diﬀerentiable activation\nfunction, although some kinds work better than others, as we discuss in Section 13.2.3.) We can\nthen pass the gradient to an optimizer, and thus minimize some training objective, as we discuss in\nSection 13.4. For this reason, the term “MLP” almost always refers to this diﬀerentiable form of the\nmodel, rather than the historical version with non-diﬀerentiable linear threshold units. 13.2.3 Activation functions\nWe are free to use any kind of diﬀerentiable activation function we like at each layer. However, if we\nuse alinearactivation function, '`(a) =c`a, then the whole model reduces to a regular linear model. To see this, note that Equation (13.3) becomes\nf(x;\u0012) =WLcL(WL\u00001cL\u00001(\u0001\u0001\u0001(W1x)\u0001\u0001\u0001))/WLWL\u00001\u0001\u0001\u0001W1x=W0x (13.10)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1053, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 861}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1054_7718d9e5", "text": "To see this, note that Equation (13.3) becomes\nf(x;\u0012) =WLcL(WL\u00001cL\u00001(\u0001\u0001\u0001(W1x)\u0001\u0001\u0001))/WLWL\u00001\u0001\u0001\u0001W1x=W0x (13.10)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.2. Multilayer perceptrons (MLPs) 423\n4\n 2\n 0 2 40.2\n0.00.20.40.60.81.01.2\nSaturating\nSaturating\nLinearSigmoid activation function\n(a)\n4\n 2\n 0 2 41.0\n0.5\n0.00.51.0Activation functions\nSigmoid\nT anh\nReLU (b)\nFigure 13.2: (a) Illustration of how the sigmoid function is linear for inputs near 0, but saturates for large\npositive and negative inputs. Adapted from 11.1 of [Gér19]. (b) Plots of some neural network activation\nfunctions. Generated by code at ﬁgures.probml.ai/book1/13.2. where we dropped the bias terms for notational simplicity. For this reason, it is important to use\nnonlinear activation functions.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1054, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 799}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1055_587978bd", "text": "Generated by code at ﬁgures.probml.ai/book1/13.2. where we dropped the bias terms for notational simplicity. For this reason, it is important to use\nnonlinear activation functions. In the early days of neural networks, a common choice was to use a sigmoid (logistic) function,\nwhich can be seen as a smooth approximation to the Heaviside function used in a perceptron:\n\u001b(a) =1\n1 +e\u0000a(13.11)\nHowever, as shown in Figure 13.2a, the sigmoid function saturates at 1 for large positive inputs,\nand at 0 for large negative inputs. Another common choice is the tanhfunction, which has a similar\nshape, but saturates at -1 and +1. See Figure 13.2b. In the saturated regimes, the gradient of the output wrt the input will be close to zero, so any\ngradient signal from higher layers will not be able to propagate back to earlier layers. This is called\nthevanishing gradient problem , and it makes it hard to train the model using gradient descent\n(see Section 13.4.2 for details).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1055, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 970}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1056_2d4e1bde", "text": "This is called\nthevanishing gradient problem , and it makes it hard to train the model using gradient descent\n(see Section 13.4.2 for details). One of the keys to being able to train very deep models is to use\nnon-saturating activation functions. Several diﬀerent functions have been proposed. The most\ncommon is rectiﬁed linear unit orReLU, proposed in [GBB11; KSH12]. This is deﬁned as\nReLU(a) = max(a;0) =aI(a>0) (13.12)\nThe ReLUfunction simply “turns oﬀ” negative inputs, and passes positive inputs unchanged: see\nFigure 13.2b for a plot, and Section 13.4.3 for more details. 13.2.4 Example models\nMLPs can be used to perform classiﬁcation and regression for many kinds of data. We give some\nexamples below. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n424 Chapter 13. Neural Networks for Structured Data\nFigure 13.3: An MLP with 2 hidden layers applied to a set of 2d points from 2 classes, shown in the top left\ncorner.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1056, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 940}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1057_fc5c80b6", "text": "CC-BY-NC-ND license\n424 Chapter 13. Neural Networks for Structured Data\nFigure 13.3: An MLP with 2 hidden layers applied to a set of 2d points from 2 classes, shown in the top left\ncorner. The visualizations associated with each hidden unit show the decision boundary at that part of the\nnetwork. The ﬁnal output is shown on the right. The input is x2R2, the ﬁrst layer activations are z12R4, the\nsecond layer activations are z22R2, and the ﬁnal logit is a32R, which is converted to a probability using the\nsigmoid function. This is a screenshot from the interactive demo at http: // playground. tensorflow. org . 13.2.4.1 MLP for classifying 2d data into 2 categories\nFigure 13.3 gives an illustration of an MLP with two hidden layers applied to a 2d input vector,\ncorresponding to points in the plane, coming from two concentric circles.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1057, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 839}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1058_4f07cc02", "text": "This model has the following\nform:\np(yjx;\u0012) = Ber(yj\u001b(a3)) (13.13)\na3=wT\n3z2+b3 (13.14)\nz2='(W2z1+b2) (13.15)\nz1='(W1x+b1) (13.16)\nHerea3is the ﬁnal logit score, which is converted to a probability via the sigmoid (logistic) function. The valuea3is computed by taking a linear combination of the 2 hidden units in layer 2, using\na3=wT\n3z2+b3. In turn, layer 2 is computed by taking a nonlinear combination of the 4 hidden units\nin layer 1, using z2='(W2z1+b2). Finally, layer 1 is computed by taking a nonlinear combination of\nthe2inputunits, using z1='(W1x+b1). Byadjustingtheparameters, \u0012= (W1;b1;W2;b2;w3;b3),\nto minimize the negative log likelihood, we can ﬁt the training data very well, despite the highly\nnonlinear nature of the decision boundary. (You can ﬁnd an interactive version of this ﬁgure at\nhttp://playground.tensorflow.org .)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1058, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 925}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1059_c4081bcd", "text": "(You can ﬁnd an interactive version of this ﬁgure at\nhttp://playground.tensorflow.org .)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.2. Multilayer perceptrons (MLPs) 425\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type) Output Shape Param #\n=================================================================\nflatten (Flatten) (None, 784) 0\n_________________________________________________________________\ndense (Dense) (None, 128) 100480\n_________________________________________________________________\ndense_1 (Dense) (None, 128) 16512\n_________________________________________________________________\ndense_2 (Dense) (None, 10) 1290\n=================================================================\nTotal params: 118,282\nTrainable params: 118,282\nNon-trainable params: 0\nTable 13.2: Structure of the MLP used for MNIST classiﬁcation. Note that 100;480 = (784 + 1)\u0002128, and\n16;512 = (128 + 1)\u0002128.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1059, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 983}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1060_801cc7e2", "text": "Note that 100;480 = (784 + 1)\u0002128, and\n16;512 = (128 + 1)\u0002128. Generated by mlp_mnist_tf.ipynb. 13.2.4.2 MLP for image classiﬁcation\nTo apply an MLP to image classiﬁcation, we need to “ ﬂatten” the 2d input into 1d vector. We can\nthen use a feedforward architecture similar to the one described in Section 13.2.4.1. For example,\nconsider building an MLP to classiﬁy MNIST digits (Section 3.5.2). These are 28\u000228 = 784 -\ndimensional. If we use 2 hidden layers with 128 units each, followed by a ﬁnal 10 way softmax layer,\nwe get the model shown in Table 13.2. We show some predictions from this model in Figure 13.4. We train it for just two “epochs” (passes\nover the dataset), but already the model is doing quite well, with a test set accuracy of 97.1%. Furthermore, the errors seem sensible, e.g., 9 is mistaken as a 3. Training for more epochs can further\nimprove test accuracy. In Chapter 14 we discuss a diﬀerent kind of model, called a convolutional neural network, which\nis better suited to images.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1060, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1005}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1061_ff703651", "text": "Training for more epochs can further\nimprove test accuracy. In Chapter 14 we discuss a diﬀerent kind of model, called a convolutional neural network, which\nis better suited to images. This gets even better performance and uses fewer parameters, by\nexploiting prior knowledge about the spatial structure of images. By contrast, with an MLP, we\ncan randomly shuﬄe (permute) the pixels without aﬀecting the output (assuming we use the same\nrandom permutation for all inputs). 13.2.4.3 MLP for text classiﬁcation\nTo apply MLPs to text classiﬁcation, we need to convert the variable-length sequence of words\nv1;:::;vT(where eachvtis a one-hot vector of length V, whereVis the vocabulary size) into a\nﬁxed dimensional vector x. The easiest way to do this is as follows. First we treat the input as an\nunordered bag of words (Section 1.5.4.1), fvtg.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1061, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 842}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1062_7b4c30cd", "text": "The easiest way to do this is as follows. First we treat the input as an\nunordered bag of words (Section 1.5.4.1), fvtg. The ﬁrst layer of the model is a E\u0002Vembedding\nmatrix W1, which converts each sparse V-dimensional vector to a dense E-dimensional embedding,\net=W1vt(see Section 20.5 for more details on word embeddings). Next we convert this set of T\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n426 Chapter 13. Neural Networks for Structured Data\n(a)\n (b)\nFigure 13.4: Results of applying an MLP (with 2 hidden layers with 128 units and 1 output layer with 10\nunits) to some MNIST images (cherry picked to include some errors). Red is incorrect, blue is correct. (a)\nAfter 1 epoch of training. (b) After 2 epochs. Generated by code at ﬁgures.probml.ai/book1/13.4. E-dimensional embeddings into a ﬁxed-sized vector using global average pooling ,e=1\nTPT\nt=1et. This can then be passed as input to an MLP.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1062, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 922}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1063_f12d73af", "text": "Generated by code at ﬁgures.probml.ai/book1/13.4. E-dimensional embeddings into a ﬁxed-sized vector using global average pooling ,e=1\nTPT\nt=1et. This can then be passed as input to an MLP. For example, if we use a single hidden layer, and a\nlogistic output (for binary classiﬁcation), we get\np(yjx;\u0012) = Ber(yj\u001b(wT\n3h+b3)) (13.17)\nh='(W2e+b2) (13.18)\ne=1\nTTX\nt=1et (13.19)\net=W1vt (13.20)\nIf we use a vocabulary size of V= 1000, an embedding size of E= 16, and a hidden layer of size\n16, we get the model shown in Table 13.3. If we apply this to the IMDB movie review sentiment\nclassiﬁcation dataset discussed in Section 1.5.2.1, we get 86% on the validation set. We see from Table 13.3 that the model has a lot of parameters, which can result in overﬁtting,\nsince the IMDB training set only has 25k examples.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1063, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 808}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1064_2d906033", "text": "We see from Table 13.3 that the model has a lot of parameters, which can result in overﬁtting,\nsince the IMDB training set only has 25k examples. However, we also see that most of the parameters\nare in the embedding matrix, so instead of learning these in a supervised way, we can perform\nunsupervised pre-training of word embedding models, as we discuss in Section 20.5. If the embedding\nmatrix W1is ﬁxed, we just have to ﬁne-tune the parameters in layers 2 and 3 for this speciﬁc labeled\ntask, which requires much less data. (See also Chapter 19, where we discuss general techniques for\ntraining with limited labeled data.)\n13.2.4.4 MLP for heteroskedastic regression\nWe can also use MLPs for regression. Figure 13.5 shows how we can make a model for heteroskedastic\nnonlinear regression. (The term “heteroskedastic” just means that the predicted output variance\nis input-dependent, as discussed in Section 2.6.3.) This function has two outputs which compute\nf\u0016(x) =E[yjx;\u0012]andf\u001b(x) =p\nV[yjx;\u0012].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1064, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 997}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1065_411b8bfb", "text": "We can share most of the layers (and hence parameters)\nbetween these two functions by using a common “ backbone ” and two output “ heads”, as shown in\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1065, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 232}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1066_978abb6e", "text": "August 27, 2021\n13.2. Multilayer perceptrons (MLPs) 427\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type) Output Shape Param #\n=================================================================\nembedding (Embedding) (None, None, 16) 160000\n_________________________________________________________________\nglobal_average_pooling1d (Gl (None, 16) 0\n_________________________________________________________________\ndense (Dense) (None, 16) 272\n_________________________________________________________________\ndense_1 (Dense) (None, 1) 17\n=================================================================\nTotal params: 160,289\nTrainable params: 160,289\nNon-trainable params: 0\nTable 13.3: Structure of the MLP used for IMDB review classiﬁcation. We use a vocabulary size of V= 1000,\nan embedding size of E= 16, and a hidden layer of size 16.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1066, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 888}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1067_391353d6", "text": "We use a vocabulary size of V= 1000,\nan embedding size of E= 16, and a hidden layer of size 16. The embedding matrix W1has size 10;000\u000216,\nthe hidden layer (labeled “dense”) has a weight matrix W2of size 16\u000216and biasb2of size 16(note that\n16\u000216 + 16 = 272 ), and the ﬁnal layer (labeled “dense_1”) has a weight vector w3of size 16and a biasb3\nof size 1. The global average pooling layer has no free parameters. Generated by mlp_imdb_tf.ipynb. y \nσ \nµ x \nFigure 13.5: Illustration of an MLP with a shared “backbone” and two output “heads”, one for predicting\nthe mean and one for predicting the variance. From https: // brendanhasz. github. io/ 2019/ 07/ 23/\nbayesian-density-net. html . Used with kind permission of Brendan Hasz. Figure 13.5. For the \u0016head, we use a linear activation, '(a) =a. For the\u001bhead, we use a softplus\nactivation,'(a) =\u001b+(a) =log(1 +ea).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1067, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 863}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1068_e493e071", "text": "html . Used with kind permission of Brendan Hasz. Figure 13.5. For the \u0016head, we use a linear activation, '(a) =a. For the\u001bhead, we use a softplus\nactivation,'(a) =\u001b+(a) =log(1 +ea). If we use linear heads and a nonlinear backbone, the overall\nmodel is given by\np(yjx;\u0012) =N\u0000\nyjwT\n\u0016f(x;wshared );\u001b+(wT\n\u001bf(x;wshared ))\u0001\n(13.21)\nFigure 13.6 shows the advantage of this kind of model on a dataset where the mean grows linearly\nover time, with seasonal oscillations, and the variance increases quadratically. (This is a simple\nexample of a stochastic volatility model ; it can be used to model ﬁnancial data, as well as the\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n428 Chapter 13. Neural Networks for Structured Data\n20\n 10\n 0 10 20 30 40 50 602\n02468\n(a)\n20\n 10\n 0 10 20 30 40 50 602\n02468\n (b)\nFigure 13.6: Illustration of predictions from an MLP ﬁt using MLE to a 1d regression dataset with growing\nnoise. (a) Output variance is input-dependent, as in Figure 13.5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1068, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 981}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1069_5a44cd37", "text": "(a) Output variance is input-dependent, as in Figure 13.5. (b) Mean is computed using same model as\nin (a), but output variance is treated as a ﬁxed parameter \u001b2, which is estimated by MLE after training, as in\nSection 11.2.3.6. Generated by code at ﬁgures.probml.ai/book1/13.6. (a)\n (b)\nFigure 13.7: A decomposition of R2into a ﬁnite set of linear decision regions produced by an MLP with\nReLUactivations with (a) one hidden layer of 25 hidden units and (b) two hidden layers. From Figure 1 of\n[HAB19]. Used with kind permission of Maksym Andriuschenko. global temperature of the earth, which (due to climate change) is increasing in mean andin variance.)\nWe see that a regression model where the output variance \u001b2is treated as a ﬁxed (input-independent)\nparameter will sometimes be underconﬁdent, since it needs to adjust to the overall noise level, and\ncannot adapt to the noise level at each point in input space.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1069, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 918}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1070_e17a5a74", "text": "13.2.5 The importance of depth\nOne can show that an MLP with one hidden layer is auniversal function approximator , meaning\nit can model any suitably smooth function, given enough hidden units, to any desired level of accuracy\n[HSW89; Cyb89; Hor91]. Intuitively, the reason for this is that each hidden unit can specify a half\nplane, and a suﬃciently large combination of these can “carve up” any region of space, to which we\ncan associate any response (this is easiest to see when using piecewise linear activation functions, as\nshown in Figure 13.7). Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.2. Multilayer perceptrons (MLPs) 429\nHowever, various arguments, both experimental and theoretical (e.g., [Has87; Mon+14; Rag+17;\nPog+17]), have shown that deep networks work better than shallow ones. The reason is that later\nlayers can leverage the features that are learned by earlier layers; that is, the function is deﬁned in a\ncompositional orhierarchical way.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1070, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 997}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1071_dce9bd00", "text": "The reason is that later\nlayers can leverage the features that are learned by earlier layers; that is, the function is deﬁned in a\ncompositional orhierarchical way. For example, suppose we want to classify DNA strings, and\nthe positive class is associated with the regular expression *AA??CGCG??AA* . Although we could ﬁt\nthis with a single hidden layer model, intuitively it will be easier to learn if the model ﬁrst learns\nto detect the AA and CG “motifs” using the hidden units in layer 1, and then uses these features\nto deﬁne a simple linear classiﬁer in layer 2, analogously to how we solved the XOR problem in\nSection 13.2.1. 13.2.6 The “deep learning revolution”\nAlthough the ideas behind DNNs date back several decades, it was not until the 2010s that they\nstarted to become very widely used. The ﬁrst area to adopt these methods was the ﬁeld of automatic\nspeech recognition (ASR), based on breakthrough results in [Dah+11].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1071, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 933}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1072_7ec836e1", "text": "The ﬁrst area to adopt these methods was the ﬁeld of automatic\nspeech recognition (ASR), based on breakthrough results in [Dah+11]. This approach rapidly became\nthe standard paradigm, and was widely adopted in academia and industry [Hin+12]. However, the moment that got the most attention was when [KSH12] showed that deep CNNs\ncould signiﬁcantly improve performance on the challenging ImageNet image classiﬁcation benchmark,\nreducing the error rate from 26% to 16% in a single year (see Figure 1.14b); this was a huge jump\ncompared to the previous rate of progress of about 2% reduction per year. The “explosion” in the usage of DNNs has several contributing factors. One is the availability\nof cheapGPUs(graphics processing units); these were originally developed to speed up image\nrendering for video games, but they can also massively reduce the time it takes to ﬁt large CNNs,\nwhich involve similar kinds of matrix-vector computations.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1072, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 941}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1073_10035c3b", "text": "Another is the growth in large labeled\ndatasets, which enables us to ﬁt complex function approximators with many parameters without\noverﬁtting. (For example, ImageNet has 1.3M labeled images, and is used to ﬁt models that have\nmillions of parameters.) Indeed, if deep learning systems are viewed as “rockets”, then large datasets\nhave been called the fuel.2\nMotivated by the outstanding empirical success of DNNs, various companies started to become\ninterested in this technology. This had led to the development of high quality open-source software\nlibraries, such as Tensorﬂow (made by Google), PyTorch (made by Facebook), and MXNet (made\nby Amazon). These libraries support automatic diﬀerentiation (see Section 13.3) and scalable\ngradient-based optimization (see Section 8.4) of complex diﬀerentiable functions.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1073, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 815}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1074_ed66aecd", "text": "These libraries support automatic diﬀerentiation (see Section 13.3) and scalable\ngradient-based optimization (see Section 8.4) of complex diﬀerentiable functions. We will use some\nof these libraries in various places throughout the book to implement a variety of models, not just\nDNNs.3\nMore details on the history of the “deep learning revolution” can be found in e.g., [Sej18; Met21]. 13.2.7 Connections with biology\nIn this section, we discuss the connections between the kinds of neural networks we have discussed\nabove, known as artiﬁcial neural networks orANNs, and real neural networks. The details on\n2. This popular analogy is due to Andrew Ng, who mentioned it in a keynote talk at the GPU Technology Conference\n(GTC) in 2015. His slides are available at https://bit.ly/38RTxzH . 3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1074, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 792}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1075_f43358a2", "text": "The details on\n2. This popular analogy is due to Andrew Ng, who mentioned it in a keynote talk at the GPU Technology Conference\n(GTC) in 2015. His slides are available at https://bit.ly/38RTxzH . 3. Note, however, that some have argued (see e.g., [BI19]) that current libraries are too inﬂexible, and put too much\nemphasis on methods based on dense matrix-vector multiplication, as opposed to more general algorithmic primitives. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n430 Chapter 13. Neural Networks for Structured Data\nFigure 13.8: Illustration of two neurons connected together in a “circuit”. The output axon of the left neuron\nmakes a synaptic connection with the dendrites of the cell on the right. Electrical charges, in the form of ion\nﬂows, allow the cells to communicate. From https: // en. wikipedia. org/ wiki/ Neuron . Used with kind\npermission of Wikipedia author BruceBlaus.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1075, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 910}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1076_ad577596", "text": "Electrical charges, in the form of ion\nﬂows, allow the cells to communicate. From https: // en. wikipedia. org/ wiki/ Neuron . Used with kind\npermission of Wikipedia author BruceBlaus. how real biological brains work are quite complex (see e.g., [Kan+12]), but we can give a simple\n“cartoon”. We start by considering a model of a single neuron. To a ﬁrst approximation, we can say that\nwhether neuron kﬁres, denoted by hk2f0;1g, depends on the activity of its inputs, denoted by\nx2RD, as well as the strength of the incoming connections, which we denote by wk2RD. We\ncan compute a weighted sum of the inputs using ak=wT\nkx. These weights can be viewed as “wires”\nconnecting the inputs xdto neuronhk; these are analogous to dendrites in a real neuron (see\nFigure 13.8). This weighted sum is then compared to a threshold, bk, and if the activation exceeds\nthe threshold, the neuron ﬁres; this is analogous to the neuron emitting an electrical output or\naction potential .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1076, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 969}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1077_27193e75", "text": "This weighted sum is then compared to a threshold, bk, and if the activation exceeds\nthe threshold, the neuron ﬁres; this is analogous to the neuron emitting an electrical output or\naction potential . Thus we can model the behavior of the neuron using hk(x) =H(wT\nkx\u0000bk),\nwhereH(a) =I(a>0)is the Heaviside function. This is called the McCulloch-Pitts model of\nthe neuron, and was proposed in 1943 [MP43]. We can combine multiple such neurons together to make an ANN. The result has sometimes been\nviewed as a model of the brain. However, ANNs diﬀers from biological brains in many ways, including\nthe following:\n•Most ANNs use backpropagation to modify the strength of their connections (see Section 13.3). However, real brains do not use backprop, since there is no way to send information backwards\nalong an axon [Ben+15b; BS16; KH19]. Instead, they use local update rules for adjusting synaptic\nstrengths. •Most ANNs are strictly feedforward, but real brains have many feedback connections.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1077, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 993}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1078_b3d5f799", "text": "Instead, they use local update rules for adjusting synaptic\nstrengths. •Most ANNs are strictly feedforward, but real brains have many feedback connections. It is believed\nthat this feedback acts like a prior, which can be combined with bottom up likelihoods from the\nsensory system to compute a posterior over hidden states of the world, which can then be used for\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.2. Multilayer perceptrons (MLPs) 431\n1950 1985 2000 2015 2056\nYear10−210−110010110210310410510610710810910101011Number of neurons (logarithmic scale)123\n456\n78\n91011\n121314\n151617\n181920\nSpongeRoundwormLeechAntBeeFrogOctopusHuman\nFigure 13.9: Plot of neural network sizes over time. Models 1, 2, 3 and 4 correspond to the perceptron\n[Ros58], the adaptive linear unit [WH60] the neocognitron [Fuk80], and the ﬁrst MLP trained by backprop\n[RHW86].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1078, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 889}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1079_c12c7d33", "text": "Models 1, 2, 3 and 4 correspond to the perceptron\n[Ros58], the adaptive linear unit [WH60] the neocognitron [Fuk80], and the ﬁrst MLP trained by backprop\n[RHW86]. Approximate number of neurons for some living organisms are shown on the right scale (the sponge\nhas 0 neurons), based on https: // en. wikipedia. org/ wiki/ List_ of_ animals_ by_ number_ of_ neurons . From Figure 1.11 of [GBC16]. Used with kind permission of Ian Goodfellow. optimal decision making (see e.g., [Doy+07]). •Most ANNs use simpliﬁed neurons consisting of a weighted sum passed through a nonlinearity,\nbut real biological neurons have complex dendritic tree structures (see Figure 13.8), with complex\nspatio-temporal dynamics. •Most ANNs are smaller in size and number of connections than biological brains (see Figure 13.9). Of course, ANNs are getting larger every week, fueled by various new hardware accelerators ,\nsuch as GPUs and TPUs(tensor processing units ), etc.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1079, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 949}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1080_1fe416d1", "text": "Of course, ANNs are getting larger every week, fueled by various new hardware accelerators ,\nsuch as GPUs and TPUs(tensor processing units ), etc. However, even if ANNs match\nbiological brains in terms of number of units, the comparison is misleading since the processing\ncapability of a biological neuron is much higher than an artiﬁcial neuron (see point above). •Most ANNs are designed to model a single function, such as mapping an image to a label, or a\nsequence of words to another sequence of words. By contrast, biological brains are very complex\nsystems, composed of multiple specialized interacting modules, which implement diﬀerent kinds\nof functions or behaviors such as perception, control, memory, language, etc (see e.g., [Sha88;\nKan+12]). Of course, there are eﬀorts to make realistic models of biological brains (e.g., the Blue Brain\nProject [Mar06; Yon19]). However, an interesting question is whether studying the brain at this\nlevel of detail is useful for “solving AI”.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1080, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 990}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1081_bdc57c89", "text": "However, an interesting question is whether studying the brain at this\nlevel of detail is useful for “solving AI”. It is commonly believed that the low level details of biological\nbrains do not matter if our goal is to build “intelligent machines”, just as aeroplanes do not ﬂap their\nwings. However, presumably “AIs” will follow similar “laws of intelligence” to intelligent biological\nagents, just as planes and birds follow the same laws of aerodynamics. Unfortunately, we do not yet know what the “laws of intelligence” are, or indeed if there even are\nsuch laws. In this book we make the assumption that any intelligent agent should follow the basic\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n432 Chapter 13. Neural Networks for Structured Data\nprinciples of information processing and Bayesian decision theory, which is known to be the optimal\nway to make decisions under uncertainty (see Section 5.1). In practice, the optimal Bayesian approach is often computationally intractable.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1081, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1006}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1082_08a24295", "text": "In practice, the optimal Bayesian approach is often computationally intractable. In the natural\nworld, biological agents have evolved various algorithmic “shortcuts” to the optimal solution; this\ncan explain many of the heuristics that people use in everyday reasoning [KST82; GTA00; Gri20]. As the tasks we want our machines to solve become harder, we may be able to gain insights from\nneuroscience and cognitive science for how to solve such tasks in an approximate way (see e.g.,\n[MWK16; Has+17; Lak+17]). However, we should also bear in mind that AI/ML systems are\nincreasingly used for safety-critical applications, in which we might want and expect the machine\nto do better than a human. In such cases, we may want more than just heuristic solutions that\noften work; instead we may want provably reliable methods, similar to other engineering ﬁelds (see\nSection 1.6.3 for further discussion). 13.3 Backpropagation\nThis section is coauthored with Mathieu Blondel.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1082, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 968}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1083_38165764", "text": "13.3 Backpropagation\nThis section is coauthored with Mathieu Blondel. In this section, we describe the famous backpropagation algorithm , which can be used to\ncompute the gradient of a loss function applied to the output of the network wrt the parameters\nin each layer. This gradient can then be passed to a gradient-based optimization algorithm, as we\ndiscuss in Section 13.4. The backpropagation algorithm was originally discovered in [BH69], and independently in [Wer74]. However, it was [RHW86] that brought the algorithm to the attention of the “mainstream” ML\ncommunity. See the wikipedia page4for more historical details. We initially assume the computation graph is a simple linear chain of stacked layers, as in an\nMLP. In this case, backprop is equivalent to repeated applications of the chain rule of calculus\n(see Equation (7.261)). However, the method can be generalized to arbitrary directed acyclic\ngraphs (DAGs), as we discuss in Section 13.3.4.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1083, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 961}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1084_23d5074d", "text": "However, the method can be generalized to arbitrary directed acyclic\ngraphs (DAGs), as we discuss in Section 13.3.4. This general procedure is often called automatic\ndiﬀerentiation orautodiﬀ . 13.3.1 Forward vs reverse mode diﬀerentiation\nConsider a mapping of the form o=f(x), wherex2Rnando2Rm. We assume that fis deﬁned\nas a composition of functions:\nf=f4\u000ef3\u000ef2\u000ef1 (13.22)\nwheref1:Rn!Rm1,f2:Rm1!Rm2,f3:Rm2!Rm3, andf4:Rm3!Rm. The intermediate\nsteps needed to compute o=f(x)arex2=f1(x),x3=f2(x2),x4=f3(x3), ando=f4(x4). We can compute the Jacobian Jf(x) =@o\n@xT2Rm\u0002nusing the chain rule:\n@o\n@x=@o\n@x4@x4\n@x3@x3\n@x2@x2\n@x=@f4(x4)\n@x4@f3(x3)\n@x3@f2(x2)\n@x2@f1(x)\n@x(13.23)\n=Jf4(x4)Jf3(x3)Jf2(x2)Jf1(x) (13.24)\n4.https://en.wikipedia.org/wiki/Backpropagation#History\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.3. Backpropagation 433\nx=x1 f1θ1\nf2x2θ2\nf3x3θ3\nf4x4θ4\no\nFigure 13.10: A simple linear-chain feedforward model with 4 layers. Here xis the input and ois the output.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1084, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1006}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1085_9a572225", "text": "August 27, 2021\n13.3. Backpropagation 433\nx=x1 f1θ1\nf2x2θ2\nf3x3θ3\nf4x4θ4\no\nFigure 13.10: A simple linear-chain feedforward model with 4 layers. Here xis the input and ois the output. From [Blo20]. We now discuss how to compute the Jacobian Jf(x)eﬃciently. Recall that\nJf(x) =@f(x)\n@x=0\nB@@f1\n@x1\u0001\u0001\u0001@f1\n@xn......... @fm\n@x1\u0001\u0001\u0001@fm\n@xn1\nCA=0\nB@rf1(x)T\n... rfm(x)T1\nCA=\u0010\n@f\n@x1;\u0001\u0001\u0001;@f\n@xn\u0011\n2Rm\u0002n(13.25)\nwhererfi(x)T2R1\u0002nis thei’th row (for i= 1 :m) and@f\n@xj2Rmis thej’th column (for j= 1 :n). Note that, in our notation, when m= 1, the gradient, denoted rf(x), has the same shape as x. It\nis therefore a column vector, while Jf(x)is a row vector. In this case, we therefore technically have\nrf(x) =Jf(x)T. We can extract the i’th row from Jf(x)by using a vector Jacobian product (VJP) of the form\neT\niJf(x), whereei2Rmis the unit basis vector. Similarly, we can extract the j’th column from\nJf(x)by using a Jacobian vector product (JVP) of the form Jf(x)ej, whereej2Rn.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1085, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 966}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1086_ad7e7395", "text": "Similarly, we can extract the j’th column from\nJf(x)by using a Jacobian vector product (JVP) of the form Jf(x)ej, whereej2Rn. This shows\nthat the computation of Jf(x)reduces to either nJVPs ormVJPs. Ifn < m, it is more eﬃcient to compute Jf(x)for each column j= 1 :nby using JVPs in a\nright-to-left manner. The right multiplication with a column vector vis\nJf(x)v=Jf4(x4)|{z}\nm\u0002m3Jf3(x3)|{z}\nm3\u0002m2Jf2(x2)|{z}\nm2\u0002m1Jf1(x1)|{z}\nm1\u0002nv|{z}\nn\u00021(13.26)\nThis can be computed using forward mode diﬀerentiation ; see Algorithm 5 for the pseudocode. Assumingm= 1andn=m1=m2=m3, the cost of computing Jf(x)isO(n3). Algorithm 5: Foward mode diﬀerentiation\n1x1:=x\n2vj:=ej2Rnforj= 1 :n\n3fork= 1 :Kdo\n4xk+1=fk(xk)\n5vj:=Jfk(xk)vjforj= 1 :n\n6Returno=xK+1,[Jf(x)]:;j=vjforj= 1 :n\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n434 Chapter 13. Neural Networks for Structured Data\nIfn>m(e.g., if the output is a scalar), it is more eﬃcient to compute Jf(x)for each row i= 1 :m\nby using VJPs in a left-to-right manner.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1086, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1009}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1087_4aca607e", "text": "Neural Networks for Structured Data\nIfn>m(e.g., if the output is a scalar), it is more eﬃcient to compute Jf(x)for each row i= 1 :m\nby using VJPs in a left-to-right manner. The left multiplication with a row vector uTis\nuTJf(x) =uT\n|{z}\n1\u0002mJf4(x4)|{z}\nm\u0002m3Jf3(x3)|{z}\nm3\u0002m2Jf2(x2)|{z}\nm2\u0002m1Jf1(x1)|{z}\nm1\u0002n(13.27)\nThis can be done using reverse mode diﬀerentiation ; see Algorithm 6 for the pseudocode. Assumingm= 1andn=m1=m2=m3, the cost of computing Jf(x)isO(n2). Algorithm 6: Reverse mode diﬀerentiation\n1x1:=x\n2fork= 1 :Kdo\n3xk+1=fk(xk)\n4ui:=ei2Rmfori= 1 :m\n5fork=K: 1do\n6uT\ni:=uT\niJfk(xk)fori= 1 :m\n7Returno=xK+1,[Jf(x)]i;:=uT\nifori= 1 :m\nBoth Algorithms 5 and 6 can be adapted to compute JVPs and VJPs against anycollection of\ninput vectors, by accepting fvjgj=1;:::;nandfuigi=1;:::;mas respective inputs. Initializing these\nvectors to the standard basis is useful speciﬁcally for producing the complete Jacobian as output.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1087, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 929}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1088_6d0cf65c", "text": "Initializing these\nvectors to the standard basis is useful speciﬁcally for producing the complete Jacobian as output. 13.3.2 Reverse mode diﬀerentiation for multilayer perceptrons\nIn the previous section, we considered a simple linear-chain feedforward model where each layer does\nnot have any learnable parameters. In this section, each layer can now have (optional) parameters\n\u00121;:::;\u00124. See Figure 13.10 for an illustration. We focus on the case where the mapping has the\nformL:Rn!R, so the output is a scalar. For example, consider `2loss for a MLP with one hidden\nlayer:\nL((x;y);\u0012) =1\n2jjy\u0000W2'(W1x)jj2\n2 (13.28)\nwe can represent this as the following feedforward model:\nL=f4\u000ef3\u000ef2\u000ef1 (13.29)\nx2=f1(x;\u00121) =W1x (13.30)\nx3=f2(x2;;) ='(x2) (13.31)\nx4=f3(x3;\u00123) =W2x3 (13.32)\nL=f4(x4;y) =1\n2jjx4\u0000yjj2(13.33)\nWe use the notation fk(xk;\u0012k)to denote the function at layer k, wherexkis the previous output\nand\u0012kare the optional parameters for this layer.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1088, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 950}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1089_fec18b35", "text": "Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.3. Backpropagation 435\nIn this example, the ﬁnal layer returns a scalar, since it corresponds to a loss function L2R. Therefore it is more eﬃcient to use reverse mode diﬀerentation to compute the gradient vectors. We ﬁrst discuss how to compute the gradient of the scalar output wrt the parameters in each layer. We can easily compute the gradient wrt the predictions in the ﬁnal layer@L\n@x4. For the gradient wrt\nthe parameters in the earlier layers, we can use the chain rule to get\n@L\n@\u00123=@L\n@x4@x4\n@\u00123(13.34)\n@L\n@\u00122=@L\n@x4@x4\n@x3@x3\n@\u00122(13.35)\n@L\n@\u00121=@L\n@x4@x4\n@x3@x3\n@x2@x2\n@\u00121(13.36)\nwhere each@L\n@\u0012k= (r\u0012kL)Tis adk-dimensional gradient row vector, where dkis the number of\nparameters in layer k. We see that these can be computed recursively, by multiplying the gradient\nrow vector at layer kby the Jacobian@xk\n@xk\u00001which is an nk\u0002nk\u00001matrix, where nkis the number\nof hidden units in layer k.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1089, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 978}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1090_88d56431", "text": "We see that these can be computed recursively, by multiplying the gradient\nrow vector at layer kby the Jacobian@xk\n@xk\u00001which is an nk\u0002nk\u00001matrix, where nkis the number\nof hidden units in layer k. See Algorithm 7 for the pseudocode. This algorithm computes the gradient of the loss wrt the parameters at each layer. It also computes\nthe gradient of the loss wrt the input, rxL2Rn, wherenis the dimensionality of the input. This\nlatter quantity is not needed for parameter learning, but can be useful for generating inputs to a\nmodel (see Section 14.6 for some applications). Algorithm 7: Backpropagation for an MLP with Klayers\n1// Forward pass\n2x1:=x\n3fork= 1 :Kdo\n4xk+1=fk(xk;\u0012k)\n5// Backward pass\n6uK+1:= 1\n7fork=K: 1do\n8gk:=uT\nk+1@fk(xk;\u0012k)\n@\u0012k\n9uT\nk:=uT\nk+1@fk(xk;\u0012k)\n@xk\n10// Output\n11ReturnL=xK+1,rxL=u1,fr\u0012kL=gk:k= 1 :Kg\nAll that remains is to specify how to compute the vector Jacobian product (VJP) of all supported\nlayers. The details of this depend on the form of the function at each layer.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1090, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1003}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1091_8e449324", "text": "The details of this depend on the form of the function at each layer. We discuss some examples\nbelow. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n436 Chapter 13. Neural Networks for Structured Data\n13.3.3 Vector-Jacobian product for common layers\nRecall that the Jacobian for a layer of the form f:Rn!Rm. is deﬁned by\nJf(x) =@f(x)\n@x=0\nB@@f1\n@x1\u0001\u0001\u0001@f1\n@xn......... @fm\n@x1\u0001\u0001\u0001@fm\n@xn1\nCA=0\nB@rf1(x)T\n... rfm(x)T1\nCA=\u0010\n@f\n@x1;\u0001\u0001\u0001;@f\n@xn\u0011\n2Rm\u0002n(13.37)\nwhererfi(x)T2Rnis thei’th row (for i= 1 :m) and@f\n@xj2Rmis thej’th column (for j= 1 :n). In this section, we describe how to compute the VJP uTJf(x)for common layers. 13.3.3.1 Cross entropy layer\nConsider a cross-entropy loss layer taking logits xand target labels yas input, and returning a\nscalar:\nz=f(x) =CrossEntropyWithLogits (y;x) =\u0000X\ncyclog(S(x)c) =\u0000X\ncyclogpc (13.38)\nwherep=S(x) =excPC\nc0=1exc0are the predicted class probabilites, and yis the true distribution over\nlabels (often a one-hot vector).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1091, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 973}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1092_61cd6306", "text": "The Jacobian wrt the input is\nJ=@z\n@x= (p\u0000y)T2R1\u0002C(13.39)\nTo see this, assume the target label is class c. We have\nz=f(x) =\u0000log(pc) =\u0000log \nexc\nP\njexj! = log0\n@X\njexj1\nA\u0000xc (13.40)\nHence\n@z\n@xi=@\n@xilogX\njexj\u0000@\n@xixc=exi\nP\njexj\u0000@\n@xixc=pi\u0000I(i=c) (13.41)\nIf we deﬁney= [I(i=c)], we recover Equation (13.39). Note that the Jacobian of this layer is a row\nvector, since the output is a scalar. 13.3.3.2 Elementwise nonlinearity\nConsider a layer that applies an elementwise nonlinearity, z=f(x) ='(x), sozi='(xi). The (i;j)\nelement of the Jacobian is given by\n@zi\n@xj=(\n'0(xi)ifi=j\n0otherwise(13.42)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.3. Backpropagation 437\nwhere'0(a) =d\nda'(a). In other words, the Jacobian wrt the input is\nJ=@f\n@x= diag('0(x)) (13.43)\nFor an arbitrary vector u, we can compute uTJby elementwise multiplication of the diagonal elements\nofJwithu.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1092, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 903}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1093_af62723b", "text": "In other words, the Jacobian wrt the input is\nJ=@f\n@x= diag('0(x)) (13.43)\nFor an arbitrary vector u, we can compute uTJby elementwise multiplication of the diagonal elements\nofJwithu. For example, if\n'(a) = ReLU(a) = max(a;0) (13.44)\nwe have\n'0(a) =(\n0a<0\n1a>0(13.45)\nThe subderivative (Section 8.1.4.1) at a= 0is any value in [0;1]. It is often taken to be 0. Hence\nReLU0(a) =H(a) (13.46)\nwhereHis the Heaviside step function. 13.3.3.3 Linear layer\nNow consider a linear layer, z=f(x;W) =Wx, where W2Rm\u0002n, sox2Rnandz2Rm. We\ncan compute the Jacobian wrt the input vector, J=@z\n@x2Rm\u0002n, as follows. Note that\nzi=nX\nk=1Wikxk (13.47)\nSo the (i;j)entry of the Jacobian will be\n@zi\n@xj=@\n@xjnX\nk=1Wikxk=nX\nk=1Wik@\n@xjxk=Wij (13.48)\nsince@\n@xjxk=I(k=j). Hence the Jacobian wrt the input is\nJ=@z\n@x=W (13.49)\nThe VJP between uT2R1\u0002mandJ2Rm\u0002nis\nuT@z\n@x=uTW2R1\u0002n(13.50)\nNow consider the Jacobian wrt the weight matrix, J=@z\n@W. This can be represented as a m\u0002(m\u0002n)\nmatrix, which is complex to deal with.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1093, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 995}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1094_c5212d86", "text": "This can be represented as a m\u0002(m\u0002n)\nmatrix, which is complex to deal with. So instead, let us focus on taking the gradient wrt a single\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n438 Chapter 13. Neural Networks for Structured Data\nx1 f3\nx2 f4x3f5\nx4f6x5f7x6\nx4x7=o\nFigure 13.11: An example of a computation graph with 2 (scalar) inputs and 1 (scalar) output. From [Blo20]. weight,Wij. This is easier to compute, since@z\n@Wijis a vector. To compute this, note that\nzk=mX\nl=1Wklxl (13.51)\n@zk\n@Wij=mX\nl=1xl@\n@WijWkl=mX\nl=1xlI(i=kandj=l) (13.52)\nHence\n@z\n@Wij=\u00000\u0001\u0001\u0001 0xj0\u0001\u0001\u0001 0\u0001T(13.53)\nwhere the non-zero entry occurs in location i. The VJP between uT2R1\u0002mand@z\n@W2Rm\u0002(m\u0002n)\ncan be represented as a matrix of shape 1\u0002(m\u0002n). Note that\nuT@z\n@Wij=mX\nk=1uk@zk\n@Wij=uixj (13.54)\nTherefore\n\u0014\nuT@z\n@W\u0015\n1;:=uxT2Rm\u0002n(13.55)\n13.3.3.4 Putting it all together\nFor an exercise that puts this all together, see Exercise 13.1.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1094, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 924}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1095_b817eca6", "text": "Note that\nuT@z\n@Wij=mX\nk=1uk@zk\n@Wij=uixj (13.54)\nTherefore\n\u0014\nuT@z\n@W\u0015\n1;:=uxT2Rm\u0002n(13.55)\n13.3.3.4 Putting it all together\nFor an exercise that puts this all together, see Exercise 13.1. 13.3.4 Computation graphs\nMLPs are a simple kind of DNN in which each layer feeds directly into the next, forming a chain\nstructure, as shown in Figure 13.10. However, modern DNNs can combine diﬀerentiable components in\nmuch more complex ways, to create a computation graph , analogous to how programmers combine\nelementary functions to make more complex ones. (Indeed, some have suggested that “deep learning”\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.3. Backpropagation 439\nbe called “ diﬀerentiable programming ”.) The only restriction is that the resulting computation\ngraph corresponds to a directed ayclic graph (DAG), where each node is a diﬀerentiable function\nof all its inputs.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1095, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 912}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1096_62549adb", "text": "For example, consider the function\nf(x1;x2) =x2ex1px1+x2ex1 (13.56)\nWe can compute this using the DAG in Figure 13.11, with the following intermediate functions:\nx3=f3(x1) =ex1(13.57)\nx4=f4(x2;x3) =x2x3 (13.58)\nx5=f5(x1;x4) =x1+x4 (13.59)\nx6=f6(x5) =px5 (13.60)\nx7=f7(x4;x6) =x4x6 (13.61)\nNote that we have numbered the nodes in topological order (parents before children). During the\nbackward pass, since the graph is no longer a chain, we may need to sum gradients along multiple\npaths. For example, since x4inﬂuencesx5andx7, we have\n@o\n@x4=@o\n@x5@x5\n@x4+@o\n@x7@x7\n@x4(13.62)\nWe can avoid repeated computation by working in reverse topological order. For example,\n@o\n@x7=@x7\n@x7=Im (13.63)\n@o\n@x6=@o\n@x7@x7\n@x6(13.64)\n@o\n@x5=@o\n@x6@x6\n@x5(13.65)\n@o\n@x4=@o\n@x5@x5\n@x4+@o\n@x7@x7\n@x4(13.66)\nIn general, we use\n@o\n@xj=X\nk2children(j)@o\n@xk@xk\n@xj(13.67)\nwhere the sum is over all children kof nodej, as shown in Figure 13.12.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1096, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 923}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1097_8bc7279a", "text": "The@o\n@xkgradient vector\nhas already been computed for each child k; this quantity is called the adjoint. This gets multiplied\nby the Jacobian@xk\n@xjof each child. The computation graph can be computed ahead of time, by using an API to deﬁne a static graph . (This is how Tensorﬂow 1 worked.) Alternatively, the graph can be computed “ just in time ”, by\ntracing the execution of the function on an input argument. (This is how Tensorﬂow eager mode\nworks, as well as JAX and PyTorch.) The latter approach makes it easier to work with a dynamic\ngraph, whose shape can change depending on the values computed by the function. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n440 Chapter 13. Neural Networks for Structured Data\nfi fjxifkxj xkparents children\nFigure 13.12: Notation for automatic diﬀerentiation at node jin a computation graph. From [Blo20].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1097, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 865}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1098_3411e9e1", "text": "CC-BY-NC-ND license\n440 Chapter 13. Neural Networks for Structured Data\nfi fjxifkxj xkparents children\nFigure 13.12: Notation for automatic diﬀerentiation at node jin a computation graph. From [Blo20]. Figure 13.13: Computation graph for an MLP with input x, hidden layer h, outputo, loss function L=`(o;y),\nan`2regularizerson the weights, and total loss J=L+s. From Figure 4.7.1 of [Zha+20]. Used with kind\npermission of Aston Zhang. Figure 13.13 shows a computation graph corresponding to an MLP with one hidden layer with\nweight decay. More precisely, the model computes the linear pre-activations z=W(1)x, the\nhidden activations h=\u001e(z), the linear outputs o=W(2)h, the lossL=`(o;y), the regularizer\ns=\u0015\n2(jjW(1)jj2\nF+jjW(2)jj2\nF), and the total loss J=L+s. 13.4 Training neural networks\nIn this section, we discuss how to ﬁt DNNs to data.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1098, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 842}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1099_cb692870", "text": "13.4 Training neural networks\nIn this section, we discuss how to ﬁt DNNs to data. The standard approach is to use maximum\nlikelihood estimation, by minimizing the NLL:\nL(\u0012) =\u0000logp(Dj\u0012) =\u0000NX\nn=1logp(ynjxn;\u0012) (13.68)\nIt is also common to add a regularizer (such as the negative log prior), as we discuss in Section 13.5. In principle we can just use the backprop algorithm (Section 13.3) to compute the gradient of\nthis loss and pass it to an oﬀ-the-shelf optimizer, such as those discussed in Chapter 8. (The Adam\noptimizer of Section 8.4.6.3 is a popular choice, due to its ability to scale to large datasets (by\nvirtue of being an SGD-type algorithm), and to converge fairly quickly (by virtue of using diagonal\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.4. Training neural networks 441\npreconditioning and momentum).) However, in practice this may not work well. In this section,\nwe discuss various problems that may arise, as well as some solutions.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1099, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 988}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1100_9a8a18a7", "text": "Training neural networks 441\npreconditioning and momentum).) However, in practice this may not work well. In this section,\nwe discuss various problems that may arise, as well as some solutions. For more details on the\npracticalities of training DNNs, see various other books, such as [HG20; Zha+20; Gér19]. In addition to practical issues, there are important theoretical issues. In particular, we note that\nthe DNN loss is not a convex objective, so in general we will not be able to ﬁnd the global optimum. Nevertheless, SGD can often ﬁnd suprisingly good solutions. The research into why this is the case is\nstill being conducted; see [Bah+20] for a recent review of some of this work. 13.4.1 Tuning the learning rate\nIt is important to tune the learning rate (step size), to ensure convergence to a good solution. We\ndiscuss this issue in Section 8.4.3.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1100, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 857}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1101_cfba70c3", "text": "13.4.1 Tuning the learning rate\nIt is important to tune the learning rate (step size), to ensure convergence to a good solution. We\ndiscuss this issue in Section 8.4.3. 13.4.2 Vanishing and exploding gradients\nWhen training very deep models, the gradient tends to become either very small (this is called the\nvanishing gradient problem ) or very large (this is called the exploding gradient problem ),\nbecause the error signal is being passed through a series of layers which either amplify or diminish it\n[Hoc+01]. (Similar problems arise in RNNs on long sequences, as we explain in Section 15.2.6.)\nTo explain the problem in more detail, consider the gradient of the loss wrt a node at layer l:\n@L\n@zl=@L\n@zl+1@zl+1\n@zl=Jlgl+1 (13.69)\nwhere Jl=@zl+1\n@zlis the Jacobian matrix, and gl+1=@L\n@zl+1is the gradient at the next layer. If Jlis\nconstant across layers, it is clear that the contribution of the gradient from the ﬁnal layer, gL, to\nlayerlwill be JL\u0000lgL.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1101, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 962}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1102_8880ec86", "text": "If Jlis\nconstant across layers, it is clear that the contribution of the gradient from the ﬁnal layer, gL, to\nlayerlwill be JL\u0000lgL. Thus the behavior of the system depends on the eigenvectors of J. Although Jis a real-valued matrix, it is not (in general) symmetric, so its eigenvalues and\neigenvectors can be complex-valued, with the imaginary components corresponding to oscillatory\nbehavior. Let \u0015be thespectral radius ofJ, which is the maximum of the absolute values of the\neigenvalues. If this is greater than 1, the gradient can explode; if this is less than 1, the gradient can\nvanish.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1102, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 592}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1103_241e688e", "text": "Let \u0015be thespectral radius ofJ, which is the maximum of the absolute values of the\neigenvalues. If this is greater than 1, the gradient can explode; if this is less than 1, the gradient can\nvanish. (Similarly, the spectral radius of W, connectingzltozl+1, determines the stability of the\ndynamical system when run in forwards mode.)\nThe exploding gradient problem can be ameliorated by gradient clipping , in which we cap the\nmagnitude of the gradient if it becomes too large, i.e., we use\ng0= min(1;c\njjgjj)g (13.70)\nThis way, the norm of g0can never exceed c, but the vector is always in the same direction as g. However, the vanishing gradient problem is more diﬃcult to solve. There are various solutions,\nsuch as the following:\n•Modify the the activation functions at each layer to prevent the gradient from becoming too large\nor too small; see Section 13.4.3. •Modify the architecture so that the updates are additive rather than multiplicative; see Sec-\ntion 13.4.4. Author: Kevin P. Murphy.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1103, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 998}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1104_c759c720", "text": "•Modify the architecture so that the updates are additive rather than multiplicative; see Sec-\ntion 13.4.4. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n442 Chapter 13. Neural Networks for Structured Data\nName Deﬁnition Range Reference\nSigmoid \u001b(a) =1\n1+e\u0000a [0;1]\nHyperbolic tangent tanh(a) = 2\u001b(2a)\u00001 [\u00001;1]\nSoftplus \u001b+(a) = log(1 + ea) [0 ;1][GBB11]\nRectiﬁed linear unit ReLU(a) = max(a;0) [0 ;1][GBB11; KSH12]\nLeaky ReLU max(a;0) +\u000bmin(a;0) [\u00001;1][MHN13]\nExponential linear unit max(a;0) + min(\u000b(ea\u00001);0) [\u00001;1][CUH16]\nSwish a\u001b(a) [ \u00001;1][RZL17]\nGELU a\b(a) [ \u00001;1][HG16]\nTable 13.4: List of some popular activation functions for neural networks. 4\n 3\n 2\n 1\n 0 1 2 3 40.5\n0.00.51.01.52.0Activation function\nsigmoid\nleaky-relu\nelu\nswish\ngelu\n(a)\n4\n 3\n 2\n 1\n 0 1 2 3 40.50\n0.25\n0.000.250.500.751.001.251.50Gradient of activation function\nsigmoid\nleaky-relu\nelu\nswish\ngelu (b)\nFigure 13.14: (a) Some popular activation functions. (b) Plot of their gradients.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1104, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 972}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1105_1e1d31ae", "text": "(b) Plot of their gradients. Generated by code at\nﬁgures.probml.ai/book1/13.14. •Modify the architecture to standardize the activations at each layer, so that the distribution of\nactivations over the dataset remains constant during training; see Section 14.2.4.1. •Carefully choose the initial values of the parameters; see Section 13.4.5. 13.4.3 Non-saturating activation functions\nIn Section 13.2.3, we mentioned that the sigmoid activation function saturates at 0 for large negative\ninputs, and at 1 for large positive inputs. It turns out that the gradient signal in these regimes is 0,\npreventing backpropagation from working. To see why the gradient vanishes, consider a layer which computes z=\u001b(Wx), where\n'(a) =\u001b(a) =1\n1 + exp(\u0000a)(13.71)\nIf the weights are initialized to be large (positive or negative), then it becomes very easy for a=Wx\nto take on large values, and hence for zto saturate near 0or1, since the sigmoid saturates, as shown\nin Figure 13.14a.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1105, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 966}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1106_6623464a", "text": "Now let us consider the gradient of the loss wrt the inputs x(from an earlier layer)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.4. Training neural networks 443\nand the parameters W. The derivative of the activation function is given by\n'0(a) =\u001b(a)(1\u0000\u001b(a)) (13.72)\nSee Figure 13.14b for a plot. In Section 13.3.3, we show that the gradient of the loss wrt the inputs\nis\n@L\n@x=WT\u000e=WTz(1\u0000z) (13.73)\nand the gradient of the loss wrt the parameters is\n@L\n@W=\u000exT=z(1\u0000z)xT(13.74)\nHence, ifzis near 0 or 1, the gradients will go to 0. One of the keys to being able to train very deep models is to use non-saturating activation\nfunctions . Several diﬀerent functions have been proposed: see Table 13.4 for a summary, and\nhttps://mlfromscratch.com/activation-functions-explained for more details. 13.4.3.1 ReLU\nThe most common is rectiﬁed linear unit orReLU, proposed in [GBB11; KSH12].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1106, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 912}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1107_bb2226da", "text": "13.4.3.1 ReLU\nThe most common is rectiﬁed linear unit orReLU, proposed in [GBB11; KSH12]. This is deﬁned\nas\nReLU(a) = max(a;0) =aI(a>0) (13.75)\nTheReLUfunction simply “turns oﬀ” negative inputs, and passes positive inputs unchanged. The\ngradient has the following form:\nReLU0(a) =I(a>0) (13.76)\nNow suppose we use this in a layer to compute z=ReLU (Wx). In Section 13.3.3, we show that the\ngradient wrt the inputs has the form\n@L\n@x=WTI(z>0) (13.77)\nand wrt the parameters has the form\n@L\n@W=I(z>0)xT(13.78)\nHence the gradient will not vanish, as long a zis positive. Unfortunately, if the weights are initialized to be large and negative, then it becomes very easy for\n(some components of) a=Wxto take on large negative values, and hence for zto go to 0. This\nwill cause the gradient for the weights to go to 0. The algorithm will never be able to escape this\nsituation, so the hidden units (components of z) will stay permanently oﬀ. This is called the “ dead\nReLU” problem [Lu+19]. Author: Kevin P. Murphy.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1107, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1009}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1108_df3350da", "text": "The algorithm will never be able to escape this\nsituation, so the hidden units (components of z) will stay permanently oﬀ. This is called the “ dead\nReLU” problem [Lu+19]. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n444 Chapter 13. Neural Networks for Structured Data\n13.4.3.2 Non-saturating ReLU\nThe problem of dead ReLU’s can be solved by using non-saturating variants of ReLU. One alternate\nis theleaky ReLU , proposed in [MHN13]. This is deﬁned as\nLReLU(a;\u000b) = max(\u000ba;a ) (13.79)\nwhere 0<\u000b< 1. The slope of this function is 1 for positive inputs, and \u000bfor negative inputs, thus\nensuring there is some signal passed back to earlier layers, even when the input is negative. See\nFigure 13.14b for a plot. If we allow the parameter \u000bto be learned, rather than ﬁxed, the leaky\nReLUis calledparametric ReLU [He+15]. Another popular choice is the ELU, proposed in [CUH16]. This is deﬁned by\nELU(a;\u000b) =(\n\u000b(ea\u00001)ifa\u00140\na ifa>0(13.80)\nThis has the advantage over leaky ReLUof being a smooth function.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1108, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1009}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1109_f72b44c0", "text": "Another popular choice is the ELU, proposed in [CUH16]. This is deﬁned by\nELU(a;\u000b) =(\n\u000b(ea\u00001)ifa\u00140\na ifa>0(13.80)\nThis has the advantage over leaky ReLUof being a smooth function. See Figure 13.14 for plot. A slight variant of ELU, known as SELU(self-normalizing ELU), was proposed in [Kla+17]. This\nhas the form\nSELU(a;\u000b;\u0015) =\u0015ELU(a;\u000b) (13.81)\nSurprisingly, they prove that by setting \u000band\u0015to carefully chosen values, this activation function\nis guaranteed to ensure that the output of each layer is standardized (provided the input is also\nstandardized), even without the use of techniques such as batchnorm (Section 14.2.4.1). This can\nhelp with model ﬁtting. 13.4.3.3 Other choices\nAs an alternative to manually discovering good activation functions, we can use blackbox optimization\nmethods to search over the space of functional forms. Such an approach was used in [RZL17], where\nthey discovered a function they call swishthat seems to do well on some image classiﬁcation\nbenchmarks.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1109, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 988}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1110_b38e1e3f", "text": "Such an approach was used in [RZL17], where\nthey discovered a function they call swishthat seems to do well on some image classiﬁcation\nbenchmarks. It is deﬁned by\nswish(a;\f) =a\u001b(\fa) (13.82)\n(The same function, under the name SiLU(for Sigmoid Linear Unit), was independently proposed\nin [HG16].) See Figure 13.14 for plot. Another popular activation function is GELU, which stands for “Gaussian Error Linear Unit”\n[HG16]. This is deﬁned as follows:\nGELU(a) =a\b(a) (13.83)\nwhere \b(a)is the cdf of a standard normal:\n\b(a) = Pr(N(0;1)\u0014a) =1\n2\u0010\n1 +erf(a=p\n2)\u0011\n(13.84)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.4. Training neural networks 445\nidentityweight layer\nweight layerrelu\nreluF(x)\u0001+\u0001xx\nF(x)x\n(a)\nX Layer blockingbackpropagation =Layer not LearningX=X+++ResidualUnits (b)\nFigure 13.15: (a) Illustration of a residual block. (b) Illustration of why adding residual connections can help\nwhen training a very deep model. Adapted from Figure 14.16 of [Gér19].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1110, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 995}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1111_7b32b557", "text": "(b) Illustration of why adding residual connections can help\nwhen training a very deep model. Adapted from Figure 14.16 of [Gér19]. We see from Figure 13.14 that this is not a convex or monontonic function, unlike most other\nactivation functions. We can think of GELU as a “soft” version of ReLU, since it replaces the step function I(a>0)\nwith the Gaussian cdf, \b(a). Alternatively, the GELU can be motivated as an adaptive version of\ndropout (Section 13.5.4), where we multiply the input by a binary scalar mask, m\u0018Ber(\b(a)),\nwhere the probability of being dropped is given by 1\u0000\b(a). Thus the expected output is\nE[a] = \b(a)\u0002a+ (1\u0000\b(a))\u00020 =a\b(a) (13.85)\nWe can approximate GELU using swish with a particular parameter setting, namely\nGELU(a)\u0019a\u001b(1:702a) (13.86)\n13.4.4 Residual connections\nOne solution to the vanishing gradient problem for DNNs is to use a residual network orResNet\n[He+16a].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1111, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 894}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1112_41cba2fe", "text": "This is a feedforward model in which each layer has the form of a residual block , deﬁned\nby\nF0\nl(x) =Fl(x) +x (13.87)\nwhereFlis a standard shallow nonlinear mapping (e.g., linear-activation-linear). The inner Fl\nfunction computes the residual term or delta that needs to be added to the input xto generate\nthe desired output; it is often easier to learn to generate a small perturbation to the input than to\ndirectly predict the output. (Residual connections are usually used in conjunction with CNNs, as\ndiscussed in Section 14.3.4, but can also be used in MLPs.)\nA model with residual connections has the same number of parameters as a model without residual\nconnections, but it is easier to train. The reason is that gradients can ﬂow directly from the output\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n446 Chapter 13. Neural Networks for Structured Data\nto earlier layers, as sketched in Figure 13.15b.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1112, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 924}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1113_3dc8831d", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n446 Chapter 13. Neural Networks for Structured Data\nto earlier layers, as sketched in Figure 13.15b. To see this, note that the activations at the output\nlayer can be derived in terms of any previous layer lusing\nzL=zl+L\u00001X\ni=lFi(zi;\u0012i): (13.88)\nWe can therefore compute the gradient of the loss wrt the parameters of the l’th layer as follows:\n@L\n@\u0012l=@zl\n@\u0012l@L\n@zl(13.89)\n=@zl\n@\u0012l@L\n@zL@zL\n@zl(13.90)\n=@zl\n@\u0012l@L\n@zL \n1 +L\u00001X\ni=l@Fi(zi;\u0012i)\n@zl! (13.91)\n=@zl\n@\u0012l@L\n@zL+ otherterms (13.92)\nThus we see that the gradient at layer ldepends directly on the gradient at layer Lin a way that is\nindependent of the depth of the network. 13.4.5 Parameter initialization\nSince the objective function for DNN training is non-convex, the way that we initialize the parameters\nof a DNN can play a big role on what kind of solution we end up with, as well as how easy the\nfunction is to train (i.e., how well information can ﬂow forwards and backwards through the model).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1113, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1000}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1114_1f99856b", "text": "In the rest of this section, we present some common heuristic methods that are used for initializing\nparameters. 13.4.5.1 Heuristic initialization schemes\nIn [GB10], they show that sampling parameters from a standard normal with ﬁxed variance can\nresult in exploding activations or gradients. To see why, consider a linear unit with no activation\nfunction given by oi=Pnin\nj=1wijxj; supposewij\u0018N(0;\u001b2), andE[xj]= 0andV[xj]=\r2, where\nwe assume xjare independent of wij. The mean and variance of the output is given by\nE[oi] =ninX\nj=1E[wijxj] =ninX\nj=1E[wij]E[xj] = 0 (13.93)\nV[oi] =E\u0002\no2\ni\u0003\n\u0000(E[oi])2=ninX\nj=1E\u0002\nw2\nijx2\nj\u0003\n\u00000 =ninX\nj=1E\u0002\nw2\nij\u0003\nE\u0002\nx2\nj\u0003\n=nin\u001b2\r2(13.94)\nTo keep the output variance from blowing up, we need to ensure nin\u001b2= 1(or some other constant),\nwhereninis thefan-inof a unit (number of incoming connections). Now consider the backwards pass.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1114, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 862}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1115_5607c9ff", "text": "Now consider the backwards pass. By analogous reasoning, we see that the variance of the gradients\ncan blow up unless nout\u001b2= 1, wherenoutis thefan-out of a unit (number of outgoing connections). Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.4. Training neural networks 447\nTo satisfy both requirements at once, we set1\n2(nin+nout)\u001b2= 1, or equivalently\n\u001b2=2\nnin+nout(13.95)\nThis is known as Xavier initialization orGlorot initialization , named after the ﬁrst author of\n[GB10]. A special case arises if\nthis is known as LeCun initialization , named after Yann LeCun, who proposed it in the 1990s. This is equivalent to Glorot initialization when nin=nout. If we use\u001b2= 2=nin, the method is called\nHe initialization , named after Ximing He, who proposed it in [He+15]. Note that it is not necessary to use a Gaussian distribution. Indeed, the above derivation just\nworked in terms of the ﬁrst two moments (mean and variance), and made no assumptions about\nGaussianity.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1115, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1001}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1116_45baf423", "text": "Note that it is not necessary to use a Gaussian distribution. Indeed, the above derivation just\nworked in terms of the ﬁrst two moments (mean and variance), and made no assumptions about\nGaussianity. For example, suppose we sample weights from a uniform distribution, wij\u0018Unif(\u0000a;a). The mean is 0, and the variance is \u001b2=a2=3. Hence we should set a=q\n6\nnin+nout. Although the above derivation assumes a linear output unit, the technique works well empirically\neven for nonlinear units. The best choice of initialization method depends on which activation\nfunction you use. For linear, tanh, logistic, and softmax, Glorot is recommended. For ReLUand\nvariants, He is recommended. For SELU, LeCun is recommended. See [Gér19] for more heuristics. 13.4.5.2 Data-driven initializations\nWe can also adopt a data-driven approach to parameter initialization. For example, [MM16] proposed\na simple but eﬀective scheme known as layer-sequential unit-variance (LSUV) initialization,\nwhich works as follows.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1116, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 995}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1117_ee309d86", "text": "For example, [MM16] proposed\na simple but eﬀective scheme known as layer-sequential unit-variance (LSUV) initialization,\nwhich works as follows. First we initialize the weights of each (fully connected or convolutional)\nlayer with orthonormal matrices, as proposed in [SMG14]. (This can be achieved by drawing from\nw\u0018N(0;I), reshaping to wto a matrix W, and then computing an orthonormal basis using QR or\nSVD decomposition.) Then, for each layer l, we compute the variance vlof the activations across a\nminibatch; we then rescale using Wl:=Wl=pvl. This scheme can be viewed as an orthonormal\ninitialization combined with batch normalization performed only on the ﬁrst mini-batch. This is\nfaster than full batch normalization, but can sometimes work just as well. 13.4.6 Parallel training\nIt can be quite slow to train large models on large datasets.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1117, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 850}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1118_1917bcdc", "text": "This is\nfaster than full batch normalization, but can sometimes work just as well. 13.4.6 Parallel training\nIt can be quite slow to train large models on large datasets. One way to speed this process up is to\nuse specialized hardware, such as graphics processing units (GPUs), which are very eﬃcient at\nperforming matrix-matrix multiplication. If we have multiple GPUs, we can sometimes further speed\nthings up. There are two main approaches: model parallelism , in which we partition the model\nbetween machines, and data parallelism , in which each machine has its own copy of the model,\nand applies it to a diﬀerent set of data. Model parallelism can be quite complicated, since it requires tight communication between machines\nto ensure they compute the correct answer. We will not discuss this further. Data parallelism is\ngenerally much simpler, since it is embarassingly parallel .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1118, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 887}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1119_ca144a59", "text": "We will not discuss this further. Data parallelism is\ngenerally much simpler, since it is embarassingly parallel . To use this to speed up training, at\neach training step t, we do the following: 1) we partition the minibatch across the Kmachines to\ngetDk\nt; 2) each machine kcomputes its own gradient, gk\nt=r\u0012L(\u0012;Dk\nt); 3) we collect all the local\ngradients on a central machine (e.g., device 0) and sum them using gt=PK\nk=1gk\nt; 4) we broadcast\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n448 Chapter 13. Neural Networks for Structured Data\nFigure 13.16: Calculation of minibatch stochastic gradient using data parallelism and two GPUs. From Figure\n12.5.2 of [Zha+20]. Used with kind permission of Aston Zhang. the summed gradient back to all devices, so ~gk\nt=gt; 5) each machine updates its own copy of the\nparameters using \u0012k\nt:=\u0012k\nt\u0000\u001at~gk\nt. See Figure 13.16 for an illustration.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1119, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 900}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1120_87662ec1", "text": "the summed gradient back to all devices, so ~gk\nt=gt; 5) each machine updates its own copy of the\nparameters using \u0012k\nt:=\u0012k\nt\u0000\u001at~gk\nt. See Figure 13.16 for an illustration. Note that steps 3 and 4 are usually combined into one atomic step; this is known as an all-reduce\noperation (where we use sum to reduce the set of (gradient) vectors into one). If each machine\nblocks until receiving the centrally aggregated gradient, gt, the method is known as synchronous\ntraining . This will give the same results as training with one machine (with a larger batchsize),\nonly faster (assuming we ignore any batch normalization layers). If we let each machine update\nits parameters using its own local gradient estimate, and not wait for the broadcast to/from the\nother machines, the method is called asynchronous training .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1120, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 814}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1121_a5968ebb", "text": "If we let each machine update\nits parameters using its own local gradient estimate, and not wait for the broadcast to/from the\nother machines, the method is called asynchronous training . This is not guaranteed to work,\nsince the diﬀerent machines may get out of step, and hence will be updating diﬀerent versions of the\nparameters; this approach has therefore been called hogwild training [Niu+11]. However, if the\nupdates are sparse, so each machine “touches” a diﬀerent part of the parameter vector, one can prove\nthat hogwild training behaves like standard synchronous SGD. 13.5 Regularization\nIn Section 13.4 we discussed computational issues associated with training (large) neural networks. In this section, we discuss statistical issues. In particular, we focus on ways to avoid overﬁtting. This\nis crucial, since large neural networks can easily have millions of parameters.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1121, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 883}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1122_745977fb", "text": "In this section, we discuss statistical issues. In particular, we focus on ways to avoid overﬁtting. This\nis crucial, since large neural networks can easily have millions of parameters. 13.5.1 Early stopping\nPerhaps the simplest way to prevent overﬁtting is called early stopping , which refers to the\nheuristic of stopping the training procedure when the error on the validation set starts to increase\n(see Figure 4.8 for an example). This method works because we are restricting the ability of the\noptimization algorithm to transfer information from the training examples to the parameters, as\nexplained in [AS19]. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.5. Regularization 449\n(a)\n4\n 2\n 0 2 44\n2\n024Deep Neural Net\nData (b)\nFigure 13.17: (a) A deep but sparse neural network. The connections are pruned using `1regularization. At\neach level, nodes numbered 0 are clamped to 1, so their outgoing weights correspond to the oﬀset/bias terms.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1122, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 979}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1123_8b7e1540", "text": "The connections are pruned using `1regularization. At\neach level, nodes numbered 0 are clamped to 1, so their outgoing weights correspond to the oﬀset/bias terms. (b) Predictions made by the model on the training set. Generated by code at ﬁgures.probml.ai/book1/13.17. 13.5.2 Weight decay\nA common approach to reduce overﬁtting is to impose a prior on the parameters, and then use\nMAP estimation. It is standard to use a Gaussian prior for the weights N(wj0;\u000b2I)and biases,\nN(bj0;\f2I). This is equivalent to `2regularization of the objective. In the neural networks literature,\nthis is called weight decay , since it encourages small weights, and hence simpler models, as in ridge\nregression (Section 11.3). 13.5.3 Sparse DNNs\nSince there are many weights in a neural network, it is often helpful to encourage sparsity. This\nallows us to perform model compression , which can save memory and time.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1123, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 897}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1124_6fed3ca6", "text": "13.5.3 Sparse DNNs\nSince there are many weights in a neural network, it is often helpful to encourage sparsity. This\nallows us to perform model compression , which can save memory and time. To do this, we can\nuse`1regularization (as in Section 11.4), or ARD (as in Section 11.7.7), or several other methods. (see [Hoe+21] for a recent review.)\nAs a simple example, Figure 13.17 shows a 5 layer MLP which has been ﬁt to some 1d regression\ndata using an `1regularizer on the weights. We see that the resulting graph topology is sparse. Of\ncourse, many other approaches to sparse estimation are possible\nDespite the intuitive appeal of sparse topology, in practice these methods are not widely used,\nsince modern GPUs are optimized for densematrix multiplication, and there are few computational\nbeneﬁts to sparse weight matrices. However, if we use methods that encourage groupsparsity, we can\nprune out whole layers of the model.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1124, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 928}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1125_6e6e6431", "text": "However, if we use methods that encourage groupsparsity, we can\nprune out whole layers of the model. This results in block sparse weight matrices, which can result in\nspeedups and memory savings (see e.g., [Sca+17; Wen+16; MAV17; LUW17]). 13.5.4 Dropout\nSuppose that we randomly (on a per-example basis) turn oﬀ all the outgoing connections from\neach neuron with probability p, as illustrated in Figure 13.18. This technique is known as dropout\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n450 Chapter 13. Neural Networks for Structured Data\n(a)\n (b)\nFigure 13.18: Illustration of dropout. (a) A standard neural net with 2 hidden layers. (b) An example of a\nthinned net produced by applying dropout with p0= 0:5. Units that have been dropped out are marked with an\nx. From Figure 1 of [Sri+14]. Used with kind permission of Geoﬀ Hinton. [Sri+14]. Dropout can dramatically reduce overﬁtting and is very widely used.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1125, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 929}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1126_714b713e", "text": "Units that have been dropped out are marked with an\nx. From Figure 1 of [Sri+14]. Used with kind permission of Geoﬀ Hinton. [Sri+14]. Dropout can dramatically reduce overﬁtting and is very widely used. Intuitively, the reason dropout\nworks well is that it prevents complex co-adaptation of the hidden units. In other words, each unit\nmust learn to perform well even if some of the other units are missing at random. This prevents the\nunits from learning complex, but fragile, dependencies on each other.5A more formal explanation,\nin terms of Gaussian scale mixture priors, can be found in [NHLS19]. We can view dropout as estimating a noisy version of the weights, \u0012lij=wlij\u000fli, where\u000fli\u0018\nBer(1\u0000p)is a Bernoulli noise term. (So if we sample \u000fli= 0, then all of the weights going out of\nunitiin layerl\u00001into anyjin layerlwill be set to 0.) At test time, we usually turn the noise oﬀ.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1126, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 883}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1127_8b8b9222", "text": "(So if we sample \u000fli= 0, then all of the weights going out of\nunitiin layerl\u00001into anyjin layerlwill be set to 0.) At test time, we usually turn the noise oﬀ. To ensure the weights have the same expectation at test time as they did during training (so the\ninput activation to the neurons is the same, on average), at test time we should use wlij=\u0012lijE[\u000fli]. For Bernoulli noise, we have E[\u000f]= 1\u0000p, so we should multiply the weights by the keep probability,\n1\u0000p, before making predictions. We can, however, use dropout at test time if we wish. The result is an ensemble of networks,\neach with slightly diﬀerent sparse graph structures. This is called Monte Carlo dropout [GG16;\nKG17], and has the form\np(yjx;D)\u00191\nSSX\ns=1p(yjx;^W\u000fs+^b) (13.96)\nwhereSis the number of samples, and we write ^W\u000fsto indicate that we are multiplying all\nthe estimated weight matrices by a sampled noise vector. This can sometimes provide a good\n5.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1127, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 924}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1128_c81f2fb1", "text": "This can sometimes provide a good\n5. Geoﬀ Hinton, who invented dropout, said he was inspired by a talk on sexual reproduction, which encourages genes\nto be individually useful (or at most depend on a small number of other genes), even when combined with random\nother genes. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.5. Regularization 451\nFigure 13.19: Flat vs sharp minima. From Figures 1 and 2 of [HS97a]. Used with kind permission of Jürgen\nSchmidhuber. approximation to the Bayesian posterior predictive distribution p(yjx;D), especially if the noise rate\nis optimized [GHK17]. 13.5.5 Bayesian neural networks\nModern DNNs are usually trained using a (penalized) maximum likelihood objective to ﬁnd a single\nsetting of parameters. However, with large models, there are often many more parameters than data\npoints, so there may be multiple possible models which ﬁt the training data equally well, yet which\ngeneralize in diﬀerent ways.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1128, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 973}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1129_0b2c13f0", "text": "It is often useful to capture the induced uncertainty in the posterior\npredictive distribution. This can be done by marginalizing out the parameters by computing\np(yjx;D) =Z\np(yjx;\u0012)p(\u0012jD)d\u0012 (13.97)\nThe result is known as a Bayesian neural network orBNN. It can be thought of as an inﬁnite\nensemble of diﬀerently weight neural networks. By marginalizing out the parameters, we can avoid\noverﬁtting [Mac95]. Bayesian marginalization is challenging for large neural networks, but also can\nlead to signiﬁcant performance gains [WI20]. For more details on the topic of Bayesian deep\nlearning , see the sequel to this book, [Mur22]. 13.5.6 Regularization eﬀects of (stochastic) gradient descent *\nSome optimization methods (in particular, second-order batch methods) are able to ﬁnd “needles\nin haystacks”, corresponding to narrow but deep “holes” in the loss landscape, corresponding to\nparameter settings with very low loss. These are known as sharp minima , see Figure 13.19(right).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1129, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 980}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1130_570d501b", "text": "These are known as sharp minima , see Figure 13.19(right). From the point of view of minimizing the empirical loss, the optimizer has done a good job. However,\nsuch solutions generally correspond to a model that has overﬁt the data. It is better to ﬁnd points\nthat correspond to ﬂat minima , as shown in Figure 13.19(left); such solutions are more robust and\ngeneralize better. To see why, note that ﬂat minima correspond to regions in parameter space where\nthere is a lot of posterior uncertainty, and hence samples from this region are less able to precisely\nmemorize irrelevant details about the training set [AS17]. SGD often ﬁnds such ﬂat minima by\nvirtue of the addition of noise, which prevents it from “entering” narrow regions of the loss landscape\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n452 Chapter 13.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1130, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 833}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1131_dc69c6af", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n452 Chapter 13. Neural Networks for Structured Data\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0051015202530Low gradient variance\n(a)\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0051015202530High gradient variance (b)\nFigure 13.20: Each curve shows how the loss varies across parameter values for a given minibatch. (a) A\nstable local minimum. (b) An unstable local minimum. Generated by code at ﬁgures.probml.ai/book1/13.20. Adapted from https: // bit. ly/ 3wTc1L6 . (see e.g., [SL18]). This is called implicit regularization . It is also possible to explicitly encourage\nSGD to ﬁnd such ﬂat minima, using entropy SGD [Cha+17], sharpness aware minimization\n[For+21], and other related techniques. Of course, the loss landscape depends not just on the parameter values, but also on the data. Since\nwe usually cannot aﬀord to do full-batch gradient descent, we will get a set of loss curves, one per\nminibatch.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1131, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 945}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1132_9426e4df", "text": "Since\nwe usually cannot aﬀord to do full-batch gradient descent, we will get a set of loss curves, one per\nminibatch. If each one of these curves corresponds to a wide basin, as shown in Figure 13.20a, we\nare at a point in parameter space that is robust to perturbations, and will likely generalize well. However, if the overall wide basin is the result of averaging over many diﬀerent narrow basins, as\nshown in Figure 13.20b, the resulting estimate will likely generalize less well. This can be formalized using the analysis in [Smi+21; BD21]. Speciﬁcally, they consider continuous\ntime gradient ﬂow which approximates the behavior of (S)GD. In [BD21], they consider full-batch\nGD, and show that the ﬂow has the form _w=\u0000rw~LGD(w), where\n~LGD(w) =L(w) +\u000f\n4jjrL(w)jj2(13.98)\nwhereL(w)is the original loss, \u000fis the learning rate, and the second term is an implicit regularization\nterm that penalizes solutions with large gradients (high curvature). In [Smi+21], they extend this analysis to the SGD case.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1132, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1004}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1133_846f1ecf", "text": "In [Smi+21], they extend this analysis to the SGD case. They show that the ﬂow has the form\n_w=\u0000rw~LSGD(w), where\n~LSGD(w) =L(w) +\u000f\n4mX\nk=1jjrLk(w)jj2(13.99)\nwheremis the number of minibatches, and Lk(w)is the loss on the k’th such minibatch. Comparing\nthis to the full-batch GD loss, we see\n~LSGD(w) =~LGD(w) +\u000f\n4mX\nk=1jjrLk(w)\u0000L(w)jj2(13.100)\nThe second term estimates the variance of the minibatch gradients, which is a measure of stability,\nand hence of generalization ability. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.6. Other kinds of feedforward networks * 453\nThe above analysis shows that SGD not only has computational advantages (since it is faster than\nfull-batch GD or second-order methods), but also statistical advantages.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1133, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 775}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1134_1cd8a591", "text": "13.6 Other kinds of feedforward networks *\n13.6.1 Radial basis function networks\nConsider a 1 layer neural net where the hidden layer is given by the feature vector\n\u001e(x) = [K(x;\u00161);:::;K(x;\u0016K)] (13.101)\nwhere\u0016k2Xare a set of Kcentroids orexemplars , andK(x;\u0016)\u00150is akernel function . We describe kernel functions in detail in Section 17.1. Here we just give an example, namely the\nGaussian kernel\nKgauss(x;c),exp\u0012\n\u00001\n2\u001b2jjc\u0000xjj2\n2\u0013\n(13.102)\nThe parameter \u001bis known as the bandwidth of the kernel. Note that this kernel is shift invariant,\nmeaning it is only a function of the distance r=jjx\u0000cjj2, so we can equivalently write this as\nKgauss(r),exp\u0012\n\u00001\n2\u001b2r2\u0013\n(13.103)\nThis is therefore called a radial basis function kernel orRBF kernel . A 1 layer neural net in which we use Equation (13.101) as the hidden layer, with RBF kernels, is\ncalled anRBF network [BL88]. This has the form\np(yjx;\u0012) =p(yjwT\u001e(x)) (13.104)\nwhere\u0012= (\u0016;w).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1134, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 927}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1135_0311b0ed", "text": "A 1 layer neural net in which we use Equation (13.101) as the hidden layer, with RBF kernels, is\ncalled anRBF network [BL88]. This has the form\np(yjx;\u0012) =p(yjwT\u001e(x)) (13.104)\nwhere\u0012= (\u0016;w). If the centroids \u0016are ﬁxed, we can solve for the optimal weights wusing\n(regularized) least squares, as discussed in Chapter 11. If the centroids are unknown, we can estimate\nthem by using an unsupervised clustering method, such as K-means (Section 21.3). Alternatively, we\ncan associate one centroid per data point in the training set, to get \u0016n=xn, where now K=N. This is an example of a non-parametric model , since the number of parameters grows (in this case\nlinearly) with the amount of data, and is not independent of N. IfK=N, the model can perfectly\ninterpolate the data, and hence may overﬁt.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1135, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 792}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1136_46b2f016", "text": "IfK=N, the model can perfectly\ninterpolate the data, and hence may overﬁt. However, by ensuring that the output weight vector\nwis sparse, the model will only use a ﬁnite subset of the input examples; this is called a sparse\nkernel machine , and will be discussed in more detail in Section 17.4.1 and Section 17.3. Another\nway to avoid overﬁtting is to adopt a Bayesian approach, by integrating out the weights w; this gives\nrise to a model called a Gaussian process , which will be discussed in more detail in Section 17.2. 13.6.1.1 RBF network for regression\nWe can use RBF networks for regression by deﬁning p(yjx;\u0012) =N(wT\u001e(x);\u001b2). For example,\nFigure 13.22 shows a 1d data set ﬁt with K= 10uniformly spaced RBF prototypes, but with the\nbandwidth ranging from small to large. Small values lead to very wiggly functions, since the predicted\nfunction value will only be non-zero for points xthat are close to one of the prototypes \u0016k.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1136, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 934}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1137_2d556b0b", "text": "Small values lead to very wiggly functions, since the predicted\nfunction value will only be non-zero for points xthat are close to one of the prototypes \u0016k. If the\nbandwidth is very large, the design matrix reduces to a constant matrix of 1’s, since each point is\nequally close to every prototype; hence the corresponding function is just a straight line. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n454 Chapter 13. Neural Networks for Structured Data\n(a)\npoly10 (b)\nrbf prototypes (c)\nFigure 13.21: (a) xor truth table. (b) Fitting a linear logistic regression classiﬁer using degree 10 polynomial\nexpansion. (c) Same model, but using an RBF kernel with centroids speciﬁed by the 4 black crosses. Generated\nby code at ﬁgures.probml.ai/book1/13.21. 13.6.1.2 RBF network for classiﬁcation\nWe can use RBF networks for binary classiﬁcation by deﬁning p(yjx;\u0012) = Ber(\u001b(wT\u001e(x))). As an\nexample, consider the data coming from the exclusive or function.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1137, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 962}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1138_194bbf1a", "text": "13.6.1.2 RBF network for classiﬁcation\nWe can use RBF networks for binary classiﬁcation by deﬁning p(yjx;\u0012) = Ber(\u001b(wT\u001e(x))). As an\nexample, consider the data coming from the exclusive or function. This is a binary-valued function\nof two binary inputs. Its truth table is shown in Figure 13.21(a). In Figure 13.1(b), we have shown\nsome data labeled by the xor function, but we have jittered the points to make the picture clearer.6\nWe see we cannot separate the data even using a degree 10 polynomial. However, using an RBF\nkernel and just 4 prototypes easily solves the problem as shown in Figure 13.1(c). 13.6.2 Mixtures of experts\nWhen considering regression problems, it is common to assume a unimodal output distribution, such\nas a Gaussian or Student distribution, where the mean and variance is some function of the input,\ni.e.,\np(yjx) =N(yjf\u0016(x);diag(\u001b+(f\u001b(x)))) (13.105)\nwhere theffunctions may be MLPs (possibly with some shared hidden units, as in Figure 13.5).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1138, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 972}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1139_8cf4442f", "text": "However, this will not work well for one-to-many functions , in which each input can have multiple\npossible outputs. Figure 13.23a gives a simple example of such a function. We see that in the middle of the plot there\nare certain xvalues for which there are two equally probable yvalues. There are many real world\nproblems of this form, e.g., 3d pose prediction of a person from a single image [Bo+08], colorization\nof a black and white image [Gua+17], predicting future frames of a video sequence [VT17], etc. Any\nmodel which is trained to maximize likelihood using a unimodal output density — even if the model\nis a ﬂexible nonlinear model, such as neural network — will work poorly on one-to-many functions\nsuch as these, since it will just produce a blurry average output. 6. Jittering is a common visualization trick in statistics, wherein points in a plot/display that would otherwise land on\ntop of each other are dispersed with uniform additive noise.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1139, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 959}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1140_c05bb947", "text": "6. Jittering is a common visualization trick in statistics, wherein points in a plot/display that would otherwise land on\ntop of each other are dispersed with uniform additive noise. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.6. Other kinds of feedforward networks * 455\n0 5 10 15 2010\n5\n05101520\n0 5 10 15 200.00.20.40.60.81.0\n2 4 6 82015105\n0 5 10 15 2010\n5\n05101520\n0 5 10 15 200.20.40.60.81.0\n2 4 6 82015105\n0 5 10 15 2010\n5\n05101520\n0 5 10 15 200.920.930.940.950.960.970.980.991.00\n2 4 6 82015105\nFigure 13.22: Linear regression using 10 equally spaced RBF basis functions in 1d. Left column: ﬁtted function. Middle column: basis functions evaluated on a grid. Right column: design matrix. Top to bottom we show dif-\nferent bandwidths for the kernel function: \u001b= 0:5;10;50. Generated by code at ﬁgures.probml.ai/book1/13.22. To prevent this problem of regression to the mean, we can use a conditional mixture model .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1140, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 957}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1141_5de4ff3d", "text": "Generated by code at ﬁgures.probml.ai/book1/13.22. To prevent this problem of regression to the mean, we can use a conditional mixture model . That is, we assume the output is a weighted mixture of Kdiﬀerent outputs, corresponding to diﬀerent\nmodes of the output distribution for each input x. In the Gaussian case, this becomes\np(yjx) =KX\nk=1p(yjx;z=k)p(z=kjx) (13.106)\np(yjx;z=k) =N(yjf\u0016;k(x);diag(f\u001b;k(x))) (13.107)\np(z=kjx) = Cat(zjS(fz(x))) (13.108)\nHeref\u0016;kpredicts the mean of the k’th Gaussian, f\u001b;kpredicts its variance terms, and fzpredicts\nwhich mixture component to use. This model is called a mixture of experts (MoE) [Jac+91;\nJJ94; YWG12; ME14]. The idea is that the k’th submodel p(yjx;z=k)is considered to be an\n“expert” in a certain region of input space. The function p(z=kjx)is called a gating function ,\nand decides which expert to use, depending on the input values. By picking the most likely expert\nfor a given input x, we can “activate” just a subset of the model.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1141, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 988}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1142_773a4df1", "text": "By picking the most likely expert\nfor a given input x, we can “activate” just a subset of the model. This is an example of conditional\ncomputation , since we decide what expert to run based on the results of earlier computations from\nthe gating network [Sha+17]. We can train this model using SGD, or using the EM algorithm (see Section 8.7.3 for details on\nthe latter method). Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n456 Chapter 13. Neural Networks for Structured Data\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Inverse problem\n(a)\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Gating functions (b)\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Expert-predictions\n(c)\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0prediction\nmode\nmean (d)\nFigure 13.23: (a) Some data from a one-to-many function. (b) The responsibilities of each expert for the\ninput domain. (c) Prediction of each expert. (d) Overeall prediction. Mean is red cross, mode is black square. Adapted from Figures 5.20 and 5.21 of [Bis06].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1142, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 1005}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1143_e7406174", "text": "(c) Prediction of each expert. (d) Overeall prediction. Mean is red cross, mode is black square. Adapted from Figures 5.20 and 5.21 of [Bis06]. Generated by code at ﬁgures.probml.ai/book1/13.23. 13.6.2.1 Mixture of linear experts\nIn this section, we consider a simple example in which we use linear regression experts and a linear\nclassiﬁcation gating function, i.e., the model has the form:\np(yjx;z=k;\u0012) =N(yjwT\nkx;\u001b2\nk) (13.109)\np(zjx;\u0012) = Cat(zjS(Vx)) (13.110)\nThe individual weighting term p(z=kjx)is called the responsibility for expertkfor inputx. In\nFigure 13.23b, we see how the gating networks softly partitions the input space amongst the K= 3\nexperts. Each expert p(yjx;z=k)corresponds to a linear regression model with diﬀerent parameters. These are shown in Figure 13.23c. If we take a weighted combination of the experts as our output, we get the red curve in Figure 13.23a,\nwhich is clearly is a bad predictor.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1143, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 925}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1144_f3742192", "text": "These are shown in Figure 13.23c. If we take a weighted combination of the experts as our output, we get the red curve in Figure 13.23a,\nwhich is clearly is a bad predictor. If instead we only predict using the most active expert (i.e., the\none with the highest responsibility), we get the discontinuous black curve, which is a much better\npredictor. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.7. Exercises 457\nFigure 13.24: Deep MOE with mexperts, represented as a neural network. From Figure 1 of [CGG17]. Used\nwith kind permission of Jacob Goldberger. 13.6.2.2 Mixture density networks\nThe gating function and experts can be any kind of conditional probabilistic model, not just a linear\nmodel. If we make them both DNNs, then resulting model is called a mixture density network\n(MDN) [Bis94; ZS14] or a deep mixture of experts [CGG17]. See Figure 13.24 for a sketch of\nthe model.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1144, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 919}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1145_e383e875", "text": "If we make them both DNNs, then resulting model is called a mixture density network\n(MDN) [Bis94; ZS14] or a deep mixture of experts [CGG17]. See Figure 13.24 for a sketch of\nthe model. 13.6.2.3 Hierarchical MOEs\nIf each expert is itself an MoE model, the resulting model is called a hierarchical mixture of\nexperts [JJ94]. See Figure 13.25 for an illustration of such a model with a two level hierarchy. An HME with Llevels can be thought of as a “soft” decision tree of depth L, where each example\nis passed through every branch of the tree, and the ﬁnal prediction is a weighted average. (We discuss\ndecision trees in Section 18.1.)\n13.7 Exercises\nExercise 13.1 [Backpropagation for a MLP]\n(Based on an exercise by Kevin Clark.)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n458 Chapter 13.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1145, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 807}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1146_7256a36a", "text": "Murphy. (C) MIT Press. CC-BY-NC-ND license\n458 Chapter 13. Neural Networks for Structured Data\nExpertNetworkGating NetworkExpertNetworkExpertNetworkExpertNetworkGating NetworkGating Networkxxxxxxxy\nFigure 13.25: A 2-level hierarchical mixture of experts as a neural network. The top gating network chooses\nbetween the left and right expert, shown by the large boxes; the left and right experts themselves choose between\ntheir left and right sub-experts. Consider the following classiﬁcation MLP with one hidden layer:\nx=input2RD(13.111)\nz=Wx+b12RK(13.112)\nh= ReLU(z)2RK(13.113)\na=Vh+b22RC(13.114)\nL=CrossEntropy (y;S(a))2R (13.115)\nwherex2RD,b12RK,W2RK\u0002D,b22RC,V2RC\u0002K, whereDis the size of the input, Kis the\nnumber of hidden units, and Cis the number of classes.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1146, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 763}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1147_7d546b2b", "text": "Show that the gradients for the parameters and\ninput are as follows:\nrVL=\u0014@L\n@V\u0015\n1;:=u2hT2RC\u0002K(13.116)\nrb2L=\u0012@L\n@b2\u0013T\n=u22RC(13.117)\nrWL=\u0014@L\n@W\u0015\n1;:=u1xT2RK\u0002D(13.118)\nrb1L=\u0012@L\n@b1\u0013T\n=u12RK(13.119)\nrxL=\u0012@L\n@x\u0013T\n=WTu12RD(13.120)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n13.7. Exercises 459\nwhere the gradients of the loss wrt the two layers (logit and hidden) are given by the following:\nu2=raL=\u0012@L\n@a\u0013T\n= (p\u0000y)2RC(13.121)\nu1=rzL=\u0012@L\n@z\u0013T\n= (VTu2)\fH(z)2RK(13.122)\nwithHis the Heaviside function. Note that, in our notation, the gradient (which has the same shape as the\nvariable with respect to which we diﬀerentiate) is equal to the Jacobian’s transpose when the variable is a\nvector and to the ﬁrst slice of the Jacobian when the variable is a matrix. Author: Kevin P. Murphy. (C) MIT Press.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1147, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 826}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1148_80a42c05", "text": "Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n\n14 Neural Networks for Images\n14.1 Introduction\nIn Chapter 13, we discussed multilayered perceptrons (MLPs) as a way to learn functions mapping\n“unstructured” input vectors x2RDto outputs. In this chapter, we extend this to the case where the\ninputxhas 2d spatial structure. (Similar ideas apply to 1d temporal structure, or 3d spatio-temporal\nstructure.)\nTo see why it is not a good idea to apply MLPs directly to image data, recall that the core\noperation in an MLP at each hidden layer is computing the activations z='(Wx), wherexis the\ninput to a layer, Ware the weights, and '()is the nonlinear activation function. Thus the j’th\nelement of the hidden layer has value zj='(wT\njx).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1148, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 746}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1149_2c1c71ef", "text": "Thus the j’th\nelement of the hidden layer has value zj='(wT\njx). We can think of this inner product operation\nas comparing the input xto a learned template or pattern wj; if the match is good (large positive\ninner product), the activation of that unit will be large (assuming a ReLUnonlinearity), signalling\nthat thej’th pattern is present in the input. However, this does not work well if the input is a variable-sized image, x2RWHC, whereWis the\nwidth,His the height, and Cis the number of input channels (e.g.,C= 3for RGB color). The\nproblem is that we would need to learn a diﬀerent-sized weight matrix Wfor every size of input\nimage. In addition, even if the input was ﬁxed size, the number of parameters needed would be\nprohibitive for reasonably sized images, since the weight matrix would have size (W\u0002H\u0002C)\u0002D,\nwhereDis the number of outputs (hidden units).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1149, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 864}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1150_eb4f7ccf", "text": "The ﬁnal problem is that a pattern that occurs\nin one location may not be recognized when it occurs in a diﬀerent location — that is, the model\nmay not exhibit translation invariance — because the weights are not shared across locations (see\nFigure 14.1). To solve these problems, we will use convolutional neural networks (CNNs), in which we\nreplace matrix multiplication with a convolution operation. We explain this in detail in Section 14.2,\nbut the basic idea is to divide the input into overlapping 2d image patches , and to compare each\npatch with a set of small weight matrices, or ﬁlters, which represent parts of an object; this is\nillustrated in Figure 14.2. We can think of this as a form of template matching . We will learn\nthese templates from data, as we explain below. Because the templates are small (often just 3x3 or\n5x5), the number of parameters is signiﬁcantly reduced.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1150, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 892}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1151_26447d96", "text": "We will learn\nthese templates from data, as we explain below. Because the templates are small (often just 3x3 or\n5x5), the number of parameters is signiﬁcantly reduced. And because we use convolution to do the\ntemplate matching, instead of matrix multiplication, the model will be translationally invariant. This\nis useful for tasks such as image classiﬁcation, where the goal is to classify if an object is present,\nregardless of its location. CNNs have many other applications besides image classiﬁcation, as we will discuss later in this\nchapter. They can also be applied to 1d inputs (see Section 15.3) and 3d inputs; however, we mostly\n462 Chapter 14.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1151, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 656}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1152_b759b4f6", "text": "They can also be applied to 1d inputs (see Section 15.3) and 3d inputs; however, we mostly\n462 Chapter 14. Neural Networks for Images\n0111\n0010\n00000010\n5 0111\n0010\n00000010\n1OUTPUT\nWEIGHTS WEIGHTS0\n1\n0\n1\n1\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n00\n1\n0\n1\n1\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0X + +0\n0\n1\n0\n0\n1\n1\n1\n0\n0\n1\n0\n0\n0\n0\n00\n1\n0\n1\n1\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0X\nFigure 14.1: Detecting patterns in 2d images using unstructured MLPs does not work well, because the method\nis not translation invariant. We can design a weight vector to act as a matched ﬁlter for detecting the\ndesired cross-shape. This will give a strong response of 5 if the object is on the left, but a weak response of 1\nif the object is shifted over to the right. Adapted from Figure 7.16 of [SAV20]. Figure 14.2: We can classify a digit by looking for certain discriminative features (image templates) occuring\nin the correct (relative) locations. From Figure 5.1 of [Cho17]. Used with kind permission of Francois Chollet. focus on the 2d case in this chapter.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1152, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 995}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1153_03d6acb8", "text": "From Figure 5.1 of [Cho17]. Used with kind permission of Francois Chollet. focus on the 2d case in this chapter. 14.2 Common layers\nIn this section, we discuss the basics of CNNs. 14.2.1 Convolutional layers\nWe start by describing the basics of convolution in 1d, and then in 2d, and then describe how they\nare used as a key component of CNNs. 14.2.1.1 Convolution in 1d\nTheconvolution between two functions, say f;g:RD!R, is deﬁned as\n[f~g](z) =Z\nRDf(u)g(z\u0000u)du (14.1)\nNow suppose we replace the functions with ﬁnite-length vectors, which we can think of as functions\ndeﬁned on a ﬁnite set of points. For example, suppose fis evaluated at the points f\u0000L;\u0000L+\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n14.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1153, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 740}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1154_21e1f9b5", "text": "For example, suppose fis evaluated at the points f\u0000L;\u0000L+\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n14.2. Common layers 463\n- - 1 2 3 4 - -\n7 6 5 - - - - - z0=x0w0= 5\n- 7 6 5 - - - - z1=x0w1+x1w0= 16\n- - 7 6 5 - - - z2=x0w2+x1w1+x2w0= 34\n- - - 7 6 5 - - z3=x1w2+x2w1+x3w0= 52\n- - - - 7 6 5 - z4=x2w2+x3w1= 45\n- - - - - 7 6 5 z5=x3w2= 28\nFigure 14.3: Discrete convolution of x= [1;2;3;4]withw= [5;6;7]to yieldz= [5;16;34;52;45;28]. We\nsee that this operation consists of “ﬂipping” wand then “dragging” it over x, multiplying elementwise, and\nadding up the results. as\nFigure 14.4: 1d cross correlation. From Figure 15.3.2 of [Zha+20]. Used with kind permission of Aston\nZhang. 1;:::; 0;1;:::;Lgto yield the weight vector (also called a ﬁlterorkernel)w\u0000L=f(\u0000L)up to\nwL=f(L). Now letgbe evaluated at points f\u0000N;:::;Ngto yield the feature vector x\u0000N=g(\u0000N)\nup toxN=g(N).", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1154, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 898}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1155_bfca849d", "text": "1;:::; 0;1;:::;Lgto yield the weight vector (also called a ﬁlterorkernel)w\u0000L=f(\u0000L)up to\nwL=f(L). Now letgbe evaluated at points f\u0000N;:::;Ngto yield the feature vector x\u0000N=g(\u0000N)\nup toxN=g(N). Then the above equation becomes\n[w~x](i) =w\u0000Lxi+L+\u0001\u0001\u0001+w\u00001xi+1+w0xi+w1xi\u00001+\u0001\u0001\u0001+wLxi\u0000L (14.2)\n(We discuss boundary conditions (edge eﬀects) later on.) We see that we “ﬂip” the weight vector w\n(since indices of ware reversed), and then “drag” it over the xvector, summing up the local windows\nat each point, as illustrated in Figure 14.3. There is a very closely related operation, in which we do not ﬂip wﬁrst:\n[w\u0003x](i) =w\u0000Lxi\u0000L+\u0001\u0001\u0001+w\u00001xi\u00001+w0xi+w1xi+1+\u0001\u0001\u0001+wLxi+L (14.3)\nThis is called cross correlation ; If the weight vector is symmetric, as is often the case, then cross\ncorrelation and convolution are the same. In the deep learning literature, the term “convolution” is\nusually used to mean cross correlation; we will follow this convention.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1155, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 934}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1156_561f01bb", "text": "In the deep learning literature, the term “convolution” is\nusually used to mean cross correlation; we will follow this convention. We can also evaluate the weights won domainf0;1;:::;L\u00001gand the features xon domain\nf0;1;:::;N\u00001g, to eliminate negative indices. Then the above equation becomes\n[w~x](i) =L\u00001X\nu=0wuxi+u (14.4)\nSee Figure 14.4 for an example. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n464 Chapter 14. Neural Networks for Images\nFigure 14.5: Illustration of 2d cross correlation. Generated by code at ﬁgures.probml.ai/book1/14.5. Adapted\nfrom Figure 6.2.1 of [Zha+20]. Figure 14.6: Convolving a 2d image (left) with a 3\u00023ﬁlter (middle) produces a 2d response map (right). The bright spots of the response map correspond to locations in the image which contain diagonal lines sloping\ndown and to the right. From Figure 5.3 of [Cho17]. Used with kind permission of Francois Chollet.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1156, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 911}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1157_8f7235c2", "text": "From Figure 5.3 of [Cho17]. Used with kind permission of Francois Chollet. 14.2.1.2 Convolution in 2d\nIn 2d, Equation (14.4) becomes\n[W~X](i;j) =H\u00001X\nu=0W\u00001X\nv=0wu;vxi+u;j+v (14.5)\nwhere the 2d ﬁlter Whas sizeH\u0002W. For example, consider convolving a 3\u00023input Xwith a\n2\u00022kernel Wto compute a 2\u00022output Y:\nY=\u0012w1w2\nw3w4\u0013\n~0\n@x1x2x3\nx4x5x6\nx7x8x91\nA (14.6)\n=\u0012(w1x1+w2x2+w3x4+x4x5) (w1x2+w2x3+w3x5+x4x6)\n(w1x4+w2x5+w3x7+x4x8) (w1x5+w2x6+w3x8+x4x9)\u0013\n(14.7)\nDraft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n14.2. Common layers 465\nSee Figure 14.5 for a visualization of this process. We can think of 2d convolution as template matching , since the output at a point (i;j)will\nbe large if the corresponding image patch centered on (i;j)is similar to W. If the template W\ncorresponds to an oriented edge, then convolving with it will cause the output heat map to “light up”\nin regions that contain edges that match that orientation, as shown in Figure 14.6.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1157, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 974}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1158_92c1f039", "text": "More generally,\nwe can think of convolution as a form of feature detection . The resulting output Y=W~Xis\ntherefore called a feature map . 14.2.1.3 Convolution as matrix-vector multiplication\nSince convolution is a linear operator, we can represent it by matrix multiplication. For example,\nconsider Equation (14.7). We can rewrite this as matrix-vector mutiplication by ﬂattening the 2d\nmatrix Xinto a 1d vector x, and multiplying by a Toeplitz-like matrix Cderived from the kernel\nW, as follows:\ny=Cx=0\nBB@w1w20w3w40 0 0 0\n0w1w20w3w40 0 0\n0 0 0 w1w20w3w40\n0 0 0 0w1w20w3w41\nCCA0\nBBBBBBBBBBBB@x1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx91\nCCCCCCCCCCCCA(14.8)\n=0\nBB@w1x1+w2x2+w3x4+w4x5\nw1x2+w2x3+w3x5+w4x6\nw1x4+w2x5+w3x7+w4x8\nw1x5+w2x6+w3x8+w4x91\nCCA(14.9)\nWe can recover the 2\u00022output by reshaping the 4\u00021vectoryback to Y.1\nThus we see that CNNs are like MLPs where the weight matrices have a special sparse structure,\nand the elements are tied across spatial locations.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1158, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 953}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1159_e88cc64f", "text": "This implements the idea of translation invariance,\nand massively reduces the number of parameters compared to a weight matrix in a standard fully\nconnected or dense layer, as used in MLPs. 14.2.1.4 Boundary conditions and padding\nIn Equation (14.7), we saw that convolving a 3\u00023image with a 2\u00022ﬁlter resulted in a 2\u00022\noutput. In general, convolving a fh\u0002fwﬁlter over an image of size xh\u0002xwproduces an output of\nsize(xh\u0000fh+ 1)\u0002(xw\u0000fw+ 1); this is called valid convolution , since we only apply the ﬁlter to\n“valid” parts of the input, i.e., we don’t let it “slide oﬀ the ends”. If we want the output to have the\nsame size as the input, we can use zero-padding , which means we add a border of 0s to the image,\nas illustrated in Figure 14.7. This is called same convolution . 1. See code.probml.ai/book1/conv2d_torch for a demo. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n466 Chapter 14.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1159, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 903}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1160_efdcc86f", "text": "This is called same convolution . 1. See code.probml.ai/book1/conv2d_torch for a demo. Author: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n466 Chapter 14. Neural Networks for Images\n00111\n01010\n0000\n0000010\n0111\n00010\n00111\n010001001111\n00010\n000000010\n01010\n00111\n0001000000111\n0010\n0010\n0000010\n0111\n0010\n0111\n01000100111\n0010\n00000010\n0010\n0111\n001000101110\n0101\n0010\n0000010\n0111\n0100\n1110\n01000101111\n0100\n00000100\n0101\n1110\n0100001011110\n01011\n0010\n0000010\n0111\n01010\n11100\n010001011111\n01010\n000001010\n01011\n11110\n0100000102\n2\n2\n00\n1\n0\n02\n2\n2\n02\n5\n2\n1Zeros\noutside\nOUTPUT\nFigure 14.7: Same-convolution (using zero-padding) ensures the output is the same size as the input. Adapted\nfrom Figure 8.3 of [SAV20]. ijffhw= 3= 3Zero Padding\n(a)\nSw= 2Sh= 2 (b)\nFigure 14.8: Illustration of padding and strides in 2d convolution. (a) We apply “same convolution” to a\n5\u00027input (with zero padding) using a 3\u00023ﬁlter to create a 5\u00027output. (b) Now we use a stride of 2, so\nthe output has size 3\u00024.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1160, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 999}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1161_5dc85ffd", "text": "(a) We apply “same convolution” to a\n5\u00027input (with zero padding) using a 3\u00023ﬁlter to create a 5\u00027output. (b) Now we use a stride of 2, so\nthe output has size 3\u00024. Adapted from Figures 14.3–14.4 of [Gér19]. In general, if the input has size xh\u0002xw, we use a kernel of size fh\u0002fw, we use zero padding on\neach side of size phandpw, then the output has the following size [DV16]:\n(xh+ 2ph\u0000fh+ 1)\u0002(xw+ 2pw\u0000fw+ 1) (14.10)\nFor example, consider Figure 14.8a. We have p= 1,f= 3,xh= 5andxw= 7, so the output has\nsize\n(5 + 2\u00003 + 1)\u0002(7 + 2\u00003 + 1) = 5\u00027 (14.11)\nIf we set 2p=f\u00001, then the output will have the same size as the input. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n14.2. Common layers 467\nFigure 14.9: Illustration of 2d convolution applied to an input with 2 channels. Generated by code at\nﬁgures.probml.ai/book1/14.9. Adapted from Figure 6.4.1 of [Zha+20].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1161, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 891}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1162_a5d9cc01", "text": "Common layers 467\nFigure 14.9: Illustration of 2d convolution applied to an input with 2 channels. Generated by code at\nﬁgures.probml.ai/book1/14.9. Adapted from Figure 6.4.1 of [Zha+20]. 14.2.1.5 Strided convolution\nSince each output pixel is generated by a weighted combination of inputs in its receptive ﬁeld\n(based on the size of the ﬁlter), neighboring outputs will be very similar in value, since their inputs\nare overlapping. We can reduce this redundancy (and speedup computation) by skipping every s’th\ninput. This is called strided convolution . This is illustrated in Figure 14.8b, where we convolve a\n5\u00027image with a 3\u00023ﬁlter with stride 2 to get a 3\u00024output. In general, if the input has size xh\u0002xw, we use a kernel of size fh\u0002fw, we use zero padding on\neach side of size phandpw, and we use strides of size shandsw, then the output has the following\nsize [DV16]:\n\u0016xh+ 2ph\u0000fh+sh\nsh\u0017\n\u0002\u0016xw+ 2pw\u0000fw+sw\nsw\u0017\n(14.12)\nFor example, consider Figure 14.8b, where we set the stride to s= 2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1162, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 992}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1163_4b1089e6", "text": "Now the output is smaller than\nthe input. \u0012\nb5 + 2\u00003 + 2\n2c;b7 + 2\u00003 + 2\n2c\u0013\n=\u0012\nb6\n2c;b4\n1c\u0013\n= 3\u00024 (14.13)\n14.2.1.6 Multiple input and output channels\nIn Figure 14.6, the input was a gray-scale image. In general, the input will have multiple channels\n(e.g., RGB, or hyper-spectral bands for satellite images). We can extend the deﬁnition of convolution\nto this case by deﬁning a kernel for each input channel; thus now Wis a 3d weight matrix or tensor. We compute the output by convolving channel cof the input with kernel W:;:;c, and then summing\nover channels:\nzi;j=b+H\u00001X\nu=0W\u00001X\nv=0C\u00001X\nc=0xsi+u;sj+v;cwu;v;c (14.14)\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n468 Chapter 14. Neural Networks for Images\nFeature map 1map 2Map 2FeatureMap 1Covolutional Layer 2Covolutional Layer 1Input Layer Filters\nChannelsRedBlueGreen\nFigure 14.10: Illustration of a CNN with 2 convolutional layers. The input has 3 color channels. The feature\nmaps at internal layers have multiple channels.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1163, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 996}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1164_5eb12957", "text": "The input has 3 color channels. The feature\nmaps at internal layers have multiple channels. The cylinders correspond to hypercolumns, which are feature\nvectors at a certain location. Adapted from Figure 14.6 of [Gér19]. wheresis the stride (which we assume is the same for both height and width, for simplicity), and b\nis the bias term. This is illustrated in Figure 14.9. Each weight matrix can detect a single kind of feature. We typically want to detect multiple kinds\nof features, as illustrated in Figure 14.2. We can do this by making Winto a 4d weight matrix. The\nﬁlter to detect feature type din input channel cis stored in W:;:;c;d. We extend the deﬁnition of\nconvolution to this case as follows:\nzi;j;d=bd+H\u00001X\nu=0W\u00001X\nv=0C\u00001X\nc=0xsi+u;sj+v;cwu;v;c;d (14.15)\nThis is illustrated in Figure 14.10. Each vertical cylindrical column denotes the set of output features\nat a given location, zi;j;1:D; this is sometimes called a hypercolumn .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1164, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 945}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1165_f987d871", "text": "Each vertical cylindrical column denotes the set of output features\nat a given location, zi;j;1:D; this is sometimes called a hypercolumn . Each element is a diﬀerent\nweighted combination of the Cfeatures in the receptive ﬁeld of each of the feature maps in the layer\nbelow.2\n14.2.1.7 1\u00021(pointwise) convolution\nSometimes we just want to take a weighted combination of the features at a given location, rather\nthan across locations. This can be done using 1x1 convolution , also called pointwise convolution . 2. In Tensorﬂow, a ﬁlter for 2d CNNs has shape (H;W;C;D ), and a minibatch of feature maps has shape (batch-size,\nimage-height, image-width, image-channels); this is called NHWC format. Other systems use diﬀerent data layouts. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021\n14.2. Common layers 469\nFigure 14.11: Mapping 3 channels to 2 using convolution with a ﬁlter of size 1\u00021\u00023\u00022. Adapted from\nFigure 6.4.2 of [Zha+20].", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1165, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 961}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1166_982962b9", "text": "August 27, 2021\n14.2. Common layers 469\nFigure 14.11: Mapping 3 channels to 2 using convolution with a ﬁlter of size 1\u00021\u00023\u00022. Adapted from\nFigure 6.4.2 of [Zha+20]. Figure 14.12: Illustration of maxpooling with a 2x2 ﬁlter and a stride of 1. Adapted from Figure 6.5.1 of\n[Zha+20]. This changes the number of channels from CtoD, without changing the spatial dimensionality:\nzi;j;d=bd+C\u00001X\nc=0xi;j;cw0;0;c;d (14.16)\nThis can be thought of as a single layer MLP applied to each feature column in parallel. 14.2.2 Pooling layers\nConvolutionwillpreserveinformationaboutthelocationofinputfeatures(moduloreducedresolution),\na property known as equivariance . In some case we want to be invariant to the location. For\nexample, when performing image classiﬁcation, we may just want to know if an object of interest\n(e.g., a face) is present anywhere in the image. One simple way to achieve this is called max pooling , which just computes the maximum over\nits incoming values, as illustrated in Figure 14.12.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1166, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 999}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1167_fd2daf2e", "text": "One simple way to achieve this is called max pooling , which just computes the maximum over\nits incoming values, as illustrated in Figure 14.12. An alternative is to use average pooling , which\nreplaces the max by the mean. In either case, the output neuron has the same response no matter\nAuthor: Kevin P. Murphy. (C) MIT Press. CC-BY-NC-ND license\n470 Chapter 14. Neural Networks for Images\nCAR\nTRUCK\nVAN\nBICYCLE\nFLATTEN POOLING CONVOLUTION + RELU POOLING CONVOLUTION + RELU INPUT SOFTMAXFULLY\nCONNECTED\nCLASSIFICATION FEATURE LEARNING\nFigure 14.13: A simple CNN for classifying images. Adapted from https: // blog. floydhub. com/\nbuilding-your-first-convnet/ . where the input pattern occurs within its receptive ﬁeld. (Note that we apply pooling to each feature\nchannel independently.)\nIf we average over all the locations in a feature map, the method is called global average pooling .", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1167, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 890}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1168_16a86577", "text": "(Note that we apply pooling to each feature\nchannel independently.)\nIf we average over all the locations in a feature map, the method is called global average pooling . Thus we can convert a H\u0002W\u0002Dfeature map into a 1\u00021\u0002Ddimensional feature map; this can\nbe reshaped to a D-dimensional vector, which can be passed into a fully connected layer to map it\nto aC-dimensional vector before passing into a softmax output. The use of global average pooling\nmeans we can apply the classiﬁer to an image of any size, since the ﬁnal feature map will always be\nconverted to a ﬁxed D-dimensional vector before being mapped to a distribution over the Cclasses. 14.2.3 Putting it all together\nA common design pattern is to create a CNN by alternating convolutional layers with max pooling\nlayers, followed by a ﬁnal linear classiﬁcation layer at the end. This is illustrated in Figure 14.13.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1168, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 876}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1169_a022211d", "text": "This is illustrated in Figure 14.13. (We omit normalization layers in this example, since the model is quite shallow.) This design pattern\nﬁrst appeared in Fukushima’s neocognitron [Fuk75], and was inspired by Hubel and Wiesel’s model\nof simple and complex cells in the human visual cortex [HW62]. In 1998 Yann LeCun used a similar\ndesign in his eponynous LeNetmodel [LeC+98], which used backpropagation and SGD to esitmate\nthe parameters. This design pattern continues to be popular in neurally-inspired models of visual\nobject recognition [RP99], as well as various practical applications (see Section 14.3 and Section 14.5). 14.2.4 Normalization layers\nThe basic design in Figure 14.13 works well for shallow CNNs, but it can be diﬃcult to scale it to\ndeeper models, due to problems with vanishing or exploding gradients, as explained in Section 13.4.2.", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1169, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 856}}
{"id": "computer_science_murphy_probabilistic_ml_chunk_1170_70ceeecf", "text": "A common solution to this problem is to add extra layers to the model, to standardize the statistics\nof the hidden units (i.e., to ensure they are zero mean and unit variance), just like we do to the\ninputs of many models. We discuss various kinds of normalization layers below. Draft of “Probabilistic Machine Learning: An Introduction”. August 27, 2021", "metadata": {"book_id": "computer_science_murphy_probabilistic_ml", "book_title": "Murphy_Probabilistic_ML", "category": "computer_science", "chunk_index": 1170, "total_chunks": 1171, "source_file": "Murphy_Probabilistic_ML.pdf", "file_type": "pdf", "char_count": 354}}
