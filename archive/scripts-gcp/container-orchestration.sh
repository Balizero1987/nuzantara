#!/bin/bash
# Advanced Container Orchestration Script for Zantara Bridge
# Handles multi-environment deployment, scaling, and monitoring

set -euo pipefail

# Configuration
PROJECT_ID="${PROJECT_ID:-involuted-box-469105-r0}"
REGION="${REGION:-asia-southeast2}"
CLUSTER_NAME="${CLUSTER_NAME:-zantara-cluster}"
NAMESPACE="${NAMESPACE:-zantara-system}"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

# Function to create GKE cluster if it doesn't exist
create_cluster() {
    log_info "Checking if GKE cluster exists..."
    
    if gcloud container clusters describe $CLUSTER_NAME --region=$REGION >/dev/null 2>&1; then
        log_info "Cluster $CLUSTER_NAME already exists"
        return 0
    fi
    
    log_info "Creating GKE cluster: $CLUSTER_NAME"
    
    gcloud container clusters create $CLUSTER_NAME \\\n        --region=$REGION \\\n        --machine-type=n2-standard-4 \\\n        --num-nodes=1 \\\n        --min-nodes=1 \\\n        --max-nodes=10 \\\n        --enable-autoscaling \\\n        --enable-autorepair \\\n        --enable-autoupgrade \\\n        --enable-network-policy \\\n        --enable-ip-alias \\\n        --network=zantara-vpc \\\n        --subnetwork=zantara-subnet-${REGION} \\\n        --cluster-secondary-range-name=pods \\\n        --services-secondary-range-name=services \\\n        --enable-shielded-nodes \\\n        --shielded-secure-boot \\\n        --shielded-integrity-monitoring \\\n        --enable-workload-identity \\\n        --workload-pool=${PROJECT_ID}.svc.id.goog \\\n        --addons=HorizontalPodAutoscaling,HttpLoadBalancing,NetworkPolicy,NodeLocalDNS \\\n        --release-channel=stable \\\n        --disk-type=pd-ssd \\\n        --disk-size=50 \\\n        --preemptible\n    \n    log_success \"GKE cluster created: $CLUSTER_NAME\"\n}\n\n# Function to configure kubectl\nconfigure_kubectl() {\n    log_info \"Configuring kubectl...\"\n    \n    gcloud container clusters get-credentials $CLUSTER_NAME \\\n        --region=$REGION \\\n        --project=$PROJECT_ID\n    \n    # Create namespace if it doesn't exist\n    kubectl create namespace $NAMESPACE --dry-run=client -o yaml | kubectl apply -f -\n    \n    log_success \"kubectl configured\"\n}\n\n# Function to setup Workload Identity\nsetup_workload_identity() {\n    log_info \"Setting up Workload Identity...\"\n    \n    # Bind Kubernetes service account to Google service account\n    gcloud iam service-accounts add-iam-policy-binding \\\n        zantara-runtime@${PROJECT_ID}.iam.gserviceaccount.com \\\n        --role=roles/iam.workloadIdentityUser \\\n        --member=\"serviceAccount:${PROJECT_ID}.svc.id.goog[${NAMESPACE}/zantara-bridge-sa]\"\n    \n    # Annotate Kubernetes service account\n    kubectl annotate serviceaccount zantara-bridge-sa \\\n        -n $NAMESPACE \\\n        iam.gke.io/gcp-service-account=zantara-runtime@${PROJECT_ID}.iam.gserviceaccount.com \\\n        --overwrite\n    \n    log_success \"Workload Identity configured\"\n}\n\n# Function to deploy monitoring stack\ndeploy_monitoring() {\n    log_info \"Deploying monitoring stack...\"\n    \n    # Add Prometheus Helm repo\n    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n    helm repo add grafana https://grafana.github.io/helm-charts\n    helm repo update\n    \n    # Install Prometheus\n    helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \\\n        --namespace monitoring \\\n        --create-namespace \\\n        --set grafana.enabled=true \\\n        --set grafana.adminPassword=admin123 \\\n        --set prometheus.prometheusSpec.retention=15d \\\n        --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=50Gi\n    \n    # Install Jaeger for distributed tracing\n    kubectl apply -f https://github.com/jaegertracing/jaeger-operator/releases/download/v1.42.0/jaeger-operator.yaml\n    \n    # Create Jaeger instance\n    cat <<EOF | kubectl apply -f -\napiVersion: jaegertracing.io/v1\nkind: Jaeger\nmetadata:\n  name: jaeger\n  namespace: monitoring\nspec:\n  strategy: production\n  storage:\n    type: elasticsearch\n    elasticsearch:\n      nodeCount: 3\n      resources:\n        requests:\n          memory: \"2Gi\"\n          cpu: \"1\"\n        limits:\n          memory: \"2Gi\"\nEOF\n    \n    log_success \"Monitoring stack deployed\"\n}\n\n# Function to deploy service mesh (Istio)\ndeploy_service_mesh() {\n    log_info \"Deploying Istio service mesh...\"\n    \n    # Download and install Istio\n    curl -L https://istio.io/downloadIstio | sh -\n    cd istio-*\n    export PATH=$PWD/bin:$PATH\n    \n    # Install Istio\n    istioctl install --set values.defaultRevision=default -y\n    \n    # Enable sidecar injection for our namespace\n    kubectl label namespace $NAMESPACE istio-injection=enabled --overwrite\n    \n    # Deploy Istio addons\n    kubectl apply -f samples/addons/\n    \n    cd ..\n    rm -rf istio-*\n    \n    log_success \"Istio service mesh deployed\"\n}\n\n# Function to setup ingress controller\nsetup_ingress() {\n    log_info \"Setting up ingress controller...\"\n    \n    # Install NGINX Ingress Controller\n    helm upgrade --install ingress-nginx ingress-nginx \\\n        --repo https://kubernetes.github.io/ingress-nginx \\\n        --namespace ingress-nginx \\\n        --create-namespace \\\n        --set controller.service.type=LoadBalancer \\\n        --set controller.service.externalTrafficPolicy=Local \\\n        --set controller.metrics.enabled=true \\\n        --set controller.podAnnotations.\"prometheus\\.io/scrape\"=true \\\n        --set controller.podAnnotations.\"prometheus\\.io/port\"=10254\n    \n    # Wait for load balancer IP\n    log_info \"Waiting for load balancer IP...\"\n    kubectl wait --namespace ingress-nginx \\\n        --for=condition=ready pod \\\n        --selector=app.kubernetes.io/component=controller \\\n        --timeout=120s\n    \n    # Create ingress for Zantara Bridge\n    cat <<EOF | kubectl apply -f -\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: zantara-bridge-ingress\n  namespace: $NAMESPACE\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/rate-limit: \"100\"\n    nginx.ingress.kubernetes.io/rate-limit-window: \"1m\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\nspec:\n  tls:\n  - hosts:\n    - api.zantara.com\n    secretName: zantara-bridge-tls\n  rules:\n  - host: api.zantara.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: zantara-bridge-service\n            port:\n              number: 80\nEOF\n    \n    log_success \"Ingress controller configured\"\n}\n\n# Function to setup cert-manager for TLS\nsetup_cert_manager() {\n    log_info \"Setting up cert-manager...\"\n    \n    # Install cert-manager\n    kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.12.0/cert-manager.yaml\n    \n    # Wait for cert-manager to be ready\n    kubectl wait --for=condition=available --timeout=300s deployment/cert-manager -n cert-manager\n    kubectl wait --for=condition=available --timeout=300s deployment/cert-manager-cainjector -n cert-manager\n    kubectl wait --for=condition=available --timeout=300s deployment/cert-manager-webhook -n cert-manager\n    \n    # Create ClusterIssuer for Let's Encrypt\n    cat <<EOF | kubectl apply -f -\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: admin@zantara.com\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    solvers:\n    - http01:\n        ingress:\n          class: nginx\nEOF\n    \n    log_success \"cert-manager configured\"\n}\n\n# Function to setup auto-scaling\nsetup_autoscaling() {\n    log_info \"Setting up advanced auto-scaling...\"\n    \n    # Install Vertical Pod Autoscaler\n    git clone https://github.com/kubernetes/autoscaler.git\n    cd autoscaler/vertical-pod-autoscaler/\n    ./hack/vpa-install.sh\n    cd ../../\n    rm -rf autoscaler\n    \n    # Create VPA for Zantara Bridge\n    cat <<EOF | kubectl apply -f -\napiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: zantara-bridge-vpa\n  namespace: $NAMESPACE\nspec:\n  targetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: zantara-bridge-blue\n  updatePolicy:\n    updateMode: \"Auto\"\n  resourcePolicy:\n    containerPolicies:\n    - containerName: zantara-bridge\n      maxAllowed:\n        cpu: 4\n        memory: 8Gi\n      minAllowed:\n        cpu: 100m\n        memory: 256Mi\nEOF\n    \n    # Install Cluster Autoscaler\n    helm upgrade --install cluster-autoscaler autoscaler/cluster-autoscaler \\\n        --repo https://kubernetes.github.io/autoscaler \\\n        --namespace kube-system \\\n        --set autoDiscovery.clusterName=$CLUSTER_NAME \\\n        --set awsRegion=$REGION \\\n        --set cloudProvider=gce \\\n        --set rbac.create=true\n    \n    log_success \"Auto-scaling configured\"\n}\n\n# Function to deploy application\ndeploy_application() {\n    log_info \"Deploying Zantara Bridge application...\"\n    \n    # Apply Kubernetes manifests\n    kubectl apply -f infrastructure/k8s-deployment.yaml\n    \n    # Wait for deployments to be ready\n    kubectl wait --for=condition=available --timeout=300s deployment/zantara-bridge-blue -n $NAMESPACE\n    \n    # Check if services are accessible\n    kubectl get services -n $NAMESPACE\n    \n    log_success \"Application deployed successfully\"\n}\n\n# Function to run chaos engineering tests\nrun_chaos_tests() {\n    log_info \"Setting up chaos engineering...\"\n    \n    # Install Chaos Mesh\n    curl -sSL https://mirrors.chaos-mesh.org/latest/install.sh | bash\n    \n    # Create a simple pod failure experiment\n    cat <<EOF | kubectl apply -f -\napiVersion: chaos-mesh.org/v1alpha1\nkind: PodChaos\nmetadata:\n  name: zantara-bridge-pod-failure\n  namespace: $NAMESPACE\nspec:\n  action: pod-failure\n  mode: fixed-percent\n  value: \"50\"\n  duration: \"30s\"\n  selector:\n    labelSelectors:\n      \"app.kubernetes.io/name\": \"zantara-bridge\"\n  scheduler:\n    cron: \"@every 1h\"\nEOF\n    \n    log_success \"Chaos engineering configured\"\n}\n\n# Function to generate deployment report\ngenerate_report() {\n    log_info \"Generating deployment report...\"\n    \n    local report_file=\"deployment-report-$(date +%Y%m%d-%H%M%S).txt\"\n    \n    cat > \"$report_file\" <<EOF\nZantara Bridge Container Orchestration Report\n=============================================\nGenerated: $(date)\nProject: $PROJECT_ID\nRegion: $REGION\nCluster: $CLUSTER_NAME\nNamespace: $NAMESPACE\n\nCluster Information:\n$(kubectl cluster-info)\n\nNodes:\n$(kubectl get nodes -o wide)\n\nDeployments:\n$(kubectl get deployments -n $NAMESPACE -o wide)\n\nServices:\n$(kubectl get services -n $NAMESPACE -o wide)\n\nPods:\n$(kubectl get pods -n $NAMESPACE -o wide)\n\nHPA Status:\n$(kubectl get hpa -n $NAMESPACE)\n\nIngress:\n$(kubectl get ingress -n $NAMESPACE)\n\nResource Usage:\n$(kubectl top nodes)\n$(kubectl top pods -n $NAMESPACE)\n\nSystem Health:\n$(kubectl get events -n $NAMESPACE --sort-by='.lastTimestamp' | tail -10)\nEOF\n    \n    log_success \"Report generated: $report_file\"\n}\n\n# Main orchestration function\nmain() {\n    local action=\"${1:-deploy}\"\n    \n    case \"$action\" in\n        \"setup\")\n            log_info \"Setting up complete container orchestration...\"\n            create_cluster\n            configure_kubectl\n            setup_workload_identity\n            deploy_monitoring\n            deploy_service_mesh\n            setup_ingress\n            setup_cert_manager\n            setup_autoscaling\n            deploy_application\n            run_chaos_tests\n            generate_report\n            log_success \"Complete setup finished!\"\n            ;;\n        \"deploy\")\n            configure_kubectl\n            deploy_application\n            generate_report\n            ;;\n        \"monitor\")\n            deploy_monitoring\n            ;;\n        \"mesh\")\n            deploy_service_mesh\n            ;;\n        \"scale\")\n            setup_autoscaling\n            ;;\n        \"chaos\")\n            run_chaos_tests\n            ;;\n        \"report\")\n            generate_report\n            ;;\n        \"status\")\n            configure_kubectl\n            log_info \"Cluster Status:\"\n            kubectl get nodes\n            log_info \"Application Status:\"\n            kubectl get all -n $NAMESPACE\n            ;;\n        \"cleanup\")\n            log_warning \"Cleaning up resources...\"\n            kubectl delete namespace $NAMESPACE --ignore-not-found\n            gcloud container clusters delete $CLUSTER_NAME --region=$REGION --quiet\n            log_success \"Cleanup completed\"\n            ;;\n        *)\n            echo \"Usage: $0 {setup|deploy|monitor|mesh|scale|chaos|report|status|cleanup}\"\n            echo \"  setup   - Complete orchestration setup\"\n            echo \"  deploy  - Deploy application only\"\n            echo \"  monitor - Setup monitoring stack\"\n            echo \"  mesh    - Deploy service mesh\"\n            echo \"  scale   - Setup auto-scaling\"\n            echo \"  chaos   - Setup chaos engineering\"\n            echo \"  report  - Generate deployment report\"\n            echo \"  status  - Show current status\"\n            echo \"  cleanup - Clean up all resources\"\n            exit 1\n            ;;\n    esac\n}\n\n# Check prerequisites\nif ! command -v gcloud >/dev/null 2>&1; then\n    log_error \"gcloud CLI not found\"\n    exit 1\nfi\n\nif ! command -v kubectl >/dev/null 2>&1; then\n    log_error \"kubectl not found\"\n    exit 1\nfi\n\nif ! command -v helm >/dev/null 2>&1; then\n    log_error \"helm not found\"\n    exit 1\nfi\n\n# Run main function\nmain \"$@\"