{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ­ THE CONVERSATION FACTORY: Synthetic Data Generation\n",
                "\n",
                "**Mission:** Turn raw Indonesian Laws into high-quality QA pairs for Nuzantara.\n",
                "**Model:** `Llama-3-8B-Instruct` (via Unsloth/HuggingFace) - Running on Colab GPU.\n",
                "**Output:** `synthetic_gold_dataset.json` (User Question + Ideal Answer + Reasoning).\n",
                "\n",
                "## ðŸ§  Strategy\n",
                "We will feed chunks of law text to Llama 3 and ask it to hallucinate *realistic* user scenarios and *perfect* legal answers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ðŸš€ Install Dependencies (Unsloth for fast inference)\n",
                "%%capture\n",
                "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
                "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ðŸ“¥ Setup & Mount Drive\n",
                "from google.colab import drive\n",
                "import os\n",
                "import json\n",
                "import glob\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Paths\n",
                "RAW_TEXT_DIR = \"/content/drive/MyDrive/nuzantara_laws_text\" # Where we saved TXTs in Phase 1\n",
                "OUTPUT_FILE = \"/content/drive/MyDrive/synthetic_gold_dataset.json\"\n",
                "\n",
                "# Check if text exists\n",
                "if not os.path.exists(RAW_TEXT_DIR):\n",
                "    print(f\"âš ï¸ Warning: {RAW_TEXT_DIR} not found. Did you run the Law Eater with text export?\")\n",
                "    # Fallback: Create dummy dir for testing\n",
                "    os.makedirs(RAW_TEXT_DIR, exist_ok=True)\n",
                "else:\n",
                "    print(f\"âœ… Found Raw Text Directory: {RAW_TEXT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ðŸ¦™ Load Llama 3 (4-bit Quantized for Colab Free Tier)\n",
                "from unsloth import FastLanguageModel\n",
                "import torch\n",
                "\n",
                "max_seq_length = 2048\n",
                "dtype = None\n",
                "load_in_4bit = True\n",
                "\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
                "    max_seq_length = max_seq_length,\n",
                "    dtype = dtype,\n",
                "    load_in_4bit = load_in_4bit,\n",
                ")\n",
                "FastLanguageModel.for_inference(model)\n",
                "\n",
                "print(\"âœ… Llama 3 Loaded! Ready to generate.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ðŸŽ­ The Prompt Template\n",
                "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
                "\n",
                "### Instruction:\n",
                "You are an expert Indonesian Legal AI trainer. Your task is to generate a synthetic QA pair based on the provided law text.\n",
                "1. **User Question**: Create a realistic question a business owner or foreigner might ask about this law. It can be slightly informal.\n",
                "2. **Ideal Answer**: Write the perfect, legally accurate answer based ONLY on the provided text. Cite the specific Article (Pasal) if mentioned.\n",
                "3. **Category**: Classify as 'Visa', 'Tax', 'Corporate', or 'General'.\n",
                "\n",
                "Output valid JSON format: {{\"question\": \"...\", \"answer\": \"...\", \"category\": \"...\"}}\n",
                "\n",
                "### Input:\n",
                "{}\n",
                "\n",
                "### Response:\n",
                "\"\"\"\n",
                "\n",
                "def generate_qa(text_chunk):\n",
                "    inputs = tokenizer(\n",
                "        [alpaca_prompt.format(text_chunk)], return_tensors = \"pt\"\n",
                "    ).to(\"cuda\")\n",
                "\n",
                "    outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True)\n",
                "    response = tokenizer.batch_decode(outputs)[0]\n",
                "    \n",
                "    # Extract JSON from response (naive parsing)\n",
                "    try:\n",
                "        json_str = response.split(\"### Response:\")[1].strip()\n",
                "        # Cleanup markdown code blocks if present\n",
                "        if \"```json\" in json_str:\n",
                "            json_str = json_str.split(\"```json\")[1].split(\"```\")[0]\n",
                "        elif \"```\" in json_str:\n",
                "            json_str = json_str.split(\"```\")[1].split(\"```\")[0]\n",
                "            \n",
                "        return json.loads(json_str)\n",
                "    except:\n",
                "        return None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ðŸ­ Production Line\n",
                "import random\n",
                "\n",
                "files = glob.glob(f\"{RAW_TEXT_DIR}/*.txt\")\n",
                "dataset = []\n",
                "\n",
                "print(f\"ðŸ“š Found {len(files)} text files. Starting generation...\")\n",
                "\n",
                "# Sample random files to avoid processing everything in one go (for demo)\n",
                "# In production, remove the slice [:10]\n",
                "for file_path in tqdm(files[:10], desc=\"Generating Data\"):\n",
                "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
                "        text = f.read()\n",
                "        \n",
                "    # Take a random chunk of 1000-2000 chars to generate a question from\n",
                "    if len(text) > 2000:\n",
                "        start = random.randint(0, len(text) - 2000)\n",
                "        chunk = text[start:start+2000]\n",
                "    else:\n",
                "        chunk = text\n",
                "        \n",
                "    qa_pair = generate_qa(chunk)\n",
                "    if qa_pair:\n",
                "        qa_pair['source_file'] = os.path.basename(file_path)\n",
                "        dataset.append(qa_pair)\n",
                "        print(f\"âœ… Generated: {qa_pair['question'][:50]}...\")\n",
                "\n",
                "# Save Dataset\n",
                "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
                "    json.dump(dataset, f, indent=2)\n",
                "    \n",
                "print(f\"ðŸŽ‰ Generation Complete! Saved {len(dataset)} pairs to {OUTPUT_FILE}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}