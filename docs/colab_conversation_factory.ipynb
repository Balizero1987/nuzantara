{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8450499",
   "metadata": {},
   "source": [
    "# ðŸ­ THE CONVERSATION FACTORY V5: High-Quality Synthetic Data (Regex Fix)\n",
    "\n",
    "**Mission:** Turn raw Indonesian Laws into high-quality QA pairs for Nuzantara.\n",
    "**Updates in V5:**\n",
    "- **Fix:** Correctly separates the Prompt from the Response before searching for JSON.\n",
    "- **Regex:** Finds the JSON object strictly within the generated answer.\n",
    "**Model:** `Llama-3-8B-Instruct` (via Unsloth) - Running on Colab GPU.\n",
    "**Output:** `synthetic_gold_dataset.json`.\n",
    "\n",
    "## ðŸš€ Setup\n",
    "We use Colab T4 or A100 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e7535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸš€ Install Dependencies (Unsloth for fast inference)\n",
    "%%capture\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
    "!pip install langchain langchain-community pypdf tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c317aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“¥ DOWNLOAD DATA (Direct from Drive)\n",
    "!gdown 1Lx4y9TQ45uBUyvNzeHiHinxo_k_WOMmm -O /content/nuzantara_laws.zip\n",
    "\n",
    "# ðŸ“¦ Unzip\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "ZIP_PATH = \"/content/nuzantara_laws.zip\"\n",
    "EXTRACT_DIR = \"/content/nuzantara_laws\"\n",
    "\n",
    "if os.path.exists(ZIP_PATH):\n",
    "    print(\"ðŸ“‚ Extracting...\")\n",
    "    with zipfile.ZipFile(ZIP_PATH, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(EXTRACT_DIR)\n",
    "    print(\"âœ… Extraction complete!\")\n",
    "    SOURCE_DIR = EXTRACT_DIR\n",
    "else:\n",
    "    print(\"âŒ Zip file not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8504ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§  CORE LOGIC INJECTION (The Butcher System V3)\n",
    "# Includes Smart Fallback for non-standard documents\n",
    "\n",
    "import re\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- CONSTANTS ---\n",
    "NOISE_PATTERNS = [\n",
    "    re.compile(r\"^Halaman\\s+\\d+\\s+dari\\s+\\d+\", re.IGNORECASE | re.MULTILINE),\n",
    "    re.compile(\n",
    "        r\"^Salinan sesuai dengan aslinya.*?(?=\\n)\",\n",
    "        re.IGNORECASE | re.MULTILINE | re.DOTALL,\n",
    "    ),\n",
    "    re.compile(r\"^PRESIDEN REPUBLIK INDONESIA\\s*\\n\", re.IGNORECASE | re.MULTILINE),\n",
    "    re.compile(r\"^\\s*-\\s*\\d+\\s*-\\s*$\", re.MULTILINE),\n",
    "    re.compile(r\"\\n{3,}\", re.MULTILINE),\n",
    "    re.compile(r\"^\\s*\\d+\\s*$\", re.MULTILINE),\n",
    "]\n",
    "\n",
    "LEGAL_TYPE_PATTERN = re.compile(\n",
    "    r\"(UNDANG-UNDANG|PERATURAN PEMERINTAH|KEPUTUSAN PRESIDEN|PERATURAN MENTERI|QANUN|PERATURAN DAERAH|PERATURAN KEPALA)\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "LEGAL_TYPE_ABBREV = {\n",
    "    \"UNDANG-UNDANG\": \"UU\",\n",
    "    \"PERATURAN PEMERINTAH\": \"PP\",\n",
    "    \"KEPUTUSAN PRESIDEN\": \"Keppres\",\n",
    "    \"PERATURAN MENTERI\": \"Permen\",\n",
    "    \"QANUN\": \"Qanun\",\n",
    "    \"PERATURAN DAERAH\": \"Perda\",\n",
    "    \"PERATURAN KEPALA\": \"Perkep\",\n",
    "}\n",
    "\n",
    "NUMBER_PATTERN = re.compile(r\"NOMOR\\s+(\\d+[A-Z]?)(?:[/-]\\d+)?\", re.IGNORECASE)\n",
    "YEAR_PATTERN = re.compile(r\"TAHUN\\s+(\\d{4})\", re.IGNORECASE)\n",
    "TOPIC_PATTERN = re.compile(\n",
    "    r\"TENTANG\\s+(.+?)(?=DENGAN RAHMAT|Menimbang|Mengingat|$)\", re.IGNORECASE | re.DOTALL\n",
    ")\n",
    "\n",
    "PASAL_PATTERN = re.compile(\n",
    "    r\"^Pasal\\s+(\\d+[A-Z]?)\\s*(.+?)(?=^Pasal\\s+\\d+|^BAB\\s+|^Penjelasan|\\Z)\",\n",
    "    re.IGNORECASE | re.MULTILINE | re.DOTALL,\n",
    ")\n",
    "BAB_PATTERN = re.compile(\n",
    "    r\"^BAB\\s+([IVX]+|[A-Z]+|\\d+)\\s+(.+?)(?=\\n|$)\", re.IGNORECASE | re.MULTILINE\n",
    ")\n",
    "AYAT_PATTERN = re.compile(\n",
    "    r\"(?:^|\\n)\\s*\\((\\d+)\\)\\s*(.+?)(?=(?:^|\\n)\\s*\\(\\d+\\)|$)\", re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "# --- CLASSES ---\n",
    "\n",
    "\n",
    "class LegalCleaner:\n",
    "    def clean(self, text: str) -> str:\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        cleaned = text\n",
    "        for pattern in NOISE_PATTERNS:\n",
    "            cleaned = pattern.sub(\"\", cleaned)\n",
    "        cleaned = re.sub(r\"\\s+\", \" \", cleaned)\n",
    "        cleaned = re.sub(r\"\\n\\s+\\n\", \"\\n\\n\", cleaned)\n",
    "        cleaned = re.sub(\n",
    "            r\"Pasal\\s+(\\d+[A-Z]?)\", r\"Pasal \\1\", cleaned, flags=re.IGNORECASE\n",
    "        )\n",
    "        return cleaned.strip()\n",
    "\n",
    "\n",
    "class LegalMetadataExtractor:\n",
    "    def extract(self, text: str) -> dict:\n",
    "        meta = {\n",
    "            \"type\": \"UNKNOWN\",\n",
    "            \"number\": \"UNKNOWN\",\n",
    "            \"year\": \"UNKNOWN\",\n",
    "            \"topic\": \"UNKNOWN\",\n",
    "        }\n",
    "\n",
    "        type_match = LEGAL_TYPE_PATTERN.search(text)\n",
    "        if type_match:\n",
    "            doc_type = type_match.group(1).upper()\n",
    "            meta[\"type\"] = doc_type\n",
    "            meta[\"type_abbrev\"] = LEGAL_TYPE_ABBREV.get(doc_type, doc_type)\n",
    "\n",
    "        num_match = NUMBER_PATTERN.search(text)\n",
    "        if num_match:\n",
    "            meta[\"number\"] = num_match.group(1)\n",
    "\n",
    "        year_match = YEAR_PATTERN.search(text)\n",
    "        if year_match:\n",
    "            meta[\"year\"] = year_match.group(1)\n",
    "\n",
    "        topic_match = TOPIC_PATTERN.search(text)\n",
    "        if topic_match:\n",
    "            meta[\"topic\"] = re.sub(r\"\\s+\", \" \", topic_match.group(1).strip())[:200]\n",
    "\n",
    "        return meta\n",
    "\n",
    "\n",
    "class LegalStructureParser:\n",
    "    def parse(self, text: str) -> dict:\n",
    "        structure = {\"batang_tubuh\": []}\n",
    "        bab_matches = list(BAB_PATTERN.finditer(text))\n",
    "        for i, match in enumerate(bab_matches):\n",
    "            bab_num = match.group(1)\n",
    "            bab_title = match.group(2).strip()\n",
    "            start = match.end()\n",
    "            end = bab_matches[i + 1].start() if i + 1 < len(bab_matches) else len(text)\n",
    "\n",
    "            # Simple pasal extraction for context finding\n",
    "            pasal_list = []\n",
    "            pasal_matches = list(PASAL_PATTERN.finditer(text[start:end]))\n",
    "            for p_match in pasal_matches:\n",
    "                pasal_list.append({\"number\": p_match.group(1)})\n",
    "\n",
    "            structure[\"batang_tubuh\"].append(\n",
    "                {\"number\": bab_num, \"title\": bab_title, \"pasal\": pasal_list}\n",
    "            )\n",
    "        return structure\n",
    "\n",
    "\n",
    "class LegalChunker:\n",
    "    def __init__(self, max_pasal_tokens=1000):\n",
    "        self.max_pasal_tokens = max_pasal_tokens\n",
    "        self.fallback_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000, chunk_overlap=200\n",
    "        )\n",
    "\n",
    "    def chunk(self, text: str, metadata: dict, structure: dict) -> list:\n",
    "        chunks = []\n",
    "        pasal_matches = list(PASAL_PATTERN.finditer(text))\n",
    "\n",
    "        # If no pasal found, use Fallback Splitter instead of sending whole text\n",
    "        if not pasal_matches:\n",
    "            raw_chunks = self.fallback_splitter.split_text(text)\n",
    "            context = self._build_context(metadata)\n",
    "            for rc in raw_chunks:\n",
    "                chunks.append(self._create_chunk(rc, context, metadata))\n",
    "            return chunks\n",
    "\n",
    "        for match in pasal_matches:\n",
    "            pasal_num = match.group(1)\n",
    "            pasal_text = match.group(2).strip()\n",
    "\n",
    "            # Find BAB context\n",
    "            bab_context = self._find_bab_for_pasal(structure, pasal_num)\n",
    "\n",
    "            # Check length - split by Ayat if needed\n",
    "            if len(pasal_text) > 3000:  # Approx chars for 1000 tokens\n",
    "                ayat_matches = list(AYAT_PATTERN.finditer(pasal_text))\n",
    "                if ayat_matches:\n",
    "                    for am in ayat_matches:\n",
    "                        ayat_num = am.group(1)\n",
    "                        ayat_text = am.group(2).strip()\n",
    "                        context = self._build_context(\n",
    "                            metadata, bab_context, f\"Pasal {pasal_num}\"\n",
    "                        )\n",
    "                        chunks.append(\n",
    "                            self._create_chunk(\n",
    "                                f\"Ayat ({ayat_num})\\n{ayat_text}\",\n",
    "                                context,\n",
    "                                metadata,\n",
    "                                pasal_num,\n",
    "                            )\n",
    "                        )\n",
    "                    continue\n",
    "\n",
    "            # Default: Pasal as chunk\n",
    "            context = self._build_context(metadata, bab_context, f\"Pasal {pasal_num}\")\n",
    "            chunks.append(self._create_chunk(pasal_text, context, metadata, pasal_num))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def _build_context(self, meta, bab=None, pasal=None):\n",
    "        parts = [\n",
    "            meta.get(\"type_abbrev\", \"UNK\"),\n",
    "            f\"NO {meta.get('number', '?')}\",\n",
    "            f\"TAHUN {meta.get('year', '?')}\",\n",
    "            f\"TENTANG {meta.get('topic', 'UNK')}\",\n",
    "        ]\n",
    "        if bab:\n",
    "            parts.append(bab)\n",
    "        if pasal:\n",
    "            parts.append(pasal)\n",
    "        return f\"[CONTEXT: {' - '.join(parts)}]\"\n",
    "\n",
    "    def _create_chunk(self, content, context, meta, pasal_num=None):\n",
    "        chunk_text = f\"{context}\\n\\n{content}\"\n",
    "        c = {\"text\": chunk_text, \"has_context\": True}\n",
    "        c.update(meta)\n",
    "        if pasal_num:\n",
    "            c[\"pasal_number\"] = pasal_num\n",
    "        return c\n",
    "\n",
    "    def _find_bab_for_pasal(self, structure, pasal_num):\n",
    "        for bab in structure.get(\"batang_tubuh\", []):\n",
    "            for p in bab.get(\"pasal\", []):\n",
    "                if p.get(\"number\") == pasal_num:\n",
    "                    return f\"BAB {bab.get('number')} - {bab.get('title', '')}\"\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"âœ… Nuzantara Legal Engine Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d6dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¦™ Load Llama 3 (4-bit Quantized)\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 4096\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"âœ… Llama 3 Loaded! Ready to generate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701bb402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ­ The Prompt Template\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "You are an expert Indonesian Legal AI trainer. Your task is to generate a synthetic QA pair based on the provided law text.\n",
    "1. **User Question**: Create a realistic question a business owner or foreigner might ask about this specific article. It can be slightly informal.\n",
    "2. **Ideal Answer**: Write the perfect, legally accurate answer based ONLY on the provided text. Cite the specific Article (Pasal) if mentioned.\n",
    "3. **Category**: Classify as 'Visa', 'Tax', 'Corporate', or 'General'.\n",
    "\n",
    "Output valid JSON format: {{\"question\": \"...\", \"answer\": \"...\", \"category\": \"...\"}}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_qa(text_chunk):\n",
    "    if len(text_chunk) > 10000:\n",
    "        text_chunk = text_chunk[:10000] + \"...\"\n",
    "\n",
    "    inputs = tokenizer([alpaca_prompt.format(text_chunk)], return_tensors=\"pt\").to(\n",
    "        \"cuda\"\n",
    "    )\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=512, use_cache=True)\n",
    "    response = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "    # ROBUST JSON EXTRACTION\n",
    "    try:\n",
    "        # 1. Split to get only the response part (avoid matching prompt examples)\n",
    "        if \"### Response:\" in response:\n",
    "            generated_text = response.split(\"### Response:\")[-1]\n",
    "        else:\n",
    "            generated_text = response\n",
    "\n",
    "        # 2. Find the first '{' and the last '}' in the GENERATED text\n",
    "        match = re.search(r\"\\{.*\\}\", generated_text, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group(0)\n",
    "            return json.loads(json_str)\n",
    "        else:\n",
    "            # Fallback: maybe it didn't output JSON at all\n",
    "            print(f\"âš ï¸ No JSON found in generated text. Raw: {generated_text[:100]}...\")\n",
    "            return None\n",
    "    except Exception:\n",
    "        # print(f\"âŒ JSON Parse Error: {e}. Raw: {response[:100]}...\") # Too noisy\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ­ Production Line (The Factory)\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Initialize Engine\n",
    "cleaner = LegalCleaner()\n",
    "extractor = LegalMetadataExtractor()\n",
    "parser = LegalStructureParser()\n",
    "chunker = LegalChunker()\n",
    "\n",
    "files = glob.glob(f\"{SOURCE_DIR}/**/*.pdf\", recursive=True)\n",
    "dataset = []\n",
    "\n",
    "print(f\"ðŸ“š Found {len(files)} PDF files. Starting generation...\")\n",
    "\n",
    "# Sample random files for demo (remove slicing for full run)\n",
    "TARGET_SAMPLES = 50  # How many QA pairs to generate for this test run\n",
    "processed_count = 0\n",
    "\n",
    "# Shuffle files to get variety\n",
    "random.shuffle(files)\n",
    "\n",
    "for file_path in tqdm(files, desc=\"Processing PDFs\"):\n",
    "    if processed_count >= TARGET_SAMPLES:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        # Check PDF Header\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            if f.read(4) != b\"%PDF\":\n",
    "                continue\n",
    "\n",
    "        # Load & Process\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        pages = loader.load()\n",
    "        raw_text = \"\\n\".join([p.page_content for p in pages])\n",
    "\n",
    "        if len(raw_text) < 100:\n",
    "            continue\n",
    "\n",
    "        cleaned_text = cleaner.clean(raw_text)\n",
    "        meta = extractor.extract(cleaned_text)\n",
    "        structure = parser.parse(cleaned_text)\n",
    "        chunks = chunker.chunk(cleaned_text, meta, structure)\n",
    "\n",
    "        # Pick 1 random chunk from this file to generate a QA pair\n",
    "        if chunks:\n",
    "            selected_chunk = random.choice(chunks)\n",
    "            qa_pair = generate_qa(selected_chunk[\"text\"])\n",
    "\n",
    "            if qa_pair:\n",
    "                qa_pair[\"source_file\"] = os.path.basename(file_path)\n",
    "                qa_pair[\"context_used\"] = selected_chunk[\"text\"]\n",
    "                dataset.append(qa_pair)\n",
    "                processed_count += 1\n",
    "                print(\n",
    "                    f\"âœ… Generated ({processed_count}/{TARGET_SAMPLES}): {qa_pair['question'][:50]}...\"\n",
    "                )\n",
    "\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# Save Dataset\n",
    "OUTPUT_FILE = \"/content/synthetic_gold_dataset.json\"\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset, f, indent=2)\n",
    "\n",
    "print(f\"ðŸŽ‰ Generation Complete! Saved {len(dataset)} pairs to {OUTPUT_FILE}\")\n",
    "print(\"â¬‡ï¸ Download the file from the Files pane on the left!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
