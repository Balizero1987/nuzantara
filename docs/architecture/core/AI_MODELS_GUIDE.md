# ðŸ¤– AI Models Guide - NUZANTARA

**Last Updated**: 2025-10-17
**Status**: âœ… Production Active on Railway

---

## ðŸ“Š Current AI Architecture

NUZANTARA uses **Collaborative Intelligence** (Claude Haiku + Sonnet PRIMARY):

| AI Model | Provider | Role | Traffic | Cost/Month |
|----------|----------|------|---------|------------|
| **Claude Haiku 3.5** | Anthropic API | PRIMARY - Fast casual | 60% | $2-4 |
| **Claude Sonnet 4.5** | Anthropic API | PRIMARY - Business complex | 35% | $4-8 |
| ZANTARA Llama 3.1 8B | RunPod (fallback) | âš ï¸ DISABLED in routing | 5% | â‚¬2-8 |
| DevAI Qwen 2.5 Coder 7B | RunPod (internal) | Code analysis only | Internal | â‚¬1-3 |

**Total Cost**: $18-45/month (vs GCP $40-165/month hosting eliminated)

---

## ðŸ§  AI Models Details

### 1. Claude Haiku 3.5 (PRIMARY)

**Use Cases**:
- Greetings and casual conversations
- Simple questions
- Fast responses needed

**Characteristics**:
- Speed: ~500ms response time
- Quality: 95% human-like
- Cost: $0.25 per 1M input tokens, $1.25 per 1M output tokens
- Max tokens: 300 (optimized for brevity)

**Integration**:
```python
# apps/backend-rag 2/backend/services/claude_haiku_service.py
client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
response = client.messages.create(
    model="claude-3-5-haiku-20241022",
    max_tokens=300,
    temperature=0.7,
    messages=[{"role": "user", "content": message}]
)
```

---

### 2. Claude Sonnet 4.5 (PRIMARY)

**Use Cases**:
- Business intelligence queries
- Complex legal/tax questions
- Multi-step reasoning
- Tool use and workflows

**Characteristics**:
- Speed: ~2-3s response time
- Quality: 92% accurate + human-like
- Cost: $3 per 1M input tokens, $15 per 1M output tokens
- Max tokens: 2000 (detailed responses)

**Integration**:
```python
# apps/backend-rag 2/backend/services/claude_sonnet_service.py
client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=2000,
    temperature=0.3,
    system=ZANTARA_SYSTEM_PROMPT,
    messages=[{"role": "user", "content": f"Context: {rag_context}\n\nQuestion: {message}"}]
)
```

---

### 3. ZANTARA Llama 3.1 8B (FALLBACK)

**Use Cases**:
- âš ï¸ **DISABLED** in routing (line 279 of main_cloud.py)
- Optional fallback only if Claude API fails
- Not actively used in production

**Characteristics**:
- Speed: ~1-2s response time
- Quality: 45% human-like (poor for casual)
- Cost: â‚¬2-8/month (RunPod serverless)
- Status: Kept as emergency fallback

**Integration**:
```python
# apps/backend-rag 2/backend/llm/zantara_client.py
# DISABLED in routing - only used if Claude unavailable
endpoint = "https://api.runpod.ai/v2/itz2q5gmid4cyt/runsync"
```

**Why Disabled**:
- Poor quality for greetings (45% vs 95% Claude)
- Formal and robotic responses
- Better as structured output generator only

---

### 4. DevAI Qwen 2.5 Coder 7B (INTERNAL)

**Use Cases**:
- Code analysis and review
- Bug detection
- Development workflows
- Internal use only

**Characteristics**:
- Speed: ~2-3s response time
- Quality: 95%+ for code tasks
- Cost: â‚¬1-3/month (RunPod serverless)
- Fine-tuned on 487 NUZANTARA code examples

**Integration**:
```python
# Internal endpoint - not exposed to users
endpoint = "https://api.runpod.ai/v2/5g2h6nbyls47i7/runsync"
```

---

## ðŸ”„ Routing Logic

### Pattern Matching Router

```python
def route_by_pattern(message: str) -> str:
    """Instant routing (no AI classification)"""

    msg_lower = message.lower()
    msg_len = len(message.split())

    # Greeting â†’ Haiku (60%)
    if any(word in msg_lower for word in ['ciao', 'hello', 'hi']):
        if msg_len < 5:
            return "haiku"

    # Business â†’ Sonnet (35%)
    if any(word in msg_lower for word in ['kitas', 'pt pma', 'visa', 'business']):
        if msg_len > 5:
            return "sonnet"

    # Default: length heuristic
    return "haiku" if msg_len < 10 else "sonnet"
```

**Accuracy**: 88-92% routing correctness
**Speed**: 0ms (instant pattern matching)
**Cost**: $0 (no classification API call)

---

## ðŸ“ˆ Modern AI Features

### Clarification Service (8.3% trigger rate)
Detects ambiguous queries like "how much" and requests clarification before processing.

### Citation Service (63.8% responses)
Adds source references to RAG-powered responses:
```
Response: "KITAS requires sponsor..."

---
**Sources:**
[1] KITAS Guide - https://... - 2024-01-15
[2] Timeline Info - https://... - 2023-12-10
```

### Follow-up Service (100% responses)
Generates 3-4 contextual follow-up questions using Claude Haiku:
```json
"followup_questions": [
  "What documents do I need?",
  "How long does the process take?",
  "What are the costs involved?"
]
```

---

## ðŸŽ¯ Performance Metrics

| Metric | Before (Llama-only) | After (Collaborative) | Improvement |
|--------|---------------------|----------------------|-------------|
| Human-like score | 45% | 92% | +47% âœ… |
| Greeting quality | 45% | 95% | +50% âœ… |
| Business accuracy | 60% | 90% | +30% âœ… |
| Avg latency | N/A | 2.1s | < 5s target âœ… |

---

## ðŸ”§ Configuration

### Environment Variables (Railway)

```bash
# Claude API (PRIMARY)
ANTHROPIC_API_KEY=sk-ant-api03-...

# RunPod (FALLBACK)
RUNPOD_LLAMA_ENDPOINT=https://api.runpod.ai/v2/itz2q5gmid4cyt/runsync
RUNPOD_API_KEY=rpa_...

# DevAI (INTERNAL)
RUNPOD_QWEN_ENDPOINT=https://api.runpod.ai/v2/5g2h6nbyls47i7/runsync
```

### Enable/Disable Models

```python
# To re-enable Llama routing (not recommended):
# apps/backend-rag 2/backend/app/main_cloud.py:279
# Change: if False and should_use_llama:
# To:     if should_use_llama:
```

---

## ðŸ“š Additional Resources

- **System Architecture**: `docs/architecture/SYSTEM_ARCHITECTURE_COMPLETE.md`
- **Project Context**: `docs/PROJECT_CONTEXT.md`
- **Deployment**: `docs/deployment/deploy-rag-backend.md`

---

**Version**: 6.0.0
**Platform**: Railway Production
**Status**: âœ… All AI models operational

*From Zero to Infinity âˆž* ðŸŒ¸
