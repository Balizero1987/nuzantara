{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§  Nuzantara Synthetic Data Generator\n",
                "\n",
                "This notebook generates high-quality synthetic Question-Answer pairs from Indonesian legal text using open-source LLMs (Llama 3 / Mistral).\n",
                "\n",
                "**Goal**: Create a \"Gold Dataset\" to train and test Nuzantara.\n",
                "\n",
                "### Steps:\n",
                "1.  **Setup**: Install dependencies.\n",
                "2.  **Load Data**: Paste your legal text or load from file.\n",
                "3.  **Generate**: AI creates user personas and questions.\n",
                "4.  **Save**: Export as JSON for Nuzantara."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 1. Install Dependencies\n",
                "!pip install -q -U torch transformers accelerate bitsandbytes\n",
                "!pip install -q -U langchain langchain-community"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2. Load Model (Llama 3 8B or Mistral 7B)\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
                "\n",
                "# Use a powerful but efficient model\n",
                "MODEL_ID = \"unsloth/llama-3-8b-Instruct-bnb-4bit\" # Faster & Free on Colab\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_ID,\n",
                "    quantization_config={\"load_in_4bit\": True},\n",
                "    device_map=\"auto\",\n",
                ")\n",
                "\n",
                "pipe = pipeline(\n",
                "    \"text-generation\",\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    max_new_tokens=512,\n",
                "    temperature=0.7,\n",
                "    top_p=0.95,\n",
                "    repetition_penalty=1.15\n",
                ")\n",
                "\n",
                "print(\"âœ… Model Loaded Successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 3. Define the Generator Function\n",
                "\n",
                "def generate_synthetic_data(legal_text, num_pairs=5):\n",
                "    prompt = f\"\"\"\n",
                "    You are an expert legal AI trainer.\n",
                "    Your task is to generate {num_pairs} diverse Question-Answer pairs based strictly on the provided legal text.\n",
                "    \n",
                "    **Scenarios to simulate:**\n",
                "    1. A confused foreigner (simple English).\n",
                "    2. A professional lawyer (formal Indonesian).\n",
                "    3. A digital nomad (casual slang).\n",
                "    \n",
                "    **Format:**\n",
                "    Return a JSON list of objects with 'question', 'answer', 'context_used', and 'persona'.\n",
                "    \n",
                "    **Legal Text:**\n",
                "    {legal_text}\n",
                "    \n",
                "    **Output (JSON only):**\n",
                "    \"\"\"\n",
                "    \n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that outputs only valid JSON.\"},\n",
                "        {\"role\": \"user\", \"content\": prompt},\n",
                "    ]\n",
                "    \n",
                "    prompt_formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
                "    \n",
                "    outputs = pipe(prompt_formatted)\n",
                "    return outputs[0][\"generated_text\"].split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].strip()\n",
                "\n",
                "print(\"âœ… Generator Function Ready!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 4. Run Generation (Paste Text Here)\n",
                "\n",
                "legal_text_input = \"\"\"\n",
                "Pasal 4\n",
                "(1) Orang Asing yang berada di Wilayah Indonesia wajib memiliki Izin Tinggal.\n",
                "(2) Izin Tinggal sebagaimana dimaksud pada ayat (1) diberikan kepada Orang Asing sesuai dengan Visa yang dimilikinya.\n",
                "\"\"\"\n",
                "\n",
                "result = generate_synthetic_data(legal_text_input, num_pairs=3)\n",
                "print(result)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}