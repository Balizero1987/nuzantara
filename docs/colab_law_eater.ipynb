{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ü¶ñ THE LAW EATER: Massive PDF Ingestion (OpenAI Edition)\n",
                "\n",
                "**Mission:** Ingest thousands of Indonesian Law PDFs into Qdrant.\n",
                "**Model:** `text-embedding-3-small` (1536 dimensions) - **Compatible with Nuzantara Main DB**.\n",
                "**Target:** Qdrant Cloud (`legal_unified` collection).\n",
                "**Bonus:** Exports raw text to Drive for the \"Conversation Factory\".\n",
                "\n",
                "## üöÄ Setup\n",
                "We use Colab for its high bandwidth and processing power (PDF parsing)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q qdrant-client langchain langchain-community langchain-openai langchain-text-splitters pypdf tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Authentication & Configuration\n",
                "from google.colab import userdata\n",
                "import os\n",
                "\n",
                "try:\n",
                "    QDRANT_URL = userdata.get('QDRANT_URL')\n",
                "    QDRANT_API_KEY = userdata.get('QDRANT_API_KEY')\n",
                "except Exception:\n",
                "    QDRANT_URL = input(\"Enter Qdrant URL: \")\n",
                "    QDRANT_API_KEY = input(\"Enter Qdrant API Key: \")\n",
                "\n",
                "# Hardcoded API Key as requested\n",
                "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY') or input(\"Enter OpenAI API Key: \")\n",
                "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
                "\n",
                "# TARGET COLLECTION (Main Legal DB)\n",
                "COLLECTION_NAME = \"legal_unified\"\n",
                "VECTOR_SIZE = 1536 # OpenAI Standard"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üì• DOWNLOAD DATA (Direct from your Link)\n",
                "# We use gdown to grab the zip file directly from the link you provided.\n",
                "!gdown 1Lx4y9TQ45uBUyvNzeHiHinxo_k_WOMmm -O /content/nuzantara_laws.zip\n",
                "\n",
                "# üì¶ Unzip & Setup Paths\n",
                "import os\n",
                "import zipfile\n",
                "\n",
                "ZIP_PATH = \"/content/nuzantara_laws.zip\"\n",
                "EXTRACT_DIR = \"/content/nuzantara_laws\"\n",
                "TEXT_OUTPUT_DIR = \"/content/nuzantara_laws_text\"\n",
                "\n",
                "if os.path.exists(ZIP_PATH):\n",
                "    print(f\"üì¶ Found zip: {ZIP_PATH}\")\n",
                "    if not os.path.exists(EXTRACT_DIR):\n",
                "        print(\"üìÇ Extracting...\")\n",
                "        with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
                "            zip_ref.extractall(EXTRACT_DIR)\n",
                "        print(\"‚úÖ Extraction complete!\")\n",
                "    else:\n",
                "        print(\"‚úÖ Already extracted.\")\n",
                "    SOURCE_DIR = EXTRACT_DIR\n",
                "else:\n",
                "    print(\"‚ùå Zip file not found! Check the download step.\")\n",
                "    SOURCE_DIR = \"/content/nuzantara_laws\" # Attempt anyway\n",
                "\n",
                "if not os.path.exists(TEXT_OUTPUT_DIR):\n",
                "    os.makedirs(TEXT_OUTPUT_DIR)\n",
                "    print(f\"‚úÖ Created text output directory: {TEXT_OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üß† The Engine: OpenAI Embeddings\n",
                "from langchain_openai import OpenAIEmbeddings\n",
                "\n",
                "print(\"‚è≥ Initializing OpenAI Embeddings...\")\n",
                "embeddings = OpenAIEmbeddings(\n",
                "    model=\"text-embedding-3-small\",\n",
                "    dimensions=1536\n",
                ")\n",
                "print(\"‚úÖ Embeddings Ready (1536 dims)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üìÑ Processing Logic\n",
                "from langchain_community.document_loaders import PyPDFLoader\n",
                "# ROBUST IMPORT for Text Splitter\n",
                "try:\n",
                "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "except ImportError:\n",
                "    try:\n",
                "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "    except ImportError:\n",
                "        from langchain_community.text_splitter import RecursiveCharacterTextSplitter\n",
                "\n",
                "import re\n",
                "from pathlib import Path\n",
                "\n",
                "text_splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size=1000,\n",
                "    chunk_overlap=200,\n",
                "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
                ")\n",
                "\n",
                "def extract_metadata_from_filename(filename):\n",
                "    \"\"\"Simple regex to guess metadata from filename like 'UU_1_2023.pdf'\"\"\"\n",
                "    meta = {\"source\": filename, \"type\": \"UNKNOWN\", \"year\": \"UNKNOWN\", \"number\": \"UNKNOWN\"}\n",
                "    \n",
                "    # Try to find year (4 digits)\n",
                "    year_match = re.search(r'(19|20)\\d{2}', filename)\n",
                "    if year_match:\n",
                "        meta['year'] = year_match.group(0)\n",
                "        \n",
                "    # Try to find type\n",
                "    if \"UU\" in filename.upper(): meta['type'] = \"UNDANG_UNDANG\"\n",
                "    elif \"PP\" in filename.upper(): meta['type'] = \"PERATURAN_PEMERINTAH\"\n",
                "    elif \"PERPRES\" in filename.upper(): meta['type'] = \"PERATURAN_PRESIDEN\"\n",
                "    \n",
                "    return meta\n",
                "\n",
                "def process_pdf(file_path):\n",
                "    try:\n",
                "        loader = PyPDFLoader(file_path)\n",
                "        pages = loader.load()\n",
                "        \n",
                "        # Check if scanned (empty text)\n",
                "        total_text = \"\".join([p.page_content for p in pages])\n",
                "        if len(total_text.strip()) < 100:\n",
                "            print(f\"‚ö†Ô∏è SKIPPING {Path(file_path).name}: Likely scanned/image PDF\")\n",
                "            return []\n",
                "            \n",
                "        # SAVE RAW TEXT FOR FACTORY\n",
                "        txt_filename = Path(file_path).stem + \".txt\"\n",
                "        txt_path = os.path.join(TEXT_OUTPUT_DIR, txt_filename)\n",
                "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
                "            f.write(total_text)\n",
                "            \n",
                "        # Split\n",
                "        chunks = text_splitter.split_documents(pages)\n",
                "        \n",
                "        # Enrich Metadata\n",
                "        file_meta = extract_metadata_from_filename(Path(file_path).name)\n",
                "        for chunk in chunks:\n",
                "            chunk.metadata.update(file_meta)\n",
                "            chunk.metadata['filename'] = Path(file_path).name\n",
                "            \n",
                "        return chunks\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå ERROR processing {file_path}: {e}\")\n",
                "        return []"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üíæ Qdrant Connection\n",
                "from qdrant_client import QdrantClient\n",
                "from qdrant_client.http import models\n",
                "\n",
                "# INCREASED TIMEOUT to handle Fly.io wake-up / latency\n",
                "client = QdrantClient(\n",
                "    url=QDRANT_URL, \n",
                "    api_key=QDRANT_API_KEY,\n",
                "    timeout=60  # Increased from default to 60s\n",
                ")\n",
                "\n",
                "# FORCE RECREATE (TABULA RASA) as requested\n",
                "print(f\"‚ö†Ô∏è WIPING and Recreating collection '{COLLECTION_NAME}'...\")\n",
                "client.recreate_collection(\n",
                "    collection_name=COLLECTION_NAME,\n",
                "    vectors_config=models.VectorParams(size=VECTOR_SIZE, distance=models.Distance.COSINE),\n",
                ")\n",
                "print(\"‚úÖ Collection wiped and ready for fresh ingestion!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üöÄ EXECUTION LOOP\n",
                "import glob\n",
                "from tqdm.notebook import tqdm\n",
                "from langchain_community.vectorstores import Qdrant\n",
                "\n",
                "# Find all PDFs\n",
                "pdf_files = glob.glob(f\"{SOURCE_DIR}/**/*.pdf\", recursive=True)\n",
                "print(f\"üìö Found {len(pdf_files)} PDFs to ingest\")\n",
                "\n",
                "# Initialize VectorStore wrapper\n",
                "qdrant_store = Qdrant(\n",
                "    client=client,\n",
                "    collection_name=COLLECTION_NAME,\n",
                "    embeddings=embeddings\n",
                ")\n",
                "\n",
                "# Batch Process\n",
                "BATCH_SIZE = 10 # Process 10 PDFs at a time\n",
                "\n",
                "for i in tqdm(range(0, len(pdf_files), BATCH_SIZE), desc=\"Batch Processing\"):\n",
                "    batch_files = pdf_files[i:i+BATCH_SIZE]\n",
                "    batch_docs = []\n",
                "    \n",
                "    for pdf_file in batch_files:\n",
                "        docs = process_pdf(pdf_file)\n",
                "        batch_docs.extend(docs)\n",
                "        \n",
                "    if batch_docs:\n",
                "        # Upload to Qdrant\n",
                "        try:\n",
                "            qdrant_store.add_documents(batch_docs)\n",
                "            print(f\"‚úÖ Uploaded {len(batch_docs)} chunks from batch {i//BATCH_SIZE + 1}\")\n",
                "        except Exception as e:\n",
                "            print(f\"‚ùå FAILED to upload batch: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}