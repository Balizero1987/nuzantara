{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# ü¶ñ THE LAW EATER V7: Massive Legal Ingestion (Requests Edition)\n",
    "\n",
    "**Mission:** Ingest thousands of Indonesian Law PDFs into Qdrant with **High Quality Context Injection**.\n",
    "**Updates in V7:**\n",
    "- **Pure Requests:** Uses the standard `requests` library instead of the OpenAI client to bypass library-specific connection issues.\n",
    "- **SSL Verification Disabled:** Option to ignore SSL errors if Colab's proxy is interfering.\n",
    "- **Connectivity Test:** Verifies connection before starting.\n",
    "\n",
    "## üöÄ Setup\n",
    "We use Colab for its high bandwidth and processing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q qdrant-client langchain langchain-community langchain-text-splitters pypdf tqdm tiktoken requests tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication & Configuration\n",
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "try:\n",
    "    QDRANT_URL = userdata.get(\"QDRANT_URL\")\n",
    "    QDRANT_API_KEY = userdata.get(\"QDRANT_API_KEY\")\n",
    "except Exception:\n",
    "    QDRANT_URL = input(\"Enter Qdrant URL: \")\n",
    "    QDRANT_API_KEY = input(\"Enter Qdrant API Key: \")\n",
    "\n",
    "# OpenAI Key\n",
    "try:\n",
    "    OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
    "except Exception:\n",
    "    OPENAI_API_KEY = input(\"Enter OpenAI API Key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# TARGET COLLECTION (Main Legal DB)\n",
    "COLLECTION_NAME = \"legal_unified\"\n",
    "VECTOR_SIZE = 1536  # OpenAI Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ OPENAI CONNECTIVITY TEST (Pure Requests)\n",
    "import requests\n",
    "\n",
    "\n",
    "def test_openai_connection():\n",
    "    url = \"https://api.openai.com/v1/embeddings\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    data = {\"input\": \"Test connection\", \"model\": \"text-embedding-3-small\"}\n",
    "\n",
    "    print(\"üß™ Testing OpenAI Connection via requests...\")\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url, headers=headers, json=data, timeout=10, verify=False\n",
    "        )  # Verify=False to bypass SSL issues\n",
    "        if response.status_code == 200:\n",
    "            print(\n",
    "                \"‚úÖ OpenAI Connection SUCCESS! Vector length:\",\n",
    "                len(response.json()[\"data\"][0][\"embedding\"]),\n",
    "            )\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå OpenAI Connection FAILED! Status: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå OpenAI Connection ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "if not test_openai_connection():\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Connection test failed. The script might not work.\")\n",
    "else:\n",
    "    print(\"\\nüöÄ Connection looks good! Proceeding...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚è∞ WAKE UP & CONNECTION CHECK\n",
    "import time\n",
    "\n",
    "print(f\"üîå Connecting to {QDRANT_URL}...\")\n",
    "\n",
    "max_retries = 10\n",
    "for i in range(max_retries):\n",
    "    try:\n",
    "        response = requests.get(QDRANT_URL, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Server is AWAKE and responding!\")\n",
    "            print(f\"   Version: {response.json().get('version', 'Unknown')}\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"   üí§ Waiting for server to wake up... ({i + 1}/{max_retries}) - {e}\")\n",
    "        time.sleep(5)\n",
    "else:\n",
    "    print(\"‚ùå Server did not wake up. Check URL or Fly.io status.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† CORE LOGIC INJECTION (The Butcher System)\n",
    "import re\n",
    "\n",
    "# --- CONSTANTS ---\n",
    "NOISE_PATTERNS = [\n",
    "    re.compile(r\"^Halaman\\s+\\d+\\s+dari\\s+\\d+\", re.IGNORECASE | re.MULTILINE),\n",
    "    re.compile(\n",
    "        r\"^Salinan sesuai dengan aslinya.*?(?=\\n)\",\n",
    "        re.IGNORECASE | re.MULTILINE | re.DOTALL,\n",
    "    ),\n",
    "    re.compile(r\"^PRESIDEN REPUBLIK INDONESIA\\s*\\n\", re.IGNORECASE | re.MULTILINE),\n",
    "    re.compile(r\"^\\s*-\\s*\\d+\\s*-\\s*$\", re.MULTILINE),\n",
    "    re.compile(r\"\\n{3,}\", re.MULTILINE),\n",
    "    re.compile(r\"^\\s*\\d+\\s*$\", re.MULTILINE),\n",
    "]\n",
    "\n",
    "LEGAL_TYPE_PATTERN = re.compile(\n",
    "    r\"(UNDANG-UNDANG|PERATURAN PEMERINTAH|KEPUTUSAN PRESIDEN|PERATURAN MENTERI|QANUN|PERATURAN DAERAH|PERATURAN KEPALA)\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "LEGAL_TYPE_ABBREV = {\n",
    "    \"UNDANG-UNDANG\": \"UU\",\n",
    "    \"PERATURAN PEMERINTAH\": \"PP\",\n",
    "    \"KEPUTUSAN PRESIDEN\": \"Keppres\",\n",
    "    \"PERATURAN MENTERI\": \"Permen\",\n",
    "    \"QANUN\": \"Qanun\",\n",
    "    \"PERATURAN DAERAH\": \"Perda\",\n",
    "    \"PERATURAN KEPALA\": \"Perkep\",\n",
    "}\n",
    "\n",
    "NUMBER_PATTERN = re.compile(r\"NOMOR\\s+(\\d+[A-Z]?)(?:[/-]\\d+)?\", re.IGNORECASE)\n",
    "YEAR_PATTERN = re.compile(r\"TAHUN\\s+(\\d{4})\", re.IGNORECASE)\n",
    "TOPIC_PATTERN = re.compile(\n",
    "    r\"TENTANG\\s+(.+?)(?=DENGAN RAHMAT|Menimbang|Mengingat|$)\", re.IGNORECASE | re.DOTALL\n",
    ")\n",
    "\n",
    "PASAL_PATTERN = re.compile(\n",
    "    r\"^Pasal\\s+(\\d+[A-Z]?)\\s*(.+?)(?=^Pasal\\s+\\d+|^BAB\\s+|^Penjelasan|\\Z)\",\n",
    "    re.IGNORECASE | re.MULTILINE | re.DOTALL,\n",
    ")\n",
    "BAB_PATTERN = re.compile(\n",
    "    r\"^BAB\\s+([IVX]+|[A-Z]+|\\d+)\\s+(.+?)(?=\\n|$)\", re.IGNORECASE | re.MULTILINE\n",
    ")\n",
    "AYAT_PATTERN = re.compile(\n",
    "    r\"(?:^|\\n)\\s*\\((\\d+)\\)\\s*(.+?)(?=(?:^|\\n)\\s*\\(\\d+\\)|$)\", re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "# --- CLASSES ---\n",
    "\n",
    "\n",
    "class LegalCleaner:\n",
    "    def clean(self, text: str) -> str:\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        cleaned = text\n",
    "        for pattern in NOISE_PATTERNS:\n",
    "            cleaned = pattern.sub(\"\", cleaned)\n",
    "        cleaned = re.sub(r\"\\s+\", \" \", cleaned)\n",
    "        cleaned = re.sub(r\"\\n\\s+\\n\", \"\\n\\n\", cleaned)\n",
    "        cleaned = re.sub(\n",
    "            r\"Pasal\\s+(\\d+[A-Z]?)\", r\"Pasal \\1\", cleaned, flags=re.IGNORECASE\n",
    "        )\n",
    "        return cleaned.strip()\n",
    "\n",
    "\n",
    "class LegalMetadataExtractor:\n",
    "    def extract(self, text: str) -> dict:\n",
    "        meta = {\n",
    "            \"type\": \"UNKNOWN\",\n",
    "            \"number\": \"UNKNOWN\",\n",
    "            \"year\": \"UNKNOWN\",\n",
    "            \"topic\": \"UNKNOWN\",\n",
    "        }\n",
    "\n",
    "        type_match = LEGAL_TYPE_PATTERN.search(text)\n",
    "        if type_match:\n",
    "            doc_type = type_match.group(1).upper()\n",
    "            meta[\"type\"] = doc_type\n",
    "            meta[\"type_abbrev\"] = LEGAL_TYPE_ABBREV.get(doc_type, doc_type)\n",
    "\n",
    "        num_match = NUMBER_PATTERN.search(text)\n",
    "        if num_match:\n",
    "            meta[\"number\"] = num_match.group(1)\n",
    "\n",
    "        year_match = YEAR_PATTERN.search(text)\n",
    "        if year_match:\n",
    "            meta[\"year\"] = year_match.group(1)\n",
    "\n",
    "        topic_match = TOPIC_PATTERN.search(text)\n",
    "        if topic_match:\n",
    "            meta[\"topic\"] = re.sub(r\"\\s+\", \" \", topic_match.group(1).strip())[:200]\n",
    "\n",
    "        return meta\n",
    "\n",
    "\n",
    "class LegalStructureParser:\n",
    "    def parse(self, text: str) -> dict:\n",
    "        structure = {\"batang_tubuh\": []}\n",
    "        bab_matches = list(BAB_PATTERN.finditer(text))\n",
    "        for i, match in enumerate(bab_matches):\n",
    "            bab_num = match.group(1)\n",
    "            bab_title = match.group(2).strip()\n",
    "            start = match.end()\n",
    "            end = bab_matches[i + 1].start() if i + 1 < len(bab_matches) else len(text)\n",
    "\n",
    "            # Simple pasal extraction for context finding\n",
    "            pasal_list = []\n",
    "            pasal_matches = list(PASAL_PATTERN.finditer(text[start:end]))\n",
    "            for p_match in pasal_matches:\n",
    "                pasal_list.append({\"number\": p_match.group(1)})\n",
    "\n",
    "            structure[\"batang_tubuh\"].append(\n",
    "                {\"number\": bab_num, \"title\": bab_title, \"pasal\": pasal_list}\n",
    "            )\n",
    "        return structure\n",
    "\n",
    "\n",
    "class LegalChunker:\n",
    "    def __init__(self, max_pasal_tokens=1000):\n",
    "        self.max_pasal_tokens = max_pasal_tokens\n",
    "\n",
    "    def chunk(self, text: str, metadata: dict, structure: dict) -> list:\n",
    "        chunks = []\n",
    "        pasal_matches = list(PASAL_PATTERN.finditer(text))\n",
    "\n",
    "        # If no pasal found, treat as one chunk (e.g. short decree)\n",
    "        if not pasal_matches:\n",
    "            context = self._build_context(metadata)\n",
    "            chunks.append(self._create_chunk(text, context, metadata))\n",
    "            return chunks\n",
    "\n",
    "        for match in pasal_matches:\n",
    "            pasal_num = match.group(1)\n",
    "            pasal_text = match.group(2).strip()\n",
    "\n",
    "            # Find BAB context\n",
    "            bab_context = self._find_bab_for_pasal(structure, pasal_num)\n",
    "\n",
    "            # Check length - split by Ayat if needed\n",
    "            if len(pasal_text) > 3000:  # Approx chars for 1000 tokens\n",
    "                ayat_matches = list(AYAT_PATTERN.finditer(pasal_text))\n",
    "                if ayat_matches:\n",
    "                    for am in ayat_matches:\n",
    "                        ayat_num = am.group(1)\n",
    "                        ayat_text = am.group(2).strip()\n",
    "                        context = self._build_context(\n",
    "                            metadata, bab_context, f\"Pasal {pasal_num}\"\n",
    "                        )\n",
    "                        chunks.append(\n",
    "                            self._create_chunk(\n",
    "                                f\"Ayat ({ayat_num})\\n{ayat_text}\",\n",
    "                                context,\n",
    "                                metadata,\n",
    "                                pasal_num,\n",
    "                            )\n",
    "                        )\n",
    "                    continue\n",
    "\n",
    "            # Default: Pasal as chunk\n",
    "            context = self._build_context(metadata, bab_context, f\"Pasal {pasal_num}\")\n",
    "            chunks.append(self._create_chunk(pasal_text, context, metadata, pasal_num))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def _build_context(self, meta, bab=None, pasal=None):\n",
    "        parts = [\n",
    "            meta.get(\"type_abbrev\", \"UNK\"),\n",
    "            f\"NO {meta.get('number', '?')}\",\n",
    "            f\"TAHUN {meta.get('year', '?')}\",\n",
    "            f\"TENTANG {meta.get('topic', 'UNK')}\",\n",
    "        ]\n",
    "        if bab:\n",
    "            parts.append(bab)\n",
    "        if pasal:\n",
    "            parts.append(pasal)\n",
    "        return f\"[CONTEXT: {' - '.join(parts)}]\"\n",
    "\n",
    "    def _create_chunk(self, content, context, meta, pasal_num=None):\n",
    "        chunk_text = f\"{context}\\n\\n{content}\"\n",
    "        c = {\"text\": chunk_text, \"has_context\": True}\n",
    "        c.update(meta)\n",
    "        if pasal_num:\n",
    "            c[\"pasal_number\"] = pasal_num\n",
    "        return c\n",
    "\n",
    "    def _find_bab_for_pasal(self, structure, pasal_num):\n",
    "        for bab in structure.get(\"batang_tubuh\", []):\n",
    "            for p in bab.get(\"pasal\", []):\n",
    "                if p.get(\"number\") == pasal_num:\n",
    "                    return f\"BAB {bab.get('number')} - {bab.get('title', '')}\"\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"‚úÖ Nuzantara Legal Engine Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• DOWNLOAD DATA\n",
    "!gdown 1Lx4y9TQ45uBUyvNzeHiHinxo_k_WOMmm -O /content/nuzantara_laws.zip\n",
    "\n",
    "# üì¶ Unzip\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "ZIP_PATH = \"/content/nuzantara_laws.zip\"\n",
    "EXTRACT_DIR = \"/content/nuzantara_laws\"\n",
    "\n",
    "if os.path.exists(ZIP_PATH):\n",
    "    print(\"üìÇ Extracting...\")\n",
    "    with zipfile.ZipFile(ZIP_PATH, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(EXTRACT_DIR)\n",
    "    print(\"‚úÖ Extraction complete!\")\n",
    "    SOURCE_DIR = EXTRACT_DIR\n",
    "else:\n",
    "    print(\"‚ùå Zip file not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Qdrant Connection & RE-INIT\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    timeout=60,\n",
    "    prefer_grpc=False,  # Force HTTP to avoid some SSL issues\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"‚ö†Ô∏è WIPING and Recreating collection '{COLLECTION_NAME}' for HIGH QUALITY INGESTION...\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    if client.collection_exists(COLLECTION_NAME):\n",
    "        client.delete_collection(COLLECTION_NAME)\n",
    "        print(f\"   Deleted existing collection '{COLLECTION_NAME}'\")\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=VECTOR_SIZE, distance=models.Distance.COSINE\n",
    "        ),\n",
    "    )\n",
    "    print(\"‚úÖ Collection wiped and ready!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error recreating collection: {e}\")\n",
    "    print(\"   Trying to proceed... maybe it was already deleted/created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ EXECUTION LOOP (The Butcher)\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "import requests\n",
    "\n",
    "# Initialize Engine Components\n",
    "cleaner = LegalCleaner()\n",
    "extractor = LegalMetadataExtractor()\n",
    "parser = LegalStructureParser()\n",
    "chunker = LegalChunker()\n",
    "\n",
    "# Find all PDFs recursively\n",
    "pdf_files = glob.glob(f\"{SOURCE_DIR}/**/*.pdf\", recursive=True)\n",
    "print(f\"üìö Found {len(pdf_files)} PDFs to ingest\")\n",
    "\n",
    "\n",
    "# PURE REQUESTS EMBEDDING FUNCTION\n",
    "@retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=2, max=10))\n",
    "def robust_embed(texts):\n",
    "    url = \"https://api.openai.com/v1/embeddings\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    # Replace newlines in text to avoid issues\n",
    "    clean_texts = [t.replace(\"\\n\", \" \") for t in texts]\n",
    "\n",
    "    data = {\"input\": clean_texts, \"model\": \"text-embedding-3-small\"}\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data, timeout=20, verify=False)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"OpenAI API Error {response.status_code}: {response.text}\")\n",
    "\n",
    "    return [d[\"embedding\"] for d in response.json()[\"data\"]]\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1  # REDUCED TO 1 FOR STABILITY\n",
    "EMBED_MINI_BATCH = 5  # ULTRA SMALL BATCHES (5 chunks)\n",
    "\n",
    "for i in tqdm(range(0, len(pdf_files), BATCH_SIZE), desc=\"Batch Processing\"):\n",
    "    batch_files = pdf_files[i : i + BATCH_SIZE]\n",
    "    batch_points = []\n",
    "\n",
    "    for pdf_file in batch_files:\n",
    "        try:\n",
    "            print(f\"Processing: {os.path.basename(pdf_file)}\")\n",
    "\n",
    "            # 0. Check PDF Header (Magic Bytes) to avoid corrupt files\n",
    "            with open(pdf_file, \"rb\") as f:\n",
    "                header = f.read(4)\n",
    "                if header != b\"%PDF\":\n",
    "                    print(\n",
    "                        f\"‚ö†Ô∏è Skipping invalid PDF (bad header): {os.path.basename(pdf_file)}\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "            # 1. Load\n",
    "            loader = PyPDFLoader(pdf_file)\n",
    "            pages = loader.load()\n",
    "            raw_text = \"\\n\".join([p.page_content for p in pages])\n",
    "\n",
    "            if len(raw_text) < 100:\n",
    "                print(\"   Skipping (too short/scanned)\")\n",
    "                continue\n",
    "\n",
    "            # 2. Clean\n",
    "            cleaned_text = cleaner.clean(raw_text)\n",
    "\n",
    "            # 3. Metadata\n",
    "            meta = extractor.extract(cleaned_text)\n",
    "            if meta[\"type\"] == \"UNKNOWN\":\n",
    "                meta[\"topic\"] = os.path.basename(pdf_file)\n",
    "\n",
    "            # 4. Structure\n",
    "            structure = parser.parse(cleaned_text)\n",
    "\n",
    "            # 5. Chunk (Butcher)\n",
    "            chunks = chunker.chunk(cleaned_text, meta, structure)\n",
    "\n",
    "            # 6. Prepare for Qdrant\n",
    "            if chunks:\n",
    "                print(f\"   Embedding {len(chunks)} chunks...\")\n",
    "                # Embed in MINI-BATCHES to avoid network overload\n",
    "                texts = [c[\"text\"] for c in chunks]\n",
    "                vectors = []\n",
    "\n",
    "                # Process mini-batches\n",
    "                for k in range(0, len(texts), EMBED_MINI_BATCH):\n",
    "                    mini_batch_texts = texts[k : k + EMBED_MINI_BATCH]\n",
    "                    try:\n",
    "                        mini_batch_vectors = robust_embed(mini_batch_texts)\n",
    "                        vectors.extend(mini_batch_vectors)\n",
    "                    except Exception as e:\n",
    "                        print(f\"   ‚ùå Failed to embed batch {k}: {e}\")\n",
    "                        raise e  # Re-raise to skip this file\n",
    "\n",
    "                for j, chunk in enumerate(chunks):\n",
    "                    point_id = f\"{meta.get('type_abbrev')}-{meta.get('number')}-{meta.get('year')}_chunk_{j}_{os.urandom(4).hex()}\"\n",
    "                    point_id = re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", point_id)\n",
    "\n",
    "                    batch_points.append(\n",
    "                        models.PointStruct(\n",
    "                            id=point_id, vector=vectors[j], payload=chunk\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {os.path.basename(pdf_file)}: {e}\")\n",
    "            print(\"   ‚è≠Ô∏è SKIPPING FILE due to repeated errors.\")\n",
    "            continue  # Explicitly continue to next file\n",
    "\n",
    "    # Upload Batch\n",
    "    if batch_points:\n",
    "        try:\n",
    "            client.upsert(collection_name=COLLECTION_NAME, points=batch_points)\n",
    "            print(f\"   ‚úÖ Uploaded {len(batch_points)} points.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Upload failed: {e}\")\n",
    "\n",
    "print(\"‚úÖ Ingestion Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
