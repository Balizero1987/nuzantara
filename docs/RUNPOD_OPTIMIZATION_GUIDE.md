# üöÄ RunPod LLAMA Optimization Guide
**Velocizzare LLAMA per 5-10 minuti giornalieri**

---

## üéØ Problema

RunPod worker serverless ha **cold start** (~1-2 minuti):
- Worker in "initializing" quando idle
- Prima richiesta va in coda (IN_QUEUE)
- Successive richieste processate velocemente

**Obiettivo**: Avere worker pronto per batch giornaliero (2 AM UTC)

---

## ‚úÖ Soluzione 1: Warm-up Request (IMPLEMENTATO)

**File**: `scripts/llama_nightly_worker.py:168-202`

```python
async def _warmup_llama(self):
    """Send warm-up request to initialize worker before batch"""
    # Invia dummy request 60 secondi prima del batch
    # Worker si inizializza in background
```

**Come funziona:**
1. Worker parte (cold start ~1min)
2. Dummy request completa
3. Worker rimane "warm" per ~60s
4. Batch inizia con worker gi√† pronto

**Costo**: 1 request extra (~$0.001/giorno)

**‚úÖ Gi√† deployato nel codice**

---

## üîß Soluzione 2: RunPod Min Workers = 1

**Dashboard RunPod:**
1. https://www.runpod.io/console/serverless
2. Endpoint: `Zantara_LLAMA_3.1` (itz2q5gmid4cyt)
3. Edit ‚Üí **Scaling Configuration**:
   ```
   Min Workers: 1  (invece di 0)
   Max Workers: 3
   Idle Timeout: 60 seconds
   ```

**Costo stimato:**
- Worker idle: $0.0001/s = $8.64/giorno (sempre acceso)
- Worker active: $0.00075/s (solo durante uso)

**Compromesso**:
- ‚úÖ Zero cold start (worker sempre pronto)
- ‚ùå Costo fisso $260/mese (~‚Ç¨240)

**Raccomandazione**: Usa solo se batch √® critico (es. real-time dashboard)

---

## ‚ö° Soluzione 3: Scheduled Worker Scale-up

**RunPod API** (chiamata 5 minuti prima del batch):

```python
# Railway Cron: 01:55 UTC (5 min prima del batch)
import httpx

async def scale_up_runpod():
    """Scale RunPod to min=1 5 minutes before batch"""
    async with httpx.AsyncClient() as client:
        await client.post(
            "https://api.runpod.io/v2/itz2q5gmid4cyt/config",
            headers={"Authorization": f"Bearer {RUNPOD_API_KEY}"},
            json={"minWorkers": 1}
        )

# Railway Cron: 02:15 UTC (dopo il batch)
async def scale_down_runpod():
    """Scale down to min=0 after batch"""
    # ... (same con minWorkers: 0)
```

**Costo**: ~$0.30/giorno (worker acceso solo 15-20 minuti)

**Pro**:
- Costo ottimale ($9/mese vs $260)
- Worker pronto quando serve

**Contro**:
- Richiede 2 cron jobs extra
- Complessit√† maggiore

---

## üìä Soluzione 4: Batch Optimization

**Ridurre numero richieste:**

```python
# Invece di 10 richieste sequenziali (5 min wait x 10 = 50min)
# Usa batch inference (se supportato da vLLM)

prompts = [prompt1, prompt2, ..., prompt10]

response = await client.post(
    runpod_endpoint,
    json={
        "input": {
            "prompts": prompts,  # Batch di 10 prompts
            "max_tokens": 300,
            "temperature": 0.4
        }
    }
)
```

**Risultato**: 1 richiesta invece di 10 (50min ‚Üí 5min)

**‚ö†Ô∏è Verifica se vLLM supporta batch inference nel tuo setup**

---

## üéØ Raccomandazione Finale

### Per uso sporadico (1x/giorno):

**Combo Warm-up + Batch Optimization:**
1. ‚úÖ Warm-up request (gi√† implementato)
2. ‚úÖ Timeout lungo (300s - gi√† fatto)
3. ‚è≥ Batch inference (se supportato da vLLM)
4. ‚ùå Min Workers = 1 (troppo costoso per uso sporadico)

**Costo**: ~$0.50/mese (solo batch time)

---

## üìù Next Steps

1. **Test warm-up** (gi√† nel codice):
   ```bash
   python3 scripts/llama_nightly_worker.py --days 1 --regenerate-cultural
   # Verifica log: "üî• Warming up LLAMA worker..."
   ```

2. **Monitor RunPod dashboard**:
   - Check worker status durante batch
   - Tempo "initializing" ‚Üí "running"

3. **Se ancora lento**, considera:
   - Min Workers = 1 (costo fisso)
   - O scheduled scale-up (costo ottimale)

---

## üîç Debug RunPod Response

Se worker risponde ma content √® vuoto:

```python
# Check response format
response = await client.post(runpod_endpoint, ...)
data = response.json()

print(f"Status: {data.get('status')}")
print(f"Output keys: {data.get('output', {}).keys()}")
print(f"Full response: {data}")
```

**Formati possibili:**
- `{"status": "IN_QUEUE"}` ‚Üí Worker overloaded
- `{"status": "COMPLETED", "output": {"text": "..."}}` ‚Üí vLLM format
- `{"status": "COMPLETED", "output": {"choices": [...]}}` ‚Üí OpenAI format

Il code gi√† gestisce tutti questi casi (cultural_knowledge_generator.py:326-359)

---

‚úÖ **Warm-up implementato e ready to deploy!**
