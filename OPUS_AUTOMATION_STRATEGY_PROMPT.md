# ü§ñ OPUS AUTOMATION STRATEGY PROMPT

**Target Model**: Claude Opus 4
**Task**: Design ultimate automation strategy for ZANTARA system
**Focus**: Local AI optimization + system-wide intelligent automation
**Context Date**: November 5, 2025

---

## üìã YOUR MISSION

You are Claude Opus, the most advanced reasoning model. Your task is to design a **comprehensive automation strategy** for the ZANTARA system that:

1. **Leverages local AI models** (Qwen 2.5 7B, LLaMA 3.1 8B, Mistral 7B, TinyLLaMA) to optimize the 25,422-document knowledge base across 16 collections
2. **Operates within hardware constraints** (Mac M4, 16GB RAM, max 5-6GB available for AI)
3. **Implements intelligent automations** across all system layers
4. **Maximizes efficiency** without running all models simultaneously

Your strategy should be **production-ready**, **cost-effective**, and **technically feasible**.

---

## üéØ CURRENT SYSTEM STATE

### **Architecture Overview**

**3 Production Services:**
- **Backend-TS** (Node.js 20 + Express + TypeScript) - https://nuzantara-backend.fly.dev
  - 2 CPU cores, 2GB RAM
  - 18+ working endpoints
  - Features: 9/38 implemented (23.7%)

- **Backend-RAG** (Python 3.11 + FastAPI + ChromaDB) - https://nuzantara-rag.fly.dev
  - 2 CPU cores, 2GB RAM
  - 10GB volume (ChromaDB: 161MB)
  - 25,422 documents indexed

- **Frontend** (React + TypeScript + Vite) - https://zantara.balizero.com
  - Cloudflare Pages
  - Global CDN

**Infrastructure:**
- Fly.io (Singapore) for backends
- Cloudflare Pages for frontend
- Redis Cloud (AWS Singapore) for caching
- ChromaDB SQLite (161MB) for vector storage

**Performance Metrics:**
- Cached queries: ~120ms
- v3 unified (quick): ~500ms
- v3 comprehensive: <2s
- Uptime: 99%+
- Cache hit rate: 60-80%

---

## üìö KNOWLEDGE BASE DETAILS (Critical for Local AI Strategy)

### **Current Collections (16 total)**

**Populated Collections (10):**
1. **knowledge_base** (8,923 docs) - Blockchain, Bitcoin, Satoshi Nakamoto, Cryptocurrency
2. **kbli_unified** (8,887 docs) - KBLI 2020 Indonesian Business Classification codes
3. **legal_unified** (5,041 docs) - Indonesian Laws (UU, PP, Permen)
4. **visa_oracle** (1,612 docs) - Immigration & Visa procedures
5. **tax_genius** (895 docs) - Tax scenarios & calculations
6. **property_unified** (29 docs) - Property investment Indonesia
7. **bali_zero_pricing** (29 docs) - Service pricing
8. **property_listings** (2 docs) - Property listings
9. **tax_updates** (2 docs) - Recent tax updates
10. **legal_updates** (2 docs) - Recent legal updates

**Empty Collections (6):**
11. **kbli_comprehensive** (0 docs) - Planned: Extended KBLI
12. **kb_indonesian** (0 docs) - Planned: Indonesian language KB
13. **tax_knowledge** (0 docs) - Planned: Comprehensive tax
14. **cultural_insights** (0 docs) - Planned: Business culture
15. **zantara_memories** (0 docs) - Planned: User interaction memories
16. **property_knowledge** (0 docs) - Planned: Property market data

**Total Documents**: 25,422 across 10 active collections

### **Current Usage Patterns**
- KBLI lookups: ~40% of queries
- Legal/regulatory: ~25%
- Immigration/visa: ~20%
- Taxation: ~10%
- Property: ~3%
- Other: ~2%

### **Embedding Model**
- Model: sentence-transformers/all-MiniLM-L6-v2
- Dimensions: 384
- Languages: Multi-language support
- Performance: Fast, good accuracy

---

## üíª HARDWARE CONSTRAINTS

### **Available Resources**
- **Machine**: Mac M4
- **Total RAM**: 16GB
- **Available for AI**: 5-6GB maximum
- **CPU**: Apple Silicon (excellent for ML)
- **Storage**: Sufficient for models

### **Local AI Models Available**

**Option 1: Qwen 2.5 7B**
- Size: ~4.5GB (Q4 quantized)
- Strengths: Excellent reasoning, multi-language
- Speed: Fast on M4
- Best for: Complex queries, reasoning tasks

**Option 2: LLaMA 3.1 8B**
- Size: ~5GB (Q4 quantized)
- Strengths: General purpose, instruction following
- Speed: Good on M4
- Best for: General queries, classification

**Option 3: Mistral 7B**
- Size: ~4GB (Q4 quantized)
- Strengths: Fast, efficient, good quality
- Speed: Fastest on M4
- Best for: Quick responses, embeddings

**Option 4: TinyLLaMA**
- Size: ~1GB (Q4 quantized)
- Strengths: Ultra-fast, low resource
- Speed: Blazing fast
- Best for: Classification, routing, simple tasks

### **Constraint: Don't Run Simultaneously**
- Maximum 1-2 models loaded at once
- Smart model swapping strategy needed
- Minimize cold start times
- Intelligent routing to right model

---

## üéØ FEATURES STATUS

### **‚úÖ Implemented Features (9/38 = 23.7%)**
1. CORS & Security (Helmet, rate limiting)
2. Metrics & Observability (Prometheus)
3. Advanced Health Routes
4. Redis Cache (7 endpoints)
5. Correlation Middleware
6. Performance Routes
7. Bali Zero Chat (KBLI, pricing)
8. ZANTARA v3 Œ© (unified, collective, ecosystem)
9. Team Authentication (JWT, 22 members)

### **‚ùå Missing Features (29/38 = 76.3%)**

**Authentication & User Management (5):**
- User registration & login
- Password management
- Profile management
- Email verification
- Token refresh

**AI & Knowledge Base (4):**
- RAG query direct
- AI models list
- AI embeddings
- AI completions

**Business Logic (6):**
- KBLI complete analysis
- Legal requirements
- License check
- Compliance status
- Risk assessment
- Document preparation

**Finance & Pricing (5):**
- Pricing plans
- Price calculator
- Subscription status
- Subscription upgrade
- Invoice details

**Admin & System (6):**
- User management admin
- System analytics
- Maintenance mode
- System logs
- System backup
- Feature flags

**Utility (3):**
- File upload
- File download
- Data validation

---

## üéØ YOUR STRATEGIC OBJECTIVES

### **Primary Objective: Local AI Optimization**

Design a system where **local AI models** handle:

1. **Query Routing & Classification**
   - TinyLLaMA classifies incoming queries instantly
   - Routes to appropriate collection(s)
   - Determines query complexity
   - Decides if Claude API needed or local sufficient

2. **Embedding Generation**
   - Replace/supplement MiniLM with local models
   - Faster embedding generation
   - Better multi-language support (Indonesian especially)
   - Cost reduction (no API calls)

3. **Response Generation**
   - Simple queries: Local models respond directly
   - Complex queries: Local pre-processing + Claude final answer
   - Hybrid approach for cost optimization

4. **Collection Optimization**
   - Analyze 16 collections for redundancy
   - Propose compaction strategy
   - Suggest optimal collection structure
   - Reduce from 16 to X collections (you decide optimal number)

5. **Memory Management**
   - Smart model loading/unloading
   - Prediction of which model needed next
   - Minimize cold starts
   - Maximize cache usage

### **Secondary Objective: System-Wide Automation**

Design intelligent automations for:

1. **Knowledge Base Management**
   - Auto-update from official sources
   - Document deduplication
   - Quality scoring
   - Automatic re-indexing

2. **Caching Intelligence**
   - Predictive cache warming
   - Smart TTL calculation per query type
   - Cache invalidation strategy
   - Pattern-based prefetching

3. **Monitoring & Self-Healing**
   - Automatic performance optimization
   - Error pattern detection
   - Auto-scaling triggers
   - Health degradation recovery

4. **User Experience**
   - Query intent prediction
   - Personalization based on history
   - Proactive suggestions
   - Auto-translation (EN/ID/IT)

5. **Development & Deployment**
   - CI/CD optimization
   - Automated testing
   - Feature flag management
   - A/B testing framework

---

## üìê DESIGN CONSTRAINTS & REQUIREMENTS

### **Hard Constraints**
- ‚úÖ Maximum 5-6GB RAM for AI models
- ‚úÖ Must maintain <2s response time for 95% of queries
- ‚úÖ Must not degrade current 99%+ uptime
- ‚úÖ Must work on Mac M4 (Apple Silicon)
- ‚úÖ Must be production-deployable (not just local dev)

### **Soft Constraints**
- ‚ö†Ô∏è Prefer cost reduction (less Claude API calls)
- ‚ö†Ô∏è Prefer faster response times
- ‚ö†Ô∏è Prefer simpler architecture (maintainability)
- ‚ö†Ô∏è Prefer scalability

### **Technical Requirements**
- Must integrate with existing TypeScript backend
- Must integrate with existing Python RAG backend
- Must support 3 languages (English, Indonesian, Italian)
- Must handle 100+ concurrent users
- Must be monitorable (metrics, logs)

---

## üí° STRATEGIC QUESTIONS TO ANSWER

### **1. Collection Optimization**
- How many collections are optimal? (current: 16)
- Which collections can be merged?
- Should we separate by language or domain?
- How to handle updates efficiently?

### **2. Local AI Architecture**
- Which model for which task?
- How to orchestrate model loading/unloading?
- When to use local vs Claude API?
- How to handle model failures?

### **3. Memory Management**
- How to predict next model needed?
- How to minimize swap latency?
- Should we keep TinyLLaMA always loaded?
- Cache strategy for model outputs?

### **4. Query Processing Pipeline**
- What's the optimal flow? (classification ‚Üí routing ‚Üí processing ‚Üí response)
- Where to insert local AI?
- How to decide Claude vs local?
- How to handle multi-domain queries?

### **5. Deployment Strategy**
- Run local AI on Mac or deploy to Fly.io?
- If Fly.io, which machine size?
- How to handle model updates?
- Rollback strategy if local AI fails?

### **6. Performance Optimization**
- How to achieve <500ms with local AI?
- Caching strategy for model outputs?
- Parallel processing opportunities?
- Batch processing for efficiency?

---

## üìä SUCCESS METRICS

Your strategy should optimize for:

1. **Response Time**
   - Target: <300ms for 80% of queries (currently ~500ms)
   - Max: <2s for 95% (maintain current)

2. **Cost Reduction**
   - Target: 50-70% reduction in Claude API costs
   - Measure: API calls per day

3. **Accuracy**
   - Target: Maintain 85-95% relevance
   - Must not degrade with local AI

4. **Resource Usage**
   - Target: Stay within 5-6GB RAM limit
   - Target: <50% CPU average

5. **Uptime**
   - Target: Maintain 99%+ uptime
   - Must have fallback if local AI fails

6. **Developer Experience**
   - Target: Simple to maintain
   - Target: Easy to add new models
   - Target: Clear debugging

---

## üé® DELIVERABLES REQUESTED

Please provide a comprehensive strategy document with:

### **1. Executive Summary (1 page)**
- High-level approach
- Key innovations
- Expected benefits
- Resource requirements

### **2. Local AI Architecture (3-5 pages)**
- Model selection & roles
- Memory management strategy
- Query routing algorithm
- Fallback mechanisms
- Performance optimization

### **3. Collection Optimization Plan (2-3 pages)**
- Proposed collection structure (reduce from 16)
- Merging/splitting strategy
- Document redistribution plan
- Migration approach

### **4. System-Wide Automations (3-4 pages)**
- Knowledge base auto-update
- Caching intelligence
- Monitoring & self-healing
- Development automation

### **5. Implementation Roadmap (2 pages)**
- Phase 1: Foundation (weeks 1-2)
- Phase 2: Core Features (weeks 3-4)
- Phase 3: Optimization (weeks 5-6)
- Phase 4: Production (weeks 7-8)

### **6. Technical Specifications (2-3 pages)**
- API contracts
- Model orchestration logic
- Deployment architecture
- Monitoring setup

### **7. Risk Analysis & Mitigation (1-2 pages)**
- Technical risks
- Performance risks
- Operational risks
- Mitigation strategies

### **8. Cost-Benefit Analysis (1 page)**
- Development effort estimate
- Expected cost savings
- Performance improvements
- ROI calculation

### **9. Code Examples (2-3 pages)**
- Model orchestrator pseudocode
- Query router implementation
- Collection optimizer algorithm
- Key integration points

### **10. Testing Strategy (1-2 pages)**
- Unit tests
- Integration tests
- Performance benchmarks
- Rollback criteria

---

## üîç CONTEXT FILES PROVIDED

The following files contain detailed system information:

1. **INFRASTRUCTURE_OVERVIEW.md** (424 lines)
   - Complete architecture
   - All services & endpoints
   - Performance metrics

2. **KNOWLEDGE_BASE_MAP.md** (610 lines)
   - All 16 collections detailed
   - Document counts & content
   - Search methods

3. **SYSTEM_PROMPT_REFERENCE.md** (424 lines)
   - AI configuration
   - 22 team members
   - 8 knowledge domains

4. **WORKFLOW_COMPLETO.md** (460 lines)
   - Deployment workflows
   - Development procedures

5. **START_HERE.md** (491 lines)
   - System overview
   - Quick start guide

6. **DEV_ONBOARDING_GUIDE.md** (850+ lines)
   - Complete setup guide
   - Code patterns
   - Common issues

You should reference these files in your strategy where relevant.

---

## üéØ STRATEGIC FOCUS AREAS

### **Priority 1: Local AI for Query Processing (HIGHEST PRIORITY)**

Design the **optimal architecture** for using local AI models to handle query processing:

**Key Decisions:**
- Which model for classification? (probably TinyLLaMA)
- Which model for embeddings? (probably Mistral 7B)
- Which model for response generation? (probably Qwen 2.5 7B or LLaMA 3.1)
- How to orchestrate without exceeding 5-6GB RAM?

**Example Flow to Optimize:**
```
User Query ‚Üí [TinyLLaMA: Classify & Route] ‚Üí [Mistral: Generate Embedding]
‚Üí [ChromaDB: Vector Search] ‚Üí [Qwen/LLaMA: Generate Response OR Claude API]
‚Üí Response to User
```

**Memory Strategy:**
- Option A: Keep TinyLLaMA always loaded (1GB), swap others as needed
- Option B: Predictive loading based on query history
- Option C: Hybrid approach
- You decide best approach!

**When to use Claude API vs Local:**
- Simple queries ‚Üí Local
- Complex reasoning ‚Üí Claude
- Multi-domain queries ‚Üí ?
- Legal/compliance ‚Üí ?
- You define the rules!

### **Priority 2: Collection Optimization**

Current state: **16 collections, many underutilized**

**Problems to Solve:**
- 6 collections are empty (overhead)
- Some collections have only 2 documents (property_listings, tax_updates, legal_updates)
- Overlap between collections (e.g., legal_unified + legal_updates)
- knowledge_base (blockchain) not relevant to core business

**Your Tasks:**
1. Propose optimal number of collections (8? 10? 12?)
2. Define merging strategy
3. Explain benefits of new structure
4. Provide migration plan

**Example Proposal Structure (you create your own!):**
```
Current 16 collections ‚Üí Proposed X collections

New Structure:
1. business_intelligence (merge: kbli_unified + kbli_comprehensive)
2. legal_regulatory (merge: legal_unified + legal_updates)
3. immigration_visa (keep: visa_oracle)
4. taxation (merge: tax_genius + tax_updates + tax_knowledge)
5. property_real_estate (merge: property_unified + property_listings + property_knowledge)
6. service_pricing (keep: bali_zero_pricing)
7. cultural_business (new: cultural_insights)
8. user_memories (new: zantara_memories)

Remove: knowledge_base (blockchain - not core business)
```

### **Priority 3: Intelligent Caching**

Current: Simple Redis cache with fixed TTL

**Upgrade to Smart Caching:**
- Query similarity detection (cache similar queries)
- Predictive cache warming (pre-cache likely queries)
- Dynamic TTL based on query type
- Cache invalidation intelligence

**Example:**
```
Query: "restaurant KBLI code"
‚Üí Generate semantic hash
‚Üí Check if similar query cached (cosine similarity > 0.95)
‚Üí If yes, return cached (even if exact query different)
‚Üí If no, process and cache with smart TTL

TTL Strategy:
- KBLI queries: 7 days (data stable)
- Legal queries: 1 day (regulations change)
- Pricing queries: 1 hour (prices may update)
- Tax queries: 1 day (tax rules semi-stable)
```

### **Priority 4: Automated Knowledge Base Updates**

Current: Manual updates

**Upgrade to Automated System:**
- Scheduled scraping of official sources (JDIH, Kemenkumham, etc.)
- Automatic document processing
- Change detection
- Auto-reindexing
- Notification system

**Example Sources to Scrape:**
- https://jdih.kemenkumham.go.id (legal documents)
- https://peraturan.go.id (regulations)
- https://www.imigrasi.go.id (immigration updates)
- https://www.kemenkumham.go.id (ministry updates)

### **Priority 5: Self-Healing & Monitoring**

Current: Manual monitoring via health endpoints

**Upgrade to Intelligent System:**
- Anomaly detection (response time spikes, error rate increases)
- Automatic remediation (restart services, clear caches)
- Predictive alerts (before problems occur)
- Performance auto-optimization

---

## üöÄ EXAMPLE USE CASES TO OPTIMIZE

### **Use Case 1: KBLI Lookup**
```
Current Flow:
User: "restaurant KBLI code"
‚Üí Backend-TS receives
‚Üí Calls Claude API ($$$)
‚Üí Claude queries ChromaDB via RAG
‚Üí Response in ~500ms

Optimized Flow with Local AI:
User: "restaurant KBLI code"
‚Üí TinyLLaMA classifies: [simple KBLI lookup]
‚Üí Mistral generates embedding
‚Üí Direct ChromaDB search (no Claude)
‚Üí TinyLLaMA formats response
‚Üí Response in ~150ms, $0 cost
```

### **Use Case 2: Complex Legal Query**
```
Current Flow:
User: "Can a foreigner own a PT PMA with 100% shares in hospitality sector?"
‚Üí Claude API call
‚Üí Multiple domain search (legal + kbli + visa)
‚Üí Claude synthesizes
‚Üí Response in ~2000ms

Optimized Flow with Local AI:
User: "Can a foreigner own..."
‚Üí TinyLLaMA classifies: [complex, multi-domain]
‚Üí Qwen 2.5 pre-processes query
‚Üí Mistral generates embedding
‚Üí ChromaDB multi-collection search
‚Üí Qwen 2.5 attempts synthesis
‚Üí IF confidence < 0.8: fallback to Claude
‚Üí IF confidence >= 0.8: Qwen responds
‚Üí Response in ~800ms (local) or ~2000ms (Claude fallback)
‚Üí Cost: 70% queries answered locally
```

### **Use Case 3: Routine Tax Calculation**
```
Current Flow:
User: "Calculate PPh 21 for salary IDR 15,000,000"
‚Üí Claude API call
‚Üí Tax calculation
‚Üí Response in ~1000ms

Optimized Flow with Local AI:
User: "Calculate PPh 21..."
‚Üí TinyLLaMA classifies: [calculation, tax]
‚Üí Load pre-computed tax tables
‚Üí LLaMA 3.1 performs calculation
‚Üí Cached result (similar salary calculations common)
‚Üí Response in ~200ms, $0 cost
```

---

## üéì ADVANCED CONSIDERATIONS

### **1. Multilingual Optimization**
- Indonesian language support critical
- Qwen 2.5 excellent for Indonesian
- Translation layer vs native processing?
- When to auto-translate vs native response?

### **2. Fine-Tuning Opportunities**
- Should we fine-tune a model on our 25,422 docs?
- LoRA adapters for domain-specific knowledge?
- Cost/benefit analysis?

### **3. Hybrid Cloud-Local Strategy**
- Local AI on Mac for development
- Deploy to Fly.io GPU machines for production?
- Cost comparison: local vs cloud GPU
- Latency considerations

### **4. Progressive Enhancement**
- Phase 1: TinyLLaMA routing only
- Phase 2: Add Mistral embeddings
- Phase 3: Add Qwen/LLaMA response generation
- Phase 4: Full hybrid local+Claude system

### **5. Fail-Safe Mechanisms**
- If local AI fails ‚Üí immediate Claude fallback
- If ChromaDB slow ‚Üí cached responses
- If embeddings fail ‚Üí keyword fallback
- Graceful degradation strategy

---

## üìù OUTPUT FORMAT REQUIREMENTS

Please structure your response as:

```markdown
# ZANTARA AUTOMATION STRATEGY
## Designed by Claude Opus

### EXECUTIVE SUMMARY
[1 page high-level overview]

### PART 1: LOCAL AI ARCHITECTURE
#### 1.1 Model Selection & Roles
[Which model for what task]

#### 1.2 Memory Management
[How to stay within 5-6GB]

#### 1.3 Query Processing Pipeline
[Detailed flow diagrams]

#### 1.4 Fallback & Error Handling
[Graceful degradation]

### PART 2: COLLECTION OPTIMIZATION
#### 2.1 Current State Analysis
[Problems with 16 collections]

#### 2.2 Proposed Structure
[Optimal number & organization]

#### 2.3 Migration Plan
[How to transition]

### PART 3: SYSTEM-WIDE AUTOMATIONS
#### 3.1 Knowledge Base Auto-Update
[Scraping & processing automation]

#### 3.2 Intelligent Caching
[Semantic caching strategy]

#### 3.3 Monitoring & Self-Healing
[Anomaly detection & auto-remediation]

#### 3.4 Development Automation
[CI/CD optimization]

### PART 4: IMPLEMENTATION ROADMAP
#### Phase 1: Foundation (Weeks 1-2)
[Specific tasks with estimates]

#### Phase 2: Core Features (Weeks 3-4)
[Build main automation]

#### Phase 3: Optimization (Weeks 5-6)
[Performance tuning]

#### Phase 4: Production (Weeks 7-8)
[Deploy & monitor]

### PART 5: TECHNICAL SPECIFICATIONS
#### 5.1 API Contracts
[New endpoints needed]

#### 5.2 Model Orchestrator
[Core orchestration logic]

#### 5.3 Deployment Architecture
[Where to run what]

### PART 6: RISK ANALYSIS
[Risks & mitigation strategies]

### PART 7: COST-BENEFIT ANALYSIS
[Development cost vs savings]

### PART 8: CODE EXAMPLES
[Pseudocode for key components]

### PART 9: TESTING STRATEGY
[How to validate everything works]

### PART 10: MONITORING & METRICS
[What to track for success]
```

---

## üî• SPECIAL INSTRUCTIONS FOR OPUS

1. **Think Deep**: This is a complex optimization problem. Use your full reasoning capability.

2. **Be Specific**: Don't just say "use model X for task Y". Explain WHY, with technical justification.

3. **Consider Trade-offs**: Every decision has pros/cons. Explicitly state them.

4. **Provide Numbers**: Estimate latencies, memory usage, cost savings with calculations.

5. **Show Examples**: Code snippets, flow diagrams, configuration examples.

6. **Think Production**: Not just "cool tech" - actually deployable, maintainable, monitorable.

7. **Optimize for Indonesian Context**: This system serves Indonesian business market. Consider language, regulations, business practices.

8. **Challenge Assumptions**: If current architecture is suboptimal, propose radical changes. Be bold.

9. **Consider Failure Modes**: What breaks? How to recover? Fallback strategies?

10. **Balance Innovation & Pragmatism**: Cutting-edge + actually works in 8 weeks.

---

## ‚ö° KEY INSIGHTS TO LEVERAGE

1. **Mac M4 is Powerful**: Apple Silicon crushes ML workloads. Use this advantage.

2. **Most Queries are Simple**: 40% are KBLI lookups. Don't need Claude for these.

3. **Indonesian Language**: Qwen 2.5 excels here. Mistral also good. LLaMA decent.

4. **Collections are Bloated**: 16 collections with 6 empty. Clear optimization opportunity.

5. **Cache Hit Rate**: 60-80% means many repeated queries. Semantic caching = huge win.

6. **Current Cost**: Every Claude API call costs money. 50-70% reduction = significant savings.

7. **Response Time**: ~500ms average. Local AI can push this to ~150-300ms for simple queries.

8. **Knowledge Base**: 161MB, very manageable. Can load entirely in RAM if needed.

---

## üéØ FINAL CHALLENGE

Design a system where:

‚úÖ **80% of queries** answered by local AI (<300ms, $0 cost)
‚úÖ **20% of queries** use Claude (complex reasoning, high value)
‚úÖ **Collections reduced** from 16 to optimal number (8-12)
‚úÖ **Memory usage** stays under 6GB
‚úÖ **Performance** better than current (not worse!)
‚úÖ **Reliability** 99%+ maintained
‚úÖ **Implementable** in 8 weeks by small team
‚úÖ **Maintainable** long-term

**This is your magnum opus. Show us what Claude Opus can do.** üöÄ

---

**END OF PROMPT**

*Prepared for: Claude Opus 4*
*Prepared by: ZANTARA Dev Team*
*Date: November 5, 2025*
*Version: 1.0.0*
