# INTEL SCRAPING SYSTEM - Swiss-Watch Configuration
# All configuration centralized in one place

# ========================================
# SCRAPER CONFIGURATION
# ========================================
scraper:
  max_articles_per_source: 15
  max_content_age_days: 14  # Only scrape articles newer than this
  timeout_seconds: 45
  retry_attempts: 2
  concurrent_sites: 5
  dns_timeout: 10
  verify_ssl: false  # Gov sites have SSL issues

  # Rate limiting
  delay_min: 1
  delay_max: 3
  max_concurrent_domains: 2

  # Quality filters
  min_content_length: 100
  min_word_count: 50
  min_title_length: 10

# ========================================
# RUNPOD CONFIGURATION
# ========================================
runpod:
  endpoint: "https://api.runpod.ai/v2/itz2q5gmid4cyt"
  api_key: "${RUNPOD_API_KEY}"  # From environment
  timeout_minutes: 8
  max_articles_for_journal: 100
  max_tokens: 4000
  temperature: 0.7
  top_p: 0.9
  poll_interval_seconds: 5
  max_poll_attempts: 96

# ========================================
# OLLAMA FALLBACK
# ========================================
ollama:
  enabled: true
  base_url: "http://localhost:11434"
  model: "llama3.2"
  timeout_seconds: 60
  max_tokens: 2000
  temperature: 0.7
  max_articles_for_fallback: 50

# ========================================
# RAG BACKEND
# ========================================
rag:
  backend_url: "https://zantara-rag-backend-himaadsxua-ew.a.run.app"
  batch_size: 10
  embedding_timeout: 30
  storage_timeout: 30
  max_workers: 5

# ========================================
# CONTENT CREATION
# ========================================
content:
  anthropic_api_key: "${ANTHROPIC_API_KEY}"  # From environment
  model: "claude-3-5-haiku-20241022"
  max_tokens: 2000
  max_workers: 3

# ========================================
# FILTERS
# ========================================
filters:
  # Quality filter
  quality_threshold: 0.7
  impact_threshold: "medium"

  # Deduplication
  duplicate_similarity_threshold: 0.85
  use_cache: true
  cache_backend: "sqlite"  # sqlite | redis
  cache_ttl_days: 30

  # Date filter
  max_article_age_days: 14
  require_valid_date: true

  # LLAMA categories (use NewsIntelligentFilter)
  llama_categories:
    - "dev_code"
    - "future_trends"
    - "news"

# ========================================
# STATE MANAGEMENT
# ========================================
state:
  backend: "sqlite"  # sqlite | redis
  db_path: "INTEL_SCRAPING/data/.state/scraping_state.db"
  enable_resume: true
  checkpoint_interval: 100  # Save state every N articles

# ========================================
# MONITORING & METRICS
# ========================================
monitoring:
  enabled: true
  metrics_dir: "INTEL_SCRAPING/data/metrics"
  log_level: "INFO"  # DEBUG | INFO | WARNING | ERROR
  structured_logging: true

  # Prometheus metrics
  prometheus_enabled: false
  prometheus_port: 8000

  # Alerts
  alerts_enabled: true
  alert_channels:
    - type: "log"
    # - type: "slack"
    #   webhook_url: "${SLACK_WEBHOOK_URL}"
    # - type: "email"
    #   smtp_host: "smtp.gmail.com"
    #   smtp_port: 587
    #   from_email: "zero@balizero.com"
    #   to_emails:
    #     - "zero@balizero.com"

# ========================================
# SCHEDULING
# ========================================
scheduling:
  enabled: false
  cron_expression: "0 */6 * * *"  # Every 6 hours
  timezone: "Asia/Jakarta"

  # Per-category schedules (override global)
  category_schedules:
    ai_tech:
      cron: "0 */3 * * *"  # Every 3 hours
    visa_immigration:
      cron: "0 9 * * *"  # Daily at 9 AM

# ========================================
# EMAIL CONFIGURATION
# ========================================
email:
  enabled: true
  skip_on_ci: true  # Skip emails in GitHub Actions
  sender: "zero@balizero.com"

  # Category assignments
  regular_categories:
    visa_immigration: "consulting@balizero.com"
    tax: "faisha@balizero.com"
    property_law: "krisna@balizero.com"
    banking_finance: "surya@balizero.com"
    social_media: "sahira@balizero.com"
    employment_law: "amanda@balizero.com"
    jobs: "Anton@balizero.com"
    competitor_intel: "damar@balizero.com"
    lifestyle: "dea.au.tax@balizero.com"
    business_setup: "dea@balizero.com"
    macro_policy: "dea@balizero.com"
    regulatory_changes: "consulting@balizero.com"

  llama_categories:
    ai_tech: "zero@balizero.com"
    dev_code: "zero@balizero.com"
    future_trends: "zero@balizero.com"
    news: "zero@balizero.com"

# ========================================
# OUTPUT CONFIGURATION
# ========================================
output:
  base_dir: "INTEL_SCRAPING/data/INTEL_SCRAPING"
  raw_dir: "raw"
  filtered_dir: "filtered"
  articles_dir: "INTEL_SCRAPING/data/INTEL_SCRAPING/articles"
  journal_dir: "INTEL_SCRAPING/data/JOURNAL"

  # File formats
  save_json: true
  save_markdown: true
  save_html: false

  # Compression
  compress_old_files: true
  compress_after_days: 30
