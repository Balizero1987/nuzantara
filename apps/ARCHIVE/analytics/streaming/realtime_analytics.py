#!/usr/bin/env python3
"""
Zantara Bridge v4.1.0 - Real-time Streaming Analytics
Analytics Engine - Stream D Implementation
Real-time event processing and analytics
"""

import os
import json
import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Callable
import pandas as pd
import numpy as np
from concurrent.futures import ThreadPoolExecutor
import threading
import time

# Google Cloud Libraries
from google.cloud import pubsub_v1, bigquery, logging as cloud_logging
from google.cloud.pubsub_v1.types import PubsubMessage
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions

# Streaming Libraries
import websocket
import requests
from collections import deque, defaultdict

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ZantaraRealtimeAnalytics:
    """Real-time streaming analytics for Zantara Bridge"""
    
    def __init__(self, project_id: str = "involuted-box-469105-r0"):
        self.project_id = project_id
        self.subscription_name = "zantara-realtime-events"
        self.topic_name = "zantara-bridge-events"
        self.dataset_id = "zantara_analytics"
        
        # Initialize clients
        self.subscriber = pubsub_v1.SubscriberClient()
        self.publisher = pubsub_v1.PublisherClient()
        self.bq_client = bigquery.Client(project=project_id)
        
        # Real-time data stores
        self.event_buffer = deque(maxlen=10000)
        self.metrics_cache = defaultdict(lambda: deque(maxlen=1000))
        self.alert_thresholds = {
            'response_time_ms': 2000,
            'error_rate_percent': 5.0,
            'requests_per_second': 1000
        }
        
        # Event processors
        self.event_processors = []
        self.alert_callbacks = []
        
        # Streaming state
        self.is_streaming = False
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        logger.info("Zantara Real-time Analytics initialized")
    
    def setup_pubsub_resources(self) -> None:
        """Setup Pub/Sub topic and subscription"""
        
        logger.info("Setting up Pub/Sub resources")
        
        try:
            # Create topic
            topic_path = self.publisher.topic_path(self.project_id, self.topic_name)
            try:
                self.publisher.create_topic(request={"name": topic_path})
                logger.info(f"Created topic: {topic_path}")
            except Exception as e:
                if "already exists" in str(e).lower():
                    logger.info(f"Topic already exists: {topic_path}")
                else:
                    raise e
            
            # Create subscription
            subscription_path = self.subscriber.subscription_path(
                self.project_id, self.subscription_name
            )
            try:
                self.subscriber.create_subscription(
                    request={\n                        \"name\": subscription_path,\n                        \"topic\": topic_path,\n                        \"ack_deadline_seconds\": 60\n                    }\n                )\n                logger.info(f\"Created subscription: {subscription_path}\")\n            except Exception as e:\n                if \"already exists\" in str(e).lower():\n                    logger.info(f\"Subscription already exists: {subscription_path}\")\n                else:\n                    raise e\n                    \n        except Exception as e:\n            logger.error(f\"Error setting up Pub/Sub resources: {e}\")\n    \n    def add_event_processor(self, processor: Callable[[Dict[str, Any]], None]) -> None:\n        \"\"\"Add a custom event processor function\"\"\"\n        self.event_processors.append(processor)\n        logger.info(\"Added custom event processor\")\n    \n    def add_alert_callback(self, callback: Callable[[Dict[str, Any]], None]) -> None:\n        \"\"\"Add a custom alert callback function\"\"\"\n        self.alert_callbacks.append(callback)\n        logger.info(\"Added custom alert callback\")\n    \n    def process_event(self, event_data: Dict[str, Any]) -> None:\n        \"\"\"Process a single event through the analytics pipeline\"\"\"\n        \n        try:\n            # Add timestamp if not present\n            if 'timestamp' not in event_data:\n                event_data['timestamp'] = datetime.utcnow().isoformat()\n            \n            # Add to buffer\n            self.event_buffer.append(event_data)\n            \n            # Extract metrics\n            if event_data.get('event_type') == 'api_request':\n                self._process_api_request_event(event_data)\n            elif event_data.get('event_type') == 'system_metric':\n                self._process_system_metric_event(event_data)\n            elif event_data.get('event_type') == 'user_action':\n                self._process_user_action_event(event_data)\n            \n            # Run custom processors\n            for processor in self.event_processors:\n                try:\n                    processor(event_data)\n                except Exception as e:\n                    logger.error(f\"Error in custom processor: {e}\")\n            \n            # Check for alerts\n            self._check_alerts(event_data)\n            \n        except Exception as e:\n            logger.error(f\"Error processing event: {e}\")\n    \n    def _process_api_request_event(self, event: Dict[str, Any]) -> None:\n        \"\"\"Process API request events\"\"\"\n        \n        payload = event.get('payload', {})\n        \n        # Extract key metrics\n        response_time = payload.get('response_time_ms', 0)\n        status_code = payload.get('status_code', 200)\n        endpoint = payload.get('endpoint', 'unknown')\n        \n        # Update metrics cache\n        current_time = time.time()\n        self.metrics_cache['response_times'].append({\n            'timestamp': current_time,\n            'value': response_time,\n            'endpoint': endpoint\n        })\n        \n        self.metrics_cache['status_codes'].append({\n            'timestamp': current_time,\n            'value': status_code,\n            'endpoint': endpoint\n        })\n        \n        # Calculate real-time metrics\n        self._update_realtime_metrics()\n    \n    def _process_system_metric_event(self, event: Dict[str, Any]) -> None:\n        \"\"\"Process system metric events\"\"\"\n        \n        payload = event.get('payload', {})\n        current_time = time.time()\n        \n        # CPU and memory metrics\n        if 'cpu_usage_percent' in payload:\n            self.metrics_cache['cpu_usage'].append({\n                'timestamp': current_time,\n                'value': payload['cpu_usage_percent']\n            })\n        \n        if 'memory_usage_percent' in payload:\n            self.metrics_cache['memory_usage'].append({\n                'timestamp': current_time,\n                'value': payload['memory_usage_percent']\n            })\n    \n    def _process_user_action_event(self, event: Dict[str, Any]) -> None:\n        \"\"\"Process user action events\"\"\"\n        \n        payload = event.get('payload', {})\n        current_time = time.time()\n        \n        # User engagement metrics\n        if 'action_type' in payload:\n            self.metrics_cache['user_actions'].append({\n                'timestamp': current_time,\n                'action': payload['action_type'],\n                'user_id': payload.get('user_id', 'anonymous')\n            })\n    \n    def _update_realtime_metrics(self) -> None:\n        \"\"\"Update real-time aggregated metrics\"\"\"\n        \n        current_time = time.time()\n        window_start = current_time - 60  # 1-minute window\n        \n        # Filter recent events\n        recent_response_times = [\n            item['value'] for item in self.metrics_cache['response_times']\n            if item['timestamp'] >= window_start\n        ]\n        \n        recent_status_codes = [\n            item['value'] for item in self.metrics_cache['status_codes']\n            if item['timestamp'] >= window_start\n        ]\n        \n        if recent_response_times:\n            # Calculate metrics\n            avg_response_time = np.mean(recent_response_times)\n            p95_response_time = np.percentile(recent_response_times, 95)\n            \n            # Error rate\n            error_count = sum(1 for code in recent_status_codes if code >= 500)\n            total_requests = len(recent_status_codes)\n            error_rate = (error_count / total_requests * 100) if total_requests > 0 else 0\n            \n            # Requests per second\n            requests_per_second = total_requests / 60\n            \n            # Store aggregated metrics\n            self.metrics_cache['aggregated'].append({\n                'timestamp': current_time,\n                'avg_response_time_ms': avg_response_time,\n                'p95_response_time_ms': p95_response_time,\n                'error_rate_percent': error_rate,\n                'requests_per_second': requests_per_second,\n                'total_requests': total_requests\n            })\n    \n    def _check_alerts(self, event: Dict[str, Any]) -> None:\n        \"\"\"Check if any alert conditions are met\"\"\"\n        \n        if not self.metrics_cache['aggregated']:\n            return\n        \n        latest_metrics = self.metrics_cache['aggregated'][-1]\n        alerts = []\n        \n        # Response time alert\n        if latest_metrics['avg_response_time_ms'] > self.alert_thresholds['response_time_ms']:\n            alerts.append({\n                'type': 'HIGH_RESPONSE_TIME',\n                'severity': 'WARNING',\n                'message': f\"Average response time is {latest_metrics['avg_response_time_ms']:.1f}ms\",\n                'threshold': self.alert_thresholds['response_time_ms'],\n                'current_value': latest_metrics['avg_response_time_ms']\n            })\n        \n        # Error rate alert\n        if latest_metrics['error_rate_percent'] > self.alert_thresholds['error_rate_percent']:\n            alerts.append({\n                'type': 'HIGH_ERROR_RATE',\n                'severity': 'CRITICAL',\n                'message': f\"Error rate is {latest_metrics['error_rate_percent']:.1f}%\",\n                'threshold': self.alert_thresholds['error_rate_percent'],\n                'current_value': latest_metrics['error_rate_percent']\n            })\n        \n        # Traffic spike alert\n        if latest_metrics['requests_per_second'] > self.alert_thresholds['requests_per_second']:\n            alerts.append({\n                'type': 'HIGH_TRAFFIC',\n                'severity': 'INFO',\n                'message': f\"High traffic detected: {latest_metrics['requests_per_second']:.1f} req/s\",\n                'threshold': self.alert_thresholds['requests_per_second'],\n                'current_value': latest_metrics['requests_per_second']\n            })\n        \n        # Send alerts\n        for alert in alerts:\n            self._send_alert(alert)\n    \n    def _send_alert(self, alert: Dict[str, Any]) -> None:\n        \"\"\"Send alert through configured channels\"\"\"\n        \n        alert['timestamp'] = datetime.utcnow().isoformat()\n        alert['source'] = 'zantara_realtime_analytics'\n        \n        logger.warning(f\"ALERT: {alert['type']} - {alert['message']}\")\n        \n        # Call custom alert callbacks\n        for callback in self.alert_callbacks:\n            try:\n                callback(alert)\n            except Exception as e:\n                logger.error(f\"Error in alert callback: {e}\")\n        \n        # Store alert in BigQuery\n        self._store_alert_in_bigquery(alert)\n    \n    def _store_alert_in_bigquery(self, alert: Dict[str, Any]) -> None:\n        \"\"\"Store alert in BigQuery for historical analysis\"\"\"\n        \n        try:\n            table_id = f\"{self.project_id}.{self.dataset_id}.realtime_alerts\"\n            \n            # Prepare row\n            row = {\n                'timestamp': alert['timestamp'],\n                'alert_type': alert['type'],\n                'severity': alert['severity'],\n                'message': alert['message'],\n                'threshold_value': alert.get('threshold'),\n                'current_value': alert.get('current_value'),\n                'source': alert['source']\n            }\n            \n            # Insert row\n            errors = self.bq_client.insert_rows_json(table_id, [row])\n            if errors:\n                logger.error(f\"Error inserting alert to BigQuery: {errors}\")\n            \n        except Exception as e:\n            logger.error(f\"Error storing alert in BigQuery: {e}\")\n    \n    def get_realtime_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current real-time metrics\"\"\"\n        \n        if not self.metrics_cache['aggregated']:\n            return {'error': 'No metrics available'}\n        \n        latest_metrics = self.metrics_cache['aggregated'][-1]\n        \n        # Add additional metrics\n        current_time = time.time()\n        recent_events = [\n            event for event in self.event_buffer\n            if (current_time - time.mktime(datetime.fromisoformat(event['timestamp'].replace('Z', '+00:00')).timetuple())) < 300  # 5 minutes\n        ]\n        \n        return {\n            'timestamp': datetime.utcnow().isoformat(),\n            'performance': {\n                'avg_response_time_ms': latest_metrics['avg_response_time_ms'],\n                'p95_response_time_ms': latest_metrics['p95_response_time_ms'],\n                'error_rate_percent': latest_metrics['error_rate_percent'],\n                'requests_per_second': latest_metrics['requests_per_second']\n            },\n            'traffic': {\n                'total_requests_last_minute': latest_metrics['total_requests'],\n                'events_last_5_minutes': len(recent_events)\n            },\n            'system_health': {\n                'cpu_usage_percent': self._get_latest_metric('cpu_usage'),\n                'memory_usage_percent': self._get_latest_metric('memory_usage')\n            },\n            'buffer_status': {\n                'events_in_buffer': len(self.event_buffer),\n                'buffer_capacity': self.event_buffer.maxlen\n            }\n        }\n    \n    def _get_latest_metric(self, metric_name: str) -> Optional[float]:\n        \"\"\"Get the latest value for a metric\"\"\"\n        \n        metric_data = self.metrics_cache.get(metric_name, [])\n        if metric_data:\n            return metric_data[-1]['value']\n        return None\n    \n    def simulate_realtime_events(self, duration_seconds: int = 300) -> None:\n        \"\"\"Simulate real-time events for testing (5 minutes default)\"\"\"\n        \n        logger.info(f\"Starting real-time event simulation for {duration_seconds} seconds\")\n        \n        start_time = time.time()\n        event_count = 0\n        \n        while time.time() - start_time < duration_seconds:\n            # Simulate API request event\n            api_event = {\n                'event_type': 'api_request',\n                'event_id': f'req_{event_count}',\n                'timestamp': datetime.utcnow().isoformat(),\n                'source': 'zantara_bridge',\n                'payload': {\n                    'endpoint': np.random.choice(['/api/v1/bridge', '/api/v1/status', '/api/v1/health']),\n                    'method': np.random.choice(['GET', 'POST', 'PUT']),\n                    'status_code': np.random.choice([200, 200, 200, 200, 400, 500], p=[0.7, 0.1, 0.1, 0.05, 0.03, 0.02]),\n                    'response_time_ms': max(50, np.random.lognormal(6, 0.5)),  # Log-normal distribution\n                    'user_id': f'user_{np.random.randint(1, 1000)}',\n                    'region': np.random.choice(['asia-southeast', 'us-central', 'europe-west'])\n                }\n            }\n            \n            self.process_event(api_event)\n            event_count += 1\n            \n            # Simulate system metrics occasionally\n            if event_count % 10 == 0:\n                system_event = {\n                    'event_type': 'system_metric',\n                    'event_id': f'sys_{event_count}',\n                    'timestamp': datetime.utcnow().isoformat(),\n                    'source': 'system_monitor',\n                    'payload': {\n                        'cpu_usage_percent': np.random.normal(45, 15),\n                        'memory_usage_percent': np.random.normal(60, 20),\n                        'instance_id': f'instance_{np.random.randint(1, 3)}'\n                    }\n                }\n                \n                self.process_event(system_event)\n            \n            # Variable delay to simulate realistic traffic\n            delay = np.random.exponential(0.1)  # Average 10 events per second\n            time.sleep(delay)\n        \n        logger.info(f\"Simulation completed. Processed {event_count} events\")\n    \n    def start_streaming(self) -> None:\n        \"\"\"Start the real-time streaming analytics service\"\"\"\n        \n        if self.is_streaming:\n            logger.warning(\"Streaming is already active\")\n            return\n        \n        logger.info(\"Starting real-time streaming analytics\")\n        self.is_streaming = True\n        \n        # Setup Pub/Sub resources\n        self.setup_pubsub_resources()\n        \n        # Start event simulation in background\n        self.executor.submit(self.simulate_realtime_events, 300)  # 5 minutes\n        \n        # Start metrics reporting\n        self.executor.submit(self._metrics_reporter)\n        \n        logger.info(\"Real-time streaming analytics started\")\n    \n    def _metrics_reporter(self) -> None:\n        \"\"\"Report metrics periodically\"\"\"\n        \n        while self.is_streaming:\n            try:\n                metrics = self.get_realtime_metrics()\n                if 'error' not in metrics:\n                    logger.info(f\"Real-time metrics: {json.dumps(metrics['performance'], default=str)}\")\n                \n                time.sleep(30)  # Report every 30 seconds\n                \n            except Exception as e:\n                logger.error(f\"Error in metrics reporter: {e}\")\n                time.sleep(5)\n    \n    def stop_streaming(self) -> None:\n        \"\"\"Stop the real-time streaming analytics service\"\"\"\n        \n        logger.info(\"Stopping real-time streaming analytics\")\n        self.is_streaming = False\n        \n        # Shutdown executor\n        self.executor.shutdown(wait=True)\n        \n        logger.info(\"Real-time streaming analytics stopped\")\n    \n    def get_analytics_summary(self) -> Dict[str, Any]:\n        \"\"\"Get a summary of analytics processing\"\"\"\n        \n        current_metrics = self.get_realtime_metrics()\n        \n        return {\n            'streaming_status': 'active' if self.is_streaming else 'stopped',\n            'events_processed': len(self.event_buffer),\n            'current_metrics': current_metrics,\n            'alert_thresholds': self.alert_thresholds,\n            'processors_count': len(self.event_processors),\n            'alert_callbacks_count': len(self.alert_callbacks)\n        }\n\ndef main():\n    \"\"\"Main execution function for real-time analytics\"\"\"\n    \n    logger.info(\"Starting Zantara Bridge Real-time Analytics\")\n    \n    # Initialize analytics\n    analytics = ZantaraRealtimeAnalytics()\n    \n    # Add a sample alert callback\n    def sample_alert_callback(alert):\n        print(f\"ðŸš¨ ALERT: {alert['type']} - {alert['message']}\")\n    \n    analytics.add_alert_callback(sample_alert_callback)\n    \n    try:\n        # Start streaming\n        analytics.start_streaming()\n        \n        # Let it run for a while\n        time.sleep(60)  # Run for 1 minute\n        \n        # Get final summary\n        summary = analytics.get_analytics_summary()\n        logger.info(\"Analytics Summary:\")\n        logger.info(json.dumps(summary, indent=2, default=str))\n        \n    except KeyboardInterrupt:\n        logger.info(\"Interrupted by user\")\n    \n    finally:\n        # Stop streaming\n        analytics.stop_streaming()\n        logger.info(\"Real-time analytics completed\")\n    \n    return analytics\n\nif __name__ == \"__main__\":\n    main()"