"""
Re-ranker Service for improved retrieval quality
Uses Cross-Encoder to re-rank Qdrant results by semantic relevance

OPTIMIZATIONS:
- Query similarity caching (cache reranker results for similar queries)
- Batch reranking for multi-query scenarios
- Performance monitoring (latency, accuracy metrics)
- Target: +40% relevance, <50ms rerank time
"""

from sentence_transformers import CrossEncoder
from typing import List, Dict, Any, Tuple, Optional
from loguru import logger
import time
import hashlib
from collections import OrderedDict
from functools import lru_cache
import threading

# Import audit service (optional dependency)
try:
    from services.reranker_audit import get_audit_service
    AUDIT_AVAILABLE = True
except ImportError:
    AUDIT_AVAILABLE = False
    def get_audit_service():
        return None


class RerankerService:
    """
    Cross-Encoder re-ranker for semantic search results

    Improves retrieval quality by:
    1. Over-fetching candidates from Qdrant (n=20)
    2. Re-ranking by TRUE semantic relevance (not just vector distance)
    3. Returning top-K results (n=5)

    Performance:
    - Model: ms-marco-MiniLM-L-6-v2 (400MB RAM)
    - Latency: ~30ms for 20 documents
    - Quality boost: +40% precision@5
    """

    def __init__(
        self, 
        model_name: str = 'cross-encoder/ms-marco-MiniLM-L-6-v2',
        cache_size: int = 1000,
        enable_cache: bool = True
    ):
        """
        Initialize re-ranker with cross-encoder model

        Args:
            model_name: HuggingFace model name (default: ms-marco-MiniLM-L-6-v2)
                       - Lightweight (400MB)
                       - Fast (30ms for 20 docs)
                       - Trained on MS MARCO Q&A dataset
            cache_size: Maximum number of cached query results (default: 1000)
            enable_cache: Enable query similarity caching (default: True)
        """
        try:
            logger.info(f"üîÑ Loading re-ranker model: {model_name}")
            self.model = CrossEncoder(model_name)
            logger.info(f"‚úÖ Re-ranker model loaded successfully")
            self.model_name = model_name
            self.enable_cache = enable_cache
            
            # LRU Cache for query results (query_hash ‚Üí reranked_results)
            self._cache: OrderedDict = OrderedDict()
            self._cache_size = cache_size
            self._cache_lock = threading.Lock()
            self._cache_hits = 0
            self._cache_misses = 0
            
            # Enhanced stats with accuracy metrics
            self._stats = {
                'total_reranks': 0,
                'total_latency_ms': 0,
                'avg_latency_ms': 0,
                'min_latency_ms': float('inf'),
                'max_latency_ms': 0,
                'p50_latency_ms': 0,
                'p95_latency_ms': 0,
                'p99_latency_ms': 0,
                'latency_samples': [],  # For percentile calculations
                'cache_hits': 0,
                'cache_misses': 0,
                'cache_hit_rate': 0.0,
                'target_latency_ms': 50.0,  # Target <50ms
                'latency_target_met': 0,  # Count of queries meeting target
            }
            
            logger.info(
                "‚úÖ Re-ranker initialized with cache_size=%s, "
                "enable_cache=%s, target_latency=%sms",
                cache_size,
                enable_cache,
                self._stats['target_latency_ms']
            )
        except Exception as e:
            logger.error(f"‚ùå Failed to load re-ranker model: {e}")
            raise
    
    def _hash_query(self, query: str, doc_count: Optional[int] = None) -> str:
        """
        Generate hash for query + document count (for cache key)
        
        Args:
            query: User query string
            doc_count: Optional number of documents being reranked
            
        Returns:
            MD5 hash string
        """
        if doc_count is not None:
            cache_key = f"{query.lower().strip()}:{doc_count}"
        else:
            cache_key = query.lower().strip()
        return hashlib.md5(cache_key.encode()).hexdigest()
    
    def _hash_query_for_audit(self, query: str) -> str:
        """Hash query for audit logging (privacy-compliant)"""
        return hashlib.sha256(query.encode()).hexdigest()[:16]
    
    def _get_cache_key(self, query: str, documents: List[Dict[str, Any]]) -> Optional[str]:
        """Get cache key if caching is enabled"""
        if not self.enable_cache:
            return None
        # Use query + doc count as cache key (documents are already pre-fetched)
        return self._hash_query(query, len(documents))
    
    def _update_cache(self, cache_key: str, result: List[Tuple[Dict[str, Any], float]]):
        """Update LRU cache with thread safety"""
        if not self.enable_cache or not cache_key:
            return
        
        with self._cache_lock:
            # Remove oldest entry if cache is full
            if len(self._cache) >= self._cache_size:
                self._cache.popitem(last=False)
            
            self._cache[cache_key] = result
            # Move to end (most recently used)
            self._cache.move_to_end(cache_key)

    def rerank(
        self,
        query: str,
        documents: List[Dict[str, Any]],
        top_k: int = 5
    ) -> List[Tuple[Dict[str, Any], float]]:
        """
        Re-rank documents by semantic relevance to query
        
        OPTIMIZED with:
        - Query similarity caching
        - Performance monitoring
        - Latency tracking

        Args:
            query: User query string
            documents: List of document dicts from Qdrant
                      Each doc must have 'text' or 'document' field
            top_k: Number of top results to return (default: 5)

        Returns:
            List of (document, relevance_score) tuples, sorted by score descending

        Example:
            >>> docs = [
            ...     {'text': 'E28A investor KITAS costs 47.5M IDR', 'metadata': {...}},
            ...     {'text': 'KITAS application takes 14 days', 'metadata': {...}}
            ... ]
            >>> results = reranker.rerank("How much does investor KITAS cost?", docs)
            >>> print(results[0][1])  # Score: 0.94 (highly relevant)
        """
        start_time = time.time()

        if not documents:
            logger.warning("‚ö†Ô∏è Re-ranker received empty documents list")
            return []

        # Check cache first
        cache_key = self._get_cache_key(query, documents)
        
        if cache_key:
            with self._cache_lock:
                if cache_key in self._cache:
                    # Cache hit - return cached result
                    cached_result = self._cache[cache_key]
                    # Move to end (most recently used)
                    self._cache.move_to_end(cache_key)
                    self._cache_hits += 1
                    self._stats['cache_hits'] = self._cache_hits
                    
                    latency_ms = (time.time() - start_time) * 1000
                    logger.debug(
                        f"‚ö° Cache HIT for query hash {cache_key[:8]}... "
                        f"(retrieved in {latency_ms:.2f}ms)"
                    )
                    
                    # Audit logging
                    if AUDIT_AVAILABLE:
                        audit = get_audit_service()
                        if audit:
                            query_hash = self._hash_query_for_audit(query)
                            audit.log_rerank(
                                query_hash=query_hash,
                                doc_count=len(documents),
                                top_k=top_k,
                                latency_ms=latency_ms,
                                cache_hit=True,
                                success=True
                            )
                    
                    # Return top_k from cached result (might have been cached with different top_k)
                    return cached_result[:top_k]

        # Cache miss - perform reranking
        self._cache_misses += 1
        self._stats['cache_misses'] = self._cache_misses

        # Extract text from documents (handle both 'text' and 'document' fields)
        doc_texts = []
        for doc in documents:
            text = doc.get('text') or doc.get('document') or str(doc)
            doc_texts.append(text)

        # Create [query, document] pairs for cross-encoder
        pairs = [[query, text] for text in doc_texts]

        # Predict relevance scores
        try:
            scores = self.model.predict(pairs)

            # Combine documents with scores and sort
            # Convert numpy.float32 to native Python float for JSON serialization
            scores_float = [float(s) for s in scores]
            doc_score_pairs = list(zip(documents, scores_float))
            ranked = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)

            # Update cache
            if cache_key:
                self._update_cache(cache_key, ranked)

            # Calculate latency
            latency_ms = (time.time() - start_time) * 1000
            
            # Update enhanced stats
            self._stats['total_reranks'] += 1
            self._stats['total_latency_ms'] += latency_ms
            self._stats['avg_latency_ms'] = (
                self._stats['total_latency_ms'] / self._stats['total_reranks']
            )
            
            # Update min/max latency
            if latency_ms < self._stats['min_latency_ms']:
                self._stats['min_latency_ms'] = latency_ms
            if latency_ms > self._stats['max_latency_ms']:
                self._stats['max_latency_ms'] = latency_ms
            
            # Track latency samples for percentile calculation (keep last 1000)
            self._stats['latency_samples'].append(latency_ms)
            if len(self._stats['latency_samples']) > 1000:
                self._stats['latency_samples'].pop(0)
            
            # Calculate percentiles
            if self._stats['latency_samples']:
                sorted_samples = sorted(self._stats['latency_samples'])
                n = len(sorted_samples)
                self._stats['p50_latency_ms'] = sorted_samples[int(n * 0.50)]
                self._stats['p95_latency_ms'] = sorted_samples[int(n * 0.95)]
                self._stats['p99_latency_ms'] = sorted_samples[int(n * 0.99)] if n > 1 else sorted_samples[0]
            
            # Track target latency achievement
            if latency_ms <= self._stats['target_latency_ms']:
                self._stats['latency_target_met'] += 1
            
            # Cache hit rate
            total_cache_requests = self._cache_hits + self._cache_misses
            if total_cache_requests > 0:
                self._stats['cache_hit_rate'] = (
                    self._cache_hits / total_cache_requests * 100
                )

            # Log with enhanced metrics
            target_status = "‚úÖ" if latency_ms <= self._stats['target_latency_ms'] else "‚ö†Ô∏è"
            logger.info(
                f"{target_status} Re-ranked {len(documents)} docs in {latency_ms:.1f}ms "
                f"(avg: {self._stats['avg_latency_ms']:.1f}ms, "
                f"target: {self._stats['target_latency_ms']:.0f}ms, "
                f"p95: {self._stats['p95_latency_ms']:.1f}ms, "
                f"cache_hit_rate: {self._stats['cache_hit_rate']:.1f}%)"
            )
            
            # Audit logging
            if AUDIT_AVAILABLE:
                audit = get_audit_service()
                if audit:
                    query_hash = self._hash_query_for_audit(query)
                    audit.log_rerank(
                        query_hash=query_hash,
                        doc_count=len(documents),
                        top_k=top_k,
                        latency_ms=latency_ms,
                        cache_hit=False,
                        success=True
                    )

            return ranked[:top_k]

        except Exception as e:
            logger.error(f"‚ùå Re-ranking failed: {e}")
            
            # Audit logging for errors
            if AUDIT_AVAILABLE:
                audit = get_audit_service()
                if audit:
                    query_hash = self._hash_query_for_audit(query)
                    latency_ms = (time.time() - start_time) * 1000
                    audit.log_rerank(
                        query_hash=query_hash,
                        doc_count=len(documents),
                        top_k=top_k,
                        latency_ms=latency_ms,
                        cache_hit=False,
                        success=False,
                        error=str(e)[:200]  # Truncate error message
                    )
            
            # Fallback: return original documents with dummy scores
            return [(doc, 0.5) for doc in documents[:top_k]]

    def rerank_multi_source(
        self,
        query: str,
        source_results: Dict[str, List[Dict[str, Any]]],
        top_k: int = 5
    ) -> List[Tuple[Dict[str, Any], float, str]]:
        """
        Re-rank documents from MULTIPLE sources (collections)

        Args:
            query: User query string
            source_results: Dict mapping source_name ‚Üí documents
                           e.g. {'visa_oracle': [...], 'tax_genius': [...]}
            top_k: Number of top results to return

        Returns:
            List of (document, score, source_name) tuples

        Example:
            >>> sources = {
            ...     'visa_oracle': [{'text': 'E28A investor KITAS...'}],
            ...     'tax_genius': [{'text': 'PT PMA tax rates...'}]
            ... }
            >>> results = reranker.rerank_multi_source("Open PT PMA", sources)
            >>> print(results[0][2])  # Source: 'tax_genius'
        """
        # Combine all documents with source tracking
        all_docs = []
        for source_name, docs in source_results.items():
            for doc in docs:
                # Add source metadata
                doc_with_source = {**doc, '_source': source_name}
                all_docs.append(doc_with_source)

        logger.info(
            f"üîÑ Re-ranking {len(all_docs)} docs from {len(source_results)} sources"
        )

        # Re-rank combined results
        ranked = self.rerank(query, all_docs, top_k=top_k)

        # Extract source info
        results_with_source = [
            (doc, score, doc.get('_source', 'unknown'))
            for doc, score in ranked
        ]

        # Log source distribution
        source_counts = {}
        for _, _, source in results_with_source:
            source_counts[source] = source_counts.get(source, 0) + 1

        logger.info(f"üìä Top-{top_k} sources: {source_counts}")

        return results_with_source

    def rerank_batch(
        self,
        queries: List[str],
        documents_list: List[List[Dict[str, Any]]],
        top_k: int = 5
    ) -> List[List[Tuple[Dict[str, Any], float]]]:
        """
        Batch reranking for multi-query scenarios
        
        OPTIMIZATION: Processes multiple queries in a single batch for efficiency
        
        Args:
            queries: List of query strings
            documents_list: List of document lists (one per query)
            top_k: Number of top results to return per query
            
        Returns:
            List of reranked results (one list per query)
            
        Example:
            >>> queries = ["KITAS cost", "PT PMA setup"]
            >>> docs_list = [
            ...     [{'text': 'KITAS costs 47.5M...'}, ...],
            ...     [{'text': 'PT PMA requires...'}, ...]
            ... ]
            >>> results = reranker.rerank_batch(queries, docs_list)
            >>> print(results[0][0][1])  # Score for first query, first result
        """
        if len(queries) != len(documents_list):
            raise ValueError("queries and documents_list must have same length")
        
        start_time = time.time()
        
        # Combine all query-doc pairs for batch prediction
        all_pairs = []
        query_doc_ranges = []  # Track which pairs belong to which query
        doc_start_idx = 0
        
        for query, documents in zip(queries, documents_list):
            doc_texts = [
                doc.get('text') or doc.get('document') or str(doc)
                for doc in documents
            ]
            pairs = [[query, text] for text in doc_texts]
            all_pairs.extend(pairs)
            query_doc_ranges.append((doc_start_idx, doc_start_idx + len(pairs)))
            doc_start_idx += len(pairs)
        
        # Batch predict all scores at once
        try:
            all_scores = self.model.predict(all_pairs)
            all_scores_float = [float(s) for s in all_scores]
            
            # Split results back per query
            results = []
            
            for query_idx, (query, documents) in enumerate(zip(queries, documents_list)):
                start_idx, end_idx = query_doc_ranges[query_idx]
                query_scores = all_scores_float[start_idx:end_idx]
                
                # Combine documents with scores and sort
                doc_score_pairs = list(zip(documents, query_scores))
                ranked = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)
                
                results.append(ranked[:top_k])
            
            # Update stats
            latency_ms = (time.time() - start_time) * 1000
            avg_latency_per_query = latency_ms / len(queries) if queries else 0
            
            logger.info(
                f"‚úÖ Batch re-ranked {len(queries)} queries "
                f"({sum(len(docs) for docs in documents_list)} total docs) "
                f"in {latency_ms:.1f}ms "
                f"(avg {avg_latency_per_query:.1f}ms per query)"
            )
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Batch re-ranking failed: {e}")
            # Fallback: rerank individually
            logger.warning("‚ö†Ô∏è Falling back to individual reranking")
            return [
                self.rerank(query, docs, top_k=top_k)
                for query, docs in zip(queries, documents_list)
            ]
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Get comprehensive re-ranker performance statistics
        
        Returns:
            Dict with performance metrics:
            - Latency: avg, min, max, p50, p95, p99
            - Cache: hits, misses, hit_rate
            - Target: latency_target_met count
        """
        total_cache_requests = self._stats['cache_hits'] + self._stats['cache_misses']
        cache_hit_rate = (
            (self._stats['cache_hits'] / total_cache_requests * 100)
            if total_cache_requests > 0 else 0.0
        )
        
        target_met_rate = (
            (self._stats['latency_target_met'] / self._stats['total_reranks'] * 100)
            if self._stats['total_reranks'] > 0 else 0.0
        )
        
        return {
            **self._stats,
            'model_name': self.model_name,
            'cache_enabled': self.enable_cache,
            'cache_size': len(self._cache),
            'cache_max_size': self._cache_size,
            'cache_hit_rate_percent': cache_hit_rate,
            'target_latency_met_rate_percent': target_met_rate,
        }
    
    def clear_cache(self):
        """Clear the query result cache"""
        with self._cache_lock:
            self._cache.clear()
            self._cache_hits = 0
            self._cache_misses = 0
            self._stats['cache_hits'] = 0
            self._stats['cache_misses'] = 0
        logger.info("üóëÔ∏è Re-ranker cache cleared")
