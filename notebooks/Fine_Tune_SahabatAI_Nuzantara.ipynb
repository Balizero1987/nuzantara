{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning SahabatAI per Nuzantara Domain\n",
    "\n",
    "**Obiettivo**: Adattare SahabatAI Gemma2-9B al dominio business advisory indonesiano (KITAS, PT PMA, tax)\n",
    "\n",
    "**GPU Required**: A100 40GB (disponibile su Colab Pro)\n",
    "\n",
    "**Tempo stimato**: 2-4 ore\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Runtime ‚Üí Change runtime type ‚Üí A100 GPU**\n",
    "2. **Run cells in order** (Shift+Enter)\n",
    "3. **Upload your dataset** quando richiesto\n",
    "4. **Download fine-tuned model** alla fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "Install dependencies (takes ~5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Unsloth for fast fine-tuning (2x faster, 63% less VRAM)\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load SahabatAI Base Model\n",
    "\n",
    "Download SahabatAI Gemma2-9B (takes ~10 minutes first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct\"\n",
    "max_seq_length = 2048  # Max context length\n",
    "dtype = None  # Auto-detect\n",
    "load_in_4bit = True  # QLoRA for memory efficiency\n",
    "\n",
    "print(\"üáÆüá© Loading SahabatAI base model...\")\n",
    "print(f\"   Model: {model_name}\")\n",
    "print(f\"   Max sequence: {max_seq_length}\")\n",
    "print(f\"   4-bit quantization: {load_in_4bit}\")\n",
    "print()\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure LoRA Adapters\n",
    "\n",
    "Only train 0.16% of parameters (14.5M instead of 9B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank (16 = good balance)\n",
    "    lora_alpha=32,  # LoRA scaling\n",
    "    lora_dropout=0.05,  # Dropout for regularization\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Memory optimization\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"‚úÖ LoRA adapters configured\")\n",
    "print(f\"   Trainable parameters: {trainable:,}\")\n",
    "print(f\"   Total parameters: {total:,}\")\n",
    "print(f\"   Trainable %: {100 * trainable / total:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Upload & Prepare Dataset\n",
    "\n",
    "**Dataset Format** (JSON):\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"instruction\": \"System prompt: Kamu adalah ZANTARA...\",\n",
    "    \"input\": \"Saya mau buka PT PMA di Bali, prosesnya gimana?\",\n",
    "    \"output\": \"Wah bagus nih! Untuk PT PMA di Bali...\"\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your dataset file\n",
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "print(\"üì§ Upload your training dataset (JSON file)\")\n",
    "print(\"   Expected format: [{'instruction': ..., 'input': ..., 'output': ...}, ...]\")\n",
    "print()\n",
    "\n",
    "uploaded = files.upload()\n",
    "dataset_filename = list(uploaded.keys())[0]\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset uploaded: {dataset_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and format dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "with open(dataset_filename, 'r', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "print(f\"üìä Dataset loaded: {len(raw_data)} examples\")\n",
    "print()\n",
    "\n",
    "# Format for Gemma2 chat template\n",
    "def format_example(example):\n",
    "    \"\"\"Format single example for Gemma2\"\"\"\n",
    "    prompt = f\"\"\"<start_of_turn>user\n",
    "{example['instruction']}\n",
    "\n",
    "{example['input']}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{example['output']}<end_of_turn>\"\"\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# Format all examples\n",
    "formatted_data = [format_example(ex) for ex in raw_data]\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "print(\"‚úÖ Dataset formatted for training\")\n",
    "print(f\"   Total examples: {len(dataset)}\")\n",
    "print()\n",
    "\n",
    "# Show first example\n",
    "print(\"üìù First training example:\")\n",
    "print(\"=\"*80)\n",
    "print(dataset[0]['text'][:500] + \"...\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./nuzantara-sahabatai-lora\",\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    \n",
    "    # Precision\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    \n",
    "    # Optimizer\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=10,\n",
    "    \n",
    "    # Logging & checkpoints\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Other\n",
    "    seed=42,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration set\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized\")\n",
    "print()\n",
    "print(\"üìä Training stats:\")\n",
    "print(f\"   Total examples: {len(dataset)}\")\n",
    "print(f\"   Total steps: {len(dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n",
    "print(f\"   Estimated time: 2-4 hours on A100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Start Training! üöÄ\n",
    "\n",
    "This will take 2-4 hours. Monitor the loss - it should decrease steadily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting training...\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Train!\n",
    "trainer.train()\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "output_dir = \"./nuzantara-sahabatai-lora-final\"\n",
    "\n",
    "print(f\"üíæ Saving fine-tuned model to: {output_dir}\")\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"‚úÖ Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test Fine-Tuned Model\n",
    "\n",
    "Try some queries to see how it performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def test_query(query: str):\n",
    "    \"\"\"Test model with a query\"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"Kamu adalah ZANTARA, asisten bisnis yang membantu orang asing dengan urusan bisnis, visa, dan legal di Indonesia.\n",
    "\n",
    "PENTING:\n",
    "- Gunakan bahasa Indonesia yang natural dan conversational\n",
    "- Boleh pakai slang umum kalau konteksnya casual\n",
    "- Fokus: KITAS, PT PMA, tax, visa, business setup\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"<start_of_turn>user\n",
    "{system_prompt}\n",
    "\n",
    "{query}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=500,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the model's response\n",
    "    response = response.split(\"<start_of_turn>model\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"‚úÖ Test function ready\")\n",
    "print(\"\\nTry it with: test_query('Your Indonesian question here')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Saya mau buka usaha kopi di Bali. KBLI apa yang cocok?\",\n",
    "    \"Berapa lama proses KITAS investor?\",\n",
    "    \"Gimana cara bikin PT PMA? Ribet ga?\",\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test {i}/{len(test_queries)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print()\n",
    "    \n",
    "    response = test_query(query)\n",
    "    print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Download Fine-Tuned Model\n",
    "\n",
    "Download to your local machine for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the model directory\n",
    "!zip -r nuzantara-sahabatai-lora-final.zip ./nuzantara-sahabatai-lora-final\n",
    "\n",
    "print(\"üì¶ Model zipped\")\n",
    "print(\"\\n‚¨áÔ∏è Downloading...\")\n",
    "\n",
    "# Download\n",
    "files.download('nuzantara-sahabatai-lora-final.zip')\n",
    "\n",
    "print(\"\\n‚úÖ Download complete!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Extract zip file\")\n",
    "print(\"2. Upload to your server\")\n",
    "print(\"3. Load with: FastLanguageModel.from_pretrained('./nuzantara-sahabatai-lora-final')\")\n",
    "print(\"4. A/B test vs base SahabatAI\")\n",
    "print(\"5. If better ‚Üí deploy to production!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**What you did**:\n",
    "- ‚úÖ Loaded SahabatAI Gemma2-9B (natural Indonesian base)\n",
    "- ‚úÖ Configured LoRA adapters (parameter-efficient)\n",
    "- ‚úÖ Trained on your Nuzantara dataset\n",
    "- ‚úÖ Saved fine-tuned model\n",
    "- ‚úÖ Tested results\n",
    "- ‚úÖ Downloaded for deployment\n",
    "\n",
    "**Result**:\n",
    "SahabatAI base (natural Indonesian) + Your domain expertise (KITAS, PT PMA, tax) = Best AI for Indonesian business advisory! üèÜ\n",
    "\n",
    "**Next**:\n",
    "1. Test with Indonesian team\n",
    "2. Compare vs base SahabatAI\n",
    "3. If better ‚Üí deploy to production\n",
    "4. Collect feedback ‚Üí iterate\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Out of memory**:\n",
    "- Reduce `per_device_train_batch_size` to 2\n",
    "- Increase `gradient_accumulation_steps` to 8\n",
    "\n",
    "**Training too slow**:\n",
    "- Make sure you selected A100 GPU (not T4)\n",
    "- Runtime ‚Üí Change runtime type ‚Üí A100\n",
    "\n",
    "**Model not improving**:\n",
    "- Check dataset quality (naturalness, accuracy)\n",
    "- Increase epochs to 5\n",
    "- Adjust learning rate (try 1e-4)\n",
    "\n",
    "**Questions?**:\n",
    "- Check guide: `docs/guides/FINE_TUNING_SAHABATAI_NUZANTARA.md`\n",
    "- Review training script: `apps/backend-rag/backend/llm/train_nuzantara_sahabatai.py`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
