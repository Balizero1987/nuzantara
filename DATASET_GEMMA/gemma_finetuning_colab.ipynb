{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ Gemma2 9B Fine-Tuning - ZANTARA Dataset\n",
        "\n",
        "Fine-tuning Gemma2 9B su conversazioni indonesiane/giavanesi/italiane per migliorare la naturalezza.\n",
        "\n",
        "**Obiettivo:** Aumentare naturalezza da 67.1/100 a 85+/100\n",
        "\n",
        "**Dataset:**\n",
        "- Train: 6,000 conversazioni (79,769 messaggi)\n",
        "- Validation: 750 conversazioni (9,751 messaggi)\n",
        "- Test: 750 conversazioni (10,082 messaggi)\n",
        "\n",
        "**Metodo:** QLoRA (Quantized Low-Rank Adaptation)\n",
        "- 4-bit quantization per ridurre VRAM\n",
        "- LoRA rank 16 per training efficiente\n",
        "- Compatible con Colab Pro (A100 40GB) o Pro+ (V100 16GB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Step 0: Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q -U transformers accelerate peft bitsandbytes datasets trl huggingface_hub\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Step 1: Load Dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Dataset paths (MODIFY THESE based on your Drive structure)\n",
        "DATASET_DIR = '/content/drive/MyDrive/GEMMA_FINETUNING/splits'\n",
        "train_path = f'{DATASET_DIR}/train.jsonl'\n",
        "val_path = f'{DATASET_DIR}/validation.jsonl'\n",
        "test_path = f'{DATASET_DIR}/test.jsonl'\n",
        "\n",
        "# Verify files exist\n",
        "for path in [train_path, val_path, test_path]:\n",
        "    if os.path.exists(path):\n",
        "        size_mb = os.path.getsize(path) / 1024 / 1024\n",
        "        print(f\"‚úÖ Found: {os.path.basename(path)} ({size_mb:.1f} MB)\")\n",
        "    else:\n",
        "        print(f\"‚ùå Missing: {path}\")\n",
        "        raise FileNotFoundError(f\"Dataset file not found: {path}\")\n",
        "\n",
        "print(\"\\n‚úÖ All dataset files loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Step 2: Preview Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import json\n",
        "\n",
        "# Load datasets\n",
        "dataset = load_dataset('json', data_files={\n",
        "    'train': train_path,\n",
        "    'validation': val_path,\n",
        "    'test': test_path\n",
        "})\n",
        "\n",
        "print(\"üìä Dataset Statistics:\")\n",
        "print(f\"  Train:      {len(dataset['train']):,} conversations\")\n",
        "print(f\"  Validation: {len(dataset['validation']):,} conversations\")\n",
        "print(f\"  Test:       {len(dataset['test']):,} conversations\")\n",
        "print(f\"  Total:      {len(dataset['train']) + len(dataset['validation']) + len(dataset['test']):,} conversations\")\n",
        "\n",
        "# Preview samples\n",
        "print(\"\\nüìù Sample Conversation (Train):\")\n",
        "sample = dataset['train'][0]\n",
        "print(f\"Messages: {len(sample['messages'])}\")\n",
        "for i, msg in enumerate(sample['messages'][:4], 1):\n",
        "    role_emoji = \"üë§\" if msg['role'] == 'user' else \"ü§ñ\"\n",
        "    print(f\"  [{i}] {role_emoji} {msg['role']}: {msg['content'][:100]}...\")\n",
        "\n",
        "if len(sample['messages']) > 4:\n",
        "    print(f\"  ... ({len(sample['messages']) - 4} more messages)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîë Step 3: Login to Hugging Face (Optional)\n",
        "\n",
        "Required if you want to:\n",
        "- Save model to Hugging Face Hub\n",
        "- Access gated models\n",
        "\n",
        "Get token from: https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Login (optional)\n",
        "# Uncomment and add your token if needed\n",
        "# HF_TOKEN = \"hf_...\"\n",
        "# login(token=HF_TOKEN)\n",
        "\n",
        "print(\"‚úÖ Ready to proceed (login skipped)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Step 4: Load Gemma2 9B with 4-bit Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"google/gemma-2-9b-it\"\n",
        "\n",
        "# 4-bit quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "print(\"üì• Loading Gemma2 9B (4-bit)...\")\n",
        "print(\"‚è≥ This may take 3-5 minutes...\")\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully!\")\n",
        "print(f\"üìä Model size: ~5GB (4-bit quantized from ~18GB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Step 5: Configure LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                              # LoRA rank (higher = more parameters, slower)\n",
        "    lora_alpha=32,                     # LoRA scaling\n",
        "    target_modules=[                   # Which layers to adapt\n",
        "        \"q_proj\",\n",
        "        \"k_proj\", \n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\"\n",
        "    ],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = 0\n",
        "all_params = 0\n",
        "for _, param in model.named_parameters():\n",
        "    all_params += param.numel()\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "\n",
        "print(f\"‚úÖ LoRA configured!\")\n",
        "print(f\"üìä Trainable params: {trainable_params:,} / {all_params:,} ({100 * trainable_params / all_params:.2f}%)\")\n",
        "print(f\"üíæ Memory footprint: ~5-8GB VRAM (compatible with Colab Pro)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Step 6: Prepare Dataset for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_conversation(example):\n",
        "    \"\"\"\n",
        "    Format conversation into Gemma chat template\n",
        "    \n",
        "    Gemma format:\n",
        "    <start_of_turn>user\\n{message}<end_of_turn>\\n<start_of_turn>model\\n{response}<end_of_turn>\\n\n",
        "    \"\"\"\n",
        "    messages = example['messages']\n",
        "    \n",
        "    # Build formatted conversation\n",
        "    formatted = \"\"\n",
        "    for msg in messages:\n",
        "        role = \"user\" if msg['role'] == 'user' else \"model\"\n",
        "        formatted += f\"<start_of_turn>{role}\\n{msg['content']}<end_of_turn>\\n\"\n",
        "    \n",
        "    return {\"text\": formatted}\n",
        "\n",
        "# Apply formatting\n",
        "print(\"üîÑ Formatting datasets...\")\n",
        "train_dataset = dataset['train'].map(format_conversation, remove_columns=['messages'])\n",
        "eval_dataset = dataset['validation'].map(format_conversation, remove_columns=['messages'])\n",
        "\n",
        "print(\"‚úÖ Datasets formatted!\")\n",
        "print(f\"\\nüìù Sample formatted text:\")\n",
        "print(train_dataset[0]['text'][:500] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Step 7: Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gemma2-zantara-indonesian\",\n",
        "    num_train_epochs=3,                    # Number of epochs\n",
        "    per_device_train_batch_size=1,         # Batch size per GPU (increase if VRAM allows)\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=4,         # Effective batch size = 4\n",
        "    gradient_checkpointing=True,           # Reduce VRAM usage\n",
        "    \n",
        "    # Optimizer\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=100,\n",
        "    \n",
        "    # Logging\n",
        "    logging_steps=10,\n",
        "    logging_dir=\"./logs\",\n",
        "    \n",
        "    # Evaluation\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    \n",
        "    # Saving\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,                    # Keep only 2 best checkpoints\n",
        "    \n",
        "    # Performance\n",
        "    fp16=False,\n",
        "    bf16=True,                             # Use bfloat16 for A100\n",
        "    max_grad_norm=0.3,\n",
        "    \n",
        "    # Other\n",
        "    report_to=\"none\",                      # Disable wandb/tensorboard\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training arguments configured!\")\n",
        "print(f\"üìä Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"‚è±Ô∏è  Estimated training time: ~2-4 hours on A100\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Step 8: Initialize Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize SFTTrainer (Supervised Fine-Tuning)\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=2048,                   # Maximum sequence length\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    packing=False,                         # Don't pack multiple examples together\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer initialized!\")\n",
        "print(f\"üöÄ Ready to start training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèãÔ∏è Step 9: Start Training\n",
        "\n",
        "**‚ö†Ô∏è WARNING:** This will take 2-4 hours on A100 GPU!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "print(f\"üöÄ Starting training at {datetime.datetime.now().strftime('%H:%M:%S')}\")\n",
        "print(\"‚è≥ This will take approximately 2-4 hours...\\n\")\n",
        "\n",
        "# Train!\n",
        "trainer.train()\n",
        "\n",
        "print(f\"\\n‚úÖ Training completed at {datetime.datetime.now().strftime('%H:%M:%S')}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Step 10: Save Fine-Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save LoRA adapter\n",
        "output_dir = \"./gemma2-zantara-indonesian-final\"\n",
        "trainer.model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"‚úÖ Model saved to {output_dir}\")\n",
        "\n",
        "# Copy to Google Drive for persistence\n",
        "import shutil\n",
        "drive_output = \"/content/drive/MyDrive/GEMMA_FINETUNING/gemma2-zantara-indonesian-final\"\n",
        "shutil.copytree(output_dir, drive_output, dirs_exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Model backed up to Google Drive: {drive_output}\")\n",
        "print(f\"üì¶ Size: ~100-200MB (LoRA adapters only)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Step 11: Test Fine-Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create text generation pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95\n",
        ")\n",
        "\n",
        "# Test with Indonesian conversation\n",
        "test_prompt = \"\"\"<start_of_turn>user\n",
        "Halo! Gue mau tanya dong soal visa investor. Prosesnya gimana sih?<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "print(\"üß™ Testing fine-tuned model...\\n\")\n",
        "result = pipe(test_prompt, do_sample=True)[0]['generated_text']\n",
        "\n",
        "# Extract only the assistant's response\n",
        "response = result.split(\"<start_of_turn>model\\n\")[-1].split(\"<end_of_turn>\")[0]\n",
        "print(f\"üë§ User: Halo! Gue mau tanya dong soal visa investor. Prosesnya gimana sih?\")\n",
        "print(f\"ü§ñ Assistant: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 12: Evaluate on Test Set (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "test_dataset = dataset['test'].map(format_conversation, remove_columns=['messages'])\n",
        "\n",
        "print(\"üîç Evaluating on test set...\")\n",
        "eval_results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "\n",
        "print(\"\\nüìä Test Results:\")\n",
        "for key, value in eval_results.items():\n",
        "    print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Evaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Step 13: Push to Hugging Face Hub (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to push to Hugging Face Hub\n",
        "# HF_USERNAME = \"your-username\"\n",
        "# HF_REPO = \"gemma2-9b-zantara-indonesian\"\n",
        "\n",
        "# trainer.model.push_to_hub(f\"{HF_USERNAME}/{HF_REPO}\")\n",
        "# tokenizer.push_to_hub(f\"{HF_USERNAME}/{HF_REPO}\")\n",
        "\n",
        "# print(f\"‚úÖ Model pushed to https://huggingface.co/{HF_USERNAME}/{HF_REPO}\")\n",
        "\n",
        "print(\"‚ÑπÔ∏è  Push to Hub skipped (uncomment to enable)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Summary\n",
        "\n",
        "### What We Did\n",
        "1. ‚úÖ Loaded 7,500 Indonesian/Javanese/Italian conversations\n",
        "2. ‚úÖ Fine-tuned Gemma2 9B with QLoRA (4-bit)\n",
        "3. ‚úÖ Saved LoRA adapters (~100-200MB)\n",
        "4. ‚úÖ Tested model with natural conversations\n",
        "\n",
        "### Next Steps\n",
        "1. **Integrate into ZANTARA backend:**\n",
        "   - Load fine-tuned model in `apps/backend-rag`\n",
        "   - Replace current LLM with fine-tuned Gemma2\n",
        "\n",
        "2. **Evaluate naturalness:**\n",
        "   - Use quality analyzer from dataset generation\n",
        "   - Measure particle usage, slang density\n",
        "   - Target: 85+/100 naturalness score\n",
        "\n",
        "3. **Generate more data (if needed):**\n",
        "   - Current: 7,500 conversations\n",
        "   - Target: 24,000 conversations\n",
        "   - Re-train with expanded dataset\n",
        "\n",
        "### Model Files\n",
        "- **Local:** `./gemma2-zantara-indonesian-final/`\n",
        "- **Google Drive:** `/content/drive/MyDrive/GEMMA_FINETUNING/gemma2-zantara-indonesian-final/`\n",
        "\n",
        "### Resources\n",
        "- Training time: ~2-4 hours on A100\n",
        "- VRAM usage: ~5-8GB (4-bit + LoRA)\n",
        "- Disk space: ~100-200MB (adapters only)\n",
        "\n",
        "---\n",
        "\n",
        "**Created:** November 2025  \n",
        "**Dataset:** ZANTARA Indonesian/Javanese/Italian  \n",
        "**Model:** Gemma2 9B Instruct + QLoRA"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
