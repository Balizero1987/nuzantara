# ðŸ¤– INTEL AUTOMATION SYSTEM

**Automated Intelligence Gathering & Multi-Channel Publishing for Bali Business**

## ðŸŽ¯ Overview

Complete end-to-end automation system that:
- Scrapes 240+ sources daily across 8 categories
- Processes with local AI (LLAMA 3.2) at zero cost
- Reviews with premium AI (Claude Opus) for quality
- Publishes to 6 channels automatically

**Total Cost**: $5-10/month (95% cheaper than alternatives)
**Human Time**: 0 hours (100% automated)

## ðŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. CRAWL4AI SCRAPER â”‚ â†’ 240 sources â†’ Raw content
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2A. LLAMA RAG       â”‚ â†’ ChromaDB â†’ Searchable knowledge base
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2B. LLAMA CONTENT   â”‚ â†’ Articles â†’ 20-30 drafts/day
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. CLAUDE EDITORIAL â”‚ â†’ Review â†’ 5-10 approved/day
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. MULTI-PUBLISHER  â”‚ â†’ 6 Channels â†’ Blog, Social, Messaging
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸš€ Quick Start

### 1. Install Dependencies

```bash
# Install Python dependencies
cd scripts
pip install -r requirements.txt

# Install Ollama (for LLAMA)
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.2:3b
```

### 2. Configure API Keys

```bash
# Create .env file
cat > .env << EOF
# Claude Opus (required for editorial)
ANTHROPIC_API_KEY=sk-ant-api03-...

# Social Media (optional)
FACEBOOK_PAGE_ACCESS_TOKEN=...
TWITTER_API_KEY=...
TELEGRAM_BOT_TOKEN=...
EOF
```

### 3. Run Pipeline

```bash
# Run complete pipeline
python scripts/run_intel_automation.py

# Run specific stage
python scripts/run_intel_automation.py --stage scraping

# Test mode (limited sources)
python scripts/run_intel_automation.py --test
```

## ðŸ“ Directory Structure

```
NUZANTARA-2/
â”œâ”€â”€ INTEL_SCRAPING/           # All scraped and processed content
â”‚   â”œâ”€â”€ immigration/
â”‚   â”‚   â”œâ”€â”€ raw/             # Scraped content
â”‚   â”‚   â”œâ”€â”€ rag/             # Structured for ChromaDB
â”‚   â”‚   â”œâ”€â”€ articles/        # LLAMA generated
â”‚   â”‚   â””â”€â”€ editorial/       # Claude approved
â”‚   â”œâ”€â”€ bkpm_tax/
â”‚   â”œâ”€â”€ real_estate/
â”‚   â”œâ”€â”€ events/
â”‚   â”œâ”€â”€ social_trends/
â”‚   â”œâ”€â”€ competitors/
â”‚   â”œâ”€â”€ bali_news/
â”‚   â””â”€â”€ weekly_roundup/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ crawl4ai_scraper.py     # Stage 1: Scraping
â”‚   â”œâ”€â”€ llama_rag_processor.py  # Stage 2A: RAG
â”‚   â”œâ”€â”€ llama_content_creator.py # Stage 2B: Articles
â”‚   â”œâ”€â”€ editorial_ai.py         # Stage 3: Review
â”‚   â”œâ”€â”€ multi_channel_publisher.py # Stage 4: Publish
â”‚   â””â”€â”€ run_intel_automation.py # Main orchestrator
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ chroma_db/              # Vector database
â”‚
â””â”€â”€ .github/workflows/
    â””â”€â”€ intel-automation.yml    # GitHub Actions
```

## âš™ï¸ Configuration

### Sources Configuration

Edit `crawl4ai_scraper.py` to add/modify sources:

```python
INTEL_SOURCES = {
    "immigration": [
        {"url": "https://www.imigrasi.go.id", "tier": 1, "name": "Official"},
        # Add more sources...
    ]
}
```

### Tier System

- **Tier 1**: Official government sources (truth)
- **Tier 2**: Accredited news/business sources
- **Tier 3**: Community/forums (sentiment)

## ðŸ¤– AI Models

### LLAMA 3.2 (Local - FREE)
- RAG processing
- Content creation
- ~20 tokens/sec on MacBook
- 2GB model size

### Claude Opus 4 (Premium - $5/month)
- Editorial review
- Quality control
- Multi-channel adaptation
- Best prose quality

## ðŸ“Š Performance Metrics

### Daily Output
- 240 sources scraped
- 20-30 articles created
- 5-10 articles published
- 6 channels updated

### Processing Time
- Scraping: 30 min
- RAG Processing: 15 min
- Content Creation: 20 min
- Editorial Review: 10 min
- Publishing: 5 min
- **Total**: 80 min/day automated

## ðŸ”„ Automation

### GitHub Actions (Automatic)

The system runs automatically:
- Daily at 06:00 CET
- Weekly roundup Sunday 18:00 CET

### Manual Run

```bash
# Full pipeline
python scripts/run_intel_automation.py

# Individual stages
python scripts/crawl4ai_scraper.py
python scripts/llama_rag_processor.py
python scripts/llama_content_creator.py
python scripts/editorial_ai.py
python scripts/multi_channel_publisher.py
```

## ðŸ“± Publishing Channels

### Configured Channels
1. **Blog** - GitHub Pages (automatic)
2. **Instagram** - Carousel + caption (queued)
3. **Facebook** - Posts with links
4. **Twitter/X** - Thread format
5. **WhatsApp** - Broadcast digest
6. **Telegram** - Rich format

### Manual Posting

Content requiring manual posting is saved in:
```
INTEL_SCRAPING/manual_posting/[platform]/
```

## ðŸ” Monitoring

### Logs
```bash
# View latest log
tail -f intel_automation_*.log

# Check specific stage
grep "STAGE 3" intel_automation_*.log
```

### Reports
- `daily_summary_[date].md` - Scraping summary
- `rag_summary_[date].md` - RAG processing
- `content_summary_[date].md` - Articles created
- `editorial_summary_[date].md` - Editorial decisions
- `publishing_report_[date].md` - Publishing status

## ðŸš¨ Troubleshooting

### Common Issues

1. **Ollama not found**
   ```bash
   curl -fsSL https://ollama.com/install.sh | sh
   ```

2. **LLAMA model missing**
   ```bash
   ollama pull llama3.2:3b
   ```

3. **Claude API error**
   - Check ANTHROPIC_API_KEY
   - Verify API limits

4. **Scraping failures**
   - Check internet connection
   - Verify source URLs
   - Check rate limits

## ðŸ’° Cost Breakdown

| Component | Cost/Month |
|-----------|------------|
| Scraping (Crawl4AI) | $0 |
| LLAMA Processing | $0 |
| Claude Editorial | $5-10 |
| Publishing APIs | $0-5 |
| **Total** | **$5-15** |

## ðŸ“ˆ Optimization Tips

1. **Reduce Costs**
   - Use Gemini Pro (free) instead of Claude
   - Batch process articles
   - Cache API responses

2. **Improve Quality**
   - Fine-tune LLAMA prompts
   - Add more Tier 1 sources
   - A/B test content formats

3. **Scale Up**
   - Add more categories
   - Increase source count
   - Enable more channels

## ðŸ” Security

- Store API keys in environment variables
- Use GitHub Secrets for CI/CD
- Rotate keys regularly
- Monitor usage and costs

## ðŸ“ž Support

- **Issues**: Create GitHub issue
- **Docs**: See `/docs` folder
- **Logs**: Check `*.log` files

## ðŸŽ¯ Next Steps

1. **Test** with limited sources first
2. **Configure** API keys and channels
3. **Run** manual test
4. **Enable** GitHub Actions
5. **Monitor** quality and costs
6. **Optimize** based on results

---

**Ready to automate your intelligence gathering!** ðŸš€

*Last updated: 2025-10-07*