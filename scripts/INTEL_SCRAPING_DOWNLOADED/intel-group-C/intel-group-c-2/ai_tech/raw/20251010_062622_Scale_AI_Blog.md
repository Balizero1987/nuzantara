# AI & ML Blog | Scale AI

**Source**: Scale AI Blog
**URL**: https://scale.com/blog
**Scraped**: 2025-10-10T06:25:20.331401
**Category**: ai_tech

---

Scale AI logo
Products
Leaderboards
Enterprise
Government
Customers
Resources
Book a Demo
→
Log In
Field Unknown Model.Unknown Field
Blog

Company Updates & Technology Articles

October 7, 2025

Research
Enterprise Reinforcement Learning with Rubrics as Rewards

Many enterprise problems lack simple yes/no solutions, causing common AI training methods to fall short. Scale’s Rubrics as Rewards (RaR) method solves this by using a detailed, multi-faceted rubric for evaluation instead of a simple reward signal. This approach enables smaller, fine-tuned models to match or outperform much larger, general-purpose models on specialized tasks. For instance, on a legal analysis test set, a small Qwen3-4B model trained with RaR surpassed the performance of the much larger GPT-4.1. For enterprises, this translates directly to lower costs, more transparency, and tighter control, delivering superior performance on the complex workflows that matter most.

Read more

September 24, 2025

Product
Expanding Our Data Engine for Physical AI

Scale’s Data Engine for Physical AI is a comprehensive data collection and annotation solution that provides the massive, high-quality datasets robotics companies need to train foundation models.

Read more

September 22, 2025

Research
Introducing SEAL Showdown: Real People, Real Conversations, Real Rankings

SEAL Showdown is a new public AI leaderboard from Scale that evaluates large language models based on real-world user preferences rather than synthetic tests or hobbyist feedback. Unlike existing leaderboards, it captures granular insights by demographics, regions, professions, and use cases, drawing on millions of conversations from a diverse global contributor base. Designed to be trustworthy and resistant to gaming, SEAL Showdown sets a new standard for model evaluation by showing how AI performs for people like you.

Read more

September 19, 2025

Research
SWE-Bench Pro: Raising the Bar for Agentic Coding

Benchmarks play a critical role in measuring the progress of AI coding agents, but most fall short by relying on contaminated training data, oversimplified bug fixes, or narrow task coverage. SWE-Bench Pro solves these problems with contamination-resistant repositories, diverse and industrially relevant codebases, and human-in-the-loop curation that preserves real-world difficulty. With reproducible, end-to-end evaluation, SWE-Bench Pro sets a new gold standard for testing advanced AI developers.

Read more

September 19, 2025

Research
Advancing Agents: Introducing Scale’s Agentic Leaderboards

While today's agents show promise, the benchmarks used to evaluate them often test simple, isolated skills that don't reflect real-world work. To close this gap, Scale is launching a new suite of evaluations designed to measure an agent's ability to perform complex, end-to-end tasks. Our first two leaderboards set a new, more difficult standard for the industry. SWE-Bench Pro challenges agents with professional software engineering tasks in complex, proprietary codebases they've never seen before. MCP Atlas measures an agent's ability to skillfully orchestrate over 300 real-world digital tools to solve a single problem. Read the full post to learn about our framework for building a more reliable yardstick for the future of AI.

Read more

September 19, 2025

Research
Actions, Not Words: MCP-Atlas Raises the Bar for Agentic Evaluation

MCP-Atlas is a real-world leaderboard for agentic tool use via the Model Context Protocol. It runs 1,000 single-turn tasks across 40+ servers and 300+ tools—search, databases, filesystems, APIs, and dev tools—each requiring 3–6 calls with distractors. We score exact-answer pass rate and provide diagnostics. Early results: even the top model completes less than half of tasks, with failures concentrated in tool selection, parameter construction, and orchestration. Built for model and product teams, MCP-Atlas pinpoints what to fix.

Read more

September 19, 2025

Government
Investing in Britain's AI Talent

The future of artificial intelligence will be shaped by those who build it together. Right now, there is no more important partnership in technology than the one being forged between the United States and the United Kingdom. This transatlantic alliance, strengthened by a historic bilateral technology agreement, is creating a center of gravity for AI innovation, and at Scale AI, we are proud to be at the heart of it.

Read more

September 17, 2025

Government
From Prototype to Production: Unlocking Mission-Ready AI

This agreement, known as an Other Transaction Authority (OTA), is designed specifically to help the DoD move at speed and partner with non-traditional tech companies like Scale. It streamlines the procurement process, allowing any component across the entire DoD to access our end-to-end AI platform.

Read more

September 16, 2025

General
How Morgan Stanley deploys AI that actually works (hint: it's evals) | Human in the Loop: Episode 13

Kaitlin Elliott, who leads firmwide Generative AI Solutions at Morgan Stanley, joined us in the studio to unpack how AI evaluations powered the firm’s successful adoption of production GenAI. This is a real world case study you don't want to miss.

Read more

September 15, 2025

Research
Smoothing Out LLM Variance for Reliable Enterprise Evals

A critical challenge in enterprise AI development is the instability of LLM evaluations. Our internal testing revealed that metrics on identical A/B tests can swing by as much as 15% from one day to the next. This level of variance is large enough to invalidate results, making principled, incremental improvement a game of chance. In this post, we dive into the root cause: an industry-wide phenomenon created by the interplay of Sparse Mixture of Experts (MoE) architecture and the batched inference common to provider APIs. By implementing a "cohort of judges," a small panel of LLMs with semantically similar but varied prompts, we successfully reduce this variance by at least 50%. This creates the stable, trustworthy measurement foundation needed to confidently build and improve AI agents.

Read more
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
The future of your industry starts here
Book a Demo
→
Build AI→
PRODUCTS
Scale Data Engine
Scale GenAI Platform
Scale Donovan
GOVERNMENT
Public Sector
COMPANY
About
Careers
Security
Terms
Privacy
RESOURCES
Blog
Contact Us
Customers
Events
Documentation
Guides
Community
AI Readiness Report 2024
Research
GUIDES
Data Labeling
ML Model Training
Diffusion Models
Guide to AI for eCommerce
Computer Vision Applications
Large Language Models
FOLLOW US
Copyright © 2025 Scale AI, Inc. All rights reserved.
Terms of Use & Privacy Policy
