# Safety & responsibility | OpenAI

**Source**: OpenAI Safety
**URL**: https://openai.com/safety
**Scraped**: 2025-10-10T06:25:33.970031
**Category**: ai_tech

---

Safety & responsibility | OpenAI

Safety

Safety at every step

We believe in AI’s potential to make life better for everyone, which means making it safe for everyone

Teach

We start by teaching our AI right from wrong, filtering harmful content and responding with empathy.

Test

We conduct internal evaluations and work with experts to test real-world scenarios, enhancing our safeguards.

Share

We use real-world feedback to help make our AI safer and more helpful.

Safety doesn’t stop

Building safe AI isn’t one and done. Every day is a chance to make things better. And every step helps anticipate, evaluate, and prevent risk.

TEACH
FILTER DATA
OPENAI POLICIES
HUMAN VALUES
TEST
RED TEAMING
SYSTEM CARDS
PREPAREDNESS EVALS
SHARE
SAFETY COMMITTEES
ALPHA / BETA
GA
FEEDBACK
How we think about safety and alignment
Learn more
Leading the way in safety

We collaborate with industry leaders and policymakers on the issues that matter most.

Child safety

Creating industry-wide standards to protect children.

Private information

Protecting people’s privacy.

Deep fakes

Improving transparency in AI content.

Bias

Rigorously evaluating content to avoid reinforcing biases or stereotypes.

Elections

Partnering with governments to combat disinformation globally.

Conversations with OpenAI researchers

Get inside OpenAI with our series that breaks down a range of topics around safety and more.

Where is AI going?
Teaching ChatGPT Good Behavior
Latest news on safety
Introducing parental controls

Product
Sep 29, 2025

Our updated Preparedness Framework

Publication
Apr 15, 2025

An update on disrupting deceptive uses of AI

Safety
Oct 9, 2024

OpenAI safety practices

Safety
May 21, 2024

Go deeper on safety
Explore the safety evaluations hub
Sora 2 System Card
Sora 2’s advanced capabilities require consideration of new potential risks, including nonconsensual use of likeness or misleading generations. To address these, we worked with internal red teamers to identify new challenges and inform corresponding mitigations. We’re taking an iterative approach to safety, focusing on areas where context is especially important or where risks are still emerging and are not fully understood.
Learn more
Addendum to GPT-5 system card: GPT-5-Codex
This addendum outlines the comprehensive safety measures implemented for GPT‑5-Codex. It details both model-level mitigations, such as specialized safety training for harmful tasks and prompt injections, and product-level mitigations like agent sandboxing and configurable network access.
Learn more
GPT-5 System Card
In this system card, we label the fast, high-throughput models as gpt-5-main and gpt-5-main-mini, and the thinking models as gpt-5-thinking and gpt-5-thinking-mini. In the API, we provide direct access to the thinking model, its mini version, and an even smaller and faster nano version of the thinking model, made for developers (gpt-5-thinking-nano).
Learn more
Addendum to OpenAI o3 and o4-mini system card: OpenAI o3 Operator
This addendum to o3 and o4-mini system card describes the release of Operator, a Computer Using Agent capable of interacting with the web through its own browser. Operator is now powered by a o3-based model (instead of GPT-4o) with enhanced safety fine-tuning for computer use.
Learn more
Addendum to OpenAI o3 and o4-mini system card: Codex
This addendum to o3 and o4-mini system card describes Codex, a cloud-based coding agent powered by codex-1, a fine-tuned version of OpenAI’s o3 model tailored specifically for software engineering tasks.
Learn more
OpenAI o3 and o4-mini System Card
This is the first launch and system card to be released under Version 2 of our Preparedness Framework. OpenAI’s Safety Advisory Group (SAG) reviewed the results of our Preparedness evaluations and determined that OpenAI o3 and o4-mini do not reach the High threshold in any of our three Tracked Categories: Biological and Chemical Capability, Cybersecurity, and AI Self-improvement.
Learn more
Addendum to GPT-4o System Card: 4o image generation
This addendum to the GPT‑4o system card describes the marginal risks we’ve focused on, and the work we have done to address them.
Learn more
GPT-4.5 System Card
This system card outlines how we built and trained GPT‑4.5, evaluated its capabilities, and strengthened safety, following OpenAI’s safety process and Preparedness Framework.
Learn more
Deep research System Card
This report outlines the safety work carried out prior to releasing deep research including external red teaming, frontier risk evaluations according to our Preparedness Framework, and an overview of the mitigations we built in to address key risk areas.
Learn more
OpenAI o3-mini System Card
This report outlines the safety work carried out for the OpenAI o3-mini model, including safety evaluations, external red teaming, and Preparedness Framework evaluations.
Learn more
Operator System Card
This report outlines the safety work carried out prior to releasing Operator including external red teaming, frontier risk evaluations according to our Preparedness Framework, and an overview of the mitigations we built in to address key risk areas.
Learn more
Sora System Card
This system card outlines the resulting mitigation stack, external red teaming efforts, evaluations, and ongoing research to refine these safeguards further.
Learn more
OpenAI o1 System Card
This report outlines the safety work carried out prior to releasing OpenAI o1 and o1-mini, including external red teaming and frontier risk evaluations according to our Preparedness Framework.
Learn more
GPT-4o System Card
This system card takes a detailed look at speech-to-speech while also evaluating text and image capabilities.
Learn more
GPT-4 System Card
A system card on the safety challenges in GPT-4 and the interventions we implemented to mitigate potential harms.
Learn more
GPT-4V(ision) System Card
This system card dives deeper into the evaluations, preparation, and mitigation work done for image inputs.
Learn more
DALL·E 3 System Card
This system card details how we prepared DALL·E 3 for deployment, focusing on risk evaluation, red teaming, and mitigation.
Learn more
Preparedness Framework
A document outlining OpenAI’s processes to track, evaluate, and protect against catastrophic risks from powerful models.
Learn more
Safety and Security Committee
This new committee is responsible for making recommendations on critical safety and security decisions for all OpenAI projects.
Learn more
Product Safety Standards
A collection of resources about our safety practices across development, deployment, and the use of our models.
Learn more
