# ğŸ¤– Zantara AutoFix - Automated Testing & Deployment

Automated workflow that:
1. ğŸ§ª Runs tests (local + production)
2. ğŸ” Analyzes failures with Claude AI
3. ğŸ”§ Generates and applies fixes
4. ğŸ’¾ Commits and pushes to GitHub
5. ğŸš€ Monitors Railway deployment
6. âœ… Verifies production
7. ğŸ”„ Repeats up to 3 times

**Based on the 4-hour manual session from 2025-10-25** (see `../ZANTARA_SESSION_REPORT_2025-10-25.md`)

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
cd .autofix
pip install -r requirements.txt
```

### 2. Run AutoFix

```bash
# Make sure ANTHROPIC_API_KEY is set
export ANTHROPIC_API_KEY="your-key-here"

# Run full cycle (max 3 iterations)
python orchestrator.py

# Dry run (test without making changes)
python orchestrator.py --dry-run

# Custom max iterations
python orchestrator.py --max-iter 5
```

---

## ğŸ“Š What It Tests

### Production Tests (ONLY)
- âœ… Health check (`https://scintillating-kindness-production-47e3.up.railway.app/health`)
- âœ… **Pricing test**: Verifies "Quanto costa KITAS E23?" returns real prices (26M/28M IDR)

**Note**: AutoFix tests ONLY production, not localhost.

**If all tests pass**: âœ… Done!
**If tests fail**: Automatically analyzes, fixes, deploys, and retests.

---

## ğŸ”„ Workflow Details

```
START
  â†“
Run Tests
  â†“ FAIL
Claude Analysis (AI)
  â†“
Generate Fix (AI)
  â†“
Apply Fix
  â†“
Commit & Push
  â†“
Wait for Deploy (3-5 mins)
  â†“
Retest Production
  â†“
SUCCESS or Repeat (max 3x)
```

---

## ğŸ’° Cost

**Per cycle** (3 iterations max):
- Claude API calls: ~$0.15-0.25
- Uses the **same API key** as Claude Code
- If you have Claude Code, this uses your existing quota

**vs Manual**:
- Time: 30 min vs 4 hours (8x faster)
- Cost: $0.25 vs $100-200 developer time (400x-800x ROI)

---

## ğŸ“ Output

### State File
```
.autofix/autofix_state.json
```

Contains:
- All cycle history
- Test results
- Claude analyses
- Fixes applied
- Deploy status

### Example State
```json
{
  "cycles": [
    {
      "cycle_id": "20251025_162000",
      "status": "success",
      "iterations": [
        {
          "iteration": 1,
          "test_results": {...},
          "analysis": {
            "root_cause": "...",
            "confidence": 0.85
          },
          "fix": {
            "fixes": [...]
          }
        }
      ]
    }
  ]
}
```

---

## ğŸ¯ Real-World Examples

### Bug 1: Duplicate Responses
**Symptom**: Chat responses appear 2x-4x
**AutoFix**: Detected via production test, analyzed with Claude, added `removeAllListeners()`, deployed, verified
**Time**: ~8 minutes (vs 1 hour manual)

### Bug 2: Tools Not Loaded
**Symptom**: Backend logs "Loaded 0 tools for AI"
**AutoFix**: Detected via logs, analyzed root cause (TS backend offline), fixed `tool_executor.py`, deployed, verified
**Time**: ~10 minutes (vs 1.5 hours manual)

### Bug 3: Missing Pricing Data
**Symptom**: Railway logs "Pricing file not found"
**AutoFix**: Detected via production pricing test, analyzed `.dockerignore`, added exception, deployed, verified real prices
**Time**: ~8 minutes (vs 1.5 hours manual)

**Total**: ~26 minutes vs 4 hours manual (9x faster)

---

## âš ï¸ When AutoFix Alerts You

AutoFix will **stop and alert** if:
- âŒ Max iterations reached (3 by default)
- âŒ Claude confidence < 50%
- âŒ Build fails
- âŒ Deploy timeout (>5 minutes)
- âŒ Production goes down

**Exit codes**:
- `0` = Success
- `1` = Failed (check state.json)
- `130` = User interrupted (Ctrl+C)

---

## ğŸ› ï¸ Configuration

### Environment Variables

```bash
# Required
export ANTHROPIC_API_KEY="sk-ant-..."

# Optional
export PRODUCTION_URL="https://your-app.railway.app"
export MAX_ITERATIONS=3
export DEPLOY_TIMEOUT=300  # seconds
```

### Custom Tests

Edit `orchestrator.py` to add custom tests:

```python
def _run_custom_test(self) -> Dict:
    """Your custom test"""
    # ... your test logic
    return {"passed": True/False, "details": "..."}
```

---

## ğŸ“ˆ Monitoring

### Check State

```bash
# View last cycle
python -c "import json; print(json.dumps(json.load(open('.autofix/autofix_state.json'))['cycles'][-1], indent=2))"

# Count cycles
python -c "import json; print(len(json.load(open('.autofix/autofix_state.json'))['cycles']))"

# Success rate
python -c "import json; cycles = json.load(open('.autofix/autofix_state.json'))['cycles']; print(f'{sum(1 for c in cycles if c.get(\"status\") == \"success\") / len(cycles) * 100:.1f}% success rate')"
```

---

## ğŸš€ Advanced Usage

### Run on Schedule (Cron)

```bash
# crontab -e
# Run every night at 2 AM
0 2 * * * cd /path/to/NUZANTARA-RAILWAY/.autofix && python orchestrator.py >> autofix.log 2>&1
```

### Run After Deploy (GitHub Action)

```yaml
# .github/workflows/autofix.yml
name: AutoFix After Deploy
on:
  push:
    branches: [main]

jobs:
  autofix:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install deps
        run: pip install -r .autofix/requirements.txt
      - name: Run AutoFix
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: python .autofix/orchestrator.py
```

---

## ğŸ› Debugging

### Dry Run Mode

```bash
# Test workflow without making changes
python orchestrator.py --dry-run
```

### Verbose Logging

```bash
# Add to orchestrator.py
import logging
logging.basicConfig(level=logging.DEBUG)
```

### Manual Step-by-Step

```python
from orchestrator import AutoFixOrchestrator

orch = AutoFixOrchestrator(dry_run=True)

# Run individual steps
results = orch.run_tests()
analysis = orch.analyze_failures_with_claude(results)
fix = orch.generate_fix_with_claude(analysis)
# ... etc
```

---

## ğŸ“š Related Files

- **Session Report**: `../ZANTARA_SESSION_REPORT_2025-10-25.md` - Manual session that inspired this
- **Multi-Agent Proposal**: `../MULTI_AGENT_AUTOMATION_REQUEST.md` - Original architecture ideas

---

## ğŸ¤ Contributing

Improvements welcome! Focus areas:
1. Better error pattern recognition
2. More comprehensive tests
3. Faster deployment monitoring
4. Self-learning from successful fixes

---

## ğŸ“„ License

MIT - Use freely for your projects

---

**Made with â¤ï¸ by the Zantara team**
**Powered by Claude Code & Anthropic API**
